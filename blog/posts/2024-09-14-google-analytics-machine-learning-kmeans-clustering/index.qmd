---
title: "Messing with models: k-means clustering of Google Analytics 4 data" 
date: "2024-09-14"
author: "Collin K. Berke, Ph.D."
draft: false 
image: thumbnail.jpg
description: "A tutorial on how to perform k-means clustering using Google Analytics data"
toc: true
from: markdown+emoji
bibliography: references.bib
categories:
  - machine learning 
  - unsupervised learning
  - k-means clustering 
---

![Photo by [Daniel Fazio](https://unsplash.com/photos/gray-wire-lot-m9LlUwkPvT8)](thumbnail-wide.jpg)

Marketing campaigns target specific audiences. Some form of customer segmentation is performed to identify these audiences. These segments can be identified using several approaches. This post describes one such approach: the use of data and machine learning to specify clustering models present within [Google Analytics](https://developers.google.com/analytics) data for an e-commerce store.

The goal is simple: create customer segments a marketing team could use to craft and target specific marketing campaigns. To achieve this goal, this post overviews the use of a straightforward, useful machine learning algorithm called [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering). Packages and functions from [R](https://www.r-project.org/), a statistical programming language, are used for the segmentation process.

```{r}
#| label: setup
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(bigrquery)
library(skimr)
library(janitor)
library(factoextra)
library(ids)
library(here)
library(psych)
library(gt)
```

```{r}
#| label: vis-theme-setup
ggplot_theme <- 
  theme_bw() +
  theme(
    text = element_text(size = 12),
    title = element_text(size = 16, face = "bold")
  )
# Override the default ggplot2 theme
theme_set(ggplot_theme)
```

# Extract and explore data

First, we need some data. This section briefly discusses the data extraction process. I've detailed the extraction of Google Analytics data in a [previous post](https://www.collinberke.com/blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/). For the sake of time, then, this topic won't be discussed in-depth. If you're already familiar with how to access this data, feel free to skip this section.

The data used here is obfuscated [Google Analytics](https://developers.google.com/analytics) data for the [Google Merchandise Store](https://www.googlemerchandisestore.com/). This data is stored and accessed via [BigQuery](https://cloud.google.com/bigquery), a cloud-based data warehouse useful for analytics purposes. 

::: {.callout-warning}
This data is useful for example tutorials, so the conclusions drawn here should not be used to infer anything about true purchasing behavior. The purpose of this post is to be a tutorial on how to perform k-means clustering, rather than about deriving true conclusions about Google Merchandise store customers.
:::

Before data extraction, let's do some exploration of the source data. The focus here is to get a sense of what's in the data, while also creating an understanding of the source data's structure. This mostly involves identifying the available fields and data types.

The [`bigrquery`](https://bigrquery.r-dbi.org/) package provides the [`bq_tables_fields()`](https://bigrquery.r-dbi.org/reference/api-table.html) function to retrieve this information. The following code example shows how to use this function to return field names within the dataset's tables:

```{r}
#| label: view-bq-metadata
#| eval: FALSE
table_ecommerce <-
  bq_table(
    "bigquery-public-data",
    "ga4_obfuscated_sample_ecommerce",
    "events_20210101"
  )

bq_table_fields(table_ecommerce)
```

Then, the following code submits a SQL query to return the [`ga_obfuscated_sample_ecommerce`](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset) data from BigQuery. It's important to note, similar to what was done in [past posts](https://www.collinberke.com/blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/), I'm querying data associated with transactions occurring over the [U.S. Christmas holiday season](https://en.wikipedia.org/wiki/Christmas_and_holiday_season). Here's the query string if you want specific details:

```{r}
#| label: query-bigquery
#| eval: FALSE
query <- "
  select 
    event_date,
    user_pseudo_id,
    ecommerce.transaction_id,
    items.item_category,
    items.quantity,
    ecommerce.purchase_revenue_in_usd
  from `<your-project-name>.ga4_obfuscated_sample_ecommerce.events_*`,
  unnest(items) as items
  where _table_suffix between '20201101' and '20201231' and 
  event_name = 'purchase'
  order by user_pseudo_id, transaction_id
"
```

```{r}
#| label: extract-transaction-data
#| eval: FALSE
data_ga_transactions <- bq_project_query(
  "<your-project-name>",
  query
) |>
bq_table_download()
```

```{r}
#| label: write-read-transactions
#| eval: FALSE
#| include: FALSE
# Writing the data so the post doesn't need to query BigQuery to build
write_csv(data_ga_transactions, "data_ga_transactions.csv")
```

```{r}
#| label: import-transactions-again
#| include: FALSE
# write_csv() transforms `""` into `NA`s, so transform them back upon
# reading in the data. Doing this to make it consistent with what was 
# outputted from the BigQuery query.
data_ga_transactions <- read_csv("data_ga_transactions.csv") |>
  mutate(
    item_category = ifelse(
      is.na(item_category), 
      "", 
      as.character(item_category)
    )
  )
```

Before moving forward, some data validation is in order. Columns containing missing values are the biggest concern.

```{r}
#| label: test-na-in-cols
# Verify if there are any missing values
map(data_ga_transactions, \(x) any(is.na(x)))
```

No surprise: some features contain missing values. `transaction_id` and `quantity` both contain missing values. Our data wrangling step will need to address these issues. Let's look closer and explore why missing values might be present within the data. This code can be used to do this:

```{r}
#| label: view-missing-examples
# Examples with a missing `transaction_id`
data_ga_transactions |> filter(is.na(transaction_id))

# Examples where a `quantity` is missing
data_ga_transactions |> filter(is.na(quantity))
```

For the `transaction_id` column, 59 rows contain missing values. If you dig a little further, you'll notice the majority of these missing `transaction_id`s occur near the beginning of data collection (i.e., `2020-11-01`). This was likely a measurement issue with the Google Analytics setup, which would warrant further exploration. Since access to information about how Google Analytics was set up for the Google Merchandise Store (i.e., I can't speak with the developers), this issue can't be further explored. The only option, then, is to simply drop these rows.

Missing `quantity` values seem to be associated with examples where `transaction_id` and `item_category` both contain a `(not set)` character string. Again, this could be a measurement issue worth further exploration. Given the available information, these examples will also be dropped.

At this point, [`dplyr`'s](https://dplyr.tidyverse.org/index.html) [`glimpse()`](https://dplyr.tidyverse.org/reference/glimpse.html) and [`skimr`'s](https://docs.ropensci.org/skimr/) [`skim()`](https://docs.ropensci.org/skimr/reference/skim.html) functions are used for some additional exploratory analysis. [`glimpse()`](https://dplyr.tidyverse.org/reference/glimpse.html) is great to get info about the data's structure. [`skim()`](https://docs.ropensci.org/skimr/reference/skim.html) provides summary statistics for each variable within the dataset, regardless of type.

```{r}
#| label: view-data-structure
glimpse(data_ga_transactions)
```

```{r}
#| label: summ-dataset
skim(data_ga_transactions)
```

The data contains a total of `r format(nrow(data_ga_transactions), big.mark = ",")` rows with six features. After reviewing the data types and comparing some of the example values outputted from [`glimpse()`](https://dplyr.tidyverse.org/reference/glimpse.html), it's concerning that some variables are of type character rather than type numeric. Values of type integer would be best in this case. Let's do some more digging.

```{r}
#| label: view-transaction-id
# Only printing the first 10 values for brevity
unique(data_ga_transactions$transaction_id) |>
  head(n = 10)
```

As expected, some `transaction_id`s are of type character, which is keeping the column from being a true numeric variable. This is due to some rows containing the `(not set)` character string. Since the `transaction_id` is critical for our analysis, these rows will need to be filtered out during data wrangling.

Initial data exploration is complete. We now have enough information to begin data wrangling. The wrangling step will strive to format the data into a structure necessary for performing k-means clustering. The following code chunk contains the code to do this. After, there's a step-by-step explanation of what this code is doing.

```{r}
#| label: wrngl-user-items
data_user_items <- 
  data_ga_transactions |>
  filter(transaction_id != "(not set)") |>
  drop_na() |>
  mutate(
    user_id = random_id(), 
    .by = user_pseudo_id, 
    .after = user_pseudo_id
  ) |>
  select(-user_pseudo_id) |>
  summarise(
    quantity = sum(quantity), 
    .by = c(user_id, item_category)
  ) |>
  mutate(
    item_category = case_when(
      item_category == "" ~ "Unknown",
      TRUE ~ as.character(item_category)
    )
  ) |>
  pivot_wider(
    names_from = item_category, 
    values_from = quantity,
    values_fill = 0
  ) |>
  clean_names()
```

In the code above, the `(not set)` issue in the `transaction_id` feature is addressed first. Any row with the `(not set)` value is simply filtered from the data. [`drop_na()`](https://tidyr.tidyverse.org/reference/drop_na.html) follows. This function drops any rows that contain a `NA` value. As a result of dropping `NA`s, we're left with 11,615 rows: a loss of 1,498 examples (~11.4%). 

::: {.callout-note}
Merely dropping examples is convenient in this case, but it may not be ideal or even valid in every context. You'll want to explore what is appropriate for the data you're working with before applying this strategy.
:::

Some mutating and summarization of the data is next. The mutation step applies a random id in place of the `user_pseudo_id`. This provides an extra layer of privacy for users, since the data is being used outside of its original source system.

::: {.callout-note}
This may be a little inordinate, but I argue it's important. It's best to do what we can to create another layer of privacy to the information we're using outside of the original source system. Since this data is public, it's not too much of a concern here. However, this may be an important consideration when you're working with 'real world' Google Analytics data. 

I'm not a privacy expert, so you'll want to identify best practice for the context you work in. 
:::

With the modelling goal top-of-mind, aggregation will sum values to individual users rather than transactions. This way, we'll be able to use k-means clustering to identify customer cohorts, rather than transaction cohorts. To do this, the data is grouped by `user_id` and `item_category` and the `quantity` feature is summed. 

Post aggregation, `item_category`s that don't contain any values need to be addressed. To address this, any missing string is replaced with 'Unknown' using the [`case_when()`](https://dplyr.tidyverse.org/reference/case_when.html) function. Finally, [`tidyr`'s](https://tidyr.tidyverse.org/) [`pivot_wider()`](https://tidyr.tidyverse.org/reference/pivot_wider.html?q=pivot_wider) is used to transform the data from long format to wide format. When taken together, the transformation results in a data set where each feature, other than `user_id`, is a sum of the item categories purchased by each user during the period. Each example, then, could comprise of multiple transactions that occur over the period.

Post wrangling, [`readr`'s](https://readr.tidyverse.org/) [`write_csv()`](https://readr.tidyverse.org/reference/write_delim.html) function is used to save the data to disk. This way a request isn't sent to BigQuery every time the post is built. You don't have to do this, but it's useful for limiting query costs associated with the service, though it's pretty economical to query data in this way.

```{r}
#| label: write-data
#| eval: FALSE
write_csv(data_user_items, "data_user_items.csv")
```

```{r}
#| label: read-wrngl-data
data_user_items <- read_csv("data_user_items.csv")
```

# Data exploration for modeling

Before modeling, let's do some data exploration. The goal during exploration is to ensure data meets the assumptions of k-means clustering. First, let's get a general sense of the structure of our wrangled data. [`dplyr`'s](https://dplyr.tidyverse.org/index.html) [`glimpse()`](https://dplyr.tidyverse.org/reference/glimpse.html) is once again useful here.

```{r}
#| label: summ-glimpse-items
glimpse(data_user_items)
```

For this time period, the data contains over `r format(nrow(data_user_items), big.mark = ",")` customers and `r ncol(data_user_items) - 1` features available for the k-means clustering model. The `user_id` feature will be excluded from the modeling. Features represent counts of items purchased within each item category for each user during the period. [`skimr::skim()`](https://docs.ropensci.org/skimr/reference/skim.html) is again handy for getting a sense of the shape of the data.

```{r}
#| label: summ-user-items
skim(data_user_items)
```

Upon visual inspection, the shape of the distributions are concerning. Each histogram exhibits the presence of highly skewed data. Outliers are most likely present, and they will need to be addressed. Otherwise, they'll negatively impact the k-means clustering algorithm. The mean values for the features also indicate a likely issue: limited purchase frequency for certain item categories. 

Before moving ahead with removing examples, dropping some features might be worth exploring. The objective here is to identify item categories with limited purchase frequency. Any item category with a limited purchase frequency could likely be dropped.

```{r}
#| label: summ-glimpse-category-purchase
summarise(
  data_user_items, 
  across(apparel:black_lives_matter, sum)
) |>
  glimpse()
```

Reviewing the output, `small_goods`; `fun`; `electronics_accessories`; `notebooks_journals`; `gift_cards`; and `black_lives_matter` have a small enough purcharse frequency to be dropped. Since we don't have information on what is being purchased, the `unknown` feature is also another feature that could be dropped. Here's the code to drop these features:

```{r}
#| label: wrngl-numeric-data
data_items <- 
  data_user_items |> 
  select(
    -c(
      small_goods,
      fun,
      electronics_accessories,
      notebooks_journals,
      gift_cards,
      black_lives_matter,
      unknown
    )
  )
```

While the [`skimr::skim()`](https://docs.ropensci.org/skimr/reference/skim.html) output includes histograms, we'll use [`ggplot2`](https://ggplot2.tidyverse.org/) to examine the distributions in more detail.

```{r}
#| label: vis-item-histogram
#| fig-width: 10
#| fig-height: 7
data_item_hist <- data_items |>
  pivot_longer(
    cols = apparel:stationery,
    values_to = "items"
  ) 

ggplot(data_item_hist, aes(x = items)) +
  geom_histogram(binwidth = 1) +
  facet_wrap(~name, ncol = 4, scales = "free") +
  labs(
    title = "Distribution of purchases by item category",
    y = "",
    x = ""
  )
```

The following histograms further confirm the presence of skewed data. It also further provides evidence of another characteristic of concern: low item purchase frequency. Both of these issues will need to be addressed before applying k-means clustering. 

Let's first normalize the data, so we can get values across features to be within a similar range. To do this, we'll transform features into z-scores. The [`summary()`](https://rdrr.io/r/base/summary.html) function can be used to confirm the transformation was applied. This step can be done by utilizing the following code:

::: {.callout-note}
[`scale()`](https://rdrr.io/r/base/scale.html) accepts a dataframe with only numeric features. So, I have to remove it, then add it back.
:::

```{r}
#| label: wrngl-standardize-feats
data_items_stnd <-
  as_tibble(scale(select(data_items, -user_id))) |>
  mutate(user_id = data_items$user_id, .before = 1)

# Verify the standarization was applied
summary(data_items_stnd)
```

Reviewing the [`summary()`](https://rdrr.io/r/base/summary.html) output post normalization, the maximum values are concerning. There are some users who purchased items within categories at a very significant rate. Although this is a nice problem to have for the merchandise store (i.e., big purchases are good for the bottom line), it may cause problems when specifying the clustering algorithm.

Indeed, a couple of approaches could be taken here. For one, the outliers could be retained. This will likely highly affect the k-means algorithms' ability to effectively identify useful clusters, though. The second option is to drop examples that we consider to be outliers. Let's think this through a bit.

More than likely, any example with the number of items purchased above 3 standard deviations beyond the mean should be dropped. The merchandise store is likely meant for business-to-consumer (B2C) sales, rather than business-to-business (B2B) sales. As such, the amount of items purchased during a typical customer transaction will likely be of a volume that is reasonable for consumer purchases (e.g., who needs more than 50 items of stationery?). Such large purchases are likely a B2B transaction, where large volumes of items are being bought. Given the intent of the store to be B2C, then examples exhibiting such large purchase volumes should be dropped.

Additional information could be used to further verify this assumption. We likely have a Customer Relationship Management (CRM) system with information about who the customers of these purchases are, and thus we could use this information to confirm if a purchase was for a business or individual. Since the ability to obtain this additional information is not possible, dropping these outliers before clustering is the best option. With all that said, here's the code to further explore customers considered to be outliers. [`head()`](https://rdrr.io/r/utils/head.html) is used here to limit the output.

```{r}
#| label: view-outliers
data_items_stnd |>
  filter(if_any(-c(user_id), ~ . > 3)) |>
  head(n = 10)
```

Here's the code to filter out customers considered to be outliers:

```{r}
#| label: wrngl-no-outliers
data_items_stnd <- data_items_stnd |>
  select(-user_id) |>
  filter(!if_any(everything(), ~ . > 3)) 

summary(data_items_stnd)
```

Let's take a look at the histograms again, just to get a sense if dropping outliers helped. 

```{r}
#| label: vis-items-stnd
#| fig-width: 10
#| fig-height: 7
data_items_stnd |>
  pivot_longer(
    cols = apparel:stationery,
    values_to = "items"
  ) |> 
  ggplot(aes(x = items)) +
    geom_histogram(binwidth = 1) +
    facet_wrap(~name, ncol = 4, scales = "free") +
    labs(
      title = "Distribution of purchases by item category post wrangling",
      y = "",
      x = ""
    )
```

Although this helped with the issues caused by outliers, we still have to contend with the fact that some customers just don't buy certain items.  We'll want to keep this in mind when drawing conclusions from the final clusters.

# Determine a k-value

Now that the data is in a format acceptable for modeling, we need to explore a value for the number of cluster centers, the k-value. Various methods can be used to determine this value. I'll rely on three methods here: the elbow method; the average silhouette method; and using the realities imposed by the business case [@lantz2023]. Each will be discussed more in-depth in the following sections.

The first method is the elbow method, where we visually examine an elbow plot of the total within sum of squares based on the number of potential clusters used for the model. The [`fviz_nbclust()`](https://rpkgs.datanovia.com/factoextra/reference/fviz_nbclust.html) function from the [`factoextra`](https://rpkgs.datanovia.com/factoextra/) package is useful here. We first pass in the data we intend to use for modeling, then base R's `stats` [`kmeans()`](https://rdrr.io/r/stats/kmeans.html) function. We're also interested in creating the plot using the within sum of squares method, so we specifiy that using the `method` argument.

```{r}
#| label: viz-elbow-plot
#| fig-width: 10
#| fig-height: 7
fviz_nbclust(data_items_stnd, kmeans, method = "wss")
```

Given visual examination of the plot, six clusters seems to be a good starting point. The average silhouette method is another visulization useful for confirming the number of cluster groups for our cluster modelling.

```{r}
#| label: viz-silhouette-plot
#| fig-width: 10
#| fig-height: 7
fviz_nbclust(data_items_stnd, kmeans, method = "silhouette")
```

Just as was expected, the silhouette method provides additional evidence for 6 clusters.

Considering the business case is another useful method for determining the k-value. For instance, say our goal is to cluster the data based on the number of campaigns our marketing team has capacity to manage. The number used here is completely arbitrary (i.e., I don't have a marketing team to confirm capacity). Thus, for the sake of example, let's say we have a marketing team capable of managing 3 campaigns over the holiday season.

So, based on the information above, let's examine k-means clustering using 3 and 6 groups.

# Specify the model

Now the modeling step. We'll use base R's [`kmeans()`](https://rdrr.io/r/stats/kmeans.html) function from the `stats` package. Since a clustering model with 3 or 6 clusters is being explored here, [`purrr`'s](https://purrr.tidyverse.org/) [`map()`](https://purrr.tidyverse.org/reference/map.html) function is used to iterate the model specification. [`map()`](https://purrr.tidyverse.org/reference/map.html) returns a list object, which each element of the list is a model output. One for the three and six cluster model. [`set.seed()`](https://rdrr.io/r/base/Random.html) is also used for the reproducibility of the results. 

```{r}
#| label: mdl-initial-clusters
set.seed(20240722)
mdl_clusters <- 
  map(c(3, 6), \(x) kmeans(data_items_stnd, centers = x))
```

# Evaluating model performance

To assess model fit, we'll look to the cluster sizes for both clustering models. [`purrr`'s](https://purrr.tidyverse.org/) [`map`](https://purrr.tidyverse.org/reference/map.html) function makes this easy. Use the following code to return the `size` element of `kmeans` output:

```{r}
#| label: view-cluster-sizes
map(mdl_clusters, "size")
```

Identifying imbalanced groups is priority here. Some imbalance is tolerable, but major imbalances might indicate the presence of model fit issues. The six cluster model looks fairly balanced, where only one group includes a small subset of customers (~80 customers). The three cluster model has one larger group followed by ever decreasing sized groups. Overall, the balance across the different groups seems to be acceptable here.

Visualizing the clusters is also useful for model assessment. The [`factoextra`](https://rpkgs.datanovia.com/factoextra/) package is once again helpful. The [`fviz_cluster()`](https://rpkgs.datanovia.com/factoextra/reference/fviz_cluster.html) function from the package can be used to visualize the clusters. The function first takes our model output (`mdl_clsuters[[1]]`) and the initial data object (`data_item_stnd`) as arguments. Both the three and six cluster model are visualized using the example code below.

```{r}
#| label: vis-three-cluster-mdl
#| warning: FALSE
#| fig-width: 10
#| fig-height: 7
fviz_cluster(mdl_clusters[[1]], data = data_items_stnd, pointsize = 3) +
  labs(
    title = "Three-cluster model of Google Merchandise Store customers",
    subtitle = "View of cluster groupings during the U.S. holiday season"
  ) +
  theme_replace(ggplot_theme)
```

```{r}
#| label: vis-six-cluster-mdl
#| warning: FALSE
#| fig-width: 10
#| fig-height: 7
fviz_cluster(mdl_clusters[[2]], data = data_items_stnd, pointsize = 3) +
  labs(
    title = "Six-cluster model of Google Merchandise Store customers",
    subtitle = "View of cluster groupings during the U.S. holiday season"
  ) +
  theme_replace(ggplot_theme)
```

[`fviz_cluster()`](https://rpkgs.datanovia.com/factoextra/reference/fviz_cluster.html) uses dimension reduction methods to allow for plotting of a multi-dimensional dataset into a two-dimensional representation. The output is useful for identifying general clustering patterns within the data, which could provide additional information about the shape of the clusters. For instance, this type of visualization can be used to visually identify any outliers which may be influencing the shape clusters take.

Indeed, the plot for both models shows some cluster overlap. This is an indication that the clusters for this data may not be as distinct as we would like. There might be some common products every customer buys, and a few peripheral products that other clusters purchase beyond these common products. These initial results indicate the presence of 'upsell' opportunities for the marketing team. You have your core items that most customers purchase, but some clusters seem to purchase items beyond the core products. Thus, some of the marketing campaigns might strategise ways to highlight the upselling of some products.

# Draw conclusions about clusters

The next step is to examine the cluster centers and factor loadings. The goal is to derive conclusions about each cluster from this information. Let's first draw conclusions using our three-cluster model. We'll look to identify specific audience segments based on item category loadings.

The [`kmeans()`'s](https://rdrr.io/r/stats/kmeans.html) output has an object labelled `centers`, which is a matrix of cluster centers. We then inspect these center values for each cluster, which are on the left, and the values associated within each column.

```{r}
#| label: view-three-cluster-centers
mdl_clusters[[1]]$centers 
```

```{r}
#| label: tbl-three-cluster-loadings
#| tbl-cap: Three-cluster model factor loadings
mdl_clusters[[1]]$centers |>
  as_tibble() |>
  mutate(
    cluster = 1:3,
    across(where(is.double), \(x) round(x, digits = 4)),
    .before = 1
  ) |>
  gt() |>
  data_color(
    columns = !("cluster"),
    rows = 1:3,
    direction = "row",
    method = "numeric",
    palette = "Blues"
  ) |>
  tab_header(
    title = md(
      "**Three-cluster k-means model of Google Merchandise store customers factor loadings**"
    )
  ) |>
  tab_source_note(
    source_note = md(
      "Source: Google Analytics data for the Google Merchandise Store"
    )
  ) |>
  opt_interactive()
```

After inspecting the three-cluster k-means model factor loadings, a few groups emerge. Cluster 1 (n = 1,642) seems to be the 'anything but apparel' customers. This customer segment really doesn't purchase any specific item, but when they shop, they're purchasing items other than apparel. Perhaps a campaign could be created to improve customer's familiarity with products other than apparel that are available in the merchandise store.

Cluster 2 (n = 762) are the 'fashion fanatics'. In fact, it seems this group is mainly purchasing apparel. Maybe our product and marketing teams could consider releasing a new apparel line around the holiday season. A campaign focused on highlighting different apparel pieces could also be explored.

Cluster 3 (n = 278) are 'discount diggers'. Indeed, this data covers the holiday season, so maybe some customers around this time are trying to find a unique gift, but want to do so on a budget. Perhaps a campaign focused on 'holiday gift deals' might appeal to these types of customers.

If more nuance is required for the segmentation discussion, the factor loadings for the six-cluster model can be examined. Again, these results suggest the presence of 'anything but apparel customers'; 'fashion fanatics'; and 'discount diggers'. However, three additional groups emerge from the six cluster model.

```{r}
#| label: view-six-cluster-centers
#| include: FALSE
mdl_clusters[[2]]$centers
```

```{r}
#| label: tbl-six-cluster-loadings
#| tbl-cap: Six-cluster model factor loadings
mdl_clusters[[2]]$centers |>
  as_tibble() |>
  mutate(
    cluster = 1:6,
    across(where(is.double), \(x) round(x, digits = 4)),
    .before = 1
  ) |>
  gt() |>
  data_color(
    columns = !("cluster"),
    rows = 1:6,
    direction = "row",
    method = "numeric",
    palette = "Blues"
  ) |>
  tab_header(
    title = md(
      "**Six-cluster k-means model of Google Merchandise store customers factor loadings**"
    )
  ) |>
  tab_source_note(
    source_note = md(
      "Source: Google Analytics data for the Google Merchandise Store"
    )
  ) |>
  opt_interactive()
```

The first additional segment emerging from the six-cluster model includes 'lifestyle looters' (n = 223): the customers who purchase products that fit or enhance their lifestyle. Perhaps there's room for a campaign focused on highlighting how the Google Merchandise Store's products fit within the lives of its customers: most likely people who work in tech.

The second segment are the 'brand buyers' (n = 296). These customers are mostly interested in purchasing branded items. Thus, a campaign highlighting the various branded items that are available might be explored.

The final group to emerge is our 'accessory enthusiasts' (n = 139). These are customers most interested in purchasing accessories. Perhaps a focus on accessories could be another campaign our marketing team might look at to create.

Depending on the model reviewed, clustering resulted in the identification of three customer segments campaigns could be targeted: 'anything but apparel', 'fashion fanatics', and 'discount diggers'. If an expanded list of segments was required, the six-cluster model provides additional information. This includes segments like 'lifestyle looters', 'brand buyers', and 'accessory enthusiasts'. Indeed, the segment names are up for debate. I would lean on my marketing team to workshop them some more. Analysts are poor at naming things.

# Wrap up

This post was a tutorial on how to perform clustering using [Google Analytics](https://developers.google.com/analytics) e-commerce data. The k-means algorithm was used to identify clusters within [obfuscated analytics data](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset) from the Google Merchandise store. Information about the clusters was used to generate various customer segments. The intent was to use these clusters to better inform future marketing campaigns and targeting strategies.

The [`kmeans()`](https://rdrr.io/r/stats/kmeans.html) function from the base R `stats` package was used to specify two clustering models. The elbow method, silhouette method, and information about the business case were used to determine the k-values for the clustering models. The [`fviz_nbclust()`](https://rpkgs.datanovia.com/factoextra/reference/fviz_nbclust.html) function from the [`factoextra`](https://rpkgs.datanovia.com/factoextra/) package was useful for creating visualizations to further confirm the selected k-values. It was determined both a three and six cluster model would be effective to meet our modelling goal for this data. Lastly, the [`fviz_cluster()`](https://rpkgs.datanovia.com/factoextra/reference/fviz_cluster.html) function from the [`factoextra`](https://rpkgs.datanovia.com/factoextra/) package was used to create visualizations of each model's clusters. 

In truth, this dataset lacked the presence of any interesting cluster groups. I was hoping for some more cross-product segments that could be used for customer segmentation identification. Unfortunately, this wasn't the case. Many of the Google Merchandise Store's customers fell within a single item-category. This was likely due to low purchase frequency for item categories, which likely is due to customers only buying one or two products with each purchase. Nonetheless, we were able to still identify various customer segments useful for targeting purposes and provide some additional support for potential marketing campaigns.

So there you have it, another messing with models post. I hope you found something useful. If not, I hope it was somewhat informative and you found a few takeaways. 

Until next time, keep messing with models.
