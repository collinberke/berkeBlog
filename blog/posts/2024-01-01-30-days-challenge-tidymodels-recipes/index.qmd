---
title: "30 day tidymodels `recipes` challenge"
date: "2024-01-01"
author: "Collin K. Berke, Ph.D."
draft: false 
image: thumbnail.jpg
description: "Learning how to use the `recipes` package, one day at a time"
toc: true
from: markdown+emoji
categories:
  - machine learning
  - feature engineering
  - tidymodels
  - data wrangling
---

![Photo by [Nicolas Gras](https://unsplash.com/photos/assorted-cookware-set-UiGsP8TvOJQ)](thumbnail-wide.jpg)

# Background

Before the holidays, I came across [Emil Hvitfeldt's](https://www.linkedin.com/in/emilhvitfeldt/) `#adventofsteps` LinkedIn [posts](https://www.linkedin.com/feed/hashtag/?keywords=adventofsteps). Following a model popularized by [advent of code](https://adventofcode.com/2023/about)--an annual tradition of online programming puzzles based on the theme of an [advent calendar](https://en.wikipedia.org/wiki/Advent_calendar)--these posts provided daily examples on the use of various `step_*` functions from the `tidymodels`' [`recipes`](https://recipes.tidymodels.org/index.html) package. This post, with a slight spin, is inspired by these posts. 

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(here)
library(corrr)
library(skimr)
library(janitor)
library(textrecipes)
tidymodels_prefer()
```

# My spin on this

One of my personal goals this coming year is to learn and practice using the different tidymodels' packages. To complete this goal, I thought a 30 day `recipes` challenge would be a good start. Each day during this 30 day personal challenge, I will focus on learning and creating some daily notes about one functionality of the `recipes` package. First, I start with the basics (e.g., how to create a recipe object). Then, I'll focus on describing the various `step_*` functions. 

To keep me on track, while also avoiding making this a chore, I'm going to place a 1-hour a day stopgap on studying, practicing, and documenting what I've learned. Depending on my schedule and motivation, I may work ahead on some material, but I will strive to update this post once a day.

Given the time constraint I'm imposing on myself, some of my daily notes or examples may result in an incomplete description of functionality. In cases like this, I'll try to link to relevant documentation for you to follow up and learn more. Please be flexible with any grammar and spelling errors during this challenge, as I'll likely edit very little until the end of the 30 days, if at all. 

Since the aim of this post is to document what I'm learning, all errors are completely mine. I highly suggest following up with the `recipes` package's [documentation](https://recipes.tidymodels.org/) and the [Tidy Modeling with R](https://www.tmwr.org/) book following a review of these notes. Both do a more thorough job overviewing the package's functionality.

# What I intend to get out of this challenge 

By the end of this challenge, I hope to have pushed myself to learn more about how to use `tidymodels`'s `recipe` package, and to create several example use cases of different functionality.

# Day 01 - Create a recipe

First off, what is a recipe? According to the docs:

> A recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.

So, I start this personal challenge by overviewing how to create a recipe object with the `recipes` package. The `recipe()` function is used to create a recipe object. 

When creating a recipe, we need to consider what **roles** variables take. In simple modeling tasks, you'll just have outcomes and predictors. However, variables may take on other roles (i.e., IDs). As such, the `recipe()` function provides multiple means for specifying the role of a variable: 

1. The formula 
2. Manually updating roles using the `update_role()` function.

Let's use the `credit_data` from tidymodels' `modeldata` package. You can get more information about this data by running `?credit_data` in your console.

```{r day-01-data}
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)

# Create splits for examples
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

```{r day-01-recipe-formula}
# No outcome variable, `.` is a shortcut for **all** variables
credit_rec <- recipe(~., data = credit_train)

# Outcome with specific variables to be included within model
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)

# Recipe uses `data` only as a template, all the data is not needed
# Useful in cases when you're working with large data
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = head(credit_train)
)
```

```{r day-01-recipe-update}
# Use `update_role()` to specify variable roles
credit_rec_update <- recipe(credit_train) |>
  update_role(Status, new_role = "outcome") |>
  update_role(
    Seniority, Home, Time, Age, Marital, Records, 
    Job, Expenses, Income, Assets, Debt, Amount, 
    Price, new_role = "predictor"
  )

credit_rec_update
```

The `update_role()` function is useful in cases where you might have an ID variable you don't want to include within your model.

```{r day-01-create-data-w-id}
credit_data_id <- credit_data |>
  mutate(id = 1:n(), .before = 1)

set.seed(2)
credit_id_split <- 
  initial_split(credit_data_id, prop = 0.8, strata = Status)
credit_id_train <- training(credit_id_split)
credit_id_test <- testing(credit_id_split)
```

```{r day-01-recipe-w-ids}
# Manually add an 'id' role to a variable
credit_id_rec <- recipe(credit_id_train) |>
  update_role(id, new_role = "id") |>
  update_role(Status, new_role = "outcome") |>
  update_role(
    Seniority, Home, Time, Age, Marital, Records, 
    Job, Expenses, Income, Assets, Debt, Amount, 
    Price, new_role = "predictor"
  ) 

credit_id_rec
```

In case you ever need to remove a role, you can use `remove_role()`.

```{r day-01-recipe-no-id}
credit_no_id_rec <- credit_id_rec |>
  remove_role(id, old_role = "id")

# id will be assigned and 'undeclared' role
credit_no_id_rec
```

Each recipe has its own summary method. We can wrap the recipe object within `summary()` to output more information about each variable and its assigned role.

```{r day-01-summary}
# Formula specified recipe
summary(credit_rec)

# Manually specified using `update_role()`
summary(credit_rec_update)

# Recipe with a variable holding the 'id' role
summary(credit_id_rec)
```

# Day 02 - How to use `prep()` and `bake()` 

Let's stick with the credit data for today's examples.

```{r day-02-data}
# Same code from day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)

# Create splits for our day 2 examples
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

We're going to continue to use the previously specified limited model from day 01 for our examples.

```{r day-02-recipe}
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)
```

Now that we know how to specify a recipe, we need to learn how to use `recipes`' `prep()` and `bake()` functions. `prep()` calculates any intermediate values required for preprocessing. `bake()` applies the preprocessing steps--using any intermediate values--to our testing and training data.

`prep()` and `bake()` [can be confusing](https://stackoverflow.com/questions/62189885/what-is-the-difference-among-prep-bake-juice-in-the-r-package-recipes) at first. However, I like the following analogy from the [R4DS learning community's Q&A](https://youtu.be/xygnYlku-w4?feature=shared&t=1822) with the authors of the [Tidy Modeling with R book](https://www.tmwr.org/):

> They're analogous to `fit()` and `predict()` ... `prep()` is like fitting where you're estimating stuff and `bake()` is like you're applying it.
>
> \- Max Kuhn

For a more formal treatment, the `prep()` docs state:

> For a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets. 

The `bake()` docs state:

> For a recipe with at least one preprocessing operation that has been trained by `prep()`, apply the computations to new data.

Why two separate functions? Some preprocessing steps need an *intermediate calculation* step to be performed before applying the recipe to the data (e.g., `step_normalize()` and `step_center()`; more on this later). To better articulate this point, I'm going to fast-forward a bit in our challenge and apply the `step_center()` function to our recipe. `step_center()` is used to center variables.

When centering a variable, we need to make an intermediate calculation (i.e., `prep()`) before applying the calculation to perform the centering to our data (i.e., `bake()`).

For our example, say we want to center the `Debt` variable. To do this, we can simply add `step_center(Debt)` to our recipe. When we pipe the recipe object to `prep()`, the mean is calculated in the background to perform the preprocessing step. 

```{r day-02-prep}
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
) |>
  step_center(Debt) |>
  prep() 
```
We can see this calculated value by using the `number` argument in the `tidy.recipe()` method. 

```{r day-02-prep-tidy}
# Print a summary of the recipe steps to be performed
tidy(credit_rec)

# Print additional information about the first recipe step
tidy(credit_rec, number = 1)
```

Take note, though, the `Debt` variable has not been centered yet, and we are still working with a recipe object.

We then apply the centering transformation to the data by piping the prepped recipe to `bake()`. We can apply the preprocessing to the training data by passing the `NULL` to the `new_data` argument. `bake()` returns a tibble with our transformed variable using our training data. 

```{r day-02-baked}
credit_baked <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
) |>
  step_center(Debt) |>
  prep() |>
  bake(new_data = NULL)

credit_baked
```

Most likely, you won't use `prep()` and `bake()` for other modeling tasks. However, they'll be important as we continue exploring the `recipes` package in the coming days. 

# Day 03 - Selector functions

Remaining consistent, let's continue using the `credit_data` data for some of today's examples. We'll also use the `Chicago` data set for a couple additional examples. You can read more about this data by running `?Chicago` in your console.

Here we'll get our data and split it into training and testing for both data sets.

```{r day-03-credit-data}
# Same code from day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

```{r day-03-Chicago-data}
data(Chicago, package = "modeldata")
glimpse(Chicago)

# For reproducibility
set.seed(2)
chicago_split <- initial_split(Chicago, prop = 0.8)
chicago_train <- training(chicago_split)
chicago_test <- testing(chicago_split)
```

When using `recipes`, we often need to select a group of variables (e.g., all predictors, all numeric variables, all categorical variables, etc.) to apply preprocessing steps. Indeed, we certainly could just explicitly specify each variable by name within our recipe. There's a better way, though. Use [*selector functions*](https://recipes.tidymodels.org/reference/has_role.html).

Selector functions can be used to choose variables based on:

1. Variable names
2. Current role
3. Data type
4. Any combination of the above three

The first set of selectors comes from the [`tidyselect`](https://tidyselect.r-lib.org) package, which allows you to make selections based on variable names. Some common ones include:

* `tidyselect::starts_with()`
* `tidyselect::ends_with()`
* `tidyselect::contains()`
* `tidyselect::everything()`

Check out `recipes`' `?selections` and the `tidyselect` [docs](https://tidyselect.r-lib.org) for a more exhaustive list of available selection functions. Included above are the ones I commonly use. Here are a few examples of how to use these selector functions to center variables.

```{r day-03-center-chicago-weather}
# Apply the centering to variables that start with the *weather* prefix
chicago_rec <- 
  recipe(ridership ~ ., data = chicago_train) |>
  step_center(starts_with("weather")) |>
  prep() |>
  bake(new_data = NULL)

chicago_rec |> select(starts_with("weather"))
```

Selections also allows us to use the `-` to exclude specific variables or groupings of variables while using selector functions.

```{r day-03-center-some-numeric}
chicago_rec <- 
  recipe(ridership ~ ., data = chicago_train) |>
  step_center(-date, -starts_with("weather")) |>
  prep() |>
  bake(new_data = NULL)

chicago_rec

# To show centering was not applied to variables with the *weather* prefix
chicago_rec |> select(starts_with("weather"))
```

`recipes` provides functions to select variables based on role and type. This includes the [`has_role()`](https://recipes.tidymodels.org/reference/has_role.html) and [`has_type()`](https://recipes.tidymodels.org/reference/has_role.html) functions.

```{r day-03-select-by-role}
# Simplified recipe, applying centering to variables with predictor role 
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)  |>
  step_center(has_role("predictor")) |>
  prep() |>
  bake(new_data = NULL)

credit_rec
```

```{r day-03-select-by-type}
# Applying centering to variables with type numeric
credit_rec_type <- recipe(Status ~ ., data = credit_train) |>
  step_center(has_type(match = "numeric")) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_type
```

Although `has_role()` and `has_type()` are available, you'll most likely rely on functions that are more specific. The docs state (`?has_role`):

> **In most cases**, the right approach for users will be to use the predictor-specific selectors such as `all_numeric_predictors()` and `all_nominal_predictors()`.

These include functions to select variables based on type:

* `all_numeric()` - includes all numeric variables.
* `all_nominal()` - includes both character and factor variables.

```{r day-03-select-rec-type}
# Center **all** numeric variables
credit_rec_type <- recipe(
  Status ~ Debt + Income + Assets,
  data = credit_train
) |>
  step_center(all_numeric()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_type 
```

Functions to select by role:

* `all_predictors()`
* `all_outcomes()`

```{r day-03-select-rec-role}
# Center all predictors
credit_rec_role <- 
  recipe(
    Status ~ Debt + Income + Assets, 
    data = credit_train
  ) |>
  step_center(all_predictors()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_role
```

Functions to select variables that intersect by role and type:

* `all_numeric_predictors()`
* `all_nominal_predictors()`

```{r day-03-select-num-predictors}
credit_rec_num_pred <- 
  recipe(Status ~ ., data = credit_train) |>
  step_center(all_numeric_predictors()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_num_pred
```

Selector functions will become useful as we continue to explore the `step_*` functions within the `recipes` package.

# Day 04 - Create dummy variables using `step_dummy()`

Before starting our overview of `recipes`' `step_*` functions, we need a bit of direction on what preprocessing steps might be required or beneficial to apply. **The type of data preprocessing is determined by the model being fit.** As a starting point, the [Tidy Modeling with R](https://www.tmwr.org/pre-proc-table) book provides an [appendix](https://www.tmwr.org/pre-proc-table) with a table of preprocessing recommendations based on the types of models being used. This table is separate from the types of feature engineering that may be applied, but it's a good baseline for determining the initial `step_*` functions to be included within a recipe. 

[Dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) is the first preprocessing method highlighted in this appendix. That is, the encoding of qualitative predictors into numeric predictors. Closely related is [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics). When dummy variables are created, most commonly, nominal variable columns are converted into separate columns of 1's and 0's. `recipes`' [`step_dummy()`](https://recipes.tidymodels.org/reference/step_dummy.html) function performs these preprocessing operations.

Let's continue using the `credit_data` for today's examples. Take note, this data contains some `NA`'s. To address this issue, I'm just going to drop any cases with a missing value using dplyr's `drop_na()` function. Indeed, this issue could be addressed with [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) through the use of `recipes`' `step_impute_*` functions (more on this in the coming days).  

```{r day-04-data}
# Same code as day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

credit_data <- credit_data |>
  drop_na()
```

```{r day-04-split}
# Create the split, training and testing data
set.seed(20230104)
credit_split <- initial_split(credit_data, prop = 0.8)
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

Here's the recipe we'll use. I'm gonna keep it simple, so it's easier to observe the results of adding `step_dummy()` to our recipe.

```{r day-04-rec}
credit_rec <- 
  recipe(
    Status ~ Job + Home + Marital, 
    data = credit_train
  ) 
```

Let's create dummy variables from the `Job` column. But first, let's take a look at how many different variable levels there are.

```{r day-04-job-cats}
unique(credit_data$Job)
```

Since we have four levels (`freelance`, `fixed`, `partime`, `others`), the `step_dummy()` function will create three columns. The `fixed` `Job` level will be the reference group, since it's the first level specified for the factor.

```{r day-04-dummy-job}
credit_rec |>
  step_dummy(Job) |>
  prep() |>
  bake(new_data = NULL)
```

Take note of the naming conventions applied to the new dummy columns. `step_dummy()` uses the following naming convention `variable-name_variable-level`. This makes it easier to know what variable the dummy variables originated.

Say you don't want to drop the original column when the dummy variables are created. We can pass `TRUE` to the `keep_original_cols` argument. This will retain the original column, while also creating the dummy variables.

```{r day-04-dummy-keep-col}
credit_rec |>
  step_dummy(Job, keep_original_cols = TRUE) |>
  prep() |>
  bake(new_data = NULL)
```

What about one-hot encoding? To apply one-hot encoding we specify `FALSE` to the `one_hot` argument within the function. The preprocessed, baked data will now contain four columns. One column for each level of the source column.

```{r day-04-one-hot-encoding}
credit_rec |>
  step_dummy(Job, one_hot = TRUE) |>
  prep() |>
  bake(new_data = NULL)
```

We can scale this preprocessing to all nominal predictors by using, you guessed it, selector functions.

```{r day-04-nominal-dummy-variables}
credit_rec |>
  step_dummy(all_nominal_predictors()) |>
  prep() |>
  bake(new_data = NULL)
```

That's a lot of additional columns. How can we keep track of all these additional columns and how they were preprocessed? We can `summary` and `tidy` our prepped recipe. Summarizing the prepped recipe is useful because of the `source` column that gets outputted. In our example, the source column of the returned tibble contains two values: original (i.e., the column was an original column in the data set) and derived (i.e., a column created from the preprocessing step). When we `tidy()` the recipe object returned from `step_dummy()`, a tibble with two columns is returned: `terms` and `columns`. `terms` represents the original variable the dummy variables were created from. `columns` represents the newly preprocessed dummy variable.

```{r day-04-dummy-tidy}
# Prep our dummy variables
credit_rec <- 
  credit_rec |>
  step_dummy(all_nominal_predictors()) |>
  prep()

summary(credit_rec)

# View what preprocessing steps are applied
tidy(credit_rec)

# Drill down and view what was done in during this specific step 
tidy(credit_rec, number = 1)
```

When it comes to specifying interactions within a model, there are some special considerations when using dummy variables. I don't have much time to discuss this today, but I hope to address it on a future day of this challenge. I suggest reviewing the 'Interactions with Dummy Variables' section from the 'Dummies' vignette (`vignettes("Dummies", package = "recipes")`) for more information.

One more thing, `step_dummy()` is useful for straight forward dummy variable creation. However, `recipes` also has some other closely related `step_*` functions. Here is a list of a few from the 'Dummies' vignette:

* `step_other()` - collapses infrequently occurring levels into an 'other' category.
* `step_holiday()` - creates dummy variables from dates to capture holidays. Useful when working with time series data.
* `step_zv()` - removes dummy variables that are zero-variance.

I look to highlight the use of some of these `step_*` functions in the coming days.

# Day 05 - Create a binary indicator variable for holidays using `step_holiday()` 

Staying on the topic of dummy variables, I wanted to take a day to focus on the use of `recipes`' `step_holiday()` function. It seems to be pretty useful when working with time series data.

For today's example, I'm going to use some obfuscated, simulated [Google Analytics ecommerce data](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset). This emulates data closely related to what would be collected for the [Google Merchandise Store](https://shop.googlemerchandisestore.com/). You can learn more about this data by clicking on the previously linked docs. Let's do some data wrangling.

Some notes about what wrangling was done:

* Parse the `event_date` column into a date variable.
* Calculate the revenue generated from the purchase of items based on quantity.
* Retain only relevant columns.

For simplicity, I'm not going to create a testing training split for this data.

```{r day-05-data}
data_ga <- 
  read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  ) |>
  mutate(
    event_date = ymd(event_date),
    revenue = price_in_usd * quantity
  ) |>
  select(event_date, transaction_id, item_category, revenue)
```

Let's start on our recipe. Since we have an id variable, `transaction_id`, let's update the recipe to change it's role to `id`. Once we do that, we can pass the `event_date` to the `step_holiday()` function. Before we bake our recipe, I wanna `prep()` and summarise the preprocessing to see what columns will get added.

```{r day-05-rec}
ga_rec <- recipe(revenue ~ ., data = data_ga) |>
  update_role(transaction_id, new_role = "id") |>
  step_holiday(event_date) |>
  prep() 

summary(ga_rec)
```

Note, three new columns will be added once the recipe is baked. This includes:

* `event_date_LaborDay` - a dummy variable to represent an item purchases on [Labor Day](https://en.wikipedia.org/wiki/Labor_Day). 
* `event_date_NewYearsDay` - a dummy variable to represent item purchases on New Years Day. 
* `event_date_ChristmasDay` - a dummy variable to represent item purchases made on Christmas Day.

You can see the variables that get added by baking the recipe.

```{r day-05-rec-baked}
bake(ga_rec, new_data = NULL)
```

Labor Day, New Years Day, and Christmas Day are the default holidays preprocessed by the function. You can modify this by passing a character vector of holidays to `step_holiday()`'s `holidays` argument. For instance, say we wanted to create dummy variables for [Boxing Day](https://en.wikipedia.org/wiki/Boxing_Day) and the [United State's Thanksgiving Day](https://en.wikipedia.org/wiki/Thanksgiving_(United_States)) holiday, while excluding Labor Day. The following code will specify this preprocessing step for us:

```{r day-05-rec-expanded-holidays}
ga_rec_holidays <- recipe(revenue ~ ., data = data_ga) |>
  update_role(transaction_id, new_role = "transaction_id") |>
  step_holiday(
    event_date, 
    holidays = c("USThanksgivingDay", "ChristmasDay", "BoxingDay", "NewYearsDay")
  ) |>
  prep()

summary(ga_rec_holidays)
```

Now we have a dummy variable for all four of these holidays. Let's bake our recipe and see the final result.

```{r day-05-rec-expanded-holidays-baked}
bake(ga_rec_holidays, new_data = NULL)
```

Indeed, there are many holidays that could be specified for dummy variable creation. All the available holidays can be seen by running `timeDate::listHolidays()` in your console. Last time I checked, there were 118 available holidays.

# Day 06 - Use `step_zv()` to drop variables with one value 

For today, I'm focusing on `recipes`' `step_zv()` function. This function is a filter function, which drops variables that only contain one value.

At first, I didn't really understand why `step_zv()` was made available. Why would you want a step to drop variables within a recipe? Then it clicked working on yesterday's example using the obfuscated [Google Analytics data](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset) for the [Google Merchandise store](https://shop.googlemerchandisestore.com/). 

But first, let's get our data again and specify our recipe. I'm going to keep things simple here. First, I'm just going to use `data_ga`, which was previously wrangled in yesterday's post (check it out if you want more info). Second, I'm going to skip creating a testing and training split. Lastly, I'm going to create dummy variables using `step_holiday()`, just to show how `step_zv()` can be useful.

```{r day-06-data}
ga_rec <- recipe(revenue ~ ., data = data_ga) |>
  step_holiday(event_date)

summary(ga_rec)
```

Let's take a closer look at our data. You'll notice the range of the `event_date` is a subset of data. `data_ga`'s `event_date` ranges between the US holiday season. It starts right before Christmas and moves into the first month of the new year. 

```{r day-06-view-date-range}
c(
  min_date = min(data_ga$event_date), 
  max_date = max(data_ga$event_date)
)
```

If you remember from yesterday's post, one of the default holidays for `step_holiday()` is Labor Day. As such, a dummy variable with all `0`'s will be created for the Labor Day holiday. Purchases made on these dates were not included within this data.

```{r day-06-no-labor-day}
ga_prep <- prep(ga_rec)

tidy(ga_prep, number = 1)

# Check the unique values
bake(ga_prep, new_data = NULL) |> 
  select(event_date_LaborDay) |>
  distinct(event_date_LaborDay)
```

As such, this variable is not very useful and should be dropped before being applied within our model. This is why `step_zv()` can be handy, especially in situations where you have a lot of variables that could only have one value. `step_zv()` makes it easy to drop all unnecessary variables in one step, while allowing you to continue working with a recipe object. 

Indeed, keen observers might note this step could be mitigated by modifying the `holiday` argument in `step_holiday()`. However, the function's utility extends beyond just `step_holiday()`. You might even consider useful as a final step you apply to every recipe.

```{r day-06-rec-with-step-zv}
ga_rec_drop <- recipe(revenue ~ ., data = data_ga) |>
  step_holiday(event_date) |>
  step_zv(all_predictors()) |>
  prep()

ga_rec_drop

tidy(ga_rec_drop)
```

Take note, the `prep()` output informs us of the variables that were dropped when the step was applied. This is something to keep an eye on, just in case you need to explore situations where many variables are dropped, and you need to explore what your recipe is actually doing. 

For completeness, lets `bake()` our final recipe.

```{r day-06-rec-bake}
ga_rec <- recipe(revenue ~., data = data_ga) |>
  step_holiday(event_date) |>
  step_zv(all_predictors()) |>
  prep() |>
  bake(new_data = NULL)

ga_rec

glimpse(ga_rec)
```

# Day 07 - Use `step_impute_*()` functions for imputation

The `recipes` package makes it easy to perform [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) tasks. As of this writing, `recipes` had the following functions to perform different methods of imputation: 

* `step_impute_bag()`
* `step_impute_knn()`
* `step_impute_linear()`
* `step_impute_lower()`
* `step_impute_mean()`
* `step_impute_median()`
* `step_impute_mode()`
* `step_impute_roll()`

For today's examples, I'm going to highlight the use of `step_impute_mean()`, `step_input_median()`, and `step_input_mode()`. First, though, we need some data with missing values. Let's switch it up a bit and use the [Palmer Station penguin data](https://github.com/allisonhorst/palmerpenguins) (run ?penguins in your console to get more information about the data). In brief, these data represent different measurements of various [penguin](https://en.wikipedia.org/wiki/Penguin) species in Antarctica.

```{r day-07-data}
data(penguins, package = "modeldata") 

# Add an id column
penguins <- 
  penguins |> mutate(id = 1:n(), .before = everything())
```

This data set contains some missing values that could be addressed using imputation methods. Let's take a moment and explore the data a little further.

```{r day-07-explore-data}
glimpse(penguins)

# What columns have missing data?
map_df(penguins, \(x) any(is.na(x)))

# What percentage of data is missing in each column?
map(penguins, \(x) mean(is.na(x)))

# Missing data examples
missing_examples <- c(4, 12, 69, 272)
penguins |> slice(missing_examples)
```

The following columns contain missing data (included are the variable types):

* `bill_length_mm` - double
* `bill_depth_mm` - double
* `flipper_length_mm` - integer
* `body_mass_g` - integer
* `sex` - factor 

You'll also notice some of these variables are of various types (i.e. factor, double, or integer). Indeed, the variable type will determine the method of imputation applied. 

```{r day-07-split}
set.seed(20240107)
penguins_split <- initial_split(penguins, prop = .8)

penguins_tr <- training(penguins_split)
penguins_te <- testing(penguins_split)
```

Let's start by highlighting how to apply [mean substitution](https://en.wikipedia.org/wiki/Imputation_(statistics)#Mean_substitution) as our imputation method. Specifically, let's apply this step to our first numeric variable with missing values, `bill_length_mm`. 

::: {.callout-note}
Take note of the importance of the use of `prep()` here. Remember, some recipe steps need to calculate an intermediate value before applying it to the final baked data. This is highlighted with the `tidy(penquin_rec, number = 1)` in the code below.
:::

```{r day-07-recipe}
penguins_rec <- recipe(~ ., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_mean(bill_length_mm) |>
  prep()

summary(penguins_rec)
tidy(penguins_rec, number = 1)
```

```{r day-07-recipe-bake}
penguins_baked <- bake(penguins_rec, new_data = NULL)

penguins_baked

# Imputation should result in a complete column of data
any(is.na(penguins_baked$bill_length_mm))

# The missing values have now been substituted
penguins_baked |> filter(id %in% missing_examples)
```

`step_impute_mean()` also includes a `trim` argument, which trims observations from the end of the variable before the mean is computed. This is also a tuning parameter, which can be used in any [hyperparameter tuning](https://www.tmwr.org/tuning) applied within your modeling. I would like to explore this more, but it's outside the scope of this post. Just to highlight the use of the `trim` argument, here's some example code:

```{r day-07-impute-mean-trim}
penguin_mean_trim_rec <- recipe(~ ., data = penguins_tr) |>
  step_impute_mean(bill_length_mm, trim = .5) |>
  prep()

# Notice how the intermediate calculation changed because
# we trimmed the observations used to make the mean calculation
tidy(penguin_mean_trim_rec, number = 1)
```

Let's bake this recipe for completeness.

```{r day-07-impute-mean-trim-baked}
penguins_mean_trim_baked <- 
  bake(penguin_mean_trim_rec, new_data = NULL)

penguins_mean_trim_baked

# The missing values have now been imputed
penguins_mean_trim_baked |> filter(id %in% missing_examples)
```

Mean substitution is just one imputation step. The `recipes` package also includes the `step_impute_median()` and `step_impute_mode()`. These step functions have similar syntax, just a different calculated metric is applied in the background. Let's apply `step_impute_median()` to `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. 

In addition, we'll apply `step_impute_mode()` to impute values for the missing data within the `sex` variable. Take note, the docs for this function state:

> Impute nominal data using the most common value.

So, it only seems `step_impute_mode()` can only be used to impute missing values for nominal variables. 

```{r day-07-other-numeric-imputation}
penguin_rec <- recipe(~ ., data = penguins_tr) |>
  step_impute_mean(bill_length_mm) |>
  step_impute_median(
    bill_depth_mm, 
    flipper_length_mm, 
    body_mass_g
  ) |>
  step_impute_mode(sex) |>
  prep()

penguin_rec

tidy(penguin_rec)
```

Let's take a look at the calculated values for all these steps.

```{r day-07-view-step-calcs}
map(1:3, \(x) tidy(penguin_rec, number = x))
```

As always, let's bake this recipe and look at the final data, which should now contain no missing data.

```{r day-07-bake-final-rec}
baked_penguin <- bake(penguin_rec, new_data = NULL)

baked_penguin

map_df(baked_penguin, \(x) any(is.na(x)))

baked_penguin |> filter(id %in% missing_examples)
```

That's all the time I have for today. Tomorrow I'll pick up exploring some more of the other `step_impute_*` functions.

# Day 08 - Use bagged tree models to impute missing data with `step_impute_bag()`

To start, I wanted to highlight a really good, simplified definition of imputation from the [Feature Engineering and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/) book by Max Kuhn and Kjell Johnson.

> Imputation uses information and relationships among the non-missing predictors to provide an estimate to fill in the missing values.

Yesterday we used the `step_impute_mean()`, `step_impute_median()`, and `step_impute_mode()` functions to calculate missing values. However, we can also use tree-based methods, which uses information from different variables rather than just values in rows, to perform our imputation step.

To be honest, this imputation method was beyond my current knowledge set. Thus, my explanation of what is happening on the backend may be quite general. However, check out the ['Trees' section](https://bookdown.org/max/FES/imputation-methods.html) from the [Feature Engineering and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/) book for a good starting point to learn more. The book does suggest using bagged models can produce reasonable outputs, which results in values to be produced within the range of the training data. Such methods also retains all predictors, unlike when case-wise deletion is used to manage missing data.

`recipes`' `step_impute_bag()` function is used to impute missing data using bagged tree models. To highlight the use of this step, let's go back to using the `penguins` data from yesterday.

```{r day-08-data}
data(penguins, package = "modeldata")
missing_examples <- c(4, 12, 69, 272)

# Create an id variable
penguins <- 
  penguins |> 
  mutate(id = 1:n(), .before = everything())

glimpse(penguins)

# Print the missing examples
penguins |> filter(id %in% missing_examples)
```

We'll now create our training and testing split.

```{r day-08-split}
set.seed(20240108)
penguins_split <- initial_split(penguins, prop = 0.8)
penguins_tr <- training(penguins_split)
penguins_te <- testing(penguins_split)
```

To start small, let's use a bagged tree model to impute values for the missing data in the `bill_length_mm` variable. The syntax is pretty straightforward:

```{r day-08-recipe}
penguins_rec <- recipe (~ ., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_bag(bill_length_mm) |>
  prep()

penguins_rec
```

Before we `bake()` our recipe, let's `tidy()` our prepped recipe a bit to see what's happening under the hood.

```{r day-08-tidy-rec}
tidy(penguins_rec)

tidy(penguins_rec, number = 1)
```

Tidying down to the bagging step, you'll see this step outputs a tibble with three columns:

1. `terms` - the selectors or variables selected. 
2. `model` - the bagged tree model object.
3. `id` - a unique id for the step being applied in the recipe.

Let's bake the recipe and see the result of our imputation step.

```{r day-08-baked-recipe}
baked_penguin <- bake(penguin_rec, new_data = NULL)

baked_penguin

baked_penguin |> filter(id %in% missing_examples)
```

`step_impute_bagged()` also has several options to modify the imputation method. First, it has an `impute_with` argument that allows you to be selective about what variables are used as predictors in the bagged tree model. We'll specify these variables by passing them into the `imp_vars()` function to the argument.

This argument accepts the various [selector functions](https://recipes.tidymodels.org/articles/Selecting_Variables.html) as well. For instance, the default for the argument is the `all_predictors()` function. The following code uses this argument to limit the imputation to the `bill_depth_mm` and `sex` variables (I'm not a biologist, so I have no idea if this is actually a good approach).

I did come across a cryptic warning when first doing this, though. This warning also resulted in the imputation step to not be applied.

```{r day-08-cryptic-warning}
penguin_rec <- recipe(~ ., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_bag(
    bill_length_mm,
    impute_with = imp_vars(bill_depth_mm, sex)
  ) |>
  prep()
```

I assumed this was because all the predictors used to create the model for imputation had missing values. So, I applied some imputation to these first before applying the `step_impute_bag()` and the warning went away. However, I'm unsure if this was the initial problem. I might submit an issue to the [`recipes` GitHub repo](https://github.com/tidymodels/recipes) to which I'll link later. Nevertheless, I got the example to work. Here's the code:

```{r day-08-impute-selective}
penguin_rec <- recipe(~., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_mean(bill_depth_mm) |>
  step_impute_mode(sex) |>
  step_impute_bag(
    bill_length_mm,
    impute_with = imp_vars(bill_depth_mm)
  ) |>
  prep() 

baked_penguin <- bake(penguin_rec, new_data = NULL)

baked_penguin |> filter(id %in% missing_examples)
```

Given we're using a bagged tree model to perform imputation, we can modify the number of bagged trees used in each model in the `step_impute_bag()` function. To do this, we just pass a value to the `trees` argument. Indeed, its suggested to keep this value [between 25 - 50 trees](https://bookdown.org/max/FES/imputation-methods.html).

```{r day-08-set-trees}
# The default is 25 trees
penguin_rec <- recipe(~., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_bag(bill_length_mm, trees = 50) |>
  prep()

baked_penguin <- bake(penguin_rec, new_data = NULL)

baked_penguin |> filter(id %in% missing_examples)
```

The last `step_impute_bag()` argument I'll highlight is `options`. `ipred::ipredbagg()` implements the bagged model used for this imputation step. Thus, the `options` argument is used to pass arguments to this function. For example, if we want to speed up execution, we can lower the `nbagg` argument, the number of bootstrap replications applied, and indicate we don't want to return a data frame of predictors by setting `keepX = FALSE`.

```{r day-08-pass-options}
penguin_rec <- recipe(~., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_bag(
    bill_length_mm,
    options = list(nbagg = 2, keepX = FALSE)
  ) |>
  prep()

baked_penguin <- bake(penguin_rec, new_data = NULL)

baked_penguin

baked_penguin |> filter(id %in% missing_examples)
```

Just for the heck of it, let's apply `step_impute_bag()` to all predictor variables in our recipe.

```{r day-08-final-rec}
penguin_rec <- recipe(~., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_bag(all_predictors()) |>
  prep()

baked_penguin <- bake(penguin_rec, new_data = NULL)

baked_penguin

# There should now be no missing data
map_df(baked_penguin, \(x) any(is.na(x)))

baked_penguin |> filter(id %in% missing_examples)
```

That's it for today. Tomorrow I'll focus on the use of `step_impute_knn()`.

# Day 09 - Impute missing values using `step_impute_knn()`

Today, we're focusing on imputing missing data using `recipes`' `step_impute_knn()` function. In short, this function uses a k-nearest neighbors approach to impute missing values.

For today's examples, I'm going to stick with the `penguins` data we've been using the past few days. Given this data is relatively small (n = 344), it's a [good candidate](https://bookdown.org/max/FES/imputation-methods.html) for using a k-nearest neighbor approach to imputation.

```{r day-09-data}
data(penguins, package = "modeldata")

# Create a row id
penguins <- penguins |>
  mutate(id = 1:n(), .before = everything())

glimpse(penguins)

# Percent missing for each column
map(penguins, \(x) mean(is.na(x)))
```

Just for a refresher, let's peek at a few of the missing values.

```{r day-09-mising}
missing_examples <- c(4, 12, 69, 272)

penguins |> filter(id %in% missing_examples)
```

Let's create our training and testing split.

```{r day-09-split}
set.seed(20240109)
penguins_split <- initial_split(penguins, prop = 0.8)

penguins_tr <- training(penguins_split)
penguins_te <- testing(penguins_split)
```

Since imputation can be applied to either numeric or nominal data, `step_impute_knn()` uses [Gower's distance](https://www.jstor.org/stable/2528823) for calculating nearest neighbors (you can learn more by running `?step_impute_knn` in your console). Once the neighbors are calculated, nominal variables are predicted using the mean, and numeric data is predicted using the mode. The number of neighbors can be set by specifying the `neighbors` argument of the function, which can also be used for [hyperparameter tuning](https://www.tmwr.org/tuning).

Let's start by imputing values for our missing data in the `sex` column. Here's the code for this initial recipe.

```{r day-09-rec}
penguins_rec <- recipe(~., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_knn(sex) |>
  prep()

penguins_rec
```

Before we bake and examine what the imputation step does, let's drill down and see what's occurring at the prep stage. `tidy()` will be used to do this. Similar to `step_impute_bag()`, a tibble of `terms`, `predictors`, `neighbors` (specific to k-nearest neighbors), and an `id` is returned.

```{r day-09-peek-rec}
tidy(penguins_rec)

# Drill down into the specific impute_knn step
tidy(penguins_rec, number = 1)
```

Let's bake our recipe and examine the result of our imputation step.

```{r day-09-bake-rec}
baked_penguins <- bake(penguins_rec, new_data = NULL)

baked_penguins

baked_penguins |> 
  filter(id %in% missing_examples) |> 
  relocate(sex, .after = 1) 
```

As mentioned before, `neighbors` is an argument to set the number of neighbors to use in our estimation. The function defaults to five, but we can modify this to any integer value. It is suggested that [5 - 10 neighbors is a sensible default](https://bookdown.org/max/FES/imputation-methods.html). However, this is dependent on the data you are working with. For our next example, let's constrain this parameter to 3, while also applying our imputation step to all numeric predictors.

```{r day-09-neighbors-constrain}
penguins_rec <- recipe(~., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_knn(all_numeric_predictors(), neighbors = 3) |>
  prep()

penguins_rec
```

```{r day-09-bake-neighbors-constrain}
baked_penguin <- bake(penguins_rec, new_data = NULL)

baked_penguin |> filter(id %in% missing_examples)
```

Just like `step_impute_bag()`, `step_impute_knn()` provides both an `impute_with` and `options` argument. We can be explicit about the variables to use with our knn calculations by passing a comma-separated list of names to the `imp_vars()` function to the `impute_with` arugment. `options` accepts a list of arguments. These get passed along to the underlying `gower::gower_topn()` function running under the hood, which performs the k-nearest neighbors calculation using Gower's distance. According to the docs, the only two options accepted are:

1. `nthread` - specify the number of threads to use for parallelization.
2. `eps` - optional option for variable ranges (I'm not quite sure what this does).

My assumption is these options can be used to optimize the run-time for our calculations. However, I would consult the documentation for the `gower::gower_topn()` function to verify. The key takeaway here is that `step_impute_knn()` provides an interface to configure options for the function it wraps.

Here's an example constraining our `sex` imputation to the `bill_length` and `bill_depth_mm` variables.

```{r day-09-specific-variables}
penguins_rec <- recipe(~., data = penguins_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_knn(
    sex, 
    impute_with = imp_vars(bill_depth_mm, bill_length_mm)
  ) |>
  prep()

penguins_rec
```

Let's bake our final example and examine what happened.

```{r day-09-final-bake}
baked_penguin <- bake(penguins_rec, new_data = NULL)

baked_penguin |> 
  filter(id %in% missing_examples) |>
  relocate(sex, .after = 1)
```

So there you have it, another example of a `step_impute_*` function. Tomorrow I'll continue exploring imputation steps by highlighting the use of the `step_impute_linear()` function.

# Day 10 - Impute missing values using a linear model with `step_impute_linear()`

So here we are, day 10. We continue our overview of `recipes`' imputation steps. Specifically, I'm going to highlight the use of `step_impute_linear()` for today. `step_impute_linear()` uses linear regression models to impute missing data. Indeed, when there is a strong, linear relationship between a complete predictor variable and one that requires imputation (i.e., contains missing data), [linear methods for imputation may be a good approach](http://www.feat.engineering/imputation-methods). Such a method is also really quick and requires few computational resources to calculate.

For today's examples, we're going back to our `credit_data` data. You can read more about this data by running `?credit_data` in your console. 

```{r day-10-data}
data(credit_data, package = "modeldata")

credit_data <- credit_data |>
  mutate(id = seq_len(nrow(credit_data)), .before = everything()) |>
  as_tibble()

glimpse(credit_data)

map(credit_data, \(x) mean(is.na(x)))
```

Let's peek at some missing data examples.

```{r day-10-view-missing}
missing_examples <- c(114, 195, 206, 242, 496)

credit_data |> filter(id %in% missing_examples) 
```

As mentioned before, linear imputation methods are useful in cases where you have a strong, linear relationship between a complete variable (i.e., contains no missing data) and one where imputation is to be applied. For our example, let's use linear methods to impute values for missing data in the `Income` variable. The `Senority` variable is complete, so we'll use it for our imputation step. Although the relationship between these two variables isn't a strong, linear one, let's use it for example sake.  

```{r day-10-correlation-senority-income}
# Use corrr::correlate() to examine correlation among numeric variables
correlate(credit_data)
```

We start off by creating our testing and training split.

```{r day-10-split}
set.seed(20240110)
credit_split <- initial_split(credit_data, prop = .80)

credit_tr <- training(credit_split)
credit_te <- testing(credit_split)
```

::: {.callout-note}
The docs mention imputed variables must be of type `numeric`. Also, this method requires predictors to be complete cases. As such, the imputation model will only use training set predictors that don't have any missing values.
:::

Just like we did with other imputation methods that use a modeling approach, we can be specific about what variables to use as predictors. We do this by passing them to the `impute_with` argument, which are wrapped in the `imp_vars()` function. Here's what this looks like: 

```{r day-10-recipe}
credit_rec <- recipe(~., data = credit_tr) |>
  update_role(id, new_role = "id") |>
  step_impute_linear(Income, impute_with = imp_vars(Seniority)) |>
  prep(credit_tr)

credit_rec
```

Before we bake our recipe, let's take a look at what's happening under the hood with `tidy()`.

```{r day-10-tidy-prep}
tidy(credit_rec)

tidy(credit_rec, number = 1)
```

A tibble is outputted with three columns: 

1. `terms` - the variable we're seeking to replace missing values with imputed values.
2. `model` - the model object used to calculate the imputed value. Note, the model object is `lm`.
3. `id` - a unique id for the step being performed. 

Ready, get set, bake.

```{r day-10-bake-rec}
credit_baked <- bake(credit_rec, new_data = NULL)

# View some of the imputed values
credit_baked |> filter(id %in% missing_examples)
```

The `recipes`' `step_impute_linear()` documentation also has a really good example of how to visualize the imputed data, using a dataset with complete values. It's purpose is to show a comparison between the original values and the newly imputed values.

The following code is adapted from this example. Here I show the regression line used for the imputed `Income` values based on `Seniority`. It's just a linear regression. If you're interested in seeing the full example, run `?step_impute_linear()` in your console, and scroll down to the examples section.

```{r day-10-show-impute-model}
ggplot(credit_baked, aes(x = Seniority, y = Income)) +
  geom_abline(col = "green") +
  geom_point(alpha = .3) +
  labs(title = "Imputed Values")
```

Again, this relationship is not a strong, linear one. Therefore, a linear imputation method of estimation may not be the best approach for this data. Nevertheless, it serves as an example of how to do it using the `recipes` package.

Day 10, check. Another 20 to go. Tomorrow I'll continue my exploration of `step_impute_*()` functions by highlighting the use of the `step_impute_lower()` function.

# Day 11 - Impute values using `step_impute_lower()` 

`step_impute_lower()` is our focus for today. According to the docs, `step_impute_lower()` calculates a variable's minimum, simulates a value between zero and the minimum, and imputes this value for any cases at the minimum value. This imputation method is useful when we have non-negative numeric data, where values cannot be measured below a known value. 

For today's examples, I'm going to use the `modeldata` package's `crickets` data. This data comes from a study examining the relationship between chirp rates and temperature for two different cricket species (run ?crickets in your console to learn more). 

```{r day-11-data}
data(crickets, package = "modeldata")

glimpse(crickets)

skim(crickets)
```

We're also going to modify the data a bit to better highlight what `step_impute_lower()` is doing. Here I'm just truncating all temp values less than or equal to 21 to 21.

```{r day-11-wrngl-data}
# Create a floor at temp = 21
crickets <- crickets |> 
  mutate(temp = case_when(
    temp <= 21 ~ 21,
    TRUE ~ as.double(temp)
  ))

print(crickets, n = 50)
```

For simplicity, I'm not going to create a testing and training split and will use the full data for our example. Let's start with our recipe.

```{r day-11-rec}
crickets_rec <- recipe(~., data = crickets) |>
  step_impute_lower(temp) |>
  prep()

crickets_rec
```

Let's take a quick peek at what's happening under the hood by tidying our recipe object. 

```{r day-11-tidy-rec}
tidy(crickets_rec)

tidy(crickets_rec, number = 1)
```

You'll notice the `value` column in the tibble represents the minimum value within the column. Using this value, `step_impute_lower()` will replace these values with any value between 0 and 21. Let's bake this recipe and verify this is the case.

```{r day-11-bake-rec}
baked_crickets <- bake(crickets_rec, new_data = NULL)

print(baked_crickets, n = 50)
```

Great! All values of 21 have now been imputed with a value between 0 and the minimum value for the column, 21.

We can also create a plot to highlight what `step_impute_lower()` is doing. This approach is adapted from the example in the docs (`?step_impute_lower()`).

```{r day-11-rec-plot}
plot(baked_crickets$temp, crickets$temp,
  ylab = "pre-imputation", xlab = "imputed"
)
```

`step_impute_lower()` is pretty straightforward in my opinion. Give it a try. That's it for today. A short one. Tomorrow is our final `step_impute_*()` function, `step_impute_roll()`. 

# Day 12 - Impute values using a rolling window via `step_impute_roll()`

We're going to round out our discussion of `step_impute_*()` functions by highlighting the use of `step_impute_roll()`. `step_impute_roll()` utilizes window functions to calculate missing values. At first, I had trouble understanding how window functions are utilized. However, seeing a simplified example helped me better understand what was occurring.

Let's get some data for today's examples. I'm going back to our obfuscated [Google Analytics data](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset) from the [Google Merchandise store](https://shop.googlemerchandisestore.com/) for today's examples.

```{r day-12-data}
data_ga <- 
  read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  ) |>
  mutate(
    event_date = ymd(event_date),
    revenue = price_in_usd * quantity
  ) 

glimpse(data_ga)

skim(data_ga)
```

To make it clear what `step_impute_roll()` is doing, I'm going to wrangle the data to only look at total revenue for `Clearance` item purchases for a specific date range (2 weeks). Since this data is complete, I will introduce some missing values into the data.

```{r day-12-wrngl-data}
data_ga <- data_ga |>
  select(event_date, transaction_id, item_category, revenue) |>
  filter(item_category == "Clearance") |>
  group_by(event_date) |>
  summarise(total_rev = sum(revenue))  |>
  filter(
    event_date >= as_date('2020-12-14') & 
    event_date <= as_date('2020-12-27')
  )

# Introduce some NAs for the example
data_ga$total_rev[c(1, 6, 7, 8, 14)] <- NA
```

Let's get our recipe set up to the point of prepping it.

```{r day-12-rec-to-prep}
ga_rec <- recipe(~., data = data_ga) |>
  update_role(event_date, new_role = "data_ref") |>
  step_impute_roll(total_rev, window = 3) |>
  prep()

ga_rec

tidy(ga_rec)
tidy(ga_rec, number = 1)
```

Not too much useful information here. Let's move forward with bake and see the imputed values. Take note, we will still have a missing value (more on this later).

```{r day-12-bake-rec}
bake(ga_rec, new_data = NULL)
```

We now have a complete data set. But, how were these imputed values calculated?  If you peek at `step_impute_roll()`'s arguments, you'll notice it contains a `statistic` argument set to `median`. It should be pretty intuitive that we are calculating the median here, but the median of what? It's the median of our window we set in the function, which was `window = 3`.

```{r day-12-peek-func}
args(step_impute_roll)
```

We need to be aware of some important notes regarding our calcuation of the median here. Let's put our baked data side-by-side with our old data, just so it's easier to see what was imputed and how it was calculated.

```{r day-12-bake-w-old-rev}
baked_ga <- bake(ga_rec, new_data = NULL) |>
  rename(new_rev = total_rev) |>
  left_join(
    data_ga |> rename(old_rev = total_rev)
  )
```

Let's start with the tails, the missing values at row 1 and 14. Since these values lack a value above and below, they default to using the series `1:3` and `12:14` for imputation. When making the calculation for the window, only complete values are passed to the calculation. Take for example the missing value at row 1. The 324 imputed value comes from calculating the median between 323 and 324.

```{r day-12-calc-window-median}
# We're rounding up here for the imputed value 
median(c(323, 324))
```

The same calculation is being done for the 14th value, which looks like this:

```{r day-12-calc-last-tail-median}
median(c(73, 19))
```

This brings up the interesting case for the imputed NA value at row 7, or the lack there of. Since all the values within the window are `NA`, an `NA` is imputed (i.e., you can't calculate a median with no known values). To fix this, we would need to expand our window to include more values. We do this by setting `window = 5` within the function.

```{r day-12-expand-rec-window}
baked_ga <- recipe(~., data = data_ga) |>
  update_role(event_date, new_role = "date_ref") |>
  step_impute_roll(total_rev, window = 5) |>
  prep() |>
  bake(new_data = NULL)

# Now we have complete values, since we expanded the window
baked_ga
```

What about other functions to calculate values? To do this, `step_impute_roll()` has the `statistic` argument. Say instead of the median we want to calculate our imputed value using the mean. We just do the following:

```{r day-12-recipe-mean}
ga_mean_rec <- recipe(~., data_ga) |>
  update_role(event_date, new_role = "ref_date") |>
  step_impute_roll(total_rev, window = 5, statistic = mean) |>
  prep() |>
  bake(new_data = NULL)

ga_mean_rec
```

::: {.callout-note}
According to the docs, the statistic function you use should:

1. contain a single argument for the data to compute the imputed value.
2. return a double precision value.

They also note only complete values will be passed to the functions specified with the `statistic` argument.
:::

That's a wrap for `step_imputation_*()` functions. Next we're going to focus on step functions to decorrelate predictors.

# Day 13 - Use `step_corr()` to remove highly correlated predictors

Starting today, I'm going to begin highlighting `step_*()` functions useful for performing decorrelation preprocessing steps. These include steps to filter out highly correlated predictors, using principal component analysis, or other model-based methods.

I'm starting out simple here by focusing on the use of `step_corr()`. According to the docs, `step_corr()` will

> potentially remove variables that have large absolute correations with other variables.

Using some threshold value, `step_corr()` will identify column combinations where a minimum number of columns are removed and all the absolute correlations between columns are below a specified threshold.

::: {.callout-important}
`step_corr()` will potentially remove columns.
:::

Before using `step_corr()` let's highlight some of the important arguments of the function.

* `threshold` - used as the absolute correlation cut off value. The function defaults to .9. This is the only tuning parameter for the function.
* `method` - the method used to make correlation calculations, defaults to `pearson`.

Let's get our data together. For today's example, I'm going back to our `penguins` data. You can learn more about this data by running `?penguins` in your console or by reviewing my past examples using this data.

```{r day-13-data}
data(penguins, package = "modeldata")

penguins

glimpse(penguins)

skim(penguins)
```

Now we create our testing training split.

```{r day-13-split}
set.seed(20240113)
penguins_split <- initial_split(penguins, prop = .8)

penguins_tr <- training(penguins_split)
penguins_te <- testing(penguins_split)
```

Quickly, let's use `corrr::correlate()` to explore correlations between variables in our training data. The code looks like this:

```{r day-13-var-correlations}
correlate(penguins_tr)
```

You'll notice a highly positive correlation between `flipper_length_mm` and `body_mass_g` (.872). Although I'm not making a causal argument here, it would seem feasible to assume that as a penguin's flippers get longer, their body mass would increase. The question now is, if we were using this data to train a model, would the inclusion of both these variables improve our model? Or, could we simplify model estimation by eliminating one of these variables? Perhaps we can achieve the same amount of accuracy while also specifying the most parsimonious model.

Here we'll create our recipe to address these two, highly correlated variables within our data.

```{r day-13-rec}
penguins_rec <- recipe(~., data = penguins_tr) |>
  step_corr(all_numeric_predictors(), threshold = .8) |>
  prep()

penguins_rec
```

The prepped recipe now tells us the `flipper_length_mm` variable will be removed once we bake the data. This is a good thing to keep an eye on, verifying the step removed expected columns.

When you drill down into the step using `tidy()`, you'll notice a tibble that highlights what column will be removed once we `bake()` the recipe.

```{r day-13-tidy-rec}
tidy(penguins_rec)

tidy(penguins_rec, number = 1)
```

```{r day-13-bake}
baked_penguin <- bake(penguins_rec, new_data = NULL)

baked_penguin
```

You can verify this highly correlated variable is mitigated by checking out the correlations between variables in our baked data set:

```{r day-13-check-bake-corr}
correlate(baked_penguin)
```

So there you have it, our first `step_*()` function to perform decorrelation preprocessing. Tomorrow I'll be focusing on `step_pca()`, which will use principal components analysis to manage correlations among predictors.

# Day 14 - Perform dimension reduction with `step_pca()`

For today, I'm overviewing `step_pca()`. `step_pca()` performs dimension reduction using [principal components analysis](https://en.wikipedia.org/wiki/Principal_component_analysis). Dimension reduction seeks to combine predictors into latent predictors, while also [retaining as much information](http://www.feat.engineering/numeric-many-to-many) from the full data set as possible. Principal component analysis can seem like an advanced topic, but the [@statquest YouTube channel](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw) has a good step-by-step [video](https://www.youtube.com/watch?v=FgakZw6K1QQ) on how it's performed, in general terms.

We'll use the `mlc_churn` data from the 'modeldata' package for today's examples. This data is an artificial data set representing [customer churn](https://en.wikipedia.org/wiki/Customer_attrition) (i.e., the loss of customers to a service). You can learn more about this data by running `?mlc_churn` within your console. But here's some basic data exploration code to get a sense of what's included within the data.

```{r day-14-data}
data(mlc_churn, package = "modeldata")

glimpse(mlc_churn)

skim(mlc_churn)
```

Let's create our training and testing split.

```{r day-14-split}
set.seed(20240114)
churn_split <- initial_split(mlc_churn, prop = .8)

churn_tr <- training(churn_split)
churn_te <- testing(churn_split)
```

Before we apply the `step_pca()` function, we need to first center and scale our data. To do this, we'll first use `step_normalize()` on all numeric variables within the data set. Normalizing will place all numeric data on the same scale, which is required step to complete before performing principal components analysis. 

Post center and scaling of our variables, we'll use `step_pca()` on `all_numeric()` predictors. We use the `num_comp = 3` argument to specify how many components will be outputted from our principal components analysis. 

Let's `prep()` and `tidy()` our recipe to peek under the hood to get a better sense of what's happening with this recipe step.

```{r day-14-rec}
churn_rec <- recipe(churn ~ ., data = churn_tr) |>
  update_role(state, area_code, new_role = "ref_var") |>
  update_role(ends_with("plan"), new_role = "plan_var") |>
  step_normalize(all_numeric()) |>
  step_pca(all_numeric_predictors(), num_comp = 3) |>
  prep()

churn_rec
```

It's also important to point out that the `tidy()` method for `step_pca()` has two type options:

1. `type = "coef"`
2. `type = "variance"`

Each of these options modify what gets printed to the console. I've added some comments below on what gets outputted for each.

```{r day-14-rec-peek}
tidy(churn_rec)

# Output the variable loadings for each component
tidy(churn_rec, number = 2, type = "coef")

# Output the variance each component accounts for
tidy(churn_rec, number = 2, type = "variance")
```

Great, let's bake our recipe. You'll notice `step_pca()` reduced all numeric data into our three components, `PC1`, `PC2`, and `PC3`. 

```{r day-14-bake}
bake(churn_rec, new_data = NULL)
```

Constraining by component works, but `step_pca()` also provides a `threshold` argument. In other words, we can specify the total variance that should be covered by components, and `step_pca()` will create the required number of components based on this threshold. Let's say we want our PCA to create a number of components to cover 70% of the variability in our variables. We can do this by using the following recipe code:

```{r day-14-rec-threshold}
churn_rec_thresh <- recipe(churn ~ ., data = churn_tr) |>
  update_role(state, area_code, new_role = "ref_var") |>
  update_role(ends_with("plan"), new_role = "plan_var") |>
  step_normalize(all_numeric()) |>
  step_pca(all_numeric_predictors(), threshold = .7) |>
  prep()

churn_rec_thresh
```

Let's `tidy()` our recipe to see what's happening here.

```{r day-14-rec-threshold-peek}
tidy(churn_rec_thresh)

# variable loadings
tidy(churn_rec_thresh, number = 2, type = "coef")

# variance accounted for
tidy(churn_rec_thresh, number = 2, type = "variance")
```

Now, we'll bake our `churn_rec_thresh` recipe.

```{r day-14-threshold-bake}
bake(churn_rec_thresh, new_data = NULL)
```

To meet our threshold, the `step_pca()` recipes step calculated and returned seven principal components, `PC1` - `PC7`.

`step_pca()` makes it pretty easy to do dimension reduction utilizing principal components analysis. Give it a try.

Another day down. See you tomorrow. 

# Day 15 - Use `step_ica()` for signal extraction

Here we are, the halfway point :tada:.

Today, I'm overviewing the use of `step_ica()`. This step is used to make transformations in signal processing. In general terms, [independent component analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis) is a dimensionality reduction technique that attempts to isolate signal from noise. 

Before reviewing this step, I had to do some research on what ICA is and how it is used. Here are a few sources I found useful to gain an intuitive sense of this step:

* [Making sense of independent component analysis](https://stats.stackexchange.com/questions/162288/making-sense-of-independent-component-analysis) [Cross Validated](https://stats.stackexchange.com/) post.
* [Introduction to ICA: Independent Component Analysis](https://towardsdatascience.com/introduction-to-ica-independent-component-analysis-b2c3c4720cd9) by [Jonas Dieckmann](https://medium.com/@jonas_dieckmann)
* [Independent Component Analysis (ICA) and Automated Component Labeling — EEG Example](https://betterprogramming.pub/independent-component-analysis-ica-and-automated-component-labeling-eeg-example-fccfe9b0ea64) by [Bartek Kulas](https://bartek-kulas.medium.com/)

Across many of these sources, two examples are commonly presented to more clearly explain the purpose of ICA. First, the [cocktail party problem](https://en.wikipedia.org/wiki/Cocktail_party_effect), where we can isolate individual conversations among many during a party. The second comes from audio recording. Take for example a situation where you have multiple microphones. These microphones are being used to capture the sound of several audio sources (e.g., various instruments), and you're attempting to isolate the sound from a single source. Both are situations where you're trying to isolate the signal from surrounding noise. Indeed, these are simplified examples. The linked sources above provide more sophisticated explanations.

Before we use `step_ica()`, we'll need some data. I'm using a portion of the data from the [Independent Component Analysis (ICA) and Automated Component Labeling — EEG Example](https://betterprogramming.pub/independent-component-analysis-ica-and-automated-component-labeling-eeg-example-fccfe9b0ea64). This data comes from the use of [Electroencephalography (EEG)](https://en.wikipedia.org/wiki/Electroencephalography). It was collected to examine EEG correlates of a genetic predisposition to alcoholism, and it was made available via [Kaggle](https://www.kaggle.com/datasets/nnair25/Alcoholics/data).

::: {.callout-warning}
I am not an EEG professional, and I rarely work with this type of data. The approach I highlight here may not be best practice.
:::

This data represents EEG sensor measurements of electrical activity during an experimental session for one participant. There are multiple sensors, which collect data intended to observe six known brain wave patterns (again, I'm not an expert here, so this is certainly a more complex topic then I'm making it out to be here). Let's use `step_ica()` to see if we can transform this data into six different signals.

```{r day-15-data}
data_eeg <- 
  read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_eeg.csv")
  ) |>
  clean_names() |>
  rename(id = x1)

glimpse(data_eeg)

skim(data_eeg)
```

I'll have to do some data wrangling here to work with the data first, though. Specifically, I need to go from long to wide data. This will give each EEG sensor measurement its own column.

```{r day-15-wrngl-data}
data_eeg <- data_eeg |>
  select(sensor_position, sensor_value, time) |>
  pivot_wider(
    names_from = sensor_position, 
    values_from = sensor_value
  )

data_eeg
```

In addition, `step_ica()` requires a few packages to be installed.  This includes `dimRed` and `fastICA`. If these are not installed beforehand, an error will be returned.

Now we create our recipe. Just like principal components analysis, [we need to center and scale our data](http://www.feat.engineering/numeric-many-to-many). We achieve this by applying `step_center()` and `step_scale()` to `all_numeric()` variables. Since we're attempting to create columns to represent  the six brain waves, we'll set `num_comp` to 6.

```{r day-15-rec}
eeg_rec <- recipe(~., data = data_eeg) |>
  update_role(time, new_role = "time_ref") |>
  step_center(all_numeric()) |>
  step_scale(all_numeric()) |>
  step_ica(all_numeric(), num_comp = 6) |>
  prep()
```

Let's `tidy()` our recipe to get a better sense of what's happening here. You'll notice it's very similar to principal components analysis.

```{r day-15-tidy-rec}
tidy(eeg_rec, number = 3)
```

Now let's bake our data and see what the result of the `step_ica()` function.

```{r day-15-baked-rec}
baked_eeg <- bake(eeg_rec, new_data = NULL) |>
  bind_cols(data_eeg |> select(time))
```

For the heck of it, let's plot our baked data. Do you see any of the common [brain wave types](https://en.wikipedia.org/wiki/Electroencephalography#Wave%20patterns:~:text=prominent%20alpha%2Drhythm-,Wave%20patterns,-%5Bedit%5D) in these plots? Remember, this is only one participant, so if additional data were included, the patterns might become more apparent.

```{r day-15-vis-bake}
walk(
  baked_eeg |> select(-time), 
  ~plot(baked_eeg$time, .x, type = "line") 
)
```

There you have it, another `step_*()` function to try out. See you tomorrow. 

# Day 16 - Use `step_other()` to collapse low occurring categorical levels into `other`

For today, I'm going to highlight the use of `step_other()`. `step_other()` is used to collapse infrequent categorical values into an `other` category. To do this, the function has a `threshold` argument to modify the cutoff for determining values within this category. Let's highlight its use with an example.

For data, I'm going back to our Google Analytics data. I've used this data for several examples already, so I'm not going to detail it too much here. However, here is some quick data exploration code for you to review.

```{r day-16-data}
data_ga <- read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  )

glimpse(data_ga)

skim(data_ga)
```

First, let's create a split.

```{r day-16-data-split}
split_ga <- initial_split(data_ga, prop = .8)

data_ga_tr <- training(split_ga)
data_ga_te <- testing(split_ga)
```

I'm interested in the `item_category` variable here. Let's get a sense of the unique values and counts of each value within the data. 

```{r day-16-explore-items}
data_ga_tr |>
  count(item_category, sort = TRUE) |>
  mutate(prop = n / sum(n)) |>
  print(n = 25) 
```

Indeed, there are certainly some item categories that are purchased less than 5% of the time in our training data. Let's create our recipe, where we focus on using `step_other()` to create an `other` category.

```{r day-16-rec}
# threshold defaults to .05
ga_rec <- recipe(~., data = data_ga_tr) |>
  step_other(item_category) |>
  prep()

ga_rec
```

We'll now `tidy()` our recipe to drill down and get more information on what's happening. When we look deeper into the first step, a tibble describing what columns will be affected by the step and what variables will not be converted into an `other` category is returned. In our case, `Accessories`, `Apparel`, `Bags`, `Campus Collection`, `New`, `Office`, and `Shop by Brand` will remain as factor levels. All others will be converted to the `other` category.

```{r day-16-tidy-rec}
tidy(ga_rec)
tidy(ga_rec, number = 1)
```

We can bake to see what happens as a result.

```{r day-16-bake-rec}
baked_ga <- bake(ga_rec, new_data = NULL)

baked_ga

baked_ga |>
  count(item_category, sort = TRUE)
```

Say we want to modify the threshold to be 10%. All we need to do is pass this value to the function via the `threshold` argument in `step_other()`. This looks like this:

```{r day-16-modify-threshold}
ga_rec_thresh <- recipe(~., data = data_ga_tr) |>
  step_other(item_category, threshold = .1) |>
  prep() |>
  bake(new_data = NULL)

ga_rec_thresh 

ga_rec_thresh |>
  count(item_category, sort = TRUE)
```

This might be too restrictive. You can also pass an integer to `threshold` to use a frequency as a cutoff. This looks like:

```{r day-16-modify-threshold-integer}
ga_rec_thresh <- recipe(~., data = data_ga_tr) |>
  step_other(item_category, threshold = 300) |>
  prep() |>
  bake(new_data = NULL)

ga_rec_thresh

ga_rec_thresh |>
  count(item_category, sort = TRUE)
```

If you don't like the `other` category label, you can modify it by passing a string to the `other`argument.

```{r day-16-modify-other-label}
ga_rec_thresh <- recipe(~., data = data_ga_tr) |>
  step_other(item_category, threshold = 300, other = "Other items") |>
  prep() |>
  bake(new_data = NULL)

ga_rec_thresh

ga_rec_thresh |>
  count(item_category, sort = TRUE)
```

Check mark next to another day. `step_other()` is pretty useful in cases where you have many factor levels in a categorical variable. Take it for a test drive. See you tomorrow. 

# Day 17 - Convert date data into factor or numeric variables with `step_date()`

Do you ever work with date variables? Do these tasks often require you to parse date variables into various components (e.g., month, year, day of the week, etc.)? `recipes`' `step_date()` simplifies these operations.

Let's continue using our Google Analytics ecommerce data from yesterday. I've overviewed this data a few times now, so I'm not going to discuss it in depth here. Rather, I'm going to jump right into today's examples.

```{r day-17-data}
data_ga <- read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  )

set.seed(20240117)
ga_split <- initial_split(data_ga, prop = .8)
ga_tr <- training(ga_split)
ga_te <- testing(ga_split)
```

Within this data is an `event_date` column. Let's say I want to create two new columns from this data: `month` and `year`. First, we need to utilize `step_mutate()` to convert `event_date` to type `date` (i.e., `20201202` to `2020-12-02`). `lubridate`'s `ymd()` function is used to do this. We do this by doing the following:

```{r day-17-rec}
ga_rec <- recipe(~., data = ga_tr) |>
  step_mutate(event_date = ymd(event_date)) |>
  step_date(event_date, features = c("month", "year")) |>
  prep()

ga_rec

tidy(ga_rec, number = 1)
```

Let's bake our recipe and take a look at what's happening. You should notice two new variables are created, each prefixed with the original variable name `event_date_`. The data now contains `event_date_month` and `event_date_year`.

```{r day-17-bake-rec}
bake(ga_rec, new_data = NULL) |>
  relocate(starts_with("event_"), .after = 1)
```

Say we don't like the use of the abbreviation for `event_date_month`, we can change this by setting the `abbr` argument to `FALSE` in our recipe.

```{r day-17-rec-no-abbr}
ga_rec <- recipe(~., data = ga_tr) |>
  step_mutate(event_date = ymd(event_date)) |>
  step_date(event_date, features = c("month", "year"), abbr = FALSE) |>
  prep()

ga_rec

bake(ga_rec, new_data = NULL) |>
  relocate(starts_with("event_"), .after = 1)
```

Also of note is `step_date()` converted the `event_date_month` column into an ordered factor for us.

```{r day-17-month-factor}
bake(ga_rec, new_data = NULL) |>
  pull(event_date_month) |>
  levels()
```

`step_date()`'s `feature` argument has many different options. This includes:

* `month`
* `dow` (day of week)
* `doy` (day of year)
* `week`
* `month`
* `decimal` (decimal date)
* `quarter`
* `semester`
* `year`

Just for the heck of it, let's create features using all of the available options.

```{r day-17-all-features}
ga_rec <- recipe(~., data = ga_tr) |>
  step_mutate(event_date = ymd(event_date)) |>
  step_date(
    event_date, 
    features = c("month", "dow", "doy", "week", "decimal", "quarter", "semester", "year")
  ) |>
  prep()

ga_rec

baked_ga <- bake(ga_rec, new_data = NULL) |>
  relocate(starts_with("event_"), .after = 1)

baked_ga
```

::: {.callout-note}
Unlike other `step_*()` functions, `step_date` does not remove the original column. If needed you can modify this with the `keep_original_columns` argument.
:::

```{r day-17-glimpse-baked-data}
glimpse(baked_ga)
```

Pretty neat! `step_date()` provides some pretty good utility, especially if you tend to work with a lot of dates needing to be transformed into different representations. We'll see you tomorrow everybody. 

# Day 18 - Create a lagged variable using `step_lag()`

Welcome back, fellow readers! Another day, another `step_*()` function. 

Currently, I tend to work with timeseries data. As such, I need to create lagged variables from time-to-time. `recipes`' `step_lag()` function is handy in these situations. Let's highlight an example using this function. But first, we need some data.

Once again, I'm going to use the obfuscated Google Analytics ecommerce data. You can read more about this data in my previous posts. I'm not going to detail it much here, other than importing it, doing a little wrangling to calculate total revenue and total number of items purchased, and creating a training testing split. Here's all the code to do this:

```{r day-18-data}
data_ga <- 
  read_csv(here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv"))

data_ga <- data_ga |>
  group_by(event_date) |>
  summarise(
    items_purchased = sum(quantity),
    revenue = sum(item_revenue_in_usd)
  )
```

```{r day-18-split}
set.seed(20240118)
ga_split <- initial_split(data_ga, prop = .8)
ga_tr <- training(ga_split)
ga_te <- testing(ga_split)
```

Let's create our recipe, prep, and bake it. The `tidy()` method doesn't provide too much useful information about our step, other than what columns lagged columns will be created from, so no use in looking at it here. The important argument is `lag`. This argument is where we specify how much we want to lag our variable by. 

```{r day-18-recipe}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_mutate(event_date = ymd(event_date)) |>
  step_lag(items_purchased, revenue, lag = 1) |>
  prep()

bake(ga_rec, new_data = NULL)
```

The lag argument also makes it convenient to create multiple lagged variables in one step. We just need to pass along a vector of positive vectors (e.g., `1:3`) to the `lag` argument. This looks like this:

```{r day-18-more-lag}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_mutate(event_date = ymd(event_date)) |>
  step_lag(items_purchased, revenue, lag = 1:3) |>
  prep()

ga_rec

baked_ga <- bake(ga_rec, new_data = NULL)
glimpse(baked_ga)
```

Say we want a variable lagged by 1, 3, and 5 values. We can do this by doing the following:

```{r day-18-even-more-lag}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_mutate(event_date = ymd(event_date)) |>
  step_lag(items_purchased, revenue, lag = c(1, 3, 5)) |>
  prep()

ga_rec

baked_ga <- bake(ga_rec, new_data = NULL)
glimpse(baked_ga)
```

Not happy with the default `NA` value. We can change that by passing a value to the `default` argument.

```{r day-18-lag-default}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_mutate(event_date = ymd(event_date)) |>
  step_lag(items_purchased, revenue, lag = 1:3, default = 0) |>
  prep()

ga_rec

baked_ga <- bake(ga_rec, new_data = NULL)
glimpse(baked_ga)
```

The `prefix` argument also makes it easy to modify the prefix of the outputted column names.

```{r day-18-lag-change-prefix}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_mutate(event_date = ymd(event_date)) |>
  step_lag(items_purchased, revenue, lag = 1:3, prefix = "diff_") |>
  prep()

ga_rec

baked_ga <- bake(ga_rec, new_data = NULL)
glimpse(baked_ga)
```

There you have it, another useful `step_*()` function. `step_lag()` is pretty straightforward, but the `lag` argument makes it easy to create many different lagged variables. See you tomorrow. 

# Day 19 - Use `step_log()` to log transform variables

Let's pivot topics at this point in the post. Another type of preprocessing step `recipes` can perform is data transformations. For today's example, I'll spend some time learning about `step_log()`. `step_log()` creates a recipe step that will log transform data.

Before highlighting the use of this step function, I wanted to revisit this topic from my Stats 101 course. To refresh my memory, I went to find explanations on what a log transformation is, while also looking for views on why you might employ it as a data preprocessing step in machine learning contexts.

What is a log transformation? In simple terms, it's using a mathematical function (i.e., a logarithim) to transform variables from one representation to another. The @statquest YouTube channel has a pretty good [video](https://www.youtube.com/watch?v=VSi0Z04fWj0) explaining what a log transformation does when applied to a variable.

Why use a log transformation? In most of what I've read, a log transformation is used to address skewed distributions, where it seeks to make the distribution more normal. This normality is important because some models (e.g., linear regression) assume variables are normally distributed. In addition, when employed in various modeling contexts, its application may lead to more accurate model predictions. Several of the articles I read stated log transformations are useful when working in domains that tend to have variables with skewed distributions, like financial data (i.e., salaries, housing prices, etc.). This Stack Exchange [post](https://datascience.stackexchange.com/questions/40089/what-is-the-reason-behind-taking-log-transformation-of-few-continuous-variables) provides a pretty good explanation on the use of a log transformation.

To highlight the use of this step function, let's continue using our obfuscated Google Analytics data. Here's the code to get started with this data. 

```{r day-19-data}
data_ga <- read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  )

set.seed(20240117)
ga_split <- initial_split(data_ga, prop = .8)
ga_tr <- training(ga_split)
ga_te <- testing(ga_split)

skim(ga_tr)
```

Let's take a quick look at `item_revenue_in_usd`. Using `skim()`, this variable ranges from $1 to $704. Now let's peek at the distribution of the `item_revenue_in_usd` using base R plotting. 

```{r day-19-vis-item-rev-hist}
hist(ga_tr$item_revenue_in_usd)
```

Indeed, this variable is skewed to the right. A log transformation would be appropriate here. Let's do this by using `step_log()`.

```{r day-19-rec}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_log(item_revenue_in_usd) |>
  prep()

ga_rec

bake_ga <- bake(ga_rec, new_data = NULL)
```

Now that we transformed the variable, let's explore the histogram 
of our baked variable and see its effect on the distribution.

```{r day-19-vis-item-rev-bake-hist}
hist(bake_ga$item_revenue_in_usd)
```

Not bad. The transformed `item_revenue_in_usd` now exhibits a more normal distribution.

`step_log()` also has a few useful arguments. The first useful argument is `base`. When taking the log, R defaults to using the natural log, or `exp(1)`. Say we want to log using base 10? This is certainly useful when dealing with money, since money makes more since in multiples of 10 (e.g., $100, $1,000, $10,000, etc.). Lets change the base of our recipe and see what happens.

```{r day-19-change-base}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_log(item_revenue_in_usd, base = 10) |>
  prep()

ga_rec

bake_ga <- bake(ga_rec, new_data = NULL)
```

```{r day-19-change-base-bake}
hist(bake_ga$item_revenue_in_usd)
```

The second useful argument is the `offset` argument. This argument accepts a value to offset the data prior to logging, which helps us avoid `log(0)`. Here's some example code:

```{r day-19-offset}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_log(item_revenue_in_usd, offset = 5) |>
  prep()

ga_rec

bake_ga <- bake(ga_rec, new_data = NULL)
```

```{r day-19-offset-bake-hist}
hist(bake_ga$item_revenue_in_usd)
```

The final useful argument is `signed`, which accepts a boolean. Setting this argument to `TRUE` will make `step_log()` calculate the signed log. This is useful in cases where you have negative values, since a standard log transformation can't be used to transform negative values. I found this [article](https://www.r-statistics.com/2013/05/log-transformations-for-skewed-and-wide-distributions-from-practical-data-science-with-r/) useful for getting an intuitive sense of what the signed log is and how it can useful for modeling. Here's the code to serve as an example, even though we don't have any negative values in our data:

```{r day-19-signed-log}
ga_rec <- recipe(~., data = ga_tr) |>
  update_role(event_date, new_role = "date_ref") |>
  step_log(item_revenue_in_usd, signed = TRUE) |>
  prep()

ga_rec

bake_ga <- bake(ga_rec, new_data = NULL)
```

```{r day-19-signed-log-bake}
hist(bake_ga$item_revenue_in_usd)
```

That's it for today. `step_log()` is a great first step in learning all the transformations `recipes` can do. I'm looking forward to tomorrow. 

# Day 20 - Use `step_sqrt()` to apply a square root transformation 

Welcome back fellow learners. I had to take a couple days off because my schedule didn't allow me the time to focus on this post. Nevertheless, let's get back to it. 

Today I'm focusing on another transformation function, `step_sqrt()`. `step_sqrt()` applies a square root transformation to variables. This function is similair to `step_log()`--it's just applying a different function to the variable.

The square root transformation is pretty straightforward, but I did some digging to more deeply understand what it is and why it's useful. Here's what I came up with:

* [What could be the reason for using square root transformation on data? - Cross Validated post](https://stats.stackexchange.com/questions/11359/what-could-be-the-reason-for-using-square-root-transformation-on-data)

* [Why is the square root transformation recommended for count data? - Cross Validated post](https://stats.stackexchange.com/questions/46418/why-is-the-square-root-transformation-recommended-for-count-data)

Let's get our data. I'm going to continue using the obfuscated Google Analytics ecommerce data we've been working with. 

```{r day-20-data}
data_ga <- read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  )

set.seed(20240122)
ga_split <- initial_split(data_ga, prop = .8)
ga_tr <- training(ga_split)
ga_te <- testing(ga_split)

skim(ga_tr)
```

We do have some count data, `quantity`. This is the number of times a specific item was purchased. Let's take a look at its distribution before we apply our transformation.

```{r day-20-vis-quantity-distribution}
hist(ga_tr$quantity)
```

Indeed, this data seems to follow a Poisson Distribution. Let's see if we can transform it to be more normal (Gaussian) with a square root transformation.

```{r day-20-rec}
ga_rec <- recipe(~., data = ga_tr) |>
  step_sqrt(quantity) |>
  prep()

ga_rec

baked_ga <- bake(ga_rec, new_data = NULL)

baked_ga
```

Now we'll check out the effect of the transformation by looking at a histogram of the baked data's `quantity` variable.

```{r day-20-vis-baked-hist}
hist(baked_ga$quantity)
```

We can further explore the effect of this transformation by plotting the un-transformed variable on a scatter plot with the baked variable.

```{r day-20-vis-baked-scatter}
plot(ga_tr$quantity, baked_ga$quantity)
```

Eh, looking at the histogram it seems this transformation didn't really help very much. My assumption is this is due to many items only being purchased once. Thus, no transformation would likely be helpful in this case. Nonetheless, `step_sqrt()` is a pretty simple transformation function. You might find it useful when working with your data.

# Day 21 - Apply the Box-Cox transformation using `step_BoxCox()` 

Here we are, another day of transformations. To get us started, I turned to [ChatGPT](https://chat.openai.com/) for a joke about data transformations. 

Hey ChatGPT, tell me a joke about data transformations (prompt).

> Why do data transformations never get invited to parties?
>
> Because they always skew the conversation and standard deviations can be quite awkward!

Not bad ... Okay, let's talk about `step_BoxCox()`. This function applies a [Box Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation) to our variables. I haven't used this transformation in some time, so I scoured the internet to get back up to speed. I found these sources helpful in reminding me what this transformation is and how it can be useful:

* [Transforming the response(Y) in regression: Box Cox transformation (7 mins)](https://www.youtube.com/watch?v=zYeTyEWt7Cw) from the @PhilChanstats YouTube channel
* [Box Cox transformation formula in regression analysis ](https://www.youtube.com/watch?v=2gVA3TudAXI&t=4s) from the @PhilChanstats YouTube channel
* [Box-Cox Transformation + R Demo](https://www.youtube.com/watch?v=vGOpEpjz2Ks) from the @mathetal YouTube channel

Though the following is a simplified explanation, many of these sources mention the Box Cox transformation attempts to make our distribution more normal. It does this by focusing on the tails of the distribution. Box Cox can be applied in situations where variance is not equal (i.e., heteroscedasticity). This transformation also allows us to better meet the assumptions of our models, and its application can result in a better predictive model. Lastly, many of these sources suggest this transformation is applied to the outcome variable of our model.

Let's go back to our Google Analytics obfuscated ecommerce data for today's examples. 

```{r day-21-data}
data_ga <- read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  )

data_ga <- data_ga |>
  group_by(event_date) |>
  summarise(
    items_purchased = sum(quantity),
    revenue = sum(item_revenue_in_usd)
  )

set.seed(20240122)
ga_split <- initial_split(data_ga, prop = .8)
ga_tr <- training(ga_split)
ga_te <- testing(ga_split)
```

Since we can use the Box Cox transformation to rescale a variable to be more normal, let's see if the `revenue` variable will be a good candidate. You'll notice the application of the Box Cox transformation is pretty straightforward (like most of `recipes`' transformation functions) with `step_BoxCox()`. 

```{r day-21-vis-hist-revenue}
hist(ga_tr$revenue)
```

Indeed, this varible exhibits a slight right skew. Let's use the `step_BoxCox()` function here to perform the transformation.

```{r day-21-recipe}
ga_rec <- recipe(revenue ~ ., data = ga_tr) |>
  step_BoxCox(revenue) |>
  prep()

ga_rec

baked_ga <- bake(ga_rec, new_data = NULL)

baked_ga
```

Not too bad. The distribution has been rescaled to resemble a slightly more normal distribution. The following histogram shows the effect of this transformation on the distribution. 

```{r day-21-vis-hist-transform}
hist(baked_ga$revenue)
```

If for some reason you wanted to inspect the lambda value used by the transformation, you can inspect it by tidying the recipe. The outputted table shows lambda in the `value` column.

```{r day-21-tidy-recipe}
tidy(ga_rec, number = 1)
```

`step_BoxCox()` is pretty simple, but useful. You might give it a try if you have some data of positive values exihibiting skewness. Until tomorrow, keep having fun with `recipes`.

# Day 22 - Apply an inverse transformation using `step_inverse()`

I'm keeping things simple today; `step_inverse()` is my focus. This step function will inverse transform our data. Just like the other transformation functions, `recipes` makes it pretty easy to perform.

Let's continue using our Google Analytics ecommerce data for today's example.

```{r day-22-data}
data_ga <- read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  )

data_ga <- data_ga |>
  group_by(event_date) |>
  summarise(
    items_purchased = sum(quantity),
    revenue = sum(item_revenue_in_usd)
  )

set.seed(20240122)
ga_split <- initial_split(data_ga, prop = .8)
ga_tr <- training(ga_split)
ga_te <- testing(ga_split)
```

Just as a quick reminder, let's take a look at the histogram of `items_purchased`.

```{r day-22-vis-items-purchased-hist}
hist(ga_tr$items_purchased)
```

To apply an inverse transformation to our variable, we do the following in our recipe.

```{r day-22-rec}
ga_rec <- recipe(~., data = ga_te) |>
  step_inverse(items_purchased) |>
  prep()

ga_rec
```

Now bake.

```{r day-22-baked-rec}
ga_baked <- bake(ga_rec, new_data = NULL)

ga_baked
```

Here's the effect of the transformation on our variable in two plots.

```{r day-22-vis-effect-trans}
hist(ga_baked$items_purchased)

plot(ga_te$items_purchased, ga_baked$items_purchased)
```

The inverse did a pretty good job of making this distribution more normal. 

Like I said, keeping things simple today with a straight forward `step_*()` function. `recipes` makes it pretty easy to perform the inverse transformation with the `step_inverse()` function. 

# Day 23 - Use extension packages to get even more steps

Recently, I've felt these posts have become a little stale and templated. So today, I'm switching it up a bit. I'm highlighting `recipes` extension packages. I'm also using this post for a bit of inspiration; I'm exploring other `step_*` functions to highlight. As such, I most likely won't describe specific examples from each package in today's post (there's too many to cover). However, I'm aiming to list them here and highlight some packages that would be useful for the work I do.

Several packages are available that provide additional `step_*()` functions, which are outside the core functionality of the `recipes` package. At the time of this writing, these include but are not limited to:

* [`actxps`](https://mattheaphy.github.io/actxps/)
* [`bestNormalize`](https://petersonr.github.io/bestNormalize/)
* [`customsteps`](https://github.com/smaakage85/customsteps)
* [`embed`](https://embed.tidymodels.org/)
* [`extrasteps`](https://emilhvitfeldt.github.io/extrasteps/)
* [`tfhub`](https://github.com/rstudio/tfhub)
* [`healthcareai`](https://docs.healthcare.ai/)
* [`healthyR.ai`](https://www.spsanderson.com/healthyR.ai/)
* [`healthyR.ts`](https://www.spsanderson.com/healthyR.ts/)
* [`hydrorecipes`](https://github.com/jkennel/hydrorecipes)
* [`MachineShop`](https://brian-j-smith.github.io/MachineShop/)
* [`measure`](https://brian-j-smith.github.io/MachineShop/)
* [`nestedmodels`](https://ashbythorpe.github.io/nestedmodels/)
* [`sparseR`](https://petersonr.github.io/sparseR/)
* [`textrecipes`](https://textrecipes.tidymodels.org/)
* [`themis`](https://themis.tidymodels.org/)
* [`timetk`](https://business-science.github.io/timetk/)

The `tidymodels` site provides a [table](https://www.tidymodels.org/find/recipes/) of a good majority of `step_*()` functions made available via these extension packages. Shout out to @jonthegeek from the R4DS Online Learning Community for pointing me to this table, and much praise to `@Emil Hvitfeldt` for pointing me in the direction of additional `recipes` extension packages I wasn't aware. Keep in mind, not all of these extension packages are made available on [CRAN](https://cran.r-project.org/).

Taking a moment to read the descriptions of each extension package, I found `textrecipes`, `themis`, and `timetk` to be the most applicable to my work. Here's a brief description of what each does. I also included some functions that could be interesting to explore in future posts. Note, some of these packages are meant to be applied in a specific domain, so their purpose may not solely be to be an extension of `recipes` (e.g., `timetk`).

* The `textrecipes` package provides extra `step_*()` functions to preprocess text data. Looking through the package's [`reference`](https://textrecipes.tidymodels.org/reference/index.html) section, the `step_tokenize()` family of functions look useful. I also find the token modification (e.g., `step_stem()` and `step_stopwords()`) and text cleaning functions (e.g. `step_clean_levels()`) to be intriguing and worth further exploration.

* The `themis` package provides additional `step_*()` functions to assist in the preprocessing of unbalanced data. Specifically, this package provides functions to apply over-sampling or under-sampling methods to unbalance the data used for modeling. There's several functions in here that seem to be useful when dealing with unbalanced data. I highly suggest checking out its [reference page](https://themis.tidymodels.org/reference/index.html) to get an idea of what all is available. 

* The `timetk` package does more than extend `recipes`. It's main focus is to assist in the analysis of timeseries data. The package contains some similar functions we've highlighted before (e.g., `step_holiday_signature()` and `step_box_cox()`). However, there's some additional functions of interest. `step_ts_pad()` seems useful in cases where you need to add rows to fill in gaps. `step_ts_clean()` helps to clean outliers and missing data for timeseries analysis. `step_diff()` also looks helpful if you need to create a differenced predictor. If you're working with timeseries, you might want to explore this package some more.

Indeed, the `tidymodels` ecosystem is quite vast. This is especially true in the area of preprocessing steps achieved by extension packages for the `recipes` package.

I wanted to shake it up a little bit. Today's post was a little different, but in a good, refreshing way. See you tomorrow.

# Day 24 - Use `step_tokenize()` from `textrecipes` to convert a character predictor into a `token` variable

Building on yesterday's post, I'll begin exploring `recipes` extension packages' functions. `step_tokenize()` from [`textrecipes`](https://textrecipes.tidymodels.org/) is first up. This function is useful when we need to convert a character variable into a token variable. If you're unfamiliar with the concept of tokenization, the [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/tidytext) provides an overview.

I'll keep things simple here by using a pared down data set. This will make it easier to see what this `step_function()` is doing. How about some [quotes](https://stats.stackexchange.com/questions/726/famous-statistical-quotations?page=1&tab=scoredesc#tab-top) about statistics?

```{r day-24-data}
quotes <- tibble(
  text = c(
    "All models are wrong, but some are useful.",
    "Statisticians, like artists, have the bad habit of falling in love with their models.",
    "If you torture the data enough, nature will always confess.",
    "All generalizations are false, including this one."
  )
)
```

Let's create our recipe to tokenize by word. 

```{r day-24-rec}
quotes_rec <- recipe(~ text, data = quotes) |>
  step_tokenize(text) |>
  prep() 

quotes_rec

baked_quotes <- bake(quotes_rec, new_data = NULL)

baked_quotes
```

One thing I didn't expect, but learned, was `step_tokenize()` creates a `tknlist` column. Indeed, I expected the column to still be a character where every token was placed on it's own row, like what happens when you use [`tidytext`](https://juliasilge.github.io/tidytext/) functions. This certainly makes the object more compact and easier to work with, an excellent design decision in my opinion. To see the tokens, though, we need to use `recipes` `show_tokens()` function.

```{r day-24-show-tokens}
recipe(~ text, data = quotes) |>
  step_tokenize(text) |>
  show_tokens(text)
```

`step_tokenize()` defaults to tokenizing by word. However, there are several ways we can tokenize by. To do this, we modify the `tokenizers` argument. For example, say we want to tokenize by character instead of word. We do the following:

```{r day-24-tokenize-by-char}
recipe(~ text, data = quotes) |>
  step_tokenize(text, token = "characters") |>
  show_tokens(text)
```

How about by word stems?

```{r day-24-tokenize-by-stem}
recipe(~ text, data = quotes) |>
  step_tokenize(text, token = "word_stems") |>
  show_tokens(text)
```

ngrams?

```{r day-24-tokenize-by-ngram}
recipe(~ text, data = quotes) |>
  step_tokenize(text, token = "ngrams") |>
  show_tokens(text)
```

Say we only want 2 words in our ngram tokens. In this case, we need to pass argument values to the `tokenizers::tokenize_ngrams()` function using `step_tokenize()`'s `options` argument. Here we pass all the argument values in a list.

```{r day-24-tokenize-by-2-ngram}
recipe(~ text, data = quotes) |>
  step_tokenize(
    text, 
    token = "ngrams",
    options = list(n = 2)
  ) |>
  show_tokens(text)
```

Going further, say we want to also exclude stop words (e.g., the, is, and) from being included within our ngrams. We just pass this option along in our list of options.

```{r day-24-tokenize-by-2-ngram-no-stop}
recipe(~ text, data = quotes) |>
  step_tokenize(
    text, 
    token = "ngrams",
    options = list(n = 2, stopwords = c("the", "is", "and"))
  ) |>
  show_tokens(text)
```

That's all the time I have for today. There's some other things `step_tokenize()` can do. I highly suggest checking out it's [documentation](https://textrecipes.tidymodels.org/reference/step_tokenize.html) to see all of its functionality.

# Day 25 - Use `step_clean_level()` to clean categorical levels

Keeping our focus on `textrecipes`, I'm going to highlight the use of `step_clean_levels()`. This function is useful for cleaning up the text used within character or factor variables. Specifically, this function will clean text values in our variable to only include:

* letters
* numbers
* underscores

Let's apply this text cleaning recipe step to the `item_name` variable in our obfuscated Google Analytics data. In fact, to make it easier to see what's happening, I'll `select()` just the `item_name` within our example data set. I'll also go ahead and just create our training and testing split here as well.

```{r day-25-data}
data_ga <- read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  ) |>
select(item_name)

set.seed(20240127)
ga_split <- initial_split(data_ga, prop = .8)
ga_tr <- training(ga_split)
ga_te <- testing(ga_split)
```

If you scan through the items, you'll begin to notice some of the text could be cleaned. For example, the data contains item names like:

* `Google Medium Pet Collar (Red/Yellow)` 
* `#IamRemarkable Unisex T-Shirt`
* `Android SM S/F18 Sticker Sheet`

If you've ever worked with ecommerce or website data, these strings are actually not too bad. However, we can use `step_clean_levels()` to make them easier to compute on.

Here's our recipe.

```{r day-25-rec}
ga_rec <- recipe(~., data = ga_te) |>
  step_clean_levels(item_name) |>
  prep()

ga_rec
```

`step_clean_levels()`' tidy method is pretty informative. It provides one column with the original value and a second column with the cleaned value. Take a look at what it will do to our values once baked. 

```{r day-25-tidy-rec}
tidy(ga_rec, number = 1)
```

::: {.callout-note}
It's interesting to see that the hashtag (i.e., `#`) was converted to the value `number`. I couldn't find any options in `step_clean_levels()`to modify this, so you may need to first do some pre-cleaning before applying this function.
:::

Now, we bake. 

::: {.callout-note}
`step_clean_levels()` will convert character vectors to factors for us.
:::

```{r day-25-rec-bake}
bake(ga_rec, new_data = NULL)
```

All in all, a pretty useful step function from `textrecipes`. Take some time to apply the `step_clean_levels()` to your data. It might save you a great deal of time if you're working with some messy text data.
