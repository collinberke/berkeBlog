---
title: "30 day tidymodels `recipes` challenge"
date: "2024-01-01"
author: "Collin K. Berke, Ph.D."
draft: false 
image: thumbnail.jpg
description: "Learning how to use the `recipes` package, one day at a time"
toc: true
categories:
  - machine learning
  - feature engineering
  - tidymodels
  - data wrangling
---

![Photo by [Nicolas Gras](https://unsplash.com/photos/assorted-cookware-set-UiGsP8TvOJQ)](thumbnail-wide.jpg)

# Background

Before the holidays, I came across [Emil Hvitfeldt's](https://www.linkedin.com/in/emilhvitfeldt/) `#adventofsteps` LinkedIn [posts](https://www.linkedin.com/feed/hashtag/?keywords=adventofsteps). Following a model popularized by [advent of code](https://adventofcode.com/2023/about)--an annual tradition of online programming puzzles based on the theme of an [advent calendar](https://en.wikipedia.org/wiki/Advent_calendar)--these posts provided daily examples on the use of various `step_*` functions from the `tidymodels`' [`recipes`](https://recipes.tidymodels.org/index.html) package. This post, with a slight spin, is inspired by these posts. 

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(here)
tidymodels_prefer()
```

# My spin on this

One of my personal goals this coming year is to learn and practice using the different tidymodels' packages. To complete this goal, I thought a 30 day `recipes` challenge would be a good start. Each day during this 30 day personal challenge, I will focus on learning and creating some daily notes about one functionality of the `recipes` package. First, I start with the basics (e.g., how to create a recipe object). Then, I'll focus on describing the various `step_*` functions. 

To keep me on track, while also avoiding making this a chore, I'm going to place a 1-hour a day stopgap on studying, practicing, and documenting what I've learned. Depending on my schedule and motivation, I may work ahead on some material, but I will strive to update this post once a day.

Given the time constraint I'm imposing on myself, some of my daily notes or examples may result in an incomplete description of functionality. In cases like this, I'll try to link to relevant documentation for you to follow up and learn more. Please be flexible with any grammar and spelling errors during this challenge, as I'll likely edit very little until the end of the 30 days, if at all. 

Since the aim of this post is to document what I'm learning, all errors are completely mine. I highly suggest following up with the `recipes` package's [documentation](https://recipes.tidymodels.org/) and the [Tidy Modeling with R](https://www.tmwr.org/) book following a review of these notes. Both do a more thorough job overviewing the package's functionality.

# What I intend to get out of this challenge 

By the end of this challenge, I hope to have pushed myself to learn more about how to use `tidymodels`'s `recipe` package, and to create several example use cases of different functionality.

# Day 01 - Create a recipe

First off, what is a recipe? According to the docs:

> A recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.

So, I start this personal challenge by overviewing how to create a recipe object with the `recipes` package. The `recipe()` function is used to create a recipe object. 

When creating a recipe, we need to consider what **roles** variables take. In simple modeling tasks, you'll just have outcomes and predictors. However, variables may take on other roles (i.e., IDs). As such, the `recipe()` function provides multiple means for specifying the role of a variable: 

1. The formula 
2. Manually updating roles using the `update_role()` function.

Let's use the `credit_data` from tidymodels' `modeldata` package. You can get more information about this data by running `?credit_data` in your console.

```{r day-01-data}
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)

# Create splits for examples
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

```{r day-01-recipe-formula}
# No outcome variable, `.` is a shortcut for **all** variables
credit_rec <- recipe(~., data = credit_train)

# Outcome with specific variables to be included within model
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)

# Recipe uses `data` only as a template, all the data is not needed
# Useful in cases when you're working with large data
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = head(credit_train)
)
```

```{r day-01-recipe-update}
# Use `update_role()` to specify variable roles
credit_rec_update <- recipe(credit_train) |>
  update_role(Status, new_role = "outcome") |>
  update_role(
    Seniority, Home, Time, Age, Marital, Records, 
    Job, Expenses, Income, Assets, Debt, Amount, 
    Price, new_role = "predictor"
  )

credit_rec_update
```

The `update_role()` function is useful in cases where you might have an ID variable you don't want to include within your model.

```{r day-01-create-data-w-id}
credit_data_id <- credit_data |>
  mutate(id = 1:n(), .before = 1)

set.seed(2)
credit_id_split <- 
  initial_split(credit_data_id, prop = 0.8, strata = Status)
credit_id_train <- training(credit_id_split)
credit_id_test <- testing(credit_id_split)
```

```{r day-01-recipe-w-ids}
# Manually add an 'id' role to a variable
credit_id_rec <- recipe(credit_id_train) |>
  update_role(id, new_role = "id") |>
  update_role(Status, new_role = "outcome") |>
  update_role(
    Seniority, Home, Time, Age, Marital, Records, 
    Job, Expenses, Income, Assets, Debt, Amount, 
    Price, new_role = "predictor"
  ) 

credit_id_rec
```

In case you ever need to remove a role, you can use `remove_role()`.

```{r day-01-recipe-no-id}
credit_no_id_rec <- credit_id_rec |>
  remove_role(id, old_role = "id")

# id will be assigned and 'undeclared' role
credit_no_id_rec
```

Each recipe has its own summary method. We can wrap the recipe object within `summary()` to output more information about each variable and its assigned role.

```{r day-01-summary}
# Formula specified recipe
summary(credit_rec)

# Manually specified using `update_role()`
summary(credit_rec_update)

# Recipe with a variable holding the 'id' role
summary(credit_id_rec)
```

# Day 02 - How to use `prep()` and `bake()` 

Let's stick with the credit data for today's examples.

```{r day-02-data}
# Same code from day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)

# Create splits for our day 2 examples
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

We're going to continue to use the previously specified limited model from day 01 for our examples.

```{r day-02-recipe}
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)
```

Now that we know how to specify a recipe, we need to learn how to use `recipes`' `prep()` and `bake()` functions. `prep()` calculates any intermediate values required for preprocessing. `bake()` applies the preprocessing steps--using any intermediate values--to our testing and training data.

`prep()` and `bake()` [can be confusing](https://stackoverflow.com/questions/62189885/what-is-the-difference-among-prep-bake-juice-in-the-r-package-recipes) at first. However, I like the following analogy from the [R4DS learning community's Q&A](https://youtu.be/xygnYlku-w4?feature=shared&t=1822) with the authors of the [Tidy Modeling with R book](https://www.tmwr.org/):

> They're analogous to `fit()` and `predict()` ... `prep()` is like fitting where you're estimating stuff and `bake()` is like you're applying it.
>
> \- Max Kuhn

For a more formal treatment, the `prep()` docs state:

> For a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets. 

The `bake()` docs state:

> For a recipe with at least one preprocessing operation that has been trained by `prep()`, apply the computations to new data.

Why two separate functions? Some preprocessing steps need an *intermediate calculation* step to be performed before applying the recipe to the data (e.g., `step_normalize()` and `step_center()`; more on this later). To better articulate this point, I'm going to fast-forward a bit in our challenge and apply the `step_center()` function to our recipe. `step_center()` is used to center variables.

When centering a variable, we need to make an intermediate calculation (i.e., `prep()`) before applying the calculation to perform the centering to our data (i.e., `bake()`).

For our example, say we want to center the `Debt` variable. To do this, we can simply add `step_center(Debt)` to our recipe. When we pipe the recipe object to `prep()`, the mean is calculated in the background to perform the preprocessing step. 

```{r day-02-prep}
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
) |>
  step_center(Debt) |>
  prep() 
```
We can see this calculated value by using the `number` argument in the `tidy.recipe()` method. 

```{r day-02-prep-tidy}
# Print a summary of the recipe steps to be performed
tidy(credit_rec)

# Print additional information about the first recipe step
tidy(credit_rec, number = 1)
```

Take note, though, the `Debt` variable has not been centered yet, and we are still working with a recipe object.

We then apply the centering transformation to the data by piping the prepped recipe to `bake()`. We can apply the preprocessing to the training data by passing the `NULL` to the `new_data` argument. `bake()` returns a tibble with our transformed variable using our training data. 

```{r day-02-baked}
credit_baked <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
) |>
  step_center(Debt) |>
  prep() |>
  bake(new_data = NULL)

credit_baked
```

Most likely, you won't use `prep()` and `bake()` for other modeling tasks. However, they'll be important as we continue exploring the `recipes` package in the coming days. 

# Day 03 - Selector functions

Remaining consistent, let's continue using the `credit_data` data for some of today's examples. We'll also use the `Chicago` data set for a couple additional examples. You can read more about this data by running `?Chicago` in your console.

Here we'll get our data and split it into training and testing for both data sets.

```{r day-03-credit-data}
# Same code from day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

```{r day-03-Chicago-data}
data(Chicago, package = "modeldata")
glimpse(Chicago)

# For reproducibility
set.seed(2)
chicago_split <- initial_split(Chicago, prop = 0.8)
chicago_train <- training(chicago_split)
chicago_test <- testing(chicago_split)
```

When using `recipes`, we often need to select a group of variables (e.g., all predictors, all numeric variables, all categorical variables, etc.) to apply preprocessing steps. Indeed, we certainly could just explicitly specifiy each variable by name within our recipe. There's a better way, though. Use [*selector functions*](https://recipes.tidymodels.org/reference/has_role.html).

Selector functions can be used to choose variables based on:

1. Variable names
2. Current role
3. Data type
4. Any combination of the above three

The first set of selectors comes from the [`tidyselect`](https://tidyselect.r-lib.org) package, which allows you to make selections based on variable names. Some common ones include:

* `tidyselect::starts_with()`
* `tidyselect::ends_with()`
* `tidyselect::contains()`
* `tidyselect::everything()`

Check out `recipes`' `?selections` and the `tidyselect` [docs](https://tidyselect.r-lib.org) for a more exhaustive list of available selection functions. Included above are the ones I commonly use. Here are a few examples of how to use these selector functions to center variables.

```{r day-03-center-chicago-weather}
# Apply the centering to variables that start with the *weather* prefix
chicago_rec <- 
  recipe(ridership ~ ., data = chicago_train) |>
  step_center(starts_with("weather")) |>
  prep() |>
  bake(new_data = NULL)

chicago_rec |> select(starts_with("weather"))
```

Selections also allows us to use the `-` to exclude specific variables or groupings of variables while using selector functions.

```{r day-03-center-some-numeric}
chicago_rec <- 
  recipe(ridership ~ ., data = chicago_train) |>
  step_center(-date, -starts_with("weather")) |>
  prep() |>
  bake(new_data = NULL)

chicago_rec

# To show centering was not applied to variables with the *weather* prefix
chicago_rec |> select(starts_with("weather"))
```

`recipes` provides functions to select variables based on role and type. This includes the [`has_role()`](https://recipes.tidymodels.org/reference/has_role.html) and [`has_type()`](https://recipes.tidymodels.org/reference/has_role.html) functions.

```{r day-03-select-by-role}
# Simplified recipe, applying centering to variables with predictor role 
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)  |>
  step_center(has_role("predictor")) |>
  prep() |>
  bake(new_data = NULL)

credit_rec
```

```{r day-03-select-by-type}
# Applying centering to variables with type numeric
credit_rec_type <- recipe(Status ~ ., data = credit_train) |>
  step_center(has_type(match = "numeric")) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_type
```

Although `has_role()` and `has_type()` are available, you'll most likely rely on functions that are more specific. The docs state (`?has_role`):

> **In most cases**, the right approach for users will be to use the predictor-specific selectors such as `all_numeric_predictors()` and `all_nominal_predictors()`.

These include functions to select variables based on type:

* `all_numeric()` - includes all numeric variables.
* `all_nominal()` - includes both character and factor variables.

```{r day-03-select-rec-type}
# Center **all** numeric variables
credit_rec_type <- recipe(
  Status ~ Debt + Income + Assets,
  data = credit_train
) |>
  step_center(all_numeric()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_type 
```

Functions to select by role:

* `all_predictors()`
* `all_outcomes()`

```{r day-03-select-rec-role}
# Center all predictors
credit_rec_role <- 
  recipe(
    Status ~ Debt + Income + Assets, 
    data = credit_train
  ) |>
  step_center(all_predictors()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_role
```

Functions to select variables that intersect by role and type:

* `all_numeric_predictors()`
* `all_nominal_predictors()`

```{r day-03-select-num-predictors}
credit_rec_num_pred <- 
  recipe(Status ~ ., data = credit_train) |>
  step_center(all_numeric_predictors()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_num_pred
```

Selector functions will become useful as we continue to explore the `step_*` functions within the `recipes` package.

# Day 04 - Create dummy variables using `step_dummy()`

Before starting our overview of `recipes`' `step_*` functions, we need a bit of direction on what preprocessing steps might be required or beneficial to apply. **The type of data preprocessing is determined by the model being fit.** As a starting point, the [Tidy Modeling with R](https://www.tmwr.org/pre-proc-table) book provides an [appendix](https://www.tmwr.org/pre-proc-table) with a table of preprocessing recommendations based on the types of models being used. This table is separate from the types of feature engineering that may be applied, but it's a good baseline for determining the initial `step_*` functions to be included within a recipe. 

[Dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) is the first preprocessing method highlighted in this appendix. That is, the encoding of qualitative predictors into numeric predictors. Closely related is [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics). When dummy variables are created, most commonly, nominal variable columns are converted into separate columns of 1's and 0's. `recipes`' [`step_dummy()`](https://recipes.tidymodels.org/reference/step_dummy.html) function performs these preprocessing operations.

Let's continue using the `credit_data` for today's examples. Take note, this data contains some `NA`'s. To address this issue, I'm just going to drop any cases with a missing value using dplyr's `drop_na()` function. Indeed, this issue could be addressed with [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) through the use of `recipes`' `step_impute_*` functions (more on this in the coming days).  

```{r day-04-data}
# Same code as day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

credit_data <- credit_data |>
  drop_na()
```

```{r day-04-split}
# Create the split, training and testing data
set.seed(20230104)
credit_split <- initial_split(credit_data, prop = 0.8)
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

Here's the recipe we'll use. I'm gonna keep it simple, so it's easier to observe the results of adding `step_dummy()` to our recipe.

```{r day-04-rec}
credit_rec <- 
  recipe(
    Status ~ Job + Home + Marital, 
    data = credit_train
  ) 
```

Let's create dummy variables from the `Job` column. But first, let's take a look at how many different variable levels there are.

```{r day-04-job-cats}
unique(credit_data$Job)
```

Since we have four levels (`freelance`, `fixed`, `partime`, `others`), the `step_dummy()` function will create three columns. The `fixed` `Job` level will be the reference group, since it's the first level specified for the factor.

```{r day-04-dummy-job}
credit_rec |>
  step_dummy(Job) |>
  prep() |>
  bake(new_data = NULL)
```

Take note of the naming conventions applied to the new dummy columns. `step_dummy()` uses the following naming convention `variable-name_variable-level`. This makes it easier to know what variable the dummy variables originated.

Say you don't want to drop the original column when the dummy variables are created. We can pass `TRUE` to the `keep_original_cols` argument. This will retain the original column, while also creating the dummy variables.

```{r day-04-dummy-keep-col}
credit_rec |>
  step_dummy(Job, keep_original_cols = TRUE) |>
  prep() |>
  bake(new_data = NULL)
```

What about one-hot encoding? To apply one-hot encoding we specify `FALSE` to the `one_hot` argument within the function. The preprocessed, baked data will now contain four columns. One column for each level of the source column.

```{r day-04-one-hot-encoding}
credit_rec |>
  step_dummy(Job, one_hot = TRUE) |>
  prep() |>
  bake(new_data = NULL)
```

We can scale this preprocessing to all nominal predictors by using, you guessed it, selector functions.

```{r day-04-nominal-dummy-variables}
credit_rec |>
  step_dummy(all_nominal_predictors()) |>
  prep() |>
  bake(new_data = NULL)
```

That's a lot of additional columns. How can we keep track of all these additional columns and how they were preprocessed? We can `summary` and `tidy` our prepped recipe. Summarizing the prepped recipe is useful because of the `source` column that gets outputted. In our example, the source column of the returned tibble contains two values: original (i.e., the column was an original column in the data set) and derived (i.e., a column created from the preprocessing step). When we `tidy()` the recipe object returned from `step_dummy()`, a tibble with two columns is returned: `terms` and `columns`. `terms` represents the original variable the dummy variables were created from. `columns` represents the newly preprocessed dummy variable.

```{r day-04-dummy-tidy}
# Prep our dummy variables
credit_rec <- 
  credit_rec |>
  step_dummy(all_nominal_predictors()) |>
  prep()

summary(credit_rec)

# View what preprocessing steps are applied
tidy(credit_rec)

# Drill down and view what was done in during this specific step 
tidy(credit_rec, number = 1)
```

When it comes to specifying interactions within a model, there are some special considerations when using dummy variables. I don't have much time to discuss this today, but I hope to address it on a future day of this challenge. I suggest reviewing the 'Interactions with Dummy Variables' section from the 'Dummies' vignette (`vignettes("Dummies", package = "recipes")`) for more information.

One more thing, `step_dummy()` is useful for straight forward dummy variable creation. However, `recipes` also has some other closely related `step_*` functions. Here is a list of a few from the 'Dummies' vignette:

* `step_other()` - collapses infrequently occurring levels into an 'other' category.
* `step_holiday()` - creates dummy variables from dates to capture holidays. Useful when working with time series data.
* `step_zv()` - removes dummy variables that are zero-variance.

I look to highlight the use of some of these `step_*` functions in the coming days.

# Day 05 - Create a binary indicator variable for holidays using `step_holiday()` 

Staying on the topic of dummy variables, I wanted to take a day to focus on the use of `recipes`' `step_holiday()` function. It seems to be pretty useful when working with time series data.

For today's example, I'm going to use some obsfucated, simulated [Google Analytics ecommerce data](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset). This emulates data closely related to what would be collected for the [Google Merchandise Store](https://shop.googlemerchandisestore.com/). You can learn more about this data by clicking on the previously linked docs. Let's do some data wrangling.

Some notes about what wrangling was done:

* Parse the `event_date` column into a date variable.
* Calculate the revenue generated from the purchase of items based on quantity.
* Retain only relevant columns.

For simplicity, I'm not going to create a testing training split for this data.

```{r day-05-data}
data_ga <- 
  read_csv(
    here("blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv")
  ) |>
  mutate(
    event_date = ymd(event_date),
    revenue = price_in_usd * quantity
  ) |>
  select(event_date, transaction_id, item_category, revenue)
```

Let's start on our recipe. Since we have an id variable, `transaction_id`, let's update the recipe to change it's role to `id`. Once we do that, we can pass the `event_date` to the `step_holiday()` function. Before we bake our recipe, I wanna `prep()` and summarise the preprocessing to see what columns will get added.

```{r day-05-rec}
ga_rec <- recipe(revenue ~ ., data = data_ga) |>
  update_role(transaction_id, new_role = "id") |>
  step_holiday(event_date) |>
  prep() 

summary(ga_rec)
```

Note, three new columns will be added once the recipe is baked. This includes:

* `event_date_LaborDay` - a dummy variable to represent an item purchases on [Labor Day](https://en.wikipedia.org/wiki/Labor_Day). 
* `event_date_NewYearsDay` - a dummy variable to represent item purchases on New Years Day. 
* `event_date_ChristmasDay` - a dummy variable to represent item purchases made on Christmas Day.

You can see the variables that get added by baking the recipe.

```{r day-05-rec-baked}
bake(ga_rec, new_data = NULL)
```

Labor Day, New Years Day, and Christmas Day are the default holidays preprocessed by the function. You can modify this by passing a character vector of holidays to `step_holiday()`'s `holidays` argument. For instance, say we wanted to create dummy variables for [Boxing Day](https://en.wikipedia.org/wiki/Boxing_Day) and the [United State's Thanksgiving Day](https://en.wikipedia.org/wiki/Thanksgiving_(United_States)) holiday, while excluding Labor Day. The following code will specify this preprocessing step for us:

```{r day-05-rec-expanded-holidays}
ga_rec_holidays <- recipe(revenue ~ ., data = data_ga) |>
  update_role(transaction_id, new_role = "transaction_id") |>
  step_holiday(
    event_date, 
    holidays = c("USThanksgivingDay", "ChristmasDay", "BoxingDay", "NewYearsDay")
  ) |>
  prep()

summary(ga_rec_holidays)
```

Now we have a dummy variable for all four of these holidays. Let's bake our recipe and see the final result.

```{r day-05-rec-expanded-holidays-baked}
bake(ga_rec_holidays, new_data = NULL)
```

Indeed, there are many holidays that could be specified for dummy variable creation. All the available holdidays can be seen by running `timeDate::listHolidays()` in your console. Last time I checked, there were 118 available holidays.

# Day 06 - Use `step_zv()` to drop variables with one value 

For today, I'm focusing on `recipes`' `step_zv()` function. This function is a filter function, which drops variables that only contain one value.

At first, I didn't really understand why `step_zv()` was made available. Why would you want a step to drop variables within a recipe? Then it clicked working on yesterday's example using the obfuscated [Google Analytics data](https://developers.google.com/analytics/bigquery/web-ecommerce-demo-dataset) for the [Google Merchandise store](https://shop.googlemerchandisestore.com/). 

But first, let's get our data again and specify our recipe. I'm going to keep things simple here. First, I'm just going to use `data_ga`, which was previously wrangled in yesterday's post (check it out if you want more info). Second, I'm going to skip creating a testing and training split. Lastly, I'm going to create dummy variables using `step_holiday()`, just to show how `step_zv()` can be useful.

```{r day-06-data}
ga_rec <- recipe(revenue ~ ., data = data_ga) |>
  step_holiday(event_date)

summary(ga_rec)
```

Let's take a closer look at our data. You'll notice the range of the `event_date` is a subset of data. `data_ga`'s `event_date` ranges between the US holiday season. It starts right before Christmas and moves into the first month of the new year. 

```{r day-06-view-date-range}
c(
  min_date = min(data_ga$event_date), 
  max_date = max(data_ga$event_date)
)
```

If you remember from yesterday's post, one of the default holidays for `step_holiday()` is Labor Day. As such, a dummy variable with all `0`'s will be created for the Labor Day holiday. Purchases made on these dates were not included within this data.

```{r day-06-no-labor-day}
ga_prep <- prep(ga_rec)

tidy(ga_prep, number = 1)

# Check the unique values
bake(ga_prep, new_data = NULL) |> 
  select(event_date_LaborDay) |>
  distinct(event_date_LaborDay)
```

As such, this variable is not very useful and should be dropped before being applied within our model. This is why `step_zv()` can be handy, especially in situations where you have a lot of variables that could only have one value. `step_zv()` makes it easy to drop all unnecessary variables in one step, while allowing you to continue working with a recipe object. 

Indeed, keen observers might note this step could be mitigated by modifying the `holiday` argument in `step_holiday()`. However, the function's utility extends beyond just `step_holiday()`. You might even consider useful as a final step you apply to every recipe.

```{r day-06-rec-with-step-zv}
ga_rec_drop <- recipe(revenue ~ ., data = data_ga) |>
  step_holiday(event_date) |>
  step_zv(all_predictors()) |>
  prep()

ga_rec_drop

tidy(ga_rec_drop)
```

Take note, the `prep()` output informs us of the variables that were dropped when the step was applied. This is something to keep an eye on, just in case you need to explore situations where many variables are dropped, and you need to explore what your recipe is actually doing. 

For completeness, lets `bake()` our final recipe.

```{r day-06-rec-bake}
ga_rec <- recipe(revenue ~., data = data_ga) |>
  step_holiday(event_date) |>
  step_zv(all_predictors()) |>
  prep() |>
  bake(new_data = NULL)

ga_rec

glimpse(ga_rec)
```
