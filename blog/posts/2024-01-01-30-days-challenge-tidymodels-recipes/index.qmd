---
title: "30 day tidymodels `recipes` challenge"
date: "2024-01-01"
author: "Collin K. Berke, Ph.D."
draft: false 
image: thumbnail.jpg
description: "Learning how to use the `recipes` package, one day at a time"
toc: true
categories:
  - machine learning
  - feature engineering
  - tidymodels
  - data wrangling
---

![Photo by [Nicolas Gras](https://unsplash.com/photos/assorted-cookware-set-UiGsP8TvOJQ)](thumbnail-wide.jpg)

# Background

Before the holidays, I came across [Emil Hvitfeldt's](https://www.linkedin.com/in/emilhvitfeldt/) `#adventofsteps` LinkedIn [posts](https://www.linkedin.com/feed/hashtag/?keywords=adventofsteps). Following a model popularized by [advent of code](https://adventofcode.com/2023/about)--an annual tradition of online programming puzzles based on the theme of an [advent calendar](https://en.wikipedia.org/wiki/Advent_calendar)--these posts provided daily examples on the use of various `step_*` functions from the `tidymodels`' [`recipes`](https://recipes.tidymodels.org/index.html) package. This post, with a slight spin, is inspired by these posts. 

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
```

# My spin on this

One of my personal goals this coming year is to learn and practice using the different tidymodels' packages. To complete this goal, I thought a 30 day `recipes` challenge would be a good start. Each day during this 30 day personal challenge, I will focus on learning and creating some daily notes about one functionality of the `recipes` package. First, I start with the basics (e.g., how to create a recipe object). Then, I'll focus on describing the various `step_*` functions. 

To keep me on track, while also avoiding making this a chore, I'm going to place a 1-hour a day stopgap on studying, practicing, and documenting what I've learned. Depending on my schedule and motivation, I may work ahead on some material, but I will strive to update this post once a day.

Given the time constraint I'm imposing on myself, some of my daily notes or examples may result in an incomplete description of functionality. In cases like this, I'll try to link to relevant documentation for you to follow up and learn more. Please be flexible with any grammar and spelling errors during this challenge, as I'll likely edit very little until the end of the 30 days, if at all. 

Since the aim of this post is to document what I'm learning, all errors are completely mine. I highly suggest following up with the `recipes` package's [documentation](https://recipes.tidymodels.org/) and the [Tidy Modeling with R](https://www.tmwr.org/) book following a review of these notes. Both do a more thorough job overviewing the package's functionality.

# What I intend to get out of this challenge 

By the end of this challenge, I hope to have pushed myself to learn more about how to use `tidymodels`'s `recipe` package, and to create several example use cases of different functionality.

# Day 01 - Create a recipe

First off, what is a recipe? According to the docs:

> A recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.

So, I start this personal challenge by overviewing how to create a recipe object with the `recipes` package. The `recipe()` function is used to create a recipe object. 

When creating a recipe, we need to consider what **roles** variables take. In simple modeling tasks, you'll just have outcomes and predictors. However, variables may take on other roles (i.e., IDs). As such, the `recipe()` function provides multiple means for specifying the role of a variable: 

1. The formula 
2. Manually updating roles using the `update_role()` function.

Let's use the `credit_data` from tidymodels' `modeldata` package. You can get more information about this data by running `?credit_data` in your console.

```{r day-01-data}
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)

# Create splits for examples
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

```{r day-01-recipe-formula}
# No outcome variable, `.` is a shortcut for **all** variables
credit_rec <- recipe(~., data = credit_train)

# Outcome with specific variables to be included within model
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)

# Recipe uses `data` only as a template, all the data is not needed
# Useful in cases when you're working with large data
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = head(credit_train)
)
```

```{r day-01-recipe-update}
# Use `update_role()` to specify variable roles
credit_rec_update <- recipe(credit_train) |>
  update_role(Status, new_role = "outcome") |>
  update_role(
    Seniority, Home, Time, Age, Marital, Records, 
    Job, Expenses, Income, Assets, Debt, Amount, 
    Price, new_role = "predictor"
  )

credit_rec_update
```

The `update_role()` function is useful in cases where you might have an ID variable you don't want to include within your model.

```{r day-01-create-data-w-id}
credit_data_id <- credit_data |>
  mutate(id = 1:n(), .before = 1)

set.seed(2)
credit_id_split <- 
  initial_split(credit_data_id, prop = 0.8, strata = Status)
credit_id_train <- training(credit_id_split)
credit_id_test <- testing(credit_id_split)
```

```{r day-01-recipe-w-ids}
# Manually add an 'id' role to a variable
credit_id_rec <- recipe(credit_id_train) |>
  update_role(id, new_role = "id") |>
  update_role(Status, new_role = "outcome") |>
  update_role(
    Seniority, Home, Time, Age, Marital, Records, 
    Job, Expenses, Income, Assets, Debt, Amount, 
    Price, new_role = "predictor"
  ) 

credit_id_rec
```

In case you ever need to remove a role, you can use `remove_role()`.

```{r day-01-recipe-no-id}
credit_no_id_rec <- credit_id_rec |>
  remove_role(id, old_role = "id")

# id will be assigned and 'undeclared' role
credit_no_id_rec
```

Each recipe has its own summary method. We can wrap the recipe object within `summary()` to output more information about each variable and its assigned role.

```{r day-01-summary}
# Formula specified recipe
summary(credit_rec)

# Manually specified using `update_role()`
summary(credit_rec_update)

# Recipe with a variable holding the 'id' role
summary(credit_id_rec)
```

# Day 02 - How to use `prep()` and `bake()` 

Let's stick with the credit data for today's examples.

```{r day-02-data}
# Same code from day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)

# Create splits for our day 2 examples
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

We're going to continue to use the previously specified limited model from day 01 for our examples.

```{r day-02-recipe}
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)
```

Now that we know how to specify a recipe, we need to learn how to use `recipes`' `prep()` and `bake()` functions. `prep()` calculates any intermediate values required for preprocessing. `bake()` applies the preprocessing steps--using any intermediate values--to our testing and training data.

`prep()` and `bake()` [can be confusing](https://stackoverflow.com/questions/62189885/what-is-the-difference-among-prep-bake-juice-in-the-r-package-recipes) at first. However, I like the following analogy from the [R4DS learning community's Q&A](https://youtu.be/xygnYlku-w4?feature=shared&t=1822) with the authors of the [Tidy Modeling with R book](https://www.tmwr.org/):

> They're analogous to `fit()` and `predict()` ... `prep()` is like fitting where you're estimating stuff and `bake()` is like you're applying it.
>
> \- Max Kuhn

For a more formal treatment, the `prep()` docs state:

> For a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets. 

The `bake()` docs state:

> For a recipe with at least one preprocessing operation that has been trained by `prep()`, apply the computations to new data.

Why two separate functions? Some preprocessing steps need an *intermediate calculation* step to be performed before applying the recipe to the data (e.g., `step_normalize()` and `step_center()`; more on this later). To better articulate this point, I'm going to fast-forward a bit in our challenge and apply the `step_center()` function to our recipe. `step_center()` is used to center variables.

When centering a variable, we need to make an intermediate calculation (i.e., `prep()`) before applying the calculation to perform the centering to our data (i.e., `bake()`).

For our example, say we want to center the `Debt` variable. To do this, we can simply add `step_center(Debt)` to our recipe. When we pipe the recipe object to `prep()`, the mean is calculated in the background to perform the preprocessing step. 

```{r day-02-prep}
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
) |>
  step_center(Debt) |>
  prep() 
```
We can see this calculated value by using the `number` argument in the `tidy.recipe()` method. 

```{r day-02-prep-tidy}
# Print a summary of the recipe steps to be performed
tidy(credit_rec)

# Print additional information about the first recipe step
tidy(credit_rec, number = 1)
```

Take note, though, the `Debt` variable has not been centered yet, and we are still working with a recipe object.

We then apply the centering transformation to the data by piping the prepped recipe to `bake()`. We can apply the preprocessing to the training data by passing the `NULL` to the `new_data` argument. `bake()` returns a tibble with our transformed variable using our training data. 

```{r day-02-baked}
credit_baked <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
) |>
  step_center(Debt) |>
  prep() |>
  bake(new_data = NULL)

credit_baked
```

Most likely, you won't use `prep()` and `bake()` for other modeling tasks. However, they'll be important as we continue exploring the `recipes` package in the coming days. 

# Day 03 - Selector functions

Remaining consistent, let's continue using the `credit_data` data for some of today's examples. We'll also use the `Chicago` data set for a couple additional examples. You can read more about this data by running `?Chicago` in your console.

Here we'll get our data and split it into training and testing for both data sets.

```{r day-03-credit-data}
# Same code from day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

# For reproducibility
set.seed(1)
credit_split <- initial_split(credit_data, prop = 0.8, strata = Status)
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

```{r day-03-Chicago-data}
data(Chicago, package = "modeldata")
glimpse(Chicago)

# For reproducibility
set.seed(2)
chicago_split <- initial_split(Chicago, prop = 0.8)
chicago_train <- training(chicago_split)
chicago_test <- testing(chicago_split)
```

When using `recipes`, we often need to select a group of variables (e.g., all predictors, all numeric variables, all categorical variables, etc.) to apply preprocessing steps. Indeed, we certainly could just explicitly specifiy each variable by name within our recipe. There's a better way, though. Use [*selector functions*](https://recipes.tidymodels.org/reference/has_role.html).

Selector functions can be used to choose variables based on:

1. Variable names
2. Current role
3. Data type
4. Any combination of the above three

The first set of selectors comes from the [`tidyselect`](https://tidyselect.r-lib.org) package, which allows you to make selections based on variable names. Some common ones include:

* `tidyselect::starts_with()`
* `tidyselect::ends_with()`
* `tidyselect::contains()`
* `tidyselect::everything()`

Check out `recipes`' `?selections` and the `tidyselect` [docs](https://tidyselect.r-lib.org) for a more exhaustive list of available selection functions. Included above are the ones I commonly use. Here are a few examples of how to use these selector functions to center variables.

```{r day-03-center-chicago-weather}
# Apply the centering to variables that start with the *weather* prefix
chicago_rec <- 
  recipe(ridership ~ ., data = chicago_train) |>
  step_center(starts_with("weather")) |>
  prep() |>
  bake(new_data = NULL)

chicago_rec |> select(starts_with("weather"))
```

Selections also allows us to use the `-` to exclude specific variables or groupings of variables while using selector functions.

```{r day-03-center-some-numeric}
chicago_rec <- 
  recipe(ridership ~ ., data = chicago_train) |>
  step_center(-date, -starts_with("weather")) |>
  prep() |>
  bake(new_data = NULL)

chicago_rec

# To show centering was not applied to variables with the *weather* prefix
chicago_rec |> select(starts_with("weather"))
```

`recipes` provides functions to select variables based on role and type. This includes the [`has_role()`](https://recipes.tidymodels.org/reference/has_role.html) and [`has_type()`](https://recipes.tidymodels.org/reference/has_role.html) functions.

```{r day-03-select-by-role}
# Simplified recipe, applying centering to variables with predictor role 
credit_rec <- recipe(
  Status ~ Debt + Income + Assets, 
  data = credit_train
)  |>
  step_center(has_role("predictor")) |>
  prep() |>
  bake(new_data = NULL)

credit_rec
```

```{r day-03-select-by-type}
# Applying centering to variables with type numeric
credit_rec_type <- recipe(Status ~ ., data = credit_train) |>
  step_center(has_type(match = "numeric")) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_type
```

Although `has_role()` and `has_type()` are available, you'll most likely rely on functions that are more specific. The docs state (`?has_role`):

> **In most cases**, the right approach for users will be to use the predictor-specific selectors such as `all_numeric_predictors()` and `all_nominal_predictors()`.

These include functions to select variables based on type:

* `all_numeric()` - includes all numeric variables.
* `all_nominal()` - includes both character and factor variables.

```{r day-03-select-rec-type}
# Center **all** numeric variables
credit_rec_type <- recipe(
  Status ~ Debt + Income + Assets,
  data = credit_train
) |>
  step_center(all_numeric()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_type 
```

Functions to select by role:

* `all_predictors()`
* `all_outcomes()`

```{r day-03-select-rec-role}
# Center all predictors
credit_rec_role <- 
  recipe(
    Status ~ Debt + Income + Assets, 
    data = credit_train
  ) |>
  step_center(all_predictors()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_role
```

Functions to select variables that intersect by role and type:

* `all_numeric_predictors()`
* `all_nominal_predictors()`

```{r day-03-select-num-predictors}
credit_rec_num_pred <- 
  recipe(Status ~ ., data = credit_train) |>
  step_center(all_numeric_predictors()) |>
  prep() |>
  bake(new_data = NULL)

credit_rec_num_pred
```

Selector functions will become useful as we continue to explore the `step_*` functions within the `recipes` package.

# Day 04 - Create dummy variables using `step_dummy()`

Before starting our overview of `recipes`' `step_*` functions, we need a bit of direction on what preprocessing steps might be required or beneficial to apply. **The type of data preprocessing is determined by the model being fit.** As a starting point, the [Tidy Modeling with R](https://www.tmwr.org/pre-proc-table) book provides an [appendix](https://www.tmwr.org/pre-proc-table) with a table of preprocessing recommendations based on the types of models being used. This table is separate from the types of feature engineering that may be applied, but it's a good baseline for determining the initial `step_*` functions to be included within a recipe. 

[Dummy variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) is the first preprocessing method highlighted in this appendix. That is, the encoding of qualitative predictors into numeric predictors. Closely related is [one-hot encoding](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics). When dummy variables are created, most commonly, nominal variable columns are converted into separate columns of 1's and 0's. `recipes`' [`step_dummy()`](https://recipes.tidymodels.org/reference/step_dummy.html) function performs these preprocessing operations.

Let's continue using the `credit_data` for today's examples. Take note, this data contains some `NA`'s. To address this issue, I'm just going to drop any cases with a missing value using dplyr's `drop_na()` function. Indeed, this issue could be addressed with [imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)) through the use of `recipes`' `step_impute_*` functions (more on this in the coming days).  

```{r day-04-data}
# Same code as day 01
data(credit_data, package = "modeldata")
glimpse(credit_data)

credit_data <- credit_data |>
  drop_na()
```

```{r day-04-split}
# Create the split, training and testing data
set.seed(20230104)
credit_split <- initial_split(credit_data, prop = 0.8)
credit_train <- training(credit_split)
credit_test <- testing(credit_split)
```

Here's the recipe we'll use. I'm gonna keep it simple, so it's easier to observe the results of adding `step_dummy()` to our recipe.

```{r day-04-rec}
credit_rec <- 
  recipe(
    Status ~ Job + Home + Marital, 
    data = credit_train
  ) 
```

Let's create dummy variables from the `Job` column. But first, let's take a look at how many different variable levels there are.

```{r day-04-job-cats}
unique(credit_data$Job)
```

Since we have four levels (`freelance`, `fixed`, `partime`, `others`), the `step_dummy()` function will create three columns. The `fixed` `Job` level will be the reference group, since it's the first level specified for the factor.

```{r day-04-dummy-job}
credit_rec |>
  step_dummy(Job) |>
  prep() |>
  bake(new_data = NULL)
```

Take note of the naming conventions applied to the new dummy columns. `step_dummy()` uses the following naming convention `variable-name_variable-level`. This makes it easier to know what variable the dummy variables originated.

Say you don't want to drop the original column when the dummy variables are created. We can pass `TRUE` to the `keep_original_cols` argument. This will retain the original column, while also creating the dummy variables.

```{r day-04-dummy-keep-col}
credit_rec |>
  step_dummy(Job, keep_original_cols = TRUE) |>
  prep() |>
  bake(new_data = NULL)
```

What about one-hot encoding? To apply one-hot encoding we specify `FALSE` to the `one_hot` argument within the function. The preprocessed, baked data will now contain four columns. One column for each level of the source column.

```{r day-04-one-hot-encoding}
credit_rec |>
  step_dummy(Job, one_hot = TRUE) |>
  prep() |>
  bake(new_data = NULL)
```

We can scale this preprocessing to all nominal predictors by using, you guessed it, selector functions.

```{r day-04-nominal-dummy-variables}
credit_rec |>
  step_dummy(all_nominal_predictors()) |>
  prep() |>
  bake(new_data = NULL)
```

That's a lot of additional columns. How can we keep track of all these additional columns and how they were preprocessed? We can `summary` and `tidy` our prepped recipe. Summarizing the prepped recipe is useful because of the `source` column that gets outputted. In our example, the source column of the returned tibble contains two values: original (i.e., the column was an original column in the data set) and derived (i.e., a column created from the preprocessing step). When we `tidy()` the recipe object returned from `step_dummy()`, a tibble with two columns is returned: `terms` and `columns`. `terms` represents the original variable the dummy variables were created from. `columns` represents the newly preprocessed dummy variable.

```{r day-04-dummy-tidy}
# Prep our dummy variables
credit_rec <- 
  credit_rec |>
  step_dummy(all_nominal_predictors()) |>
  prep()

summary(credit_rec)

# View what preprocessing steps are applied
tidy(credit_rec)

# Drill down and view what was done in during this specific step 
tidy(credit_rec, number = 1)
```

When it comes to specifying interactions within a model, there are some special considerations when using dummy variables. I don't have much time to discuss this today, but I hope to address it on a future day of this challenge. I suggest reviewing the 'Interactions with Dummy Variables' section from the 'Dummies' vignette (`vignettes("Dummies", package = "recipes")`) for more information.

One more thing, `step_dummy()` is useful for straight forward dummy variable creation. However, `recipes` also has some other closely related `step_*` functions. Here is a list of a few from the 'Dummies' vignette:

* `step_other()` - collapses infrequently occurring levels into an 'other' category.
* `step_holiday()` - creates dummy variables from dates to capture holidays. Useful when working with time series data.
* `step_zv()` - removes dummy variables that are zero-variance.

I look to highlight the use of some of these `step_*` functions in the coming days.
