{
  "hash": "1f3ce73967ba712e48a3a3f8530cf67b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women's volleyball data\"\nauthor: \"Collin K. Berke, Ph.D.\"\ndate: \"2023-12-07\"\ndescription: \"Using tidymodels to predict wins and losses for volleyball matches\"\ntoc: true\ncategories:\n  - tutorial\n  - tidymodels\n  - classification\n  - decision tree\n  - logistic regression\n---\n\n\n\nThe initial rounds of the NCAA women's volleyball [tournament](https://www.ncaa.com/news/volleyball-women/article/2023-11-18/2023-ncaa-volleyball-tournament-schedule-dates-di-womens-championship) have just begun. As such, I felt it was a good opportunity to understand more about the game while learning to specify models using [Big Ten women's volleyball](https://bigten.org/sports/wvball) match data and the [tidymodels](https://www.tidymodels.org/) framework. This post sought to specify a predictive model of wins and loses. It then used this model to explore and predict match outcomes of the #1 team going into the tournament, the [Nebraska Cornhuskers](https://huskers.com/sports/volleyball).\n\nThis post overviews the use of the tidymodels framework to fit and train predictive models. Specifically, it aims to be an introductory tutorial on the use of [tidymodels](https://www.tidymodels.org/) to split data into test and training sets, specify a model, and assess model fit using both the training and testing data. To do this, I explored the fit of two binary classification models to NCAA Big Ten women's volleyball match data, with the goal to predict wins and loses.\n\nBeing a high-level overview, this post will not cover topics like feature engineering, resampling techniques, hyperparameter tuning, or ensemble methods. Most assuredly, additional modeling procedures would lead to improved model predictions. As such, I plan to write future posts overviewing these topics.\n\nLet's attach the libraries we'll need for the session.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(here)\nlibrary(glue)\nlibrary(rpart.plot)\nlibrary(patchwork)\ntidymodels_prefer()\n```\n:::\n\n\n\n# Establish a modeling goal\n\nFirst things first, we need to establish a modeling goal. Given the scope of this post, I have the following goal:\n\nCreate a simple binary classification model to predict wins and loses using NCAA Big Ten women's volleyball match data.\n\n::: {.callout-note}\nThere might be a slight issue of data leakage in this example. [In a previous post](https://www.collinberke.com/til/posts/2023-10-22-correlations-with-corrr/), I explored correlations using similar data. Since this post aims to be a tutorial, I'm simply making note of it here, and I will not address further. Indeed, it is considered best practice to perform exploratory analysis only on the test set rather than all the data when fitting predictive models.\n:::\n\n\n\n\n\n\n\nWe'll start by importing our data using `readr::read_csv()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball <-\n  read_csv(\n    glue(\n      here(\"blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/\"),\n      \"2023-12-04_bigten_vb_data.csv\"\n    )\n  ) |>\n  select(-ms)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1613 Columns: 28\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (3): team_name, opponent, w_l\ndbl  (24): set_wins, set_loss, s, kills, errors, total_attacks, hit_pct, assists, aces, serr, di...\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n# Data background\n\nThe data represents up to date match-by-match offensive and defensive statistics for NCAA Big Ten women's volleyball teams. The data includes matches from the 2021, 2022, and 2023 seasons, where the earliest recorded match is 2021-01-22. The data is updated up to 2023-11-25. To see the variables within the data, we can run the `dplyr::glimpse()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(data_vball)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 1,613\nColumns: 27\n$ date              <date> 2021-01-22, 2021-01-23, 2021-01-29, 2021-01-30, 2021-02-05, 2021-02-06,…\n$ team_name         <chr> \"Illinois Fighting Illini\", \"Illinois Fighting Illini\", \"Illinois Fighti…\n$ opponent          <chr> \"@ Iowa\", \"@ Iowa\", \"Wisconsin\", \"Wisconsin\", \"@ Penn St.\", \"@ Penn St.\"…\n$ w_l               <chr> \"win\", \"win\", \"loss\", \"loss\", \"loss\", \"loss\", NA, NA, \"loss\", \"loss\", NA…\n$ set_wins          <dbl> 3, 3, 0, 1, 1, 2, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 3, 3, 3, 3, 0, 3, 3, 3, …\n$ set_loss          <dbl> 1, 1, 3, 3, 3, 3, 0, 0, 3, 3, 0, 0, 3, 3, 3, 3, 1, 2, 0, 1, 3, 2, 2, 0, …\n$ s                 <dbl> 4, 4, 3, 4, 4, 5, 0, 0, 4, 5, 0, 0, 3, 3, 5, 3, 4, 5, 3, 4, 3, 5, 5, 3, …\n$ kills             <dbl> 55, 49, 36, 48, 49, 55, 0, 0, 47, 59, 0, 0, 36, 41, 71, 32, 51, 63, 41, …\n$ errors            <dbl> 28, 21, 19, 22, 19, 31, 0, 0, 29, 29, 0, 0, 19, 25, 33, 29, 34, 30, 14, …\n$ total_attacks     <dbl> 139, 122, 101, 129, 112, 167, 0, 0, 137, 178, 0, 0, 111, 118, 185, 124, …\n$ hit_pct           <dbl> 0.194, 0.230, 0.168, 0.202, 0.268, 0.144, 0.000, 0.000, 0.131, 0.169, 0.…\n$ assists           <dbl> 48, 44, 27, 45, 47, 54, 0, 0, 43, 53, 0, 0, 31, 36, 64, 31, 44, 53, 35, …\n$ aces              <dbl> 14, 10, 3, 7, 5, 9, 0, 0, 6, 11, 0, 0, 5, 6, 5, 7, 12, 5, 7, 8, 2, 2, 12…\n$ serr              <dbl> 10, 11, 9, 10, 11, 14, 0, 0, 10, 6, 0, 0, 10, 11, 6, 10, 12, 14, 8, 10, …\n$ digs              <dbl> 61, 46, 34, 47, 48, 57, 0, 0, 53, 76, 0, 0, 35, 39, 90, 45, 67, 60, 33, …\n$ block_solos       <dbl> 1, 2, 1, 1, 0, 3, 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1, 1, 2, 0, 0, 3, 3, …\n$ block_assists     <dbl> 25, 22, 4, 12, 14, 16, 0, 0, 12, 16, 0, 0, 12, 8, 22, 6, 10, 14, 12, 27,…\n$ opp_kills         <dbl> 36, 37, 48, 55, 52, 63, 56, 63, 44, 39, 57, 33, 32, 52, 31, 44, 43, 63, …\n$ opp_errors        <dbl> 25, 24, 9, 14, 14, 21, 15, 18, 15, 14, 18, 12, 24, 21, 20, 29, 14, 25, 0…\n$ opp_total_attacks <dbl> 131, 117, 105, 123, 124, 148, 141, 168, 103, 102, 179, 97, 135, 139, 93,…\n$ opp_hit_pct       <dbl> 0.084, 0.111, 0.371, 0.333, 0.306, 0.284, 0.291, 0.268, 0.282, 0.245, 0.…\n$ opp_assists       <dbl> 30, 36, 42, 46, 48, 56, 48, 52, 38, 35, 52, 30, 30, 51, 29, 34, 38, 57, …\n$ opp_aces          <dbl> 6, 1, 4, 5, 5, 9, 2, 2, 2, 4, 4, 2, 4, 8, 1, 1, 6, 6, 0, 0, 0, 0, 3, 2, …\n$ opp_serr          <dbl> 2, 8, 9, 11, 15, 10, 12, 10, 3, 8, 6, 5, 9, 11, 3, 8, 6, 7, 0, 0, 0, 0, …\n$ opp_digs          <dbl> 45, 43, 39, 47, 42, 67, 49, 77, 47, 39, 72, 49, 55, 64, 30, 48, 56, 80, …\n$ opp_block_solos   <dbl> 2, 1, 2, 1, 2, 3, 4, 1, 2, 1, 2, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 4, 3, …\n$ opp_block_assists <dbl> 18, 20, 12, 14, 12, 20, 14, 24, 12, 12, 24, 28, 29, 16, 4, 14, 24, 28, 0…\n```\n\n\n:::\n:::\n\n\n\nMost of the variable names should be informative, aside from `s`. This variable represents the total number of sets played within a match. If you're unfamiliar with the sport, you might find the following [list](https://en.wikipedia.org/wiki/Volleyball_jargon) helpful.\n\n# Wrangle our data\n\nBefore fitting any models, a few data wrangling steps will need to take place. First, `NA` values need to be removed. `NA`s are present for two reasons:\n\n1. Cancelled matches due to the COVID pandemic in 2021.\n2. The pre-wrangled data contains matches that have not taken place in the 2023 season.\n\nSecond, the `w_l` column (i.e., wins and losses) needs to be mutated into a factor with two levels (win and loss). It's generally best practice to mutate categorical variables into factors when using the tidymodels framework.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball <- data_vball |>\n  drop_na(w_l) |>\n  mutate(w_l = factor(w_l, levels = c(\"loss\", \"win\")))\n```\n:::\n\n\n\n# Check for any class imbalances\n\nLet's check for the presence of any class imbalances. Take note, this is the only exploratory analysis we'll do before splitting our data into a training and testing set. Additional exploratory analysis will be done on the training set. This is done to prevent [data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)) between the different modeling steps.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data_vball, aes(x = w_l)) +\n  geom_bar(fill = \"#191970\", alpha = 0.8) +\n  theme_minimal() +\n  labs(x = \"\") +\n  theme(axis.text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/vis-class-imbalances-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe bar chart indicates the presence of a slight imbalance between wins and losses (win = 877; loss = 678). During the initial split of our data, we'll employ a stratified sampling technique to account for this imbalance. This is done to ensure our splits avoid major class imbalances within the training and testing sets. \n\n# Determine the data budget \n\nAt this point, we need to split our data into testing and training data sets. [`rsample`'s](https://rsample.tidymodels.org/index.html), a tidymodels' package, `initial_split()` function makes this easy. While using this function, we'll pass along our data object, `data_vball`, and values to two arguments: `prop` and `strata`. \n\nThe `prop` argument specifies the proportion of data to be retained in the training and testing data sets. It defaults to `3/4` or 75%. For our specific modeling case, we'll specify we want to devote 80% of our data to the training set, with the remaining 20% going to the testing set. Given the data contains 1,555 rows, 1,243 rows will be allocated to the training set while the remaining 312 rows will be held out for the test set. Since class imbalances were identified, a stratified sampling technique will be used. To do this, we pass the name of the class variable to the `strata` argument. For reproducibility, `set.seed(2023)` is set before the split is specified.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\ndata_vball_split <- initial_split(data_vball, prop = .80, strata = w_l)\n```\n:::\n\n\n\nLet's take a quick look the `data_vball_split` object.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<1243/312/1555>\n```\n\n\n:::\n:::\n\n\n\nYou'll notice this doesn't print out any data. Rather, information about the split is printed to the console. `initial_split()` only creates an object with the information on how the split is to be performed. To perform the split, we need to use `rsample`'s `training()` and `testing()` functions. Simply, we do the following:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball_train <- training(data_vball_split)\ndata_vball_test <- testing(data_vball_split)\n```\n:::\n\n\n\nWhen these two objects are printed, you'll see two tibbles. The first tibble is the training data set. The second tibble is the testing data set. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball_train\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,243 × 27\n   date       team_name    opponent w_l   set_wins set_loss     s kills errors total_attacks hit_pct\n   <date>     <chr>        <chr>    <fct>    <dbl>    <dbl> <dbl> <dbl>  <dbl>         <dbl>   <dbl>\n 1 2021-01-29 Illinois Fi… Wiscons… loss         0        3     3    36     19           101   0.168\n 2 2021-01-30 Illinois Fi… Wiscons… loss         1        3     4    48     22           129   0.202\n 3 2021-02-06 Illinois Fi… @ Penn … loss         2        3     5    55     31           167   0.144\n 4 2021-02-19 Illinois Fi… Ohio St. loss         1        3     4    47     29           137   0.131\n 5 2021-02-20 Illinois Fi… Ohio St. loss         2        3     5    59     29           178   0.169\n 6 2021-03-05 Illinois Fi… Nebraska loss         0        3     3    36     19           111   0.153\n 7 2021-03-06 Illinois Fi… Nebraska loss         0        3     3    41     25           118   0.136\n 8 2021-03-12 Illinois Fi… @ Minne… loss         2        3     5    71     33           185   0.205\n 9 2021-03-13 Illinois Fi… @ Minne… loss         0        3     3    32     29           124   0.024\n10 2021-04-02 Illinois Fi… @ Purdue loss         0        3     3    33     22           127   0.087\n# ℹ 1,233 more rows\n# ℹ 16 more variables: assists <dbl>, aces <dbl>, serr <dbl>, digs <dbl>, block_solos <dbl>,\n#   block_assists <dbl>, opp_kills <dbl>, opp_errors <dbl>, opp_total_attacks <dbl>,\n#   opp_hit_pct <dbl>, opp_assists <dbl>, opp_aces <dbl>, opp_serr <dbl>, opp_digs <dbl>,\n#   opp_block_solos <dbl>, opp_block_assists <dbl>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 312 × 27\n   date       team_name    opponent w_l   set_wins set_loss     s kills errors total_attacks hit_pct\n   <date>     <chr>        <chr>    <fct>    <dbl>    <dbl> <dbl> <dbl>  <dbl>         <dbl>   <dbl>\n 1 2021-01-23 Illinois Fi… @ Iowa   win          3        1     4    49     21           122   0.23 \n 2 2021-02-05 Illinois Fi… @ Penn … loss         1        3     4    49     19           112   0.268\n 3 2021-03-19 Illinois Fi… Maryland win          3        1     4    51     34           162   0.105\n 4 2021-08-27 Illinois Fi… UC Sant… win          3        2     5    63     33           178   0.169\n 5 2021-09-10 Illinois Fi… SMU @ O… win          3        0     3    41     11            98   0.306\n 6 2021-09-17 Illinois Fi… @ Illin… win          3        0     3    43     10            88   0.375\n 7 2021-09-18 Illinois Fi… North T… win          3        0     3    40     17           102   0.225\n 8 2021-09-25 Illinois Fi… Northwe… win          3        2     5    65     19           137   0.336\n 9 2021-10-03 Illinois Fi… Wiscons… loss         1        3     4    48     20           142   0.197\n10 2021-10-23 Illinois Fi… Michigan win          3        1     4    55     21           155   0.219\n# ℹ 302 more rows\n# ℹ 16 more variables: assists <dbl>, aces <dbl>, serr <dbl>, digs <dbl>, block_solos <dbl>,\n#   block_assists <dbl>, opp_kills <dbl>, opp_errors <dbl>, opp_total_attacks <dbl>,\n#   opp_hit_pct <dbl>, opp_assists <dbl>, opp_aces <dbl>, opp_serr <dbl>, opp_digs <dbl>,\n#   opp_block_solos <dbl>, opp_block_assists <dbl>\n```\n\n\n:::\n:::\n\n\n\nEach of these data sets will be used at specific points during the modeling procedure. The training will be used for model specification and assessment. The testing will be used to assess the final model fit. But first, let's do some exploratory data analysis on our training data.\n\n# Perform exploratory data analysis\n\n## Feature exploration\n\nGiven the number of features in the data, we can easily obtain summary information using [`skimr::skim()`](https://docs.ropensci.org/skimr/index.html).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(data_vball_train)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |                 |\n|:------------------------|:----------------|\n|Name                     |data_vball_train |\n|Number of rows           |1243             |\n|Number of columns        |27               |\n|_______________________  |                 |\n|Column type frequency:   |                 |\n|character                |2                |\n|Date                     |1                |\n|factor                   |1                |\n|numeric                  |23               |\n|________________________ |                 |\n|Group variables          |None             |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|team_name     |         0|             1|  13|  25|     0|       14|          0|\n|opponent      |         0|             1|   3|  40|     0|      329|          0|\n\n\n**Variable type: Date**\n\n|skim_variable | n_missing| complete_rate|min        |max        |median     | n_unique|\n|:-------------|---------:|-------------:|:----------|:----------|:----------|--------:|\n|date          |         0|             1|2021-01-22 |2023-11-25 |2022-09-09 |      228|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts         |\n|:-------------|---------:|-------------:|:-------|--------:|:------------------|\n|w_l           |         0|             1|FALSE   |        2|win: 701, los: 542 |\n\n\n**Variable type: numeric**\n\n|skim_variable     | n_missing| complete_rate|   mean|    sd|    p0|    p25|    p50|    p75|   p100|hist  |\n|:-----------------|---------:|-------------:|------:|-----:|-----:|------:|------:|------:|------:|:-----|\n|set_wins          |         0|             1|   2.03|  1.22|  0.00|   1.00|   3.00|   3.00|   3.00|▃▂▁▂▇ |\n|set_loss          |         0|             1|   1.65|  1.31|  0.00|   0.00|   2.00|   3.00|   3.00|▆▃▁▂▇ |\n|s                 |         0|             1|   3.68|  0.76|  3.00|   3.00|   4.00|   4.00|   5.00|▇▁▅▁▃ |\n|kills             |         0|             1|  47.02| 11.49| 15.00|  39.00|  46.00|  55.00|  84.00|▁▆▇▅▁ |\n|errors            |         0|             1|  18.86|  6.57|  3.00|  14.00|  19.00|  23.00|  44.00|▃▇▇▂▁ |\n|total_attacks     |         0|             1| 125.31| 30.19| 63.00| 101.00| 121.00| 148.00| 237.00|▃▇▆▂▁ |\n|hit_pct           |         0|             1|   0.23|  0.09| -0.10|   0.17|   0.23|   0.30|   0.54|▁▃▇▅▁ |\n|assists           |         0|             1|  43.23| 10.84| 15.00|  36.00|  43.00|  50.50|  75.00|▂▆▇▃▁ |\n|aces              |         0|             1|   5.18|  2.86|  0.00|   3.00|   5.00|   7.00|  18.00|▅▇▃▁▁ |\n|serr              |         0|             1|   7.97|  3.41|  1.00|   5.00|   8.00|  10.00|  23.00|▅▇▅▁▁ |\n|digs              |         0|             1|  51.55| 15.14| 16.00|  40.00|  49.00|  61.00| 108.00|▂▇▅▂▁ |\n|block_solos       |         0|             1|   1.64|  1.62|  0.00|   1.00|   1.00|   2.00|  12.00|▇▂▁▁▁ |\n|block_assists     |         0|             1|  14.25|  6.78|  0.00|  10.00|  14.00|  18.00|  38.00|▂▇▆▂▁ |\n|opp_kills         |         0|             1|  43.64| 15.01|  0.00|  34.00|  44.00|  54.00|  84.00|▁▃▇▆▁ |\n|opp_errors        |         0|             1|  19.34|  7.05|  0.00|  15.00|  20.00|  24.00|  46.00|▁▆▇▂▁ |\n|opp_total_attacks |         0|             1| 121.76| 36.90|  0.00| 100.00| 119.00| 148.00| 237.00|▁▂▇▅▁ |\n|opp_hit_pct       |         0|             1|   0.19|  0.10| -0.14|   0.13|   0.20|   0.25|   0.50|▁▃▇▅▁ |\n|opp_assists       |         0|             1|  40.20| 13.99|  0.00|  31.00|  41.00|  50.00|  75.00|▁▃▇▆▁ |\n|opp_aces          |         0|             1|   4.56|  2.91|  0.00|   2.00|   4.00|   6.00|  14.00|▆▇▆▂▁ |\n|opp_serr          |         0|             1|   7.58|  3.66|  0.00|   5.00|   7.00|  10.00|  23.00|▃▇▃▁▁ |\n|opp_digs          |         0|             1|  48.99| 17.90|  0.00|  38.00|  48.00|  60.00| 108.00|▁▆▇▃▁ |\n|opp_block_solos   |         0|             1|   1.43|  1.44|  0.00|   0.00|   1.00|   2.00|  12.00|▇▂▁▁▁ |\n|opp_block_assists |         0|             1|  12.71|  7.30|  0.00|   8.00|  12.00|  18.00|  40.00|▆▇▃▁▁ |\n\n\n:::\n:::\n\n\n\nA few things to note from the initial exploratory data analysis:\n\n* Team errors, attacks, and digs distribution exhibits a slight right skew.\n* Aces, service errors, block solos, opponent aces, opponent errors, opponent block solos, and opponent block assists exhibit a greater degree of skewness to the right.\n\nAn argument could be made for further exploratory analysis of these variables, followed by some feature engineering. Although this additional work may improve our final predictive model, this post is a general overview of specifying, fitting, and assessing models using the tidymodels framework. I will thus not address these topics further. However, I intend to write a future post focusing on feature engineering using tidymodels' [`recipes` package](https://recipes.tidymodels.org/). \n\n## Examine correlations among features\n\nThe next step in the exploratory analysis is to identify the presence of any correlations among features. This can [easily be done](https://www.collinberke.com/til/posts/2023-10-22-correlations-with-corrr/) using functions from the [`corrr`](https://corrr.tidymodels.org/index.html) package. Specifically, the `correlate()` function calculates correlations among the various numeric features within our data. The output from the `correlate()` function is then passed to the `autoplot()` method, which outputs a visualization of the correlations values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball_train |>\n  correlate() |>\n  corrr::focus(-set_wins, -set_loss, -s, mirror = TRUE) |>\n  autoplot(triangular = \"lower\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNon-numeric variables removed from input: `date`, `team_name`, `opponent`, and `w_l`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the corrr package.\n  Please report the issue at <https://github.com/tidymodels/corrr/issues>.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/vis-var-correlations-1.png){width=672}\n:::\n:::\n\n\n\nThe plot indicates correlations of varying degrees among features. Feature engineering and feature reduction approaches could be used to address these correlations. However, these approaches will not be explored in this post.\n\n## Specify our models\n\nTo keep things simple, I'll explore the fit of two models to the training data. However, tidymodels has interfaces to fit a wide-range of [models](https://www.tidymodels.org/find/parsnip/), many of which are implemented via the `parsnip` package.\n\nThe models I intend to fit to our data include:\n\n1. A logistic regression using `glm`. \n2. A decision tree using `rpart`. \n\nWhen specifying a model with tidymodels, we do [three things](https://www.tmwr.org/models):\n\n1. Use [`parsnip`](https://parsnip.tidymodels.org/) functions to specify the mathematical structure of the model we intend to use (e.g., `logistic_reg()`; `decision_tree()`). \n\n2. Specify the engine we want to use to fit our model. This is done using the `set_engine()` function.\n\n3. When required, we declare the mode of the model (i.e., is it regression or classification). Some models can perform both, so we need to explicitly set the mode with the `set_mode()` function.\n\nSpecifying the two models in this post looks like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Logistic regression specification\nlog_reg_spec <-\n  logistic_reg() |>\n  set_engine(\"glm\")\n\n# Decision tree specification\ndt_spec <-\n  decision_tree() |>\n  set_engine(\"rpart\") |>\n  set_mode(\"classification\")\n```\n:::\n\n\n\nLet's take a moment to breakdown what's going on here. The calls to `logistic_regression()` and `decision_tree()` establishes the mathematical structure we want to use to fit our model to the data. `set_engine(\"glm\")` and `set_engine(\"rpart\")` specifies the model's engine, i.e., the software we want to use to fit our model. For our decision tree, since it can perform both regression and classification, we specify it's mode using `set_mode(\"classification\")`. You'll notice our logistic regression specification excludes this function. This is because logistic regression is [only used to perform classification](https://parsnip.tidymodels.org/reference/details_logistic_reg_glm.html#details), thus we don't need to set its mode.\n\nIf you're curious or want more information on what `parsnip` is doing in the background, you can pipe the model specification object to the `translate()` function. Here's what the output looks like for our decision tree specification:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndt_spec |> translate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\nModel fit template:\nrpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n```\n\n\n:::\n:::\n\n\n\nIf you're interested in viewing the types of engines available for your model, you can use `parsnip`'s `show_engines()` function. Here you'll need to pass a string character of the model function you want to explore as an argument. This is what this looks like for `logistic_reg()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_engines(\"logistic_reg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 2\n  engine    mode          \n  <chr>     <chr>         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification\n```\n\n\n:::\n:::\n\n\n\n## Create workflows\n\nFrom here, we'll create workflow objects using tidymodel's [`workflow`](https://workflows.tidymodels.org/) package. Workflow objects make it easier to work with different modeling objects by combining  objects into one object. Although this isn't too important for our current modeling task, the use of workflows will be beneficial later when we attempt to improve upon our models, like I'll do in future posts. \n\nIn this case, our model specification and model formula are combined into a workflow object. Here I just choose a few features to include within the model. For this post, I mainly focused on using team oriented features within our model to predict wins and losses. Indeed, others could have been included, as the data also contained opponent oriented statistics. To keep things simple, however, I chose to only include the following features within our model:\n\n* Hitting percentage\n* Errors\n* Block solos\n* Block assists\n* Digs\n\nThe `workflow()` function sets up the beginning of our workflow object. We'll add the model object with `add_model()`, followed by the formula object using `add_formula()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_reg_wflow <-\n  workflow() |>\n  add_model(log_reg_spec) |>\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n\ndt_wflow <-\n  workflow() |>\n  add_model(dt_spec) |>\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n```\n:::\n\n\n\nThis syntax can be a bit long, so there's a shortcut. We can pass both the model formula and the model specification as arguments to the `workflow()` function instead of using a piped chain of functions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_reg_wflow <-\n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    log_reg_spec\n  )\n\ndt_wflow <-\n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    dt_spec\n  )\n```\n:::\n\n\n\n## Fit our models\n\nNow with our models specified, we can go about fitting our model to the training data using the `fit()` method. We do the following to fit both models to the training data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_reg_fit <- log_reg_wflow |> fit(data = data_vball_train)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in y_levels$lvl: partial match of 'lvl' to 'lvls'\n```\n\n\n:::\n\n```{.r .cell-code}\ndt_fit <- dt_wflow |> fit(data = data_vball_train)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in y_levels$lvl: partial match of 'lvl' to 'lvls'\n```\n\n\n:::\n:::\n\n\n\nLet's take a look at the `log_reg_fit` and `dt_fit` fit objects.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_reg_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────────────────────────\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n── Model ───────────────────────────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:\t    1703 \nResidual Deviance: 874.4 \tAIC: 886.4\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndt_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────────────────────────\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n── Model ───────────────────────────────────────────────────────────────────────────────────────────\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct< 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists< 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists>=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct< 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct>=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs< 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs>=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct>=0.2255 642  81 win (0.1261682 0.8738318) *\n```\n\n\n:::\n:::\n\n\n\nWhen the fit objects are called, tidymodels prints information about our fitted models to the console. First, we get notified this object is a trained workflow. Second, preprocessing information is included. Since we only set a model function during preprocessing, we only see the model formula printed in this section. Lastly, tidymodels outputs model specific information and summary information about the model fit.\n\n### Explore the fit\n\nNow that we have the fit object, we can obtain more information about the fit using the `extract_fit_engine()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_reg_fit |> extract_fit_engine()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:\t    1703 \nResidual Deviance: 874.4 \tAIC: 886.4\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndt_fit |> extract_fit_engine()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct< 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists< 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists>=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct< 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct>=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs< 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs>=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct>=0.2255 642  81 win (0.1261682 0.8738318) *\n```\n\n\n:::\n:::\n\n\n\nThe output when passing the fit object to the `extract_fit_engine()` is similar to what was printed when we called the fit object alone. However, the `extract_*` family of workflow functions are great for extracting elements of a workflow. According to the docs (`?extract_fit_engine`), this family of functions are helpful when accessing elements within the fit object. This is especially helpful when needing to pass along elements of the fit object to generics like `print()`, `summary()`, and `plot()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Not evaluated to conserve space, but I encourage\n# you to run it on your own\nlog_reg_fit |> extract_fit_engine() |> plot()\n```\n:::\n\n\n\nAlthough `extract_*` functions afford convenience, the docs warn to avoid situations where you invoke a `predict()` method on the extracted object. Specifically, the docs state:\n\n> There may be preprocessing operations that `workflows` has executed on the data prior to giving it to the model. Bypassing these can lead to errors or silently generating incorrect predictions.\n\nIn other words,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# BAD, NO NO\nlog_reg_fit |> extract_fit_engine() |> predict(new_data)\n\n# Good\nlog_reg_fit |> predict(new_data)\n```\n:::\n\n\n\nThe fit object can also be passed to other generics, like `broom::tidy()`. The general `tidy()` method, when passed a fit object, is useful to view and use the coefficients table from the logistic regression model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(log_reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  term          estimate std.error statistic  p.value\n  <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    -8.55     0.769     -11.1   1.10e-28\n2 hit_pct        30.1      2.09       14.4   5.50e-47\n3 errors         -0.0128   0.0212     -0.603 5.46e- 1\n4 block_solos     0.200    0.0515      3.89  1.02e- 4\n5 block_assists   0.0876   0.0137      6.39  1.64e-10\n6 digs            0.0137   0.00703     1.96  5.05e- 2\n```\n\n\n:::\n:::\n\n\n\nBeyond summarizing the model with the coefficients table, we can also create some plots from the model's predictions from the training data. Here we need to use the `augment()` function. Later, we'll explore this function in more depth when we calculate assessment metrics. For now, I'm using it to obtain the prediction estimates for winning. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_vball_aug <- augment(log_reg_fit, data_vball_train)\n```\n:::\n\n\n\nWith this data, we can visualize these prediction estimates with the various features used within the model. Since we're creating several visualizations using similar code, I created a `plot_log_mdl()` function to simplify the plotting. Lastly, I used the [`patchwork` package](https://patchwork.data-imaginist.com/) to combine the plots into one visualization. Below is the code to create these visualizations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_log_mdl <- function(data, x_var, y_var, color) {\n  ggplot() +\n    geom_point(\n      data = data_vball_aug,\n      aes(x = {{ x_var }}, y = {{ y_var }}, color = {{ color }}),\n      alpha = .4\n    ) +\n    geom_smooth(\n      data = data_vball_aug,\n      aes(x = {{ x_var }}, y = {{ y_var }}),\n      method = \"glm\",\n      method.args = list(family = \"binomial\"),\n      se = FALSE\n    ) +\n    labs(color = \"\") +\n    theme_minimal()\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_hit_pct <-\n  plot_log_mdl(data_vball_aug, hit_pct, .pred_win, w_l)\n\nplot_errors <-\n  plot_log_mdl(data_vball_aug, errors, .pred_win, w_l)\n\nplot_block_solos <-\n  plot_log_mdl(data_vball_aug, block_solos, .pred_win, w_l) +\n  scale_x_continuous(labels = label_number(accuracy = 1))\n\nplot_block_assists <-\n  plot_log_mdl(data_vball_aug, block_assists, .pred_win, w_l)\n\nplot_digs <-\n  plot_log_mdl(data_vball_aug, digs, .pred_win, w_l)\n\nwrap_plots(\n  plot_hit_pct,\n  plot_errors,\n  plot_block_solos,\n  plot_block_assists,\n  plot_digs,\n  guides = \"collect\"\n) &\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-mdls-1.png){width=768}\n:::\n:::\n\n\n\nTo summarise our decision tree, we need to use the [`rpart.plot`](https://cran.r-project.org/web/packages/rpart.plot/) package to create a plot of the tree. The code to do this looks like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndt_fit |>\n  extract_fit_engine() |>\n  rpart.plot(roundint = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot-decision-tree-1.png){width=672}\n:::\n:::\n\n\n\nBefore transitioning to model assessment, let's explore the predictions for both models using the `augment()` function again. According to the docs,\n\n> Augment accepts a model object and a dataset and adds information about each observation in the dataset.\n\n`augment()` produces new columns from the original data set to which makes it easy to examine model predictions. For instance, we can create a data set with the `.pred_class`, `.pred_win`, and `.pred_loss` columns. `augment()` also makes a guarantee that a tibble with the same number of rows as the passed data set will be returned, and all new column names will be prefixed with a `.`. \n\nHere we'll pipe the tibble returned from `augment()` to the `relocate()` function. This will make it easier to view the variables we are interested in further examining by moving these columns to the left of the tibble.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,243 × 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   <fct> <fct>           <dbl>      <dbl> <date>     <chr>          <chr>       <dbl>    <dbl> <dbl>\n 1 loss  loss          0.0616       0.938 2021-01-29 Illinois Figh… Wiscons…        0        3     3\n 2 loss  loss          0.297        0.703 2021-01-30 Illinois Figh… Wiscons…        1        3     4\n 3 loss  loss          0.138        0.862 2021-02-06 Illinois Figh… @ Penn …        2        3     5\n 4 loss  loss          0.0572       0.943 2021-02-19 Illinois Figh… Ohio St.        1        3     4\n 5 loss  loss          0.199        0.801 2021-02-20 Illinois Figh… Ohio St.        2        3     5\n 6 loss  loss          0.0787       0.921 2021-03-05 Illinois Figh… Nebraska        0        3     3\n 7 loss  loss          0.0281       0.972 2021-03-06 Illinois Figh… Nebraska        0        3     3\n 8 loss  win           0.636        0.364 2021-03-12 Illinois Figh… @ Minne…        2        3     5\n 9 loss  loss          0.00129      0.999 2021-03-13 Illinois Figh… @ Minne…        0        3     3\n10 loss  loss          0.0114       0.989 2021-04-02 Illinois Figh… @ Purdue        0        3     3\n# ℹ 1,233 more rows\n# ℹ 20 more variables: kills <dbl>, errors <dbl>, total_attacks <dbl>, hit_pct <dbl>,\n#   assists <dbl>, aces <dbl>, serr <dbl>, digs <dbl>, block_solos <dbl>, block_assists <dbl>,\n#   opp_kills <dbl>, opp_errors <dbl>, opp_total_attacks <dbl>, opp_hit_pct <dbl>,\n#   opp_assists <dbl>, opp_aces <dbl>, opp_serr <dbl>, opp_digs <dbl>, opp_block_solos <dbl>,\n#   opp_block_assists <dbl>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, data_vball_train) |>\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,243 × 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   <fct> <fct>           <dbl>      <dbl> <date>     <chr>          <chr>       <dbl>    <dbl> <dbl>\n 1 loss  loss            0.131      0.869 2021-01-29 Illinois Figh… Wiscons…        0        3     3\n 2 loss  loss            0.131      0.869 2021-01-30 Illinois Figh… Wiscons…        1        3     4\n 3 loss  loss            0.131      0.869 2021-02-06 Illinois Figh… @ Penn …        2        3     5\n 4 loss  loss            0.131      0.869 2021-02-19 Illinois Figh… Ohio St.        1        3     4\n 5 loss  loss            0.131      0.869 2021-02-20 Illinois Figh… Ohio St.        2        3     5\n 6 loss  loss            0.131      0.869 2021-03-05 Illinois Figh… Nebraska        0        3     3\n 7 loss  loss            0.131      0.869 2021-03-06 Illinois Figh… Nebraska        0        3     3\n 8 loss  win             0.603      0.397 2021-03-12 Illinois Figh… @ Minne…        2        3     5\n 9 loss  loss            0.131      0.869 2021-03-13 Illinois Figh… @ Minne…        0        3     3\n10 loss  loss            0.131      0.869 2021-04-02 Illinois Figh… @ Purdue        0        3     3\n# ℹ 1,233 more rows\n# ℹ 20 more variables: kills <dbl>, errors <dbl>, total_attacks <dbl>, hit_pct <dbl>,\n#   assists <dbl>, aces <dbl>, serr <dbl>, digs <dbl>, block_solos <dbl>, block_assists <dbl>,\n#   opp_kills <dbl>, opp_errors <dbl>, opp_total_attacks <dbl>, opp_hit_pct <dbl>,\n#   opp_assists <dbl>, opp_aces <dbl>, opp_serr <dbl>, opp_digs <dbl>, opp_block_solos <dbl>,\n#   opp_block_assists <dbl>\n```\n\n\n:::\n:::\n\n\n\n## Model assessment\n\nSince we're fitting a binary classification model, we will use several measurements to assess model performance. Many of these measurements can be calculated using functions from the [`yardstick`](https://yardstick.tidymodels.org/) package. To start, we can calculate several measurements using the hard class predictions: a confusion matrix; accuracy; specificity; ROC curves; etc.\n\n### Create a confusion matrix\n\nFirst, let's start by creating a confusion matrix. A confusion matrix is simply a cross-tabulation of the observed and predicted classes, and it summarizes how many times the model predicted a class correctly vs. how many times it predicted it incorrectly. [The calculation of the table is pretty straight forward](https://www.youtube.com/watch?v=Kdsp6soqA7o) for a binary-classification model. The `yardstick` package makes it easy to calculate this table with the `conf_mat()` function. \n\n`conf_mat()`'s two main arguments are `truth` and `estimate`. `truth` pertains to the column containing the true class predictions (i.e., what was actually recorded). The `estimate` is the name of the column containing the discrete class prediction (i.e., the prediction made by the model).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  conf_mat(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction loss win\n      loss  438  89\n      win   104 612\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, data_vball_train) |>\n  conf_mat(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction loss win\n      loss  411  64\n      win   131 637\n```\n\n\n:::\n:::\n\n\n\nThe `conf_mat()` also has an `autoplot()` method. This makes it easier to visualize the confusion matrix, either as a `mosaic` plot or a `heatmap`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  conf_mat(truth = w_l, estimate = .pred_class) |>\n  autoplot(type = \"mosaic\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/vis-log-reg-conf-matrix-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, data_vball_train) |>\n  conf_mat(truth = w_l, estimate = .pred_class) |>\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/vis-dt-conf-matrix-1.png){width=672}\n:::\n:::\n\n\n\nA few things to note from the confusion matrices created from our two models:\n\n* The logistic regression does well predicting wins and losses, though it slightly over predicts wins in cases of losses and losses in cases of wins. However, prediction accuracy is pretty balanced.\n\n* The decision tree does better reducing cases where it predicts a loss when a win occurred, but it predicted more wins when a loss took place. Thus, the decision tree model seems fairly optimistic when it comes to predicting wins when a loss occurred.\n\nAfter examining the confusion matrix, we can move forward with calculating some quantitative summary metrics from the results of the confusion matrix, which we can use to better compare the fit between the two models.\n\n### Measure model accuracy\n\nOne way to summarize the confusion matrix is to calculate the proportion of data that is predicted correctly, also known as accuracy. `yardstick`'s `accuracy()` function simplifies this calculation for us. Again, we just pipe our `augment()` function to the `accuracy()` function, and we specify which column is the `truth` and which is the `estimate` class prediction from the model. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  accuracy(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.845\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, data_vball_train) |>\n  accuracy(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.843\n```\n\n\n:::\n:::\n\n\n\nWhen it comes to accuracy, both models are fairly similar in their ability to predict cases correctly. The logistic regression's accuracy is slightly better, though.\n\n### Measure model sensitivity and specificity\n\nSensitivity and specificity are additional assessment metrics we can calculate. Sensitivity in this case is the percentage of matches that were wins that were correctly identified by the model. Specificity is the percentage of matches that were losses that were correctly identified by the model. The @StatQuest YouTube channel has a good [video](https://www.youtube.com/watch?v=vP06aMoz4v8) breaking down how these metrics are calculated.\n\n`yardstick` makes it easy to calculate these metrics with the `sensitivity()` and `specificity()` functions. As we did with calculating accuracy, we pipe the output of the `augment()` function to the `sensitivity()` function. We also specify the column that represents the true values to the `truth` argument, then pass the class predictions made by the model to the `estimate` argument. This looks like the following for both our logistic regression and decision tree models:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  sensitivity(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 sensitivity binary         0.808\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  specificity(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 specificity binary         0.873\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, data_vball_train) |>\n  sensitivity(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 sensitivity binary         0.758\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, data_vball_train) |>\n  specificity(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 specificity binary         0.909\n```\n\n\n:::\n:::\n\n\n\nA few things to note:\n\n* The logistic regression (sensitivity = 80.8%) was much better at predicting matches that were wins than the decision tree model (sensitivity = 75.8%).\n\n* The decision tree was much better at identifying losses, though (90.9% vs. 87.3%).\n\n#### Simplify metric calculations with `metric_set()`\n\nAlthough the above code provided the output we were looking for, we can simplify our code by using `yardstick`'s `metric_set()` function. Inside `metric_set()` we specify the different metrics we want to calculate for each model. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvball_mdl_metrics <-\n  metric_set(accuracy, sensitivity, specificity)\n```\n:::\n\n\n\nThen we do as before, pipe the output from `augment()` to our metric set object `vball_mdl_metrics`, and specify the column that represents the truth and the column that represents the model's class prediction. Here's what this looks like for both our models:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.845\n2 sensitivity binary         0.808\n3 specificity binary         0.873\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, data_vball_train) |>\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.843\n2 sensitivity binary         0.758\n3 specificity binary         0.909\n```\n\n\n:::\n:::\n\n\n\nNow it's much easier to make comparisons, and we write less code for the same amount of information. A big win!\n\n### ROC curves and AUC estimates\n\n[Receiver operating characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves visually summarise classification model specificity and sensitivity using different threshold values. From this curve, an area under the curve (AUC) metric can be calculated. The AUC is a useful summary metric and can be used to compare the fit of two or more models. Again, @StatQuest has a pretty good [video](https://www.youtube.com/watch?v=4jRBRDbJemM) explaining the fundamentals of ROC curves and AUC estimates.\n\nBeing a useful way to summarise model performance, the `yardstick` package makes several functions available to calculate both the ROC curve and AUC metric. An `autoplot()` method is also available to easily plot the ROC curve for us. \n\nLet's take a look at how this is done with our logistic regression model. Here's the code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, new_data = data_vball_train) |>\n  roc_curve(truth = w_l, .pred_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,245 × 3\n     .threshold specificity sensitivity\n          <dbl>       <dbl>       <dbl>\n 1 -Inf             0                 1\n 2    0.0000835     0                 1\n 3    0.000181      0.00143           1\n 4    0.000405      0.00285           1\n 5    0.000415      0.00428           1\n 6    0.000496      0.00571           1\n 7    0.000824      0.00713           1\n 8    0.000839      0.00856           1\n 9    0.000922      0.00999           1\n10    0.000979      0.0114            1\n# ℹ 1,235 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, new_data = data_vball_train) |>\n  roc_auc(truth = w_l, .pred_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.920\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, new_data = data_vball_train) |>\n  roc_curve(truth = w_l, .pred_loss) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/vis-roc-log-reg-1.png){width=672}\n:::\n:::\n\n\n\nYou'll likely notice the syntax is pretty intuitive. You'll also notice the code is similar to our other model performance metric calculations. First we use `augment()` to create the data we need. Second, we pipe the output of the `augment()` function to either the `roc_curve()` or `roc_auc()` function. The `roc_curve()` function calculates the ROC curve values and returns a tibble, which we will later pipe to the `autoplot()` method. The `roc_auc()` function calculates the area under the curve metric.\n\nSince we're comparing two models, we perform these steps again for the decision tree model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, new_data = data_vball_train) |>\n  roc_curve(truth = w_l, .pred_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 3\n  .threshold specificity sensitivity\n       <dbl>       <dbl>       <dbl>\n1   -Inf           0          1     \n2      0.126       0          1     \n3      0.397       0.800      0.851 \n4      0.714       0.909      0.758 \n5      0.869       0.914      0.740 \n6      0.875       0.994      0.0517\n7    Inf           1          0     \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, new_data = data_vball_train) |>\n  roc_auc(truth = w_l, .pred_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.864\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(dt_fit, new_data = data_vball_train) |>\n  roc_curve(truth = w_l, .pred_loss) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/vis-roc-dt-1.png){width=672}\n:::\n:::\n\n\n\nA few notes from comparing the ROC curve and AUC metrics:\n\n* The AUC indicates a better model fit across different thresholds for the logistic regression model (AUC = .920) vs. the decision tree (AUC = .864).\n\n* When visually examining the ROC curves for both models, it seems the logistic regression model is a better fitting model for the data.\n\n# Logistic regression it is, then \n\nOur model assessment suggested logistic regression is a good candidate model to make predictions. Thus, the last step is to test our final model fit using the test set.\n\ntidymodels makes this step easy with the `last_fit()` function. This is where our previous step of specifying a workflow comes in handy. In the function, we'll pass along the `log_reg_wflow` workflow object and the `data_vball_split` object. We'll also calculate metrics by passing along `metric_set(accuracy, sensitivity, specificity, roc_auc)` to the `metrics` argument of the function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_log_reg_fit <-\n  last_fit(\n    log_reg_wflow,\n    data_vball_split,\n    metrics = metric_set(accuracy, sensitivity, specificity, roc_auc)\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n→ A | warning: partial match of 'lvl' to 'lvls'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n:::\n\n\n\nWhen you print the `final_log_reg_fit` object, a tibble is returned. The tibble contains list columns holding relevant model information, like our metrics and predictions. Take notice that all these columns are prefixed with a `.`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_log_reg_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics         .notes           .predictions .workflow \n  <list>             <chr>            <list>           <list>           <list>       <list>    \n1 <split [1243/312]> train/test split <tibble [4 × 4]> <tibble [1 × 4]> <tibble>     <workflow>\n\nThere were issues with some computations:\n\n  - Warning(s) x1: partial match of 'lvl' to 'lvls'\n\nRun `show_notes(.Last.tune.result)` for more information.\n```\n\n\n:::\n:::\n\n\n\nTo grab the information from these list columns, tidymodels makes several accessor functions available. In the case of obtaining our metrics, we can use the `collect_metrics()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(final_log_reg_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  .metric     .estimator .estimate .config        \n  <chr>       <chr>          <dbl> <chr>          \n1 accuracy    binary         0.869 pre0_mod0_post0\n2 sensitivity binary         0.846 pre0_mod0_post0\n3 specificity binary         0.886 pre0_mod0_post0\n4 roc_auc     binary         0.944 pre0_mod0_post0\n```\n\n\n:::\n:::\n\n\n\nWe can then compare the assessment metrics produced from the fit to the training set to that of the testing set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.845\n2 sensitivity binary         0.808\n3 specificity binary         0.873\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_reg_fit, data_vball_train) |>\n  roc_auc(truth = w_l, .pred_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.920\n```\n\n\n:::\n:::\n\n\n\nA few things of note:\n\n* Most of the model assessment metrics dropped between the training and testing sets. This may indicate some slight overfitting of our model. As such, it may not fully generalize when new data is used to create predictions.\n\n# Have some fun, make predictions\n\n## Extract the final workflow\n\nOnce the final candidate model is identified, we can extract the final workflow using the [`hardhat`](https://hardhat.tidymodels.org) package's `extract_workflow()` function. Here we'll use this workflow object to make predictions, but this workflow object is also useful if you intend to deploy this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit_wflow <- extract_workflow(final_log_reg_fit)\n```\n:::\n\n\n\n## Make predictions\n\nAt this point in the season, let's see how the Nebraska women's volleyball team stacked up in several of their matches using our model. First, let's examine Nebraska's win against Wisconsin, [a five set thriller](https://huskers.com/boxscore/21265).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc_mtch_one <- data_vball |>\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-10-21\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_one)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 loss       \n```\n\n\n:::\n:::\n\n\n\nAccording to our model, Nebraska should have lost this match. This makes Nebraska's win even more impressive. The grittiness to pull out a win, even when evidence suggests they shouldn't have, speaks volumes of this team. Indeed, wins and losses for volleyball matches are a function of many different factors. Factors that may not be fully captured by the data or this specific model.\n\nWhat about Nebraska's 0-3, [second match loss against Wisconsin](https://huskers.com/boxscore/21274)?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc_mtch_two <- data_vball |>\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-24\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_two)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 loss       \n```\n\n\n:::\n:::\n\n\n\nNo surprise, the model predicted Nebraska would lose this match. It's a pretty steep hill to climb when you hit a .243 and only have 5 total blocks.\n\nAnother nail-biter was Nebraska's second [match against Penn State](https://huskers.com/boxscore/21268). Let's take a look at what the model would predict. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenn_state <- data_vball |>\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-03\"))\n\npredict(final_fit_wflow, new_data = penn_state)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 win        \n```\n\n\n:::\n:::\n\n\n\nEven though the match was close, the model predicted Nebraska would win this match. It may have been a nail-biter to watch, but Nebraska played well enough to win the match, according to our model. \n\n## The NCAA tournament and our model\n\nWe're through the initial rounds of the [2023 NCAA women's volleyball tournament](https://www.ncaa.com/news/volleyball-women/article/2023-11-18/2023-ncaa-volleyball-tournament-bracket-how-watch). Let's look at a couple of scenarios for Nebraska using our final model.\n\n::: {.callout-note}\nI'm extrapolating a bit here, since the data I'm using only includes Big Ten volleyball team matches. The NCAA tournament will include teams from many other conferences, so the predictions don't fully generalize to tournament matches.\n\nWe could avert the extrapolation here by obtaining match data for all NCAA volleyball matches for the 2021, 2022, and 2023 seasons. For the sake of keeping this post manageable, I did not obtain this data.\n:::\n\nFirst, let's just say Nebraska plays to up to their regular season average for hit percentage, errors, block solos, block assists, and digs in NCAA tournament matches. What does our model predict in regards to Nebraska winning or losing a match?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseason_avg <- data_vball |>\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |>\n  summarise(across(where(is.numeric), mean)) |>\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = season_avg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 win        \n```\n\n\n:::\n:::\n\n\n\nIf Nebraska can hit at least a .290, commit less than 17 errors, have one solo block, have 16 block assists, and dig the ball roughly 48 times, then according to the model, they should win matches. Put another way, if Nebraska performs close to their regular season average for these statistics, then the model suggests they will win matches. \n\nThis is very encouraging, since the Huskers should be playing their best volleyball here at the end of the season. One would hope this means they perform near or better than their average in tournament matches.\n\nOne last scenario, let's look at the low end of Nebraska's performance this season. Specifically, let's see what the model predicts \nif Nebraska will win or lose a match at the 25% quartile for these statistics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile_25 <- data_vball |>\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |>\n  summarise(across(where(is.numeric), ~ quantile(.x, .25))) |>\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = quantile_25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 win        \n```\n\n\n:::\n:::\n\n\n\nAccording to the model, if Nebraska can perform up to their 25% quartile of their regular season statistics, the model suggests they should win matches. Matches like those in the NCAA tournament. So even if Nebraska doesn't perform to their potential or just has an off match, they should win if they can at least achieve the 25% quartile of their regular season statistics. \n\n> \"All models are wrong, but some are useful.\"\n>\n> \\- [George Box](https://en.wikipedia.org/wiki/All_models_are_wrong)\n\nAgain, many factors determine if a team wins or loses a match in volleyball (see the model's prediction for Nebraska's first match against Wisconsin). This is just one, simple model aimed at predicting wins and losses based on hit percentage, errors, block solos, block assists, and digs. A model that certainly could be improved.\n\n# Ways to improve our predictive model\n\nThis post was a brief overview of fitting predictive models using the tidymodels framework. As such, additional modeling procedures were not performed to improve the predictive performance of the model. This includes feature engineering (I can hear the volleyball fanatics groveling over my lack of per set statistics in the model), hyperparameter tuning, exploration of other models, and the use of ensemble methods. \n\nThe use of these techniques would most likely yield more accurate results from the final candidate model. Additionally, different models, not explored here, may fit the data better. Thus, I intend to write future posts exploring these topics in more depth.\n\n# Wrap-up\n\nThis post aimed to be a high-level overview of specifying predictive models using the tidymodels framework. To do this, two binary classification models predicting wins and losses were fit to Big Ten NCAA women's volleyball data. Subsequent model assessment metrics indicated logistic regression to be a candidate model to predict wins and losses when hit percentage, errors, block solos, block assists, and digs are used as features. Using this model, we estimated predictions for several matches won and lost by the Nebraska women's volleyball team. Finally, we explored some different scenarios for the Nebraska women's volleyball team and whether the model predicted if they would win matches in the NCAA tournament based on their regular season performance.\n\nIf you have any suggestions to improve upon this model or a different approach, let me know. \n\nNow, go have some fun specifying models and making predictions with [`tidymodels`](https://www.tidymodels.org/).\n\n# Resources to learn more \n\n- [Tidy Modeling with R by Max Kuhn and Julia Silge](https://www.tmwr.org/)\n- [tidymodels documentation](https://www.tidymodels.org/)\n- [Machine Learning Fundamentals: Sensitivity and Specificity from the @StatQuest YouTube Channel](https://www.youtube.com/watch?v=vP06aMoz4v8)\n- [ROC and AUC, Clearly Explained! from the @StatQuest YouTube Channel](https://www.youtube.com/watch?v=4jRBRDbJemM)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}