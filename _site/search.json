[
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A now page, why?"
  },
  {
    "objectID": "now.html#projects-im-working-on",
    "href": "now.html#projects-im-working-on",
    "title": "Now",
    "section": "Projects I’m working on",
    "text": "Projects I’m working on\n\nExperimenting more with Tableau and plotly\n\nIf the only tool you have is a hammer, it is tempting to treat everything as if it were a nail.\n- Abraham Maslow\n\nAlthough I prefer to do the majority of my analysis and visualization work using code based tools like R, I’m attempting to broaden my experience by experimenting more with different BI tools. This includes Tableau. I’ve also been experimenting with creating interactive data visualizations with plotly. To develop these skills, I’ve been contributing to the #tidytuesday social data project. Check out my recent blog posts to view my contributions. If you’re interested in what I’ve created using Tableau, check out my public profile.\n\n\nBurning it all down and building it back up, my Neovim setup\nIt was inevitable; I’m revamping my Neovim configuration. It started with a kickstarter, but now I’m looking to further customize. I’m also attempting to address some bugs that have creeped into my configuration. I’ve recently come to the conclusion that I need to burn everything down and start anew. Alas, I’m not weary. It’s another great learning experience.\n\n\nAttended Posit::conf(2023) Chicago\nI was fortunate to attend this year’s Posit conference in Chicago, a five day conference with workshops and speaker sessions. I’m still sorting through all that I learned. Here are some of the highlights:\n\nAttending the Introduction to Tidymodels workshop\nAttending the Package Development Masterclass workshop\nAll the keynote speakers, especially J.D. Long’s It’s Abstractions All the Way Down talk.\nNetworking and meeting some folks from the R4DS Online Learning Community (in person).\n\nIf you’re interested in what I’ve focused on in the past, check out my past updates"
  },
  {
    "objectID": "now.html#books-im-reading",
    "href": "now.html#books-im-reading",
    "title": "Now",
    "section": "Books I’m reading",
    "text": "Books I’m reading\n\nPractical Tableau by Ryan Sleeper\nSince I’m broadening my data visualization skills, I’ve picked up this book to develop my foundational understanding of Tableau. Reading this has also broadened my understanding of general data visualization concepts. In truth, I’ve gained a better understanding by seeing how other tools approach the creation of data visualizations.\n\n\nMachine Learning with R - Fourth Edition by Brett Lantz\nThis is a bit of a stretch read for me. I read this book when I have the time. I’m mostly using it to become more familiar with other machine learning models and techniques available in R. I’m also looking to write some more blog posts utilizing machine learning. Again, when I can find the time.\n\n\nThe Currents of Space (Galactic Empire 2) by Issac Asimov\nContinuing on with the second book in the Galactic Empire series. This book is another precursor to Asimov’s Foundation series, which I’m aiming to read in the next couple of monhts. Just some more Sci-Fi when I have time to read for fun."
  },
  {
    "objectID": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "href": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "title": "Now",
    "section": "A list of books I’ve read (ever since I’ve started keeping track)",
    "text": "A list of books I’ve read (ever since I’ve started keeping track)\n\nProfessional development reads\n\nHBR Guide to Making Every Meeting Matter from the Harvard Business Review\n\n\nThe Checklist Manifesto: How to Get Things Right by Atul Gawande\n\n\nTidy Modeling with R by Max Kuhn and Julia Silge\n\n\nPython for Data Analysis by Wes McKinney\n\n\nEngineering Production-Grade Shiny Apps by Colin Fay, Sébastien Rochette, Vincent Guyader, and Cervan Girard\n\n\nAdvanced R by Hadley Wickham. Check out past book club meeting recordings here.\n\n\nR Packages by Hadley Wickham and Jenny Bryan. Check out past book club meeting recordings here.\n\n\nVim help files maintained by Carlo Teubner\n\n\nMastering Ubuntu by Jay LaCroix\n\n\nGoogle BigQuery: The Definitive Guide by Valliappa Lakshmanan and Jordan Tigani\n\n\nMastering Shiny by Hadley Wickham. Check out the past book club meeting recordings here.\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. Check out the past book club meeting recordings here.\n\n\nDocker Deep Dive by Nigel Poulton\n\n\n\nPersonal reads\n\nThe Stars, Like Dust (Galactic Empire 1) by Issac Asimov\n\n\nRobots and Empire (The Robot Series 4) by Issac Asimov\n\n\nThe Robots of Dawn (The Robot Series 3) by Isaac Asimov\n\n\nThe Naked Sun (The Robot Series Book 2) by Isaac Asimov\n\n\nThe Caves of Steel (The Robot Series Book 1) by Isaac Asimov\n\n\nI, Robot by Isaac Asimov\n\n\nLeviathan Falls (The Expanse book 9) by James S.A. Corey\n\n\nTiamat’s Wrath (The Expanse book 8) by James S.A. Corey\n\n\nPersepolis Rising (The Expanse book 7) by James S.A. Corey\n\n\nBabylon’s Ashes (The Expanse book 6) by James S.A. Corey\n\n\nNemesis Games (The Expanse book 5) by James S.A. Corey\n\n\nCibola Burn (The Expanse book 4) by James S.A. Corey\n\n\nAbaddon’s Gate (The Expanse book 3) by James S.A. Corey\n\n\nCaliban’s War (The Expanse book 2) by James S.A. Corey\n\n\nLeviathon Wakes (The Expanse book 1) by James S.A. Corey\n\n\nThe Galaxy, and the Ground Within: A Novel (Wayfarers 4) by Becky Chambers\n\n\nRecord of a Spaceborn Few (Wayfarers 3) by Becky Chambers\n\n\nA Closed and Common Orbit (Wayfarers 2) by Becky Chambers\n\n\nThe Long Way to a Small, Angry Planet (Wayfarers 1) by Becky Chambers\n\n\nLast of the Breed by Louis L’Amour\n\n\nProject Hail Mary by Andy Weir\n\n\nFirebreak by Nicole Kornher-Stace\n\n\nDune Messiah by Frank Herbert\n\n\nDune by Frank Herbert\n\n\nThe Martian: A Novel by Andy Weir"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "title": "Combine plots using patchwork",
    "section": "",
    "text": "Image generated using the prompt ‘robot stitching a quilt of data visualizations in pop art style’ with the Bing Image Creator\nToday I learned the patchwork package makes it easy to combine multiple plots into a single plot. In this post, I overview what I’ve recently learned from using patchwork’s functions to create plot compositions."
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "title": "Combine plots using patchwork",
    "section": "Show me the money",
    "text": "Show me the money\nLet’s start with a pretty straight forward plot, total revenue over time. Here’s the code I used to wrangle the data and create the plot:\n\ndata_revenue_trend &lt;- data_google_merch |&gt;\n  group_by(event_date, transaction_id) |&gt;\n  summarise(revenue = max(purchase_revenue_in_usd)) |&gt;\n  summarise(revenue = sum(revenue))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_rev_trend &lt;- ggplot(\n  data_revenue_trend, \n  aes(x = event_date, y = revenue)\n) +\n  geom_line(linewidth = 2) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Total Revenue ($USD)\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "title": "Combine plots using patchwork",
    "section": "Items generating the most revenue",
    "text": "Items generating the most revenue\nThe data is from an online store, so we should explore items generating the most revenue. To keep things simple, I use dplyr’s slice_max(10) to create a plot of the Top 10 revenue generating items. Because we’re slicing the data, other items are excluded from the plot. The following code wrangles the data and creates the plot for us.\n\ndata_items_rev &lt;- \n  data_google_merch |&gt;\n  group_by(item_name) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_rev_items &lt;- ggplot(\n  data_items_rev, \n  aes(\n    x = fct_reorder(item_name, revenue),\n    y = revenue)) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "title": "Combine plots using patchwork",
    "section": "Product categories generating most revenue",
    "text": "Product categories generating most revenue\nThe data also categorizes items into more general groupings. As such, we can create a plot ranking product categories by amount of revenue generated. No question, it’s apparel. The code to create the plot is similar to what we did above with items.\n\ndata_cat_rev &lt;- \n  data_google_merch |&gt;\n  group_by(item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_cat_items &lt;- ggplot(\n  data_cat_rev, \n  aes(\n    x = fct_reorder(item_category, revenue),\n    y = revenue)) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "title": "Combine plots using patchwork",
    "section": "Product categories trend",
    "text": "Product categories trend\nNow that we’ve created a plot ranking product categories, let’s create a plot that breaks out several categories of interest over time. To do this, we’ll use the following code.\n\nimportant_cats &lt;- \n  c(\"Accessories\", \"Bags\", \"Apparel\", \n    \"Drinkware\", \"Lifestyle\")\n\ndata_cat_rev_trend &lt;- \n  data_google_merch |&gt;\n  filter(item_category %in% important_cats) |&gt;\n  group_by(event_date, item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_high_cat_trend &lt;- ggplot(\n  data_cat_rev_trend, \n  aes(x = event_date, y = revenue, color = item_category)) +\n  geom_line(linewidth = 2, alpha = .7) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Revenue ($USD)\", color = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "title": "Combine plots using patchwork",
    "section": "Other operators to know",
    "text": "Other operators to know\npatchwork has other operators to modify the layout of the plot composition. Each operator is listed below with a brief description of its use.\n\n+ and - - combines plots together on the same level.\n| - combines plots beside each other (i.e., packing).\n/ - places plots on top of each other (i.e., stacking).\n* - adds objects like themes and facets to all plots on the current nesting level.\n& - will add objects recursively into nested patches.\n\nThe following examples highlight the use of several of these operators."
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "title": "Calculating correlations with corrr",
    "section": "",
    "text": "Photo by Omar Flores\nToday I learned calculating, visualising, and exploring correlations is easy with the corrr package.\nIn the past, I would rely on Base R’s stats::cor() for exploring correlations. This function is a powerful tool if you’re looking to do additional analysis beyond investigating correlation coefficients. stats::cor() has its pain points, though. Sometimes, I just want a package to explore correlations quickly and easily.\nI recently stumbled across the corrr package. It met all the needs I listed above. The purpose of this post is to highlight what I’ve learned while using this package, and to demonstrate functionality I’ve found useful. To get started, let’s attach some libraries and import some example data.\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(here)"
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "title": "Calculating correlations with corrr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I was finishing this post, the Nebraska Women’s Volleyball team beat #1 Wisconsin in a five set thriller.↩︎\nThe Nebraska Women’s Volleyball team broke the World Record for a women’s sporting event on 2023-08-30. Official attendance was 92,003.↩︎"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html",
    "href": "til/posts/2023-03-14-vim-substitution/index.html",
    "title": "Find and replace in Vim",
    "section": "",
    "text": "Today I learned how to find and replace in Vim. I’ve found knowing a few variations of the substitute (:s or su for short) command to be a powerful skill to quickly and efficiently edit code and text within a file. By knowing a few simple command variations, you can greatly improve your productivity. You just have to know the different patterns and when to apply them.\nThis TIL post aims to highlight some of the basics of using Vim’s :s command. My intention is to get you up and running quickly. As such, this post provides several simple examples applying the command to some practical use cases. Although most of the examples use the R programming language, these concepts can be applied to any programming language or text editing task.\nThis post focuses on the basics. Indeed, the substitute command provides a lot of utility and different options to perform various find and replace editing tasks. If you’re looking to learn more advanced features, I suggest reading the docs (:help substitute). I also provide some additional links to other resources throughout and at the end of the post if you’re interested in learning more."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "title": "Find and replace in Vim",
    "section": "The basics",
    "text": "The basics\n:s can be used to find each occurance of a text string, and replace it with another text string. Say I have a character vector basket, and it contains an assortment of fruit. However, what if I want to replace the first apple in my basket with an orange using :s? First, I need to move my cursor to the line I want to find the first string. Then, I can enter the following into the command prompt to find the first instance of the string orange and replace it with the string apple:\n:s/orange/apple\nHere is what this looks like in action.\n\nHowever, what if I don’t want any oranges, and instead I just want apples rather than orangesin my basket. I can append the previous command with g to replace all instances of orange with apple. The g flag indicates to Vim that I want to replace globally to the current line. In other words, replace all instances on the current line.\n:s/orange/apple/g\nBelow is what this will look like in your editor.\n\nWant to find and replace text globally to the line and including multiple lines, then add % to the beginning of the command.\n:%s/orange/apple/g\n\n% is really useful if you want to refactor code efficiently within a file. Check out these two examples, one more contrived, the other a more practical, common application.\n:%s/power/horsepower/g\n:%s/data/cars_data/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "title": "Find and replace in Vim",
    "section": "Confirming replacement",
    "text": "Confirming replacement\nNot sure what all will be replaced and would rather go through each replacement step-by-step? Add c to the end of your command. Adding this flag will make Vim prompt you to confirm each replacement.\n:%s/orange/apple/gc\n\nIn the prompt, you’ll see something like replace with apple (y/n/a/q/l/^E/^Y). You’ll select the option that fulfills the action you want to perform. Here is a list of what each selection does:\n\ny - substitute this one match and move to the next (if any).\nn - skip this match and move to the next (if any).\na - substitue all (and it’s all matches) remaining matches.\nq - quit out of the prompt.\nl - subsitute this one match and quit. l is synonymous with “last”.\n^E - or Ctrl-e will scroll up.\n^Y - or Ctrl-y will scroll down.\n\nThe example above only highlights the use of y, so I suggest experimenting with each selection to get a feel for what they do."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "title": "Find and replace in Vim",
    "section": "Replacing by range",
    "text": "Replacing by range\nTake a look at the command pattern again, specifically the first portion, [range]:\n:[range]s[ubstitute]/{pattern}/{string}/[flags] count\nThe s command provides functionality to scope the find and replace operation to a specific part of your file. Indeed, this functionality was highlighted earlier when we passed % in an earlier command. % just indicated to Vim that we wanted to find and replace all lines in the file. However, we can be more specific.\nSay we now have a much larger basket, one that can hold both fruits and veggies. In the R programming language, this can be modeled using a tribble from the tribble package.\nWhat if we wanted to find the first two instances of carrots in our basket and replace it with kale. This can be done by passing a range at the start of the :s command. In this specific instance, I want to find and replace the carrots on lines 5 and 7 with the string kale, but I don’t want to change the one on line 8. To do this, I can run the following command:\n:5,7s/carrot/kale/g\n\nAnother variation is to start on your current line . and specify to Vim how many additional lines I would like to find and replace in the range. Keep in mind . represents the current line your cursor is located currently within the file. Once you postion your cursor, your command will look something like this:"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "title": "Find and replace in Vim",
    "section": "Replacing from current location to n lines",
    "text": "Replacing from current location to n lines\n:.,+2s/carrot/kale/g\n\nWhat if I had a basket with even more fruits and veggies, and I just wanted to start at my current location and replace all instances that follow? We can use the $ in the range input. The use of the dollar sign indicates to Vim that we want to replace starting at line 8 and go to the end of the file."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "title": "Find and replace in Vim",
    "section": "Replacing to end of file",
    "text": "Replacing to end of file\n:8,$s/carrot/kale/g\n\nOr, if you want to start from the current line and replace to the end of the file, you can do the following:\n:.,$s/carrot/kale/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "title": "Find and replace in Vim",
    "section": "Replacing using visual mode",
    "text": "Replacing using visual mode\nWe can also use visual mode to set the range of the find and replace operation. Just enter visual mode v or visual line mode Shift-v, highlight the range you want your find and replace operation to be applied, enter into command mode with :, and then enter your find and replace statement. Doing this will start the command line off with '&lt;,'&gt;, and you’ll just need to enter the rest of the command, the {pattern} and {string} portions.\n:'&lt;,'&gt;s/carrot/kale`"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "title": "Find and replace in Vim",
    "section": "Use objects in your search buffer",
    "text": "Use objects in your search buffer\nYour previous search history can also be used to do find and replace. Let’s go back to our miles-per-gallon plot example again. First I’ll hover my cursor over the word I want to replace and hit *. Now we can use the search value in our subsititution command. All I need to do is leave the {pattern} blank in the command. The command will look like this:\n:%s//horsepower"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "title": "Find and replace in Vim",
    "section": "Replace with what’s under your cursor",
    "text": "Replace with what’s under your cursor\nTo keep things simple, let’s go back to our first basket example. Specifically, let’s say I want to modify the string strawberry with the string banana, but use my cursor position to do this. First I have to make sure the cursor is hovering over the word I want to use for my replacement. Then, I enter the below command. When you see &lt;c-r&gt;&lt;c-w&gt;, this means you actually hit Ctrl-R and Ctrl-W on your keyboard. You’ll notice the string banana is populated into the command for us.\n%s/strawberry/&lt;c-r&gt;&lt;c-w&gt;"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Collin K. Berke, Ph.D. is a media research analyst in public media. Professionally, he uses data and media/marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. He is especially interested in the use and development of open-source statistical software (i.e. Rstats) to achieve this goal, and developing a broader understanding the role these tools can play in media, digital, and marketing analytics.\nHe has experience using different software, third-party services, and programming languages from developing several analytics projects, both in industry and academia. In regards to programming languages, he has developed projects using R, SQL, $bash, and a little bit of Python. He also has extensive experience using different analytics solutions. For data warehousing, he mostly uses database tools like Postgres and those in the Google Cloud Platform ecosystem (e.g., Google BigQuery). When it comes to automating workflows and data pipelines, he has experience implementing and working with Apache Airflow. He also has extensive experience using third-party tools and software to analyze, wrangle data, and communicate his analyses (e.g., Google Analytics, Google Sheets, Excel, Google Data Studio, and R Shiny). Most of his current work is industry related.\nCollin also serves as an adjunct instructor for the University of Nebraska-Lincoln and Southeast Community College, where he teaches courses in sports data visualization and analysis and communication specific courses. He holds a M.A. in Communication Studies from the The University of South Dakota and a Ph.D. in Media and Communication from Texas Tech University. He has also published and contributed to the publication of several academic journal articles.\nCollin is a self-proclaimed news, sports, and podcast junkie. He really enjoys listening to NPR, watching PBS (especially NOVA), and indulging in college football and baseball. At times, he will write blog posts on topics he finds interesting in these areas.\nCheck out the Now page to see what Collin is currently reading and working on."
  },
  {
    "objectID": "about.html#note-about-this-site",
    "href": "about.html#note-about-this-site",
    "title": "About",
    "section": "Note about this site",
    "text": "Note about this site\nThe views expressed on this site are my own, and they do not reflect the views of my employer, professional and/or community groups I hold membership. Any analyses hosted on this site were done for professional development or were for fun. I make every attempt to perform valid and accurate data analysis and reporting. Unless otherwise noted, none of the content on this site has been peer-reviewed, and thus any conclusions drawn or uses stemming from this work need to take these limitations into account.\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "title": "Shiny summary tiles",
    "section": "",
    "text": "Photo by Stephen Dawson\nEffective reporting tools include user interface (UI) elements to quickly and effectively communicate summary metrics. Shiny, a free software package written in the R statistical computing language, provides several tools to communicate analysis and insights. Combining several of these elements together, a developer can create user interface elements that clearly communicate important summary metrics (e.g., Key Performance Indicators) to an application’s users.\nThis post details the steps to create the following simple Shiny application. Specifically, this post overviews the use of Shiny’s built-in functions to create simple summary metric tiles. In addition, this post describes how to add styling to UI elements by applying custom css to a Shiny application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "title": "Shiny summary tiles",
    "section": "The reactive graph",
    "text": "The reactive graph\nAlthough this app is simple and most of the elements can be easily managed, it’s always good practice to see the big picture of the app by plotting out a reactive graph first. It’s also good to have the intended reactive graph available as a quick reference, just in case unexpected results and/or behaviors are displayed while developing the application, and as a method for identifying any situations where computing resources are not being used efficiently.\nBelow is the reactive graph for the application to be developed:\n\n\n\n\n\n\n\nReactive graph for summary metric tiles\n\n\n\n\nAgain, a really simple application–one input (date), a reactive expression (data()), and five outputs (users; page_view; session_start; purchase; and event_date). The graph also details the dependencies clearly, where the outputs are dependent on the reactive data() object–which in cohort with the outputs–depends on the date input."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "title": "Shiny summary tiles",
    "section": "The setup",
    "text": "The setup\nThe first step is to import the R packages used within the application. The following code chunk contains the packages used for the application. A brief description of each is included.\n\nlibrary(shiny) # The Shiny app library\nlibrary(readr) # Import data\nlibrary(dplyr) # Pipe and data manipulation\nlibrary(tidyr) # Tidying data function\nlibrary(purrr) # Used for functional programming\nlibrary(glue)  # Used for string interpolation\n\nMany of these packages are part of the tidyverse, and thus the import could be simplified to just running library(tidyverse). Be aware this may bring in unused, unneeded libraries. There is nothing wrong with this approach. However, I opted to be more verbose with this example, so as to be clear about what libraries are utilized within the example application and to have more control on what packages were imported by the application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "title": "Shiny summary tiles",
    "section": "Application layout",
    "text": "Application layout\nThe next step is to code the layout of the UI. To keep the design simple, a sidebar will contain the application’s inputs, while the outputs will be placed within the main panel of the application. The general skeleton of the layout looks like this:\n\nui &lt;- fluidPage(\n   # Inputs\n   sidebarLayout(\n      sidebarPanel()\n   ),\n   # Outputs\n   mainPanel(\n      # Summary tiles\n      fluidRow(),\n      br(),\n      # Data information output\n      fluidRow()\n   )\n)\n\nThere’s nothing too fancy about this code, outside of it establishing the general layout of the application, so not much else will be said about what each element does here. However, Chapter 6 of Mastering Shiny discusses application layout if a more detailed description is needed."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "title": "Shiny summary tiles",
    "section": "The date input",
    "text": "The date input\nThe app requirements state users need to have the ability to modify the dates to which the data represents, and the summary metric tiles will change based on this user input. However, the app will not have any user input upon startup, so it needs to default to the most recent date within the data. To meet these requirements, we use the following code:\n\n# Code excluded for brevity\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    )\n\nThe shiny:dateInput() function is used to create the HTML for the input, which resides in the application’s sidebar. The function’s id argument is given the value of date, which will establish a connection to elements within the server. More on this later. Then, a string value of Select a date for summary: is passed along to the label argument. This value will be displayed above the date input in the UI.\nSince the app won’t have an initial user input upon the startup of the application, max(ga4_date$event_date) is passed along to the value argument. This will default the input to the most recent date within the data. In addition, the functions max and min arguments are passed similar calls. However, in the case of the min argument the base R min() function is used on the ga4_data$event_date."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "title": "Shiny summary tiles",
    "section": "First iteration of the summary metric tiles",
    "text": "First iteration of the summary metric tiles\n\nThe server side\n\nThe reactive data() object\nBefore the summary metrics can be displayed in the UI, the application needs data to create the outputs. In addition, since this data will be dependent on users’ input (i.e., the user can select a new date which subsequently changes the summary metric tile), this object needs to be reactive. To do this, the following code is added to the server side of the application.\n\nserver &lt;- function(input, output, session) {\n   data &lt;- reactive({\n      ga4_data %&gt;% filter(.data[[\"event_data\"]] == input$date)\n   })\n}\n\nIn practical terms, this code just filters the data for the date being passed along as the input$date object.\nAgain, this object could be the most recent date within the data, the date set by the max argument in the dateInput() function, or it could be based on a user’s modification of the date input. Since this code was wrapped inside of the reactive({}) function, Shiny will be listening for any changes made to the to the input$date object. Any changes that occur will result in the data() reactive expression to be modified, followed by new output values being displayed via the UI.\nOne other key concept is being exhibited here, tidy evaluation, specifically data-masking. Since technically dplyr::filter() is being used inside of a function, an explicit reference to the data is required. Thus, .data[[\"event_data\"]] notation is used to make it explicit on what data will be filtered. The specifics on how to use data-masking in the context of a Shiny app is beyond the scope of this post. However, the previously linked materials provide a more detailed description of these concepts.\n\n\nThe outputs\nLooking back at the reactive graph, the application requires five outputs to be in the server. These outputs will just be simple text outputs, so the use of the shiny::renderText() function will be sufficient to meet our requirements. The format() function is also applied to comma format any outputs that contain numbers (e.g., 2,576 vs 2576). Here is what the server looks like currently:\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_date, .data[[\"event_date\"]] == input$date)\n  })\n  \n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nAs part of the functionality requirements, the app needed some UI element informing users what date is being represented in the summary tiles. The output$date object was included to meet this requirement. The output$date object, aside from using the renderText() function, includes the use of the glue::glue() function to make the outputted message more informative.\nThe {glue} package is used to manipulate string literals with the use of the curly braces (e.g., {}). When applied here, the {data()$event_date} is evaluated as an R call, its value becomes appended to the string, and the whole string is then outputted to the application’s UI.\n\n\n\nBack to the UI\nNow that there are five elements being outputted from the server, UI elements need to be included to display the rendered outputs.\nWhen making early design decisions about the application’s layout, it was decided these elements were going to reside within the main panel of the application. Another decision made was to keep the summary metric tile elements on the same row, so as to seem as though they are related to one another (i.e., related KPIs). As for the UI element informing the user on the date the summary metric tiles represent, it was decided that this element would be placed on its own row.\nTo achieve the intended design, additional Shiny layout functions were applied to the application’s code. This includes using the fluidRow() and column() functions to achieve the wanted UI organization. The following code was used to achieve the placement of the summary tiles within the application’s layout:\n\nmainPanel(\n   fluidRow(\n      column(),\n      column(),\n      column()\n   ),\n   br(),\n   fluidRow()\n)\n\nAs for the design of the summary metric tiles, each tile needed to include some type of title followed by the text representing the metric. To achieve this, the shiny::div() function was used. This function creates an individual HTML tag that outputs the text being passed along into the function. Directly below the title element, the textOutput() function is used to display the outputs coming from the application’s server. The code for one summary metric tile would look like the following:\n\ncolumn(3,\n       div(\"Unique Users\"),\n       textOutput(\"users\")\n       )\n\nBy combining these elements, the application code in its current state can be seen here:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n  )\n  \n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        column(3,\n               div(\"Users\"),\n               textOutput(\"users\")\n        ),\n        column(3,\n               div(\"Page Views\"),\n               textOutput(\"page_view\")\n        ),\n        column(3,\n               div(\"Session Starts\"),\n               textOutput(\"session_start\")\n        ),\n        column(3,\n               div(\"Purchases\"),\n               textOutput(\"purchase\")\n        )    \n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  # Text output\n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nIndeed, this code works and meets the functionality requirements. However, it’s quite verbose and contains a lot of redundant, repeated code. Different techniques could be applied to make the application more eloquent and efficient in its design. The goal of the next few sections, then, will be to simplify the application through the development of functions and applying functional programming principles.\n\nSimplifying the outputs\nReviewing the server, most of the outputs are created through the use of repeated patterns of the same code. This breaks the DRY principle (Don’t Repeat Yourself) of software development. Both functions and the application of functional programming principles will be applied to address this issue.\nAn obvious pattern used to create the outputs is output$foo &lt;- renderText(format(bar, big.mark = ',')). This pattern could be converted into a function, and then this function could be used to iterate over the several reactive objects (e.g., data()$users) with the use of a {purrr} function. Since the side-effects are intended to be used rather than outputting a list object from our iteration, purrr::walk() will do the trick.\nUtilizing this strategy simplifies our code to the following:\n\nc('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n\nIndeed, I can’t take full credit for this solution. Thanks goes to @Kent Johnson in the R4DS Slack channel for helping me out.\nThe output$date object was left out of this simplification of the code. Certainly, the function could be made to be more general and flexible to handle this repetition of the renderText() function. However, this would be over engineering a solution to the problem."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "title": "Shiny summary tiles",
    "section": "Back to the UI",
    "text": "Back to the UI\nFunctions and functional programming principles will now be used to address these same issues on the UI side of the application. Much of the repetition occurs with the use of the following pattern:\n\ncolumn(3,\n       div(\"Metric Title\"),\n       textOutput(\"metric_output\")\n       )\n\nIndeed, this pattern is applied four times. Since it was copied and pasted more than twice and breaks the DRY principle, it would be best to convert it into a function and iterate it using functional programming tools."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "title": "Shiny summary tiles",
    "section": "Simplifying the UI with functional programming",
    "text": "Simplifying the UI with functional programming\nA helper function, make_summary_tile(), is added to the setup section of the application. The function looks like this:\n\nmake_summary_tile &lt;- function(title, text_output){\n  column(2,\n         div(title),\n         textOutput(text_output)\n  )\n}\n\nThere’s nothing too fancy or complicated about this function. It simply generalizes the pattern applied within the UI side of the first iteration of our application. As for placement, this function could be defined at the top of the application file or in a separate .R file embedded in a R/ sub-folder. Both strategies would make the function available for the app. Deciding which to use comes down to the intended organizational structure of the application.\nThe next step is to apply functional programming to iterate the make_summary_tile() function over the text outputs. Since the function requires two inputs, title and text_output, they were placed inside of a tibble to improve organization of the inputs being passed to the function through pmap().\n\n# Defined in the Setup section\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Used within the UI\npmap(tiles, make_summary_tile)\n\nWhat once required sixteen line’s of code was cut in half to eight (including the explicit definition of the inputs). In addition, coding the tiles using functional programming also makes it more flexible, where summary tiles could be easily added or taken away.\nDoing this would require some slight modification to the make_summary_tile() helper function, though. That is, a width argument would need to be added to the function, so the column width could be set to accommodate the number of outputs for the UI. There are lots of different options that could be explored here. At this point, though, the solution meets the functionality requirements.\nIn its current state, the application code looks like this:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\nmake_summary_tile &lt;- function(header, text_output){\n  column(2,\n         div(header),\n         textOutput(text_output)\n  )\n}\n\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n)\n\n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        pmap(tiles, make_summary_tile)\n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  c('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n  \n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nThe application works, meets the functionality requirements, and now is written in a way that reduces repetition and redundant patterns within the code. However, the summary metric tiles just blend into the UI, and nothing about the styling communicates they contain important information.\nSince these elements are meant to highlight key, important summary metrics, they need to be styled in a way that creates contrast between themselves and the application’s background. The next section focuses on applying custom CSS to give some contrast between these elements and the application’s background."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "title": "Shiny summary tiles",
    "section": "Creating the www folder and CSS file",
    "text": "Creating the www folder and CSS file\nSince the design opted for a file-based CSS approach, a separate www sub-folder in the application’s main project directory needs to be created. Once created, the custom CSS file will be placed inside this folder. The placement of this file can be seen in this Github repo.\nThe purpose of this folder is to make the file available to the web browser when the application starts. Placement of this file is critical. If it is not placed in the www sub-folder, then the CSS file will not be available when the application starts, and any custom styling will not be applied.\nOnce the www sub-folder is created, you can create a CSS file for the application in Rstudio by clicking File, hovering over New File, and selecting CSS File. Save the file in the www sub-folder and give it an informative name. In the case of this example, the file is named app-styling.css.\nThe main goal of the styling will be to create some contrast between the summary metric tiles and the application’s background. Specifically, CSS will be used to create a container that is a different color from the application’s background and includes some shading to make it seem like the element is hovering above the application’s main page. To do this, the app-styling.css file includes the following:\n#summary-tile{\n  font-size: 25px;\n  color:White;\n  text-align: center;\n  margin: 10px;\n  padding: 5px;\n  background-color: #0A145A;\n  border-radius: 15px;\n  box-shadow: 0 5px 20px 0 rgba(0,0,0, .25);\n  transition: transform 300ms;\n}\nA detailed description on how to create CSS selectors is outside the scope of this post. However, in general terms, this selector sets several values for multiple CSS properties by defining the id, #summary-tile within the file. More about this process of creating different CSS selectors can be found here.\nNow it’s just a matter of modifying the code to call this file and pass these style values to the summary tiles within the application. The following code is added to the ui side of our application to include our app-styling.css file:\n\ntags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"app-styling.css\"))\n\nSince the styling is being applied to the summary metric tiles, the make_summary_tile() function is modified to bring in the CSS elements. A css_id argument is added to the function.\n\nmake_summary_tile &lt;- function(header, text_output, css_id){\n  column(2,\n         div(header),\n         textOutput(text_output),\n         id = css_id\n  )\n}\n\nNow that we made this modification to the make_summary_tile(), its application in the UI is also modified. Specifically, the #summary-tile CSS element is explicitly called in pmap(). To do this, the code is modified like this:\n\npmap(tiles, ~make_summary_tile(\n          header = ..1, text_output = ..2, css_id = \"summary-tile\"))\n\nThe header, text_output, and css_id arguments are now explicitly defined in the pmap() call. To refer to the first two elements in the tiles data object, the ..1 (i.e., header column) and the ..2 (i.e., text_output column) are used. Check out the pmap() docs on how to apply the ..1, ..2 (?pmap) for more information."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "",
    "text": "Image generated using the prompt ‘robot browsing an e-commerce store on laptop in pixel art, warm colors’ with the Bing Image Creator"
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#address-missing-transaction-ids",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#address-missing-transaction-ids",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Address missing transaction ids",
    "text": "Address missing transaction ids\nAfter reviewing the dataset’s structure, my first question is how many examples are missing a transaction_id? transaction_ids are critical here, as they are used to group items into transactions. To answer this question, I used the following code:\n\ndata_ga_transactions |&gt;\n  mutate(\n    has_id = case_when(\n      is.na(transaction_id) | transaction_id == \"(not set)\" ~ FALSE,\n      TRUE ~ TRUE \n    )\n  ) |&gt;\n  count(has_id) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  has_id     n  prop\n  &lt;lgl&gt;  &lt;int&gt; &lt;dbl&gt;\n1 FALSE   1498 0.114\n2 TRUE   11615 0.886\n\n\nOut of the 13,113 events, 1,498 (or 11.4%) are missing a transaction_id. Missing transaction_id’s can take two forms. First, a missing value can be an NA value. Second, missing values occur when examples contain the (not set) character string.\nI address missing values by dropping them. Indeed, other approaches are available to handle missing values. Given the data and context you’re working within, you may decide dropping over 11% of examples is not appropriate. As such, the use of nearest neighbor or imputation methods might be explored.\nAlthough the temporal aspects of the data are not relevant for this analysis, for consistency, I’m also going to parse the event_date column into type date. lubridate’s ymd() function can be used for this task.\nHere’s the code to perform the wrangling steps described above:\n\ndata_ga_transactions &lt;- \n  data_ga_transactions |&gt;\n  drop_na(transaction_id) |&gt;\n  filter(transaction_id != \"(not set)\") |&gt;\n  mutate(event_date = ymd(event_date)) |&gt;\n  arrange(event_date, transaction_id)\n\nWe can verify these steps have been applied by once again using dplyr’s glimpse() function. Base R’s summary() function is also useful to verify the data is as expected.\n\nglimpse(data_ga_transactions)\n\nRows: 11,615\nColumns: 3\n$ event_date     &lt;date&gt; 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 20…\n$ transaction_id &lt;chr&gt; \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"1342…\n$ item_name      &lt;chr&gt; \"Google Recycled Writing Set\", \"Google Emoji Sticker Pack\", \"Android Iconic…\n\n\n\nsummary(data_ga_transactions)\n\n   event_date         transaction_id      item_name        \n Min.   :2020-11-11   Length:11615       Length:11615      \n 1st Qu.:2020-11-24   Class :character   Class :character  \n Median :2020-12-04   Mode  :character   Mode  :character  \n Mean   :2020-12-04                                        \n 3rd Qu.:2020-12-13                                        \n Max.   :2020-12-31                                        \n\n\nLooks good from a structural standpoint. However, I did notice that our wrangling procedure resulted in events occurring before 2020-11-11 to be removed. This might indicate some data issues prior to this date, which might be worth further exploration. Given that I don’t have the ability to speak with the developers of the Google Merchandise Store to explore a potential measurement issue, I’m just going to move forward with the analysis. Indeed, our initial goal was to identify association rules during the holiday shopping season, so our data is still within the date range intended for our analysis."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#brief-overview-of-association-rules",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#brief-overview-of-association-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Brief overview of association rules",
    "text": "Brief overview of association rules\nAlthough others have gone more in-depth on this topic (Lantz 2023), it’s worth taking a moment to discuss what an association rule is before we use a model to create them. Let’s say we have the following five transactions:\n{t-shirt, sweater, hat}\n{t-shirt, hat}\n{socks, beanie}\n{t-shirt, sweater, hat, socks, beanie, pen}\n{socks, pen}\nEach line represents an individual transaction. What we aim to do is use an algorithm to identify rules that give us a sense if someone buys one item, what other items will they also likely buy. For example, does buying a t-shirt lead someone to also buy a hat? If so, given our data, can we quantify how confident we are in this rule? Not everyone buying a t-shirt will buy a hat.\nWhen we create rules, they’ll be made up of a left-hand (i.e., an antecedent) and right-hand side (i.e., a consequent). They’ll look something like this:\n{t-shirt} =&gt; {hat}\n\n# or\n\n{t-shirt, hat} =&gt; {sweater}\nRule interpretation is pretty straightforward. In simple terms, the first rule states that when a customer purchases a t-shirt, they’ll also likely purchase a hat. For the second, if a customer purchases a t-shirt and a hat, then they’ll also likely purchase a sweater. It’s important to recognize that the left-hand side can be one or many items. Now that we understand the general makeup of a rule, let’s explore some metrics to quantify each."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#market-basket-analysis-metrics",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#market-basket-analysis-metrics",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Market basket analysis metrics",
    "text": "Market basket analysis metrics\nBefore overviewing the model specification steps, we need to understand the key metrics calculated with a market basket analysis. Specifically, we need a little background on the following:\n\nSupport\nConfidence\nLift\n\nIn this section, I’ll spend a little time defining and describing each. However, others do a more through treatment of each metric (see Lantz 2023, chap. 8; Kadlaskar 2021; Li 2017), and I suggest checking out each for more detailed information.\n\nSupport\nSupport is calculated using the following formula:\n\\[\nsupport(x) = \\frac{count(x)}{N}\n\\]\ncount(x) is the number of transactions containing a specific set of items. N is the number of transactions within our data.\nSimply put, support is interpreted as how frequently an item occurs within the data.\n\n\nConfidence\nAlongside support, confidence is another metric provided by the analysis. It’s built upon support and calculated using the following formula:\n\\[\nconfidence(X\\rightarrow Y) = \\frac{support(X, Y)}{support(X)}\n\\]\nConfidence is a proportion of transactions containing item X (or itemset) results in the presence of item Y (or itemset).\nBoth confidence and support are important; both are parameters we’ll set when specifying our model. These two parameters are the dials we adjust to narrow or expand our rule set generated from the model.\n\n\nLift\nLift’s importance will become more evident once we specify our model and look at some rule sets. But let’s discuss its definition. It’s calculated using the following formula:\n\\[\nlift(X \\rightarrow Y) = \\frac{confidence(X \\rightarrow Y)}{support(Y)}\n\\]\nPut into simple terms, lift gives us a number of how likely one item or itemset is to be purchased to its typical rate of purchase. In even simpler terms, the higher the lift, the stronger evidence that there is a true connection between items or item sets."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#use-the-apriori-function",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#use-the-apriori-function",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Use the apriori() function",
    "text": "Use the apriori() function\nNow let’s do some modeling. Here we’ll use arules’ apriori() function to specify our model. This step requires a little trial and error, as there’s no exact method for picking support and confidence values. As such, let’s just start with apriori’s defaults for the support, confidence, and minlen parameters. The code looks like the following:\n\n# Start with the default support, confidence, minlen\n#   support: 0.1\n#   confidence: 0.8\n#   minlen: 2\nmdl_ga_rules &lt;- apriori(\n  ga_transactions, \n  parameter = list(support = 0.1, confidence = 0.8, minlen = 2)\n)\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext\n        0.8    0.1    1 none FALSE            TRUE       5     0.1      2     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 356 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[371 item(s), 3562 transaction(s)] done [0.00s].\nsorting and recoding items ... [0 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 done [0.00s].\nwriting ... [0 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nmdl_ga_rules\n\nset of 0 rules \n\n\n0 rules were created. This was due to the default support and confidence parameters being too restrictive. We will need to modify these values so the algorithm is able to identify rules from the data. However, we also need to be aware that loosening our rules could increase the size of our rule set, and this might result in a rule set unreasonably large to explore. This is where the trial and error comes into play.\nSo, then, are there any means for determining reasonable starting points? Yes, we just need to consider the business case. Let’s start with support, a parameter measuring how frequently an item or item set occurs within the transactions. A good starting point is to think about how many times a typical item might appear within a transaction throughout the measured period (Lantz 2023).\nSince this is an online merchandise store, my expectation for item and item set purchase is quite low. Thus, I’d expect a typical item to be purchased at least once a week, that is, a total of 8 times during the period. A reasonable starting point for support, then, would be:\n\n# Determining a reasonable support parameter\n8 / 3562\n\n[1] 0.002245929\n\n\nWhen it comes to confidence, it is more about picking a starting point and adjusting from there. For our specific case, I’ll start at .1.\nminlen represents the minimum length the rule (including both the right- and left-hand sides) needs to be before it’s considered for inclusion by the algorithm. It is the last parameter to be set. Our exploratory analysis identified the majority of items in a transaction were quite low, so I believe setting minlen = 2 is sufficient given our data.\nHere’s the updated code for our model with the adjusted parameters:\n\n# Use parameters we think are reasonable \n#   support: 0.002\n#   confidence: 0.1\n#   minlen: 2\nmdl_ga_rules &lt;- apriori(\n  ga_transactions, \n  parameter = list(support = 0.002, confidence = 0.1, minlen = 2)\n)\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext\n        0.1    0.1    1 none FALSE            TRUE       5   0.002      2     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 7 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[371 item(s), 3562 transaction(s)] done [0.00s].\nsorting and recoding items ... [298 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [277 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nmdl_ga_rules\n\nset of 277 rules \n\n\nWith more reasonable parameters in place, the model identified 277 rules. Is 277 rules too much, too little? You’ll have to decide. For this specific case, 277 rules seems reasonable."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#interpret-rules",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#interpret-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Interpret rules",
    "text": "Interpret rules\nLet’s use our first rule as an example, where we’ll seek to interpret it.\n\nfirst_rule &lt;- mdl_ga_rules |&gt;\n  head(1, by = \"lift\") |&gt;\n  as(\"data.frame\") |&gt;\n  tibble()\n\nfirst_rule$rules\n\n[1] \"{Google NYC Campus Mug,Google Seattle Campus Mug} =&gt; {Google Kirkland Campus Mug}\"\n\n\nWritten out, the rule means: If someone buys the Google NYC Campus Mug and the Google Seattle Campus Mug, then they’ll also likely purchase the Google Kirkland Campus Mug. Support is 0.00225, which indicates this rule is included in roughly .2% of transactions. And when these two mugs are purchased together, this rule, where the third mug is purchased, covers around 62% of these transactions. Moreover, the lift metric indicates the presence of a strong rule, where people who buy the first two mugs are more than 169 times more likely to purchase the third mug.\nWhy might this be? Perhaps it’s customers who, by purchasing the first two mugs, are primed to just go ahead and buy the third to finish the set. This might be a great cross-selling opportunity. Maybe we present this rule to our developers and suggest a store feature that encourages customers–when they buy a certain set of items–to complete sets of items within their purchases. Maybe the marketing team could think up some type of pricing scheme along with this feature to further encourage the additional purchase to complete the set.\nDespite this rule, we also need to consider it alongside the count metric. If you recall, count represents the number of transactions this rule’s item set is included. 8 transactions might be too low, and the lift metric may be biased here as a result. We may want to include additional transaction data to further explore this rule or simply be aware this limitation exists when making decisions."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#subset-rules",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#subset-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Subset rules",
    "text": "Subset rules\nIf you recall, earlier in our analysis the Google Camp Mug was identified as being a frequently purchased item. Say our marketing team wants to develop a campaign around this one product and would like to review all the rules associated with it. arules’ subset() generic function in conjunction with some infix operators (e.g., %in%) and inspect() is useful to complete this task. Here is what the code looks like to return these rules:\n\ninspect(subset(mdl_ga_rules, items %in% \"Google Camp Mug Ivory\"))\n\n    lhs                              rhs                          support     confidence coverage  \n[1] {Google Flat Front Bag Grey}  =&gt; {Google Camp Mug Ivory}      0.005053341 0.2571429  0.01965188\n[2] {Google Camp Mug Ivory}       =&gt; {Google Flat Front Bag Grey} 0.005053341 0.1016949  0.04969118\n[3] {Google Unisex Eco Tee Black} =&gt; {Google Camp Mug Ivory}      0.002245929 0.1311475  0.01712521\n[4] {Google Large Tote White}     =&gt; {Google Camp Mug Ivory}      0.002807412 0.1851852  0.01516002\n[5] {Google Magnet}               =&gt; {Google Camp Mug Ivory}      0.002807412 0.1562500  0.01796743\n[6] {Google Camp Mug Gray}        =&gt; {Google Camp Mug Ivory}      0.005053341 0.1836735  0.02751263\n[7] {Google Camp Mug Ivory}       =&gt; {Google Camp Mug Gray}       0.005053341 0.1016949  0.04969118\n    lift     count\n[1] 5.174818 18   \n[2] 5.174818 18   \n[3] 2.639252  8   \n[4] 3.726721 10   \n[5] 3.144421 10   \n[6] 3.696299 18   \n[7] 3.696299 18   \n\n\nSay for example the marketing team finds these rules are not enough to build a campaign around, so they request all rules associated with any mug. The %pin% infix operator can be used for partial matching.\n\ninspect(subset(mdl_ga_rules, items %pin% \"Mug\"))\n\nWarning in seq.default(length = NCOL(quality)): partial argument match of 'length' to 'length.out'\n\n\n     lhs                                   rhs                                    support confidence    coverage       lift count\n[1]  {Google Austin Campus Tote}        =&gt; {Google Austin Campus Mug}         0.002245929  0.6153846 0.003649635  36.533333     8\n[2]  {Google Austin Campus Mug}         =&gt; {Google Austin Campus Tote}        0.002245929  0.1333333 0.016844469  36.533333     8\n[3]  {Google Kirkland Campus Mug}       =&gt; {Google Seattle Campus Mug}        0.003368894  0.9230769 0.003649635  73.066667    12\n[4]  {Google Seattle Campus Mug}        =&gt; {Google Kirkland Campus Mug}       0.003368894  0.2666667 0.012633352  73.066667    12\n[5]  {Google Kirkland Campus Mug}       =&gt; {Google NYC Campus Mug}            0.002245929  0.6153846 0.003649635  27.061728     8\n[6]  {Google Boulder Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002245929  0.3478261 0.006457047  38.717391     8\n[7]  {Google Cambridge Campus Mug}      =&gt; {Google Boulder Campus Mug}        0.002245929  0.2500000 0.008983717  38.717391     8\n[8]  {Google Boulder Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002526670  0.3913043 0.006457047  17.207729     9\n[9]  {Google NYC Campus Mug}            =&gt; {Google Boulder Campus Mug}        0.002526670  0.1111111 0.022740034  17.207729     9\n[10] {Google NYC Campus Bottle}         =&gt; {Google NYC Campus Mug}            0.002245929  0.2962963 0.007580011  13.029721     8\n[11] {YouTube Play Mug}                 =&gt; {YouTube Leather Strap Hat Black}  0.002245929  0.1568627 0.014317799  10.347131     8\n[12] {YouTube Leather Strap Hat Black}  =&gt; {YouTube Play Mug}                 0.002245929  0.1481481 0.015160022  10.347131     8\n[13] {YouTube Play Mug}                 =&gt; {YouTube Twill Sandwich Cap Black} 0.003368894  0.2352941 0.014317799  12.325260    12\n[14] {YouTube Twill Sandwich Cap Black} =&gt; {YouTube Play Mug}                 0.003368894  0.1764706 0.019090399  12.325260    12\n[15] {Google Flat Front Bag Grey}       =&gt; {Google Camp Mug Ivory}            0.005053341  0.2571429 0.019651881   5.174818    18\n[16] {Google Camp Mug Ivory}            =&gt; {Google Flat Front Bag Grey}       0.005053341  0.1016949 0.049691185   5.174818    18\n[17] {Google Unisex Eco Tee Black}      =&gt; {Google Camp Mug Ivory}            0.002245929  0.1311475 0.017125211   2.639252     8\n[18] {Google Chicago Campus Mug}        =&gt; {Google LA Campus Mug}             0.002245929  0.1702128 0.013194834  14.099951     8\n[19] {Google LA Campus Mug}             =&gt; {Google Chicago Campus Mug}        0.002245929  0.1860465 0.012071870  14.099951     8\n[20] {Google Chicago Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002526670  0.1914894 0.013194834  11.368085     9\n[21] {Google Austin Campus Mug}         =&gt; {Google Chicago Campus Mug}        0.002526670  0.1500000 0.016844469  11.368085     9\n[22] {Google Chicago Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002526670  0.1914894 0.013194834   8.420804     9\n[23] {Google NYC Campus Mug}            =&gt; {Google Chicago Campus Mug}        0.002526670  0.1111111 0.022740034   8.420804     9\n[24] {Google LA Campus Sticker}         =&gt; {Google LA Campus Mug}             0.003649635  0.4193548 0.008702976  34.738185    13\n[25] {Google LA Campus Mug}             =&gt; {Google LA Campus Sticker}         0.003649635  0.3023256 0.012071870  34.738185    13\n[26] {Google NYC Campus Zip Hoodie}     =&gt; {Google NYC Campus Mug}            0.003088153  0.1929825 0.016002246   8.486463    11\n[27] {Google NYC Campus Mug}            =&gt; {Google NYC Campus Zip Hoodie}     0.003088153  0.1358025 0.022740034   8.486463    11\n[28] {Google LA Campus Mug}             =&gt; {Google Cambridge Campus Mug}      0.002245929  0.1860465 0.012071870  20.709302     8\n[29] {Google Cambridge Campus Mug}      =&gt; {Google LA Campus Mug}             0.002245929  0.2500000 0.008983717  20.709302     8\n[30] {Google LA Campus Mug}             =&gt; {Google Austin Campus Mug}         0.002245929  0.1860465 0.012071870  11.044961     8\n[31] {Google Austin Campus Mug}         =&gt; {Google LA Campus Mug}             0.002245929  0.1333333 0.016844469  11.044961     8\n[32] {Google LA Campus Mug}             =&gt; {Google NYC Campus Mug}            0.003930376  0.3255814 0.012071870  14.317542    14\n[33] {Google NYC Campus Mug}            =&gt; {Google LA Campus Mug}             0.003930376  0.1728395 0.022740034  14.317542    14\n[34] {Google Cambridge Campus Mug}      =&gt; {Google PNW Campus Mug}            0.002245929  0.2500000 0.008983717  20.238636     8\n[35] {Google PNW Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.1818182 0.012352611  20.238636     8\n[36] {Google Cambridge Campus Mug}      =&gt; {Google Seattle Campus Mug}        0.003088153  0.3437500 0.008983717  27.209722    11\n[37] {Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.003088153  0.2444444 0.012633352  27.209722    11\n[38] {Google Cambridge Campus Mug}      =&gt; {Google Austin Campus Mug}         0.003088153  0.3437500 0.008983717  20.407292    11\n[39] {Google Austin Campus Mug}         =&gt; {Google Cambridge Campus Mug}      0.003088153  0.1833333 0.016844469  20.407292    11\n[40] {Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.005614823  0.6250000 0.008983717  27.484568    20\n[41] {Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.005614823  0.2469136 0.022740034  27.484568    20\n[42] {Google PNW Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.004491859  0.3636364 0.012352611  28.783838    16\n[43] {Google Seattle Campus Mug}        =&gt; {Google PNW Campus Mug}            0.004491859  0.3555556 0.012633352  28.783838    16\n[44] {Google PNW Campus Mug}            =&gt; {Google NYC Campus Mug}            0.003088153  0.2500000 0.012352611  10.993827    11\n[45] {Google NYC Campus Mug}            =&gt; {Google PNW Campus Mug}            0.003088153  0.1358025 0.022740034  10.993827    11\n[46] {Google Sunnyvale Campus Mug}      =&gt; {Google Austin Campus Mug}         0.002245929  0.1600000 0.014037058   9.498667     8\n[47] {Google Austin Campus Mug}         =&gt; {Google Sunnyvale Campus Mug}      0.002245929  0.1333333 0.016844469   9.498667     8\n[48] {Google Sunnyvale Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002526670  0.1800000 0.014037058   7.915556     9\n[49] {Google NYC Campus Mug}            =&gt; {Google Sunnyvale Campus Mug}      0.002526670  0.1111111 0.022740034   7.915556     9\n[50] {Google Seattle Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002526670  0.2000000 0.012633352  11.873333     9\n[51] {Google Austin Campus Mug}         =&gt; {Google Seattle Campus Mug}        0.002526670  0.1500000 0.016844469  11.873333     9\n[52] {Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.003649635  0.2888889 0.012633352  12.703978    13\n[53] {Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.003649635  0.1604938 0.022740034  12.703978    13\n[54] {Google Large Tote White}          =&gt; {Google Camp Mug Ivory}            0.002807412  0.1851852 0.015160022   3.726721    10\n[55] {Google NYC Campus Sticker}        =&gt; {Google NYC Campus Mug}            0.003368894  0.3157895 0.010668164  13.886940    12\n[56] {Google NYC Campus Mug}            =&gt; {Google NYC Campus Sticker}        0.003368894  0.1481481 0.022740034  13.886940    12\n[57] {Google Austin Campus Mug}         =&gt; {Google NYC Campus Mug}            0.004211117  0.2500000 0.016844469  10.993827    15\n[58] {Google NYC Campus Mug}            =&gt; {Google Austin Campus Mug}         0.004211117  0.1851852 0.022740034  10.993827    15\n[59] {Google Magnet}                    =&gt; {Google Camp Mug Ivory}            0.002807412  0.1562500 0.017967434   3.144421    10\n[60] {Google Camp Mug Gray}             =&gt; {Google Camp Mug Ivory}            0.005053341  0.1836735 0.027512633   3.696299    18\n[61] {Google Camp Mug Ivory}            =&gt; {Google Camp Mug Gray}             0.005053341  0.1016949 0.049691185   3.696299    18\n[62] {Google Kirkland Campus Mug,                                                                                                \n      Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002245929  0.6666667 0.003368894  29.316872     8\n[63] {Google Kirkland Campus Mug,                                                                                                \n      Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.002245929  1.0000000 0.002245929  79.155556     8\n[64] {Google NYC Campus Mug,                                                                                                     \n      Google Seattle Campus Mug}        =&gt; {Google Kirkland Campus Mug}       0.002245929  0.6153846 0.003649635 168.615385     8\n[65] {Google Boulder Campus Mug,                                                                                                 \n      Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[66] {Google Boulder Campus Mug,                                                                                                 \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.8888889 0.002526670  98.944444     8\n[67] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Boulder Campus Mug}        0.002245929  0.4000000 0.005614823  61.947826     8\n[68] {Google Cambridge Campus Mug,                                                                                               \n      Google LA Campus Mug}             =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[69] {Google LA Campus Mug,                                                                                                      \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.5714286 0.003930376  63.607143     8\n[70] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google LA Campus Mug}             0.002245929  0.4000000 0.005614823  33.134884     8\n[71] {Google Cambridge Campus Mug,                                                                                               \n      Google PNW Campus Mug}            =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[72] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google PNW Campus Mug}            0.002245929  0.4000000 0.005614823  32.381818     8\n[73] {Google NYC Campus Mug,                                                                                                     \n      Google PNW Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.7272727 0.003088153  80.954545     8\n[74] {Google Cambridge Campus Mug,                                                                                               \n      Google Seattle Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002245929  0.7272727 0.003088153  43.175758     8\n[75] {Google Austin Campus Mug,                                                                                                  \n      Google Cambridge Campus Mug}      =&gt; {Google Seattle Campus Mug}        0.002245929  0.7272727 0.003088153  57.567677     8\n[76] {Google Austin Campus Mug,                                                                                                  \n      Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002245929  0.8888889 0.002526670  98.944444     8\n[77] {Google Cambridge Campus Mug,                                                                                               \n      Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002807412  0.9090909 0.003088153  39.977553    10\n[78] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.002807412  0.5000000 0.005614823  39.577778    10\n[79] {Google NYC Campus Mug,                                                                                                     \n      Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002807412  0.7692308 0.003649635  85.625000    10\n[80] {Google Austin Campus Mug,                                                                                                  \n      Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002526670  0.8181818 0.003088153  35.979798     9\n[81] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Austin Campus Mug}         0.002526670  0.4500000 0.005614823  26.715000     9\n[82] {Google Austin Campus Mug,                                                                                                  \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002526670  0.6000000 0.004211117  66.787500     9\n\n\nThis can also be achieved in a more tidyverse style by doing the following:\n\n# Google Camp Mug Ivory tidy way\nmdl_ga_rules |&gt;\n  as(\"data.frame\") |&gt;\n  tibble() |&gt;\n  filter(str_detect(rules, \"Google Camp Mug Ivory\"))\n\n# A tibble: 7 × 6\n  rules                                                    support confidence coverage  lift count\n  &lt;chr&gt;                                                      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 {Google Flat Front Bag Grey} =&gt; {Google Camp Mug Ivory}  0.00505      0.257   0.0197  5.17    18\n2 {Google Camp Mug Ivory} =&gt; {Google Flat Front Bag Grey}  0.00505      0.102   0.0497  5.17    18\n3 {Google Unisex Eco Tee Black} =&gt; {Google Camp Mug Ivory} 0.00225      0.131   0.0171  2.64     8\n4 {Google Large Tote White} =&gt; {Google Camp Mug Ivory}     0.00281      0.185   0.0152  3.73    10\n5 {Google Magnet} =&gt; {Google Camp Mug Ivory}               0.00281      0.156   0.0180  3.14    10\n6 {Google Camp Mug Gray} =&gt; {Google Camp Mug Ivory}        0.00505      0.184   0.0275  3.70    18\n7 {Google Camp Mug Ivory} =&gt; {Google Camp Mug Gray}        0.00505      0.102   0.0497  3.70    18\n\n\n\n# All mugs tidy way\nmdl_ga_rules |&gt;\n  as(\"data.frame\") |&gt;\n  tibble() |&gt;\n  filter(str_detect(rules, \"Mug\"))\n\n# A tibble: 82 × 6\n   rules                                                     support confidence coverage  lift count\n   &lt;chr&gt;                                                       &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 {Google Austin Campus Tote} =&gt; {Google Austin Campus Mug} 0.00225      0.615  0.00365  36.5     8\n 2 {Google Austin Campus Mug} =&gt; {Google Austin Campus Tote} 0.00225      0.133  0.0168   36.5     8\n 3 {Google Kirkland Campus Mug} =&gt; {Google Seattle Campus M… 0.00337      0.923  0.00365  73.1    12\n 4 {Google Seattle Campus Mug} =&gt; {Google Kirkland Campus M… 0.00337      0.267  0.0126   73.1    12\n 5 {Google Kirkland Campus Mug} =&gt; {Google NYC Campus Mug}   0.00225      0.615  0.00365  27.1     8\n 6 {Google Boulder Campus Mug} =&gt; {Google Cambridge Campus … 0.00225      0.348  0.00646  38.7     8\n 7 {Google Cambridge Campus Mug} =&gt; {Google Boulder Campus … 0.00225      0.25   0.00898  38.7     8\n 8 {Google Boulder Campus Mug} =&gt; {Google NYC Campus Mug}    0.00253      0.391  0.00646  17.2     9\n 9 {Google NYC Campus Mug} =&gt; {Google Boulder Campus Mug}    0.00253      0.111  0.0227   17.2     9\n10 {Google NYC Campus Bottle} =&gt; {Google NYC Campus Mug}     0.00225      0.296  0.00758  13.0     8\n# ℹ 72 more rows"
  },
  {
    "objectID": "blog/posts/2024-02-27-tidytuesday-2024-02-27-leap-day/index.html",
    "href": "blog/posts/2024-02-27-tidytuesday-2024-02-27-leap-day/index.html",
    "title": "Exploring the lifespans of historical figures born on a Leap Day",
    "section": "",
    "text": "Photo by Nick Hillier\n\n\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(here)\n\n\nBackground\nHappy belated Leap Day! This week’s #tidytuesday is focused on significant historical events and people who were born or died on a Leap Day. The aim of this post is to contribute a couple data visualizations to this social data project. Specifically, I used plotly and Tableau to create my contributions.\n\ndata_births &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-02-27-tidytuesday-2024-02-27-leap-day\",\n    \"births.csv\"\n  )\n)\n\nLet’s do a quick glimpse() and skim() of our data, just so we get an idea of what we’re working with here.\n\nglimpse(data_births)\n\nRows: 121\nColumns: 4\n$ year_birth  &lt;dbl&gt; 1468, 1528, 1528, 1572, 1576, 1640, 1692, 1724, 1736, 1792, 1812, 1828, 1836, …\n$ person      &lt;chr&gt; \"Pope Paul III\", \"Albert V\", \"Domingo Báñez\", \"Edward Cecil\", \"Antonio Neri\", …\n$ description &lt;chr&gt; NA, \"Duke of Bavaria\", \"Spanish theologian\", \"1st Viscount Wimbledon\", \"Floren…\n$ year_death  &lt;dbl&gt; 1549, 1579, 1604, 1638, 1614, 1704, 1763, 1822, 1784, 1868, 1880, 1921, 1908, …\n\n\n\nskim(data_births)\n\n\nData summary\n\n\nName\ndata_births\n\n\nNumber of rows\n121\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nperson\n0\n1.00\n6\n29\n0\n121\n0\n\n\ndescription\n1\n0.99\n12\n95\n0\n107\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear_birth\n0\n1.00\n1919.90\n101.01\n1468\n1920\n1944.0\n1976\n2004\n▁▁▁▁▇\n\n\nyear_death\n65\n0.46\n1933.61\n126.53\n1549\n1920\n1989.5\n2013\n2023\n▁▁▁▁▇\n\n\n\n\n\n\n\nData description\nThis week’s data comes from the February 29th Wikipedia page. Three data sets are made available, one focused on significant events, as well as births and deaths of historical figures that occurred on a Leap Day. Given what’s available, I was interested in exploring the age and lifespan of the historical figures born on a Leap Day. Here’s the wrangling code I created to explore the data.\n\ndata_age &lt;- data_births |&gt;\n  mutate(\n    is_alive = ifelse(is.na(year_death), 1, 0),\n    year_death = ifelse(is.na(year_death), 2024, year_death),\n    age = year_death - year_birth\n  ) |&gt;\n  arrange(desc(age)) |&gt; \n  relocate(person, description, year_birth, year_death, age)\n\ndata_age$person &lt;- factor(data_age$person, levels = data_age$person[order(data_age$year_birth)])\n\n\n\nWhat are the lifespans of historical figures born on a leap day?\nTo explore this question, I decided to create a dumbbell chart. In the chart, the blue dots represent the person’s birth year. The black dot represents the year the person died. Absence of the black dot indicates a person is still alive, while the grey line represents the person’s lifespan. If you hover over the dots, a tool tip with information about each person is shown.\n\nnot_alive &lt;- data_age |&gt; filter(is_alive == 0)\n\nplot_ly(\n  data_age, \n  color = I(\"gray80\"),\n  text = ~paste(\n    person, \"&lt;br&gt;\",\n    \"Age: \", age, \"&lt;br&gt;\",\n    description \n  ),\n  hoverinfo = \"text\"\n) |&gt;\n  add_segments(x = ~year_birth, xend = ~year_death, y = ~person, yend = ~person, showlegend = FALSE) |&gt;\n  add_markers(x = ~year_birth, y = ~person, color = I(\"#0000FF\"), name = \"Birth year\") |&gt;\n  add_markers(data = not_alive, x = ~year_death, y = ~person, color = I(\"black\"), name = \"Year passed\") |&gt;\n  layout(\n    title = list(\n      text = \"&lt;b&gt;Lifespans of historical figures born on a Leap Day&lt;/b&gt;\",\n      xanchor = \"center\",\n      yanchor = \"top\",\n      font = list(family = \"arial\", size = 24)\n    ),\n    xaxis = list(\n      title = \"Year born | Year died\"\n    ),\n    yaxis = list(\n      title = \"\"\n    )\n  )\n\n\n\n\n\n\n\nAn attempt using Tableau\nI also created a version of this visualization using Tableau. You can view my attempt here. I was required to make a few concessions with this attempt, as I was unable to have as much fine control of the plot elements as I would have liked. However, I’m happy with what turned out.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring the Lifespans of Historical Figures Born on a\n    {Leap} {Day}},\n  date = {2024-03-05},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Exploring the Lifespans of Historical\nFigures Born on a Leap Day.” March 5, 2024."
  },
  {
    "objectID": "blog/posts/2024-09-14-google-analytics-machine-learning-kmeans-clustering/index.html",
    "href": "blog/posts/2024-09-14-google-analytics-machine-learning-kmeans-clustering/index.html",
    "title": "Messing with models: k-means clustering of Google Analytics 4 data",
    "section": "",
    "text": "Photo by Daniel Fazio\n\n\nMarketing campaigns target specific audiences. Some form of customer segmentation is performed to identify these audiences. These segments can be identified using several approaches. This post describes one such approach: the use of data and machine learning to specify clustering models present within Google Analytics data for an e-commerce store.\nThe goal is simple: create customer segments a marketing team could use to craft and target specific marketing campaigns. To achieve this goal, this post overviews the use of a straightforward, useful machine learning algorithm called k-means clustering. Packages and functions from R, a statistical programming language, are used for the segmentation process.\n\nlibrary(tidyverse)\nlibrary(bigrquery)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(factoextra)\nlibrary(ids)\nlibrary(here)\nlibrary(psych)\nlibrary(gt)\n\n\nggplot_theme &lt;- \n  theme_bw() +\n  theme(\n    text = element_text(size = 12),\n    title = element_text(size = 16, face = \"bold\")\n  )\n# Override the default ggplot2 theme\ntheme_set(ggplot_theme)\n\n\nExtract and explore data\nFirst, we need some data. This section briefly discusses the data extraction process. I’ve detailed the extraction of Google Analytics data in a previous post. For the sake of time, then, this topic won’t be discussed in-depth. If you’re already familiar with how to access this data, feel free to skip this section.\nThe data used here is obfuscated Google Analytics data for the Google Merchandise Store. This data is stored and accessed via BigQuery, a cloud-based data warehouse useful for analytics purposes.\n\n\n\n\n\n\nWarning\n\n\n\nThis data is useful for example tutorials, so the conclusions drawn here should not be used to infer anything about true purchasing behavior. The purpose of this post is to be a tutorial on how to perform k-means clustering, rather than about deriving true conclusions about Google Merchandise store customers.\n\n\nBefore data extraction, let’s do some exploration of the source data. The focus here is to get a sense of what’s in the data, while also creating an understanding of the source data’s structure. This mostly involves identifying the available fields and data types.\nThe bigrquery package provides the bq_tables_fields() function to retrieve this information. The following code example shows how to use this function to return field names within the dataset’s tables:\n\ntable_ecommerce &lt;-\n  bq_table(\n    \"bigquery-public-data\",\n    \"ga4_obfuscated_sample_ecommerce\",\n    \"events_20210101\"\n  )\n\nbq_table_fields(table_ecommerce)\n\nThen, the following code submits a SQL query to return the ga_obfuscated_sample_ecommerce data from BigQuery. It’s important to note, similar to what was done in past posts, I’m querying data associated with transactions occurring over the U.S. Christmas holiday season. Here’s the query string if you want specific details:\n\nquery &lt;- \"\n  select \n    event_date,\n    user_pseudo_id,\n    ecommerce.transaction_id,\n    items.item_category,\n    items.quantity,\n    ecommerce.purchase_revenue_in_usd\n  from `&lt;your-project-name&gt;.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(items) as items\n  where _table_suffix between '20201101' and '20201231' and \n  event_name = 'purchase'\n  order by user_pseudo_id, transaction_id\n\"\n\n\ndata_ga_transactions &lt;- bq_project_query(\n  \"&lt;your-project-name&gt;\",\n  query\n) |&gt;\nbq_table_download()\n\nBefore moving forward, some data validation is in order. Columns containing missing values are the biggest concern.\n\n# Verify if there are any missing values\nmap(data_ga_transactions, \\(x) any(is.na(x)))\n\n$event_date\n[1] FALSE\n\n$user_pseudo_id\n[1] FALSE\n\n$transaction_id\n[1] TRUE\n\n$item_category\n[1] FALSE\n\n$quantity\n[1] TRUE\n\n$purchase_revenue_in_usd\n[1] FALSE\n\n\nNo surprise: some features contain missing values. transaction_id and quantity both contain missing values. Our data wrangling step will need to address these issues. Let’s look closer and explore why missing values might be present within the data. This code can be used to do this:\n\n# Examples with a missing `transaction_id`\ndata_ga_transactions |&gt; filter(is.na(transaction_id))\n\n# A tibble: 59 × 6\n   event_date user_pseudo_id transaction_id item_category quantity purchase_revenue_in_usd\n        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;                   &lt;dbl&gt;\n 1   20201101       1494019. &lt;NA&gt;           New                  1                      25\n 2   20201101       2422026. &lt;NA&gt;           New                  2                      72\n 3   20201101       2422026. &lt;NA&gt;           Fun                  1                      72\n 4   20201101      29640693. &lt;NA&gt;           Apparel              1                      55\n 5   20201101      29640693. &lt;NA&gt;           Shop by Brand        2                      59\n 6   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 7   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 8   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 9   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n10   20201101      33027284. &lt;NA&gt;           Accessories          1                      63\n# ℹ 49 more rows\n\n# Examples where a `quantity` is missing\ndata_ga_transactions |&gt; filter(is.na(quantity))\n\n# A tibble: 148 × 6\n   event_date user_pseudo_id transaction_id item_category quantity purchase_revenue_in_usd\n        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;                   &lt;dbl&gt;\n 1   20201217       1086663. (not set)      (not set)           NA                       0\n 2   20201120      11080045. (not set)      (not set)           NA                       0\n 3   20201204      12422651. (not set)      (not set)           NA                       0\n 4   20201213      13309292. (not set)      (not set)           NA                       0\n 5   20201126      13429550. (not set)      (not set)           NA                       0\n 6   20201213       1396855. (not set)      (not set)           NA                       0\n 7   20201127       1423291. (not set)      (not set)           NA                       0\n 8   20201210    1540308157. (not set)      (not set)           NA                       0\n 9   20201201      15915163. (not set)      (not set)           NA                       0\n10   20201120      15980073. (not set)      (not set)           NA                       0\n# ℹ 138 more rows\n\n\nFor the transaction_id column, 59 rows contain missing values. If you dig a little further, you’ll notice the majority of these missing transaction_ids occur near the beginning of data collection (i.e., 2020-11-01). This was likely a measurement issue with the Google Analytics setup, which would warrant further exploration. Since access to information about how Google Analytics was set up for the Google Merchandise Store (i.e., I can’t speak with the developers), this issue can’t be further explored. The only option, then, is to simply drop these rows.\nMissing quantity values seem to be associated with examples where transaction_id and item_category both contain a (not set) character string. Again, this could be a measurement issue worth further exploration. Given the available information, these examples will also be dropped.\nAt this point, dplyr’s glimpse() and skimr’s skim() functions are used for some additional exploratory analysis. glimpse() is great to get info about the data’s structure. skim() provides summary statistics for each variable within the dataset, regardless of type.\n\nglimpse(data_ga_transactions)\n\nRows: 13,113\nColumns: 6\n$ event_date              &lt;dbl&gt; 20201210, 20201210, 20201210, 20201103, 20201103, 20201103, 202011…\n$ user_pseudo_id          &lt;dbl&gt; 10111056, 10111056, 10111056, 1014825, 1014825, 1014825, 1014825, …\n$ transaction_id          &lt;chr&gt; \"741471\", \"741471\", \"741471\", \"(not set)\", \"(not set)\", \"(not set)…\n$ item_category           &lt;chr&gt; \"Apparel\", \"Apparel\", \"Apparel\", \"Shop by Brand\", \"Office\", \"Shop …\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ purchase_revenue_in_usd &lt;dbl&gt; 94, 94, 94, 183, 183, 183, 183, 183, 183, 86, 86, 86, 86, 86, 86, …\n\n\n\nskim(data_ga_transactions)\n\n\nData summary\n\n\nName\ndata_ga_transactions\n\n\nNumber of rows\n13113\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntransaction_id\n59\n1\n2\n9\n0\n3563\n0\n\n\nitem_category\n0\n1\n0\n23\n189\n22\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1.00\n20201167.86\n4.740000e+01\n20201101\n20201121\n20201202\n20201212\n20201231\n▇▁▁▂▇\n\n\nuser_pseudo_id\n0\n1.00\n289012023.38\n1.274012e+09\n1014825\n5706716\n25060616\n62921408\n9202815833\n▇▁▁▁▁\n\n\nquantity\n148\n0.99\n1.49\n2.910000e+00\n1\n1\n1\n1\n160\n▇▁▁▁▁\n\n\npurchase_revenue_in_usd\n0\n1.00\n106.68\n1.171100e+02\n0\n40\n71\n137\n1530\n▇▁▁▁▁\n\n\n\n\n\nThe data contains a total of 13,113 rows with six features. After reviewing the data types and comparing some of the example values outputted from glimpse(), it’s concerning that some variables are of type character rather than type numeric. Values of type integer would be best in this case. Let’s do some more digging.\n\n# Only printing the first 10 values for brevity\nunique(data_ga_transactions$transaction_id) |&gt;\n  head(n = 10)\n\n [1] \"741471\"    \"(not set)\" \"983645\"    \"406646\"    \"2105\"      \"886501\"    \"76937\"     \"29460\"    \n [9] \"339943\"    \"614046\"   \n\n\nAs expected, some transaction_ids are of type character, which is keeping the column from being a true numeric variable. This is due to some rows containing the (not set) character string. Since the transaction_id is critical for our analysis, these rows will need to be filtered out during data wrangling.\nInitial data exploration is complete. We now have enough information to begin data wrangling. The wrangling step will strive to format the data into a structure necessary for performing k-means clustering. The following code chunk contains the code to do this. After, there’s a step-by-step explanation of what this code is doing.\n\ndata_user_items &lt;- \n  data_ga_transactions |&gt;\n  filter(transaction_id != \"(not set)\") |&gt;\n  drop_na() |&gt;\n  mutate(\n    user_id = random_id(), \n    .by = user_pseudo_id, \n    .after = user_pseudo_id\n  ) |&gt;\n  select(-user_pseudo_id) |&gt;\n  summarise(\n    quantity = sum(quantity), \n    .by = c(user_id, item_category)\n  ) |&gt;\n  mutate(\n    item_category = case_when(\n      item_category == \"\" ~ \"Unknown\",\n      TRUE ~ as.character(item_category)\n    )\n  ) |&gt;\n  pivot_wider(\n    names_from = item_category, \n    values_from = quantity,\n    values_fill = 0\n  ) |&gt;\n  clean_names()\n\nIn the code above, the (not set) issue in the transaction_id feature is addressed first. Any row with the (not set) value is simply filtered from the data. drop_na() follows. This function drops any rows that contain a NA value. As a result of dropping NAs, we’re left with 11,615 rows: a loss of 1,498 examples (~11.4%).\n\n\n\n\n\n\nNote\n\n\n\nMerely dropping examples is convenient in this case, but it may not be ideal or even valid in every context. You’ll want to explore what is appropriate for the data you’re working with before applying this strategy.\n\n\nSome mutating and summarization of the data is next. The mutation step applies a random id in place of the user_pseudo_id. This provides an extra layer of privacy for users, since the data is being used outside of its original source system.\n\n\n\n\n\n\nNote\n\n\n\nThis may be a little inordinate, but I argue it’s important. It’s best to do what we can to create another layer of privacy to the information we’re using outside of the original source system. Since this data is public, it’s not too much of a concern here. However, this may be an important consideration when you’re working with ‘real world’ Google Analytics data.\nI’m not a privacy expert, so you’ll want to identify best practice for the context you work in.\n\n\nWith the modelling goal top-of-mind, aggregation will sum values to individual users rather than transactions. This way, we’ll be able to use k-means clustering to identify customer cohorts, rather than transaction cohorts. To do this, the data is grouped by user_id and item_category and the quantity feature is summed.\nPost aggregation, item_categorys that don’t contain any values need to be addressed. To address this, any missing string is replaced with ‘Unknown’ using the case_when() function. Finally, tidyr’s pivot_wider() is used to transform the data from long format to wide format. When taken together, the transformation results in a data set where each feature, other than user_id, is a sum of the item categories purchased by each user during the period. Each example, then, could comprise of multiple transactions that occur over the period.\nPost wrangling, readr’s write_csv() function is used to save the data to disk. This way a request isn’t sent to BigQuery every time the post is built. You don’t have to do this, but it’s useful for limiting query costs associated with the service, though it’s pretty economical to query data in this way.\n\nwrite_csv(data_user_items, \"data_user_items.csv\")\n\n\ndata_user_items &lt;- read_csv(\"data_user_items.csv\")\n\nRows: 2941 Columns: 22\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): user_id\ndbl (21): apparel, bags, shop_by_brand, drinkware, new, clearance, accessories, campus_collectio...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData exploration for modeling\nBefore modeling, let’s do some data exploration. The goal during exploration is to ensure data meets the assumptions of k-means clustering. First, let’s get a general sense of the structure of our wrangled data. dplyr’s glimpse() is once again useful here.\n\nglimpse(data_user_items)\n\nRows: 2,941\nColumns: 22\n$ user_id                 &lt;chr&gt; \"vqnzdyytdx\", \"lhkctjeylp\", \"fkgfswdpur\", \"bycrrxogpw\", \"xdecczvgb…\n$ apparel                 &lt;dbl&gt; 3, 0, 3, 0, 0, 0, 4, 15, 1, 1, 0, 1, 2, 0, 2, 3, 0, 0, 0, 0, 1, 6,…\n$ bags                    &lt;dbl&gt; 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ shop_by_brand           &lt;dbl&gt; 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, …\n$ drinkware               &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, …\n$ new                     &lt;dbl&gt; 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 6, 2, 0, 0, 0, 0, 1, 1, 0, 4, 0, 0, …\n$ clearance               &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, …\n$ accessories             &lt;dbl&gt; 0, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, …\n$ campus_collection       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0,…\n$ office                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lifestyle               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ small_goods             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ uncategorized_items     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ stationery              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ google                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ writing_instruments     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fun                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ unknown                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ electronics_accessories &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ notebooks_journals      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gift_cards              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ black_lives_matter      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nFor this time period, the data contains over 2,941 customers and 21 features available for the k-means clustering model. The user_id feature will be excluded from the modeling. Features represent counts of items purchased within each item category for each user during the period. skimr::skim() is again handy for getting a sense of the shape of the data.\n\nskim(data_user_items)\n\n\nData summary\n\n\nName\ndata_user_items\n\n\nNumber of rows\n2941\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nuser_id\n0\n1\n10\n10\n0\n2941\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\napparel\n0\n1\n1.42\n2.13\n0\n0\n1\n2\n28\n▇▁▁▁▁\n\n\nbags\n0\n1\n0.28\n2.02\n0\n0\n0\n0\n75\n▇▁▁▁▁\n\n\nshop_by_brand\n0\n1\n0.30\n1.32\n0\n0\n0\n0\n40\n▇▁▁▁▁\n\n\ndrinkware\n0\n1\n0.30\n1.49\n0\n0\n0\n0\n30\n▇▁▁▁▁\n\n\nnew\n0\n1\n0.56\n2.32\n0\n0\n0\n0\n87\n▇▁▁▁▁\n\n\nclearance\n0\n1\n0.23\n1.03\n0\n0\n0\n0\n20\n▇▁▁▁▁\n\n\naccessories\n0\n1\n0.49\n2.23\n0\n0\n0\n0\n48\n▇▁▁▁▁\n\n\ncampus_collection\n0\n1\n0.58\n3.40\n0\n0\n0\n0\n128\n▇▁▁▁▁\n\n\noffice\n0\n1\n0.56\n3.21\n0\n0\n0\n0\n80\n▇▁▁▁▁\n\n\nlifestyle\n0\n1\n0.21\n0.97\n0\n0\n0\n0\n20\n▇▁▁▁▁\n\n\nsmall_goods\n0\n1\n0.05\n0.28\n0\n0\n0\n0\n4\n▇▁▁▁▁\n\n\nuncategorized_items\n0\n1\n0.21\n1.32\n0\n0\n0\n0\n48\n▇▁▁▁▁\n\n\nstationery\n0\n1\n0.19\n2.16\n0\n0\n0\n0\n100\n▇▁▁▁▁\n\n\ngoogle\n0\n1\n0.19\n3.39\n0\n0\n0\n0\n160\n▇▁▁▁▁\n\n\nwriting_instruments\n0\n1\n0.10\n0.99\n0\n0\n0\n0\n40\n▇▁▁▁▁\n\n\nfun\n0\n1\n0.04\n1.40\n0\n0\n0\n0\n75\n▇▁▁▁▁\n\n\nunknown\n0\n1\n0.16\n1.08\n0\n0\n0\n0\n40\n▇▁▁▁▁\n\n\nelectronics_accessories\n0\n1\n0.01\n0.24\n0\n0\n0\n0\n12\n▇▁▁▁▁\n\n\nnotebooks_journals\n0\n1\n0.01\n0.30\n0\n0\n0\n0\n12\n▇▁▁▁▁\n\n\ngift_cards\n0\n1\n0.02\n0.40\n0\n0\n0\n0\n15\n▇▁▁▁▁\n\n\nblack_lives_matter\n0\n1\n0.00\n0.02\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\n\n\n\nUpon visual inspection, the shape of the distributions are concerning. Each histogram exhibits the presence of highly skewed data. Outliers are most likely present, and they will need to be addressed. Otherwise, they’ll negatively impact the k-means clustering algorithm. The mean values for the features also indicate a likely issue: limited purchase frequency for certain item categories.\nBefore moving ahead with removing examples, dropping some features might be worth exploring. The objective here is to identify item categories with limited purchase frequency. Any item category with a limited purchase frequency could likely be dropped.\n\nsummarise(\n  data_user_items, \n  across(apparel:black_lives_matter, sum)\n) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 21\n$ apparel                 &lt;dbl&gt; 4169\n$ bags                    &lt;dbl&gt; 832\n$ shop_by_brand           &lt;dbl&gt; 889\n$ drinkware               &lt;dbl&gt; 893\n$ new                     &lt;dbl&gt; 1634\n$ clearance               &lt;dbl&gt; 684\n$ accessories             &lt;dbl&gt; 1438\n$ campus_collection       &lt;dbl&gt; 1700\n$ office                  &lt;dbl&gt; 1633\n$ lifestyle               &lt;dbl&gt; 607\n$ small_goods             &lt;dbl&gt; 143\n$ uncategorized_items     &lt;dbl&gt; 608\n$ stationery              &lt;dbl&gt; 554\n$ google                  &lt;dbl&gt; 572\n$ writing_instruments     &lt;dbl&gt; 291\n$ fun                     &lt;dbl&gt; 122\n$ unknown                 &lt;dbl&gt; 463\n$ electronics_accessories &lt;dbl&gt; 35\n$ notebooks_journals      &lt;dbl&gt; 43\n$ gift_cards              &lt;dbl&gt; 46\n$ black_lives_matter      &lt;dbl&gt; 1\n\n\nReviewing the output, small_goods; fun; electronics_accessories; notebooks_journals; gift_cards; and black_lives_matter have a small enough purcharse frequency to be dropped. Since we don’t have information on what is being purchased, the unknown feature is also another feature that could be dropped. Here’s the code to drop these features:\n\ndata_items &lt;- \n  data_user_items |&gt; \n  select(\n    -c(\n      small_goods,\n      fun,\n      electronics_accessories,\n      notebooks_journals,\n      gift_cards,\n      black_lives_matter,\n      unknown\n    )\n  )\n\nWhile the skimr::skim() output includes histograms, we’ll use ggplot2 to examine the distributions in more detail.\n\ndata_item_hist &lt;- data_items |&gt;\n  pivot_longer(\n    cols = apparel:stationery,\n    values_to = \"items\"\n  ) \n\nggplot(data_item_hist, aes(x = items)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~name, ncol = 4, scales = \"free\") +\n  labs(\n    title = \"Distribution of purchases by item category\",\n    y = \"\",\n    x = \"\"\n  )\n\n\n\n\n\n\n\n\nThe following histograms further confirm the presence of skewed data. It also further provides evidence of another characteristic of concern: low item purchase frequency. Both of these issues will need to be addressed before applying k-means clustering.\nLet’s first normalize the data, so we can get values across features to be within a similar range. To do this, we’ll transform features into z-scores. The summary() function can be used to confirm the transformation was applied. This step can be done by utilizing the following code:\n\n\n\n\n\n\nNote\n\n\n\nscale() accepts a dataframe with only numeric features. So, I have to remove it, then add it back.\n\n\n\ndata_items_stnd &lt;-\n  as_tibble(scale(select(data_items, -user_id))) |&gt;\n  mutate(user_id = data_items$user_id, .before = 1)\n\n# Verify the standarization was applied\nsummary(data_items_stnd)\n\n   user_id             apparel             bags         shop_by_brand       drinkware      \n Length:2941        Min.   :-0.6655   Min.   :-0.1399   Min.   :-0.2297   Min.   :-0.2032  \n Class :character   1st Qu.:-0.6655   1st Qu.:-0.1399   1st Qu.:-0.2297   1st Qu.:-0.2032  \n Mode  :character   Median :-0.1960   Median :-0.1399   Median :-0.2297   Median :-0.2032  \n                    Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n                    3rd Qu.: 0.2734   3rd Qu.:-0.1399   3rd Qu.:-0.2297   3rd Qu.:-0.2032  \n                    Max.   :12.4797   Max.   :36.9390   Max.   :30.1695   Max.   :19.8776  \n      new            clearance        accessories     campus_collection     office       \n Min.   :-0.2391   Min.   :-0.2252   Min.   :-0.219   Min.   :-0.1701   Min.   :-0.1728  \n 1st Qu.:-0.2391   1st Qu.:-0.2252   1st Qu.:-0.219   1st Qu.:-0.1701   1st Qu.:-0.1728  \n Median :-0.2391   Median :-0.2252   Median :-0.219   Median :-0.1701   Median :-0.1728  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.:-0.2391   3rd Qu.:-0.2252   3rd Qu.:-0.219   3rd Qu.:-0.1701   3rd Qu.:-0.1728  \n Max.   :37.2019   Max.   :19.1369   Max.   :21.277   Max.   :37.4975   Max.   :24.7254  \n   lifestyle       uncategorized_items   stationery           google         writing_instruments\n Min.   :-0.2135   Min.   :-0.1566     Min.   :-0.08721   Min.   :-0.05737   Min.   :-0.1002    \n 1st Qu.:-0.2135   1st Qu.:-0.1566     1st Qu.:-0.08721   1st Qu.:-0.05737   1st Qu.:-0.1002    \n Median :-0.2135   Median :-0.1566     Median :-0.08721   Median :-0.05737   Median :-0.1002    \n Mean   : 0.0000   Mean   : 0.0000     Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000    \n 3rd Qu.:-0.2135   3rd Qu.:-0.1566     3rd Qu.:-0.08721   3rd Qu.:-0.05737   3rd Qu.:-0.1002    \n Max.   :20.4745   Max.   :36.2012     Max.   :46.21115   Max.   :47.13470   Max.   :40.4114    \n\n\nReviewing the summary() output post normalization, the maximum values are concerning. There are some users who purchased items within categories at a very significant rate. Although this is a nice problem to have for the merchandise store (i.e., big purchases are good for the bottom line), it may cause problems when specifying the clustering algorithm.\nIndeed, a couple of approaches could be taken here. For one, the outliers could be retained. This will likely highly affect the k-means algorithms’ ability to effectively identify useful clusters, though. The second option is to drop examples that we consider to be outliers. Let’s think this through a bit.\nMore than likely, any example with the number of items purchased above 3 standard deviations beyond the mean should be dropped. The merchandise store is likely meant for business-to-consumer (B2C) sales, rather than business-to-business (B2B) sales. As such, the amount of items purchased during a typical customer transaction will likely be of a volume that is reasonable for consumer purchases (e.g., who needs more than 50 items of stationery?). Such large purchases are likely a B2B transaction, where large volumes of items are being bought. Given the intent of the store to be B2C, then examples exhibiting such large purchase volumes should be dropped.\nAdditional information could be used to further verify this assumption. We likely have a Customer Relationship Management (CRM) system with information about who the customers of these purchases are, and thus we could use this information to confirm if a purchase was for a business or individual. Since the ability to obtain this additional information is not possible, dropping these outliers before clustering is the best option. With all that said, here’s the code to further explore customers considered to be outliers. head() is used here to limit the output.\n\ndata_items_stnd |&gt;\n  filter(if_any(-c(user_id), ~ . &gt; 3)) |&gt;\n  head(n = 10)\n\n# A tibble: 10 × 15\n   user_id    apparel   bags shop_by_brand drinkware    new clearance accessories campus_collection\n   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 qifwtgogoz   6.38  -0.140        -0.230    -0.203 -0.239    -0.225      -0.219             7.78 \n 2 dootkphyen  -0.665 -0.140         4.33     -0.203  2.34     -0.225      -0.219            -0.170\n 3 qksqqiqfjm  -0.665 -0.140         1.29     -0.203  0.191     0.743       0.677             0.124\n 4 cdykzwveuk  -0.665 -0.140        -0.230    10.5   -0.239    -0.225      -0.219            -0.170\n 5 yigpconufy  -0.665 -0.140        -0.230    -0.203  6.65     -0.225      -0.219            -0.170\n 6 ubblgupnrd   4.97  -0.140        -0.230     5.82  -0.239    -0.225      -0.219             1.30 \n 7 shyjuzbuci  -0.665 36.9          -0.230    -0.203 -0.239    -0.225      -0.219            -0.170\n 8 cbpqtertvd  -0.665 -0.140         4.33     -0.203 -0.239    -0.225      -0.219            -0.170\n 9 djoibydgth   0.743 -0.140         0.530     2.47   1.05     -0.225      -0.219            -0.170\n10 zqgnutolrn   3.09  -0.140        -0.230    -0.203 12.7      -0.225      -0.219            -0.170\n# ℹ 6 more variables: office &lt;dbl&gt;, lifestyle &lt;dbl&gt;, uncategorized_items &lt;dbl&gt;, stationery &lt;dbl&gt;,\n#   google &lt;dbl&gt;, writing_instruments &lt;dbl&gt;\n\n\nHere’s the code to filter out customers considered to be outliers:\n\ndata_items_stnd &lt;- data_items_stnd |&gt;\n  select(-user_id) |&gt;\n  filter(!if_any(everything(), ~ . &gt; 3)) \n\nsummary(data_items_stnd)\n\n    apparel              bags          shop_by_brand        drinkware             new          \n Min.   :-0.66549   Min.   :-0.13986   Min.   :-0.22973   Min.   :-0.20324   Min.   :-0.23910  \n 1st Qu.:-0.66549   1st Qu.:-0.13986   1st Qu.:-0.22973   1st Qu.:-0.20324   1st Qu.:-0.23910  \n Median :-0.19602   Median :-0.13986   Median :-0.22973   Median :-0.20324   Median :-0.23910  \n Mean   :-0.09975   Mean   :-0.05728   Mean   :-0.07586   Mean   :-0.08145   Mean   :-0.08105  \n 3rd Qu.: 0.27345   3rd Qu.:-0.13986   3rd Qu.:-0.22973   3rd Qu.:-0.20324   3rd Qu.:-0.23910  \n Max.   : 2.62080   Max.   : 2.82645   Max.   : 2.81020   Max.   : 2.47420   Max.   : 2.77339  \n   clearance         accessories       campus_collection      office           lifestyle       \n Min.   :-0.22516   Min.   :-0.21897   Min.   :-0.17010   Min.   :-0.17281   Min.   :-0.21349  \n 1st Qu.:-0.22516   1st Qu.:-0.21897   1st Qu.:-0.17010   1st Qu.:-0.17281   1st Qu.:-0.21349  \n Median :-0.22516   Median :-0.21897   Median :-0.17010   Median :-0.17281   Median :-0.21349  \n Mean   :-0.07933   Mean   :-0.08889   Mean   :-0.05533   Mean   :-0.09425   Mean   :-0.07426  \n 3rd Qu.:-0.22516   3rd Qu.:-0.21897   3rd Qu.:-0.17010   3rd Qu.:-0.17281   3rd Qu.:-0.21349  \n Max.   : 2.67916   Max.   : 2.91591   Max.   : 2.77268   Max.   : 2.93947   Max.   : 2.88970  \n uncategorized_items   stationery           google         writing_instruments\n Min.   :-0.15659    Min.   :-0.08721   Min.   :-0.05737   Min.   :-0.10021   \n 1st Qu.:-0.15659    1st Qu.:-0.08721   1st Qu.:-0.05737   1st Qu.:-0.10021   \n Median :-0.15659    Median :-0.08721   Median :-0.05737   Median :-0.10021   \n Mean   :-0.04983    Mean   :-0.05096   Mean   :-0.03856   Mean   :-0.05943   \n 3rd Qu.:-0.15659    3rd Qu.:-0.08721   3rd Qu.:-0.05737   3rd Qu.:-0.10021   \n Max.   : 2.87322    Max.   : 2.69069   Max.   : 1.71234   Max.   : 2.93816   \n\n\nLet’s take a look at the histograms again, just to get a sense if dropping outliers helped.\n\ndata_items_stnd |&gt;\n  pivot_longer(\n    cols = apparel:stationery,\n    values_to = \"items\"\n  ) |&gt; \n  ggplot(aes(x = items)) +\n    geom_histogram(binwidth = 1) +\n    facet_wrap(~name, ncol = 4, scales = \"free\") +\n    labs(\n      title = \"Distribution of purchases by item category post wrangling\",\n      y = \"\",\n      x = \"\"\n    )\n\n\n\n\n\n\n\n\nAlthough this helped with the issues caused by outliers, we still have to contend with the fact that some customers just don’t buy certain items. We’ll want to keep this in mind when drawing conclusions from the final clusters.\n\n\nDetermine a k-value\nNow that the data is in a format acceptable for modeling, we need to explore a value for the number of cluster centers, the k-value. Various methods can be used to determine this value. I’ll rely on three methods here: the elbow method; the average silhouette method; and using the realities imposed by the business case (Lantz 2023). Each will be discussed more in-depth in the following sections.\nThe first method is the elbow method, where we visually examine an elbow plot of the total within sum of squares based on the number of potential clusters used for the model. The fviz_nbclust() function from the factoextra package is useful here. We first pass in the data we intend to use for modeling, then base R’s stats kmeans() function. We’re also interested in creating the plot using the within sum of squares method, so we specifiy that using the method argument.\n\nfviz_nbclust(data_items_stnd, kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nGiven visual examination of the plot, six clusters seems to be a good starting point. The average silhouette method is another visulization useful for confirming the number of cluster groups for our cluster modelling.\n\nfviz_nbclust(data_items_stnd, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nJust as was expected, the silhouette method provides additional evidence for 6 clusters.\nConsidering the business case is another useful method for determining the k-value. For instance, say our goal is to cluster the data based on the number of campaigns our marketing team has capacity to manage. The number used here is completely arbitrary (i.e., I don’t have a marketing team to confirm capacity). Thus, for the sake of example, let’s say we have a marketing team capable of managing 3 campaigns over the holiday season.\nSo, based on the information above, let’s examine k-means clustering using 3 and 6 groups.\n\n\nSpecify the model\nNow the modeling step. We’ll use base R’s kmeans() function from the stats package. Since a clustering model with 3 or 6 clusters is being explored here, purrr’s map() function is used to iterate the model specification. map() returns a list object, which each element of the list is a model output. One for the three and six cluster model. set.seed() is also used for the reproducibility of the results.\n\nset.seed(20240722)\nmdl_clusters &lt;- \n  map(c(3, 6), \\(x) kmeans(data_items_stnd, centers = x))\n\n\n\nEvaluating model performance\nTo assess model fit, we’ll look to the cluster sizes for both clustering models. purrr’s map function makes this easy. Use the following code to return the size element of kmeans output:\n\nmap(mdl_clusters, \"size\")\n\n[[1]]\n[1] 1642  762  278\n\n[[2]]\n[1]  223  296 1284  660   80  139\n\n\nIdentifying imbalanced groups is priority here. Some imbalance is tolerable, but major imbalances might indicate the presence of model fit issues. The six cluster model looks fairly balanced, where only one group includes a small subset of customers (~80 customers). The three cluster model has one larger group followed by ever decreasing sized groups. Overall, the balance across the different groups seems to be acceptable here.\nVisualizing the clusters is also useful for model assessment. The factoextra package is once again helpful. The fviz_cluster() function from the package can be used to visualize the clusters. The function first takes our model output (mdl_clsuters[[1]]) and the initial data object (data_item_stnd) as arguments. Both the three and six cluster model are visualized using the example code below.\n\nfviz_cluster(mdl_clusters[[1]], data = data_items_stnd, pointsize = 3) +\n  labs(\n    title = \"Three-cluster model of Google Merchandise Store customers\",\n    subtitle = \"View of cluster groupings during the U.S. holiday season\"\n  ) +\n  theme_replace(ggplot_theme)\n\n\n\n\n\n\n\n\n\nfviz_cluster(mdl_clusters[[2]], data = data_items_stnd, pointsize = 3) +\n  labs(\n    title = \"Six-cluster model of Google Merchandise Store customers\",\n    subtitle = \"View of cluster groupings during the U.S. holiday season\"\n  ) +\n  theme_replace(ggplot_theme)\n\n\n\n\n\n\n\n\nfviz_cluster() uses dimension reduction methods to allow for plotting of a multi-dimensional dataset into a two-dimensional representation. The output is useful for identifying general clustering patterns within the data, which could provide additional information about the shape of the clusters. For instance, this type of visualization can be used to visually identify any outliers which may be influencing the shape clusters take.\nIndeed, the plot for both models shows some cluster overlap. This is an indication that the clusters for this data may not be as distinct as we would like. There might be some common products every customer buys, and a few peripheral products that other clusters purchase beyond these common products. These initial results indicate the presence of ‘upsell’ opportunities for the marketing team. You have your core items that most customers purchase, but some clusters seem to purchase items beyond the core products. Thus, some of the marketing campaigns might strategise ways to highlight the upselling of some products.\n\n\nDraw conclusions about clusters\nThe next step is to examine the cluster centers and factor loadings. The goal is to derive conclusions about each cluster from this information. Let’s first draw conclusions using our three-cluster model. We’ll look to identify specific audience segments based on item category loadings.\nThe kmeans()’s output has an object labelled centers, which is a matrix of cluster centers. We then inspect these center values for each cluster, which are on the left, and the values associated within each column.\n\nmdl_clusters[[1]]$centers \n\n     apparel        bags shop_by_brand   drinkware         new  clearance  accessories\n1 -0.4476288 -0.06097525   -0.06495514 -0.08339455 -0.09966977 -0.2251558 -0.096510610\n2  0.7127262 -0.06395056   -0.09907281 -0.07850703 -0.07644876 -0.1933938 -0.103778685\n3 -0.2720182 -0.01715296   -0.07663600 -0.07803938  0.01632421  1.0946694 -0.003105749\n  campus_collection        office   lifestyle uncategorized_items  stationery      google\n1       -0.05701557 -0.1077973177 -0.09379899        -0.073095026 -0.06268202 -0.04012104\n2       -0.09325080 -0.0992918176 -0.05330947        -0.005497098 -0.05926368 -0.04110829\n3        0.05854479 -0.0004035005 -0.01628649        -0.033980814  0.04102364 -0.02235330\n  writing_instruments\n1        -0.062586565\n2        -0.072300004\n3        -0.005490095\n\n\n\nmdl_clusters[[1]]$centers |&gt;\n  as_tibble() |&gt;\n  mutate(\n    cluster = 1:3,\n    across(where(is.double), \\(x) round(x, digits = 4)),\n    .before = 1\n  ) |&gt;\n  gt() |&gt;\n  data_color(\n    columns = !(\"cluster\"),\n    rows = 1:3,\n    direction = \"row\",\n    method = \"numeric\",\n    palette = \"Blues\"\n  ) |&gt;\n  tab_header(\n    title = md(\n      \"**Three-cluster k-means model of Google Merchandise store customers factor loadings**\"\n    )\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"Source: Google Analytics data for the Google Merchandise Store\"\n    )\n  ) |&gt;\n  opt_interactive()\n\n\n\nTable 1: Three-cluster model factor loadings\n\n\n\n\n\n\nThree-cluster k-means model of Google Merchandise store customers factor loadings\n\n\n\n\n\n\n\nSource: Google Analytics data for the Google Merchandise Store\n\n\n\n\n\n\n\n\n\n\n\n\nAfter inspecting the three-cluster k-means model factor loadings, a few groups emerge. Cluster 1 (n = 1,642) seems to be the ‘anything but apparel’ customers. This customer segment really doesn’t purchase any specific item, but when they shop, they’re purchasing items other than apparel. Perhaps a campaign could be created to improve customer’s familiarity with products other than apparel that are available in the merchandise store.\nCluster 2 (n = 762) are the ‘fashion fanatics’. In fact, it seems this group is mainly purchasing apparel. Maybe our product and marketing teams could consider releasing a new apparel line around the holiday season. A campaign focused on highlighting different apparel pieces could also be explored.\nCluster 3 (n = 278) are ‘discount diggers’. Indeed, this data covers the holiday season, so maybe some customers around this time are trying to find a unique gift, but want to do so on a budget. Perhaps a campaign focused on ‘holiday gift deals’ might appeal to these types of customers.\nIf more nuance is required for the segmentation discussion, the factor loadings for the six-cluster model can be examined. Again, these results suggest the presence of ‘anything but apparel customers’; ‘fashion fanatics’; and ‘discount diggers’. However, three additional groups emerge from the six cluster model.\n\nmdl_clusters[[2]]$centers |&gt;\n  as_tibble() |&gt;\n  mutate(\n    cluster = 1:6,\n    across(where(is.double), \\(x) round(x, digits = 4)),\n    .before = 1\n  ) |&gt;\n  gt() |&gt;\n  data_color(\n    columns = !(\"cluster\"),\n    rows = 1:6,\n    direction = \"row\",\n    method = \"numeric\",\n    palette = \"Blues\"\n  ) |&gt;\n  tab_header(\n    title = md(\n      \"**Six-cluster k-means model of Google Merchandise store customers factor loadings**\"\n    )\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"Source: Google Analytics data for the Google Merchandise Store\"\n    )\n  ) |&gt;\n  opt_interactive()\n\n\n\nTable 2: Six-cluster model factor loadings\n\n\n\n\n\n\nSix-cluster k-means model of Google Merchandise store customers factor loadings\n\n\n\n\n\n\n\nSource: Google Analytics data for the Google Merchandise Store\n\n\n\n\n\n\n\n\n\n\n\n\nThe first additional segment emerging from the six-cluster model includes ‘lifestyle looters’ (n = 223): the customers who purchase products that fit or enhance their lifestyle. Perhaps there’s room for a campaign focused on highlighting how the Google Merchandise Store’s products fit within the lives of its customers: most likely people who work in tech.\nThe second segment are the ‘brand buyers’ (n = 296). These customers are mostly interested in purchasing branded items. Thus, a campaign highlighting the various branded items that are available might be explored.\nThe final group to emerge is our ‘accessory enthusiasts’ (n = 139). These are customers most interested in purchasing accessories. Perhaps a focus on accessories could be another campaign our marketing team might look at to create.\nDepending on the model reviewed, clustering resulted in the identification of three customer segments campaigns could be targeted: ‘anything but apparel’, ‘fashion fanatics’, and ‘discount diggers’. If an expanded list of segments was required, the six-cluster model provides additional information. This includes segments like ‘lifestyle looters’, ‘brand buyers’, and ‘accessory enthusiasts’. Indeed, the segment names are up for debate. I would lean on my marketing team to workshop them some more. Analysts are poor at naming things.\n\n\nWrap up\nThis post was a tutorial on how to perform clustering using Google Analytics e-commerce data. The k-means algorithm was used to identify clusters within obfuscated analytics data from the Google Merchandise store. Information about the clusters was used to generate various customer segments. The intent was to use these clusters to better inform future marketing campaigns and targeting strategies.\nThe kmeans() function from the base R stats package was used to specify two clustering models. The elbow method, silhouette method, and information about the business case were used to determine the k-values for the clustering models. The fviz_nbclust() function from the factoextra package was useful for creating visualizations to further confirm the selected k-values. It was determined both a three and six cluster model would be effective to meet our modelling goal for this data. Lastly, the fviz_cluster() function from the factoextra package was used to create visualizations of each model’s clusters.\nIn truth, this dataset lacked the presence of any interesting cluster groups. I was hoping for some more cross-product segments that could be used for customer segmentation identification. Unfortunately, this wasn’t the case. Many of the Google Merchandise Store’s customers fell within a single item-category. This was likely due to low purchase frequency for item categories, which likely is due to customers only buying one or two products with each purchase. Nonetheless, we were able to still identify various customer segments useful for targeting purposes and provide some additional support for potential marketing campaigns.\nSo there you have it, another messing with models post. I hope you found something useful. If not, I hope it was somewhat informative and you found a few takeaways.\nUntil next time, keep messing with models.\n\n\n\n\n\nReferences\n\nLantz, Brett. 2023. Machine Learning with R. 4th ed. packt. https://www.oreilly.com/library/view/machine-learning-with/9781801071321/.\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Messing with Models: K-Means Clustering of {Google}\n    {Analytics} 4 Data},\n  date = {2024-09-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Messing with Models: K-Means Clustering of\nGoogle Analytics 4 Data.” September 14, 2024."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "",
    "text": "Photo by T K\nI had the great fortune of being a presenter at this year’s PBS TechCon conference. The focus of my talk was to introduce attendees to the principles of tidy data and discuss a data pipeline project my team has been working on at Nebraska Public Media. Here’s the session description:\nAs part of my talk, I mentioned having put together a curated list of resources others could use to learn more about the topics covered. This list can be found in the following section of this blog post. If you’re interested in discussing these topics further, please reach out."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nTidy Data paper published in the Journal of Statistical Software written by Hadley Wickham\nTidy Data Chapter published in the open source R for Data Science book written by Hadley Wickham and Garrett Grolemun\nData Organization: Organizing Data in Spreadsheets post by Karl Broman\nData Organization: Organizing Data in Spreadsheets paper published in The American Statistician written by Karl Broman and Kara Woo\nTidy data section in Data Management in Large-Scale Education Research training modules written by Crystal Lewis\nTidy Data presented by Hadley Wickham\nTowards Data Science post published on Medium summarizing tidy data written by Benedict Neo"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Join a Community",
    "text": "Join a Community\n\nR for Data Science Online Learning Community\n\nJoin the Slack workspace\n@Collin Berke to get a hold of me in the workspace"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Open source workflow management tool",
    "text": "Open source workflow management tool\n\nApache Airflow"
  },
  {
    "objectID": "blog/posts/2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel/index.html",
    "href": "blog/posts/2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel/index.html",
    "title": "Exploring the relationship between trash processed by Mr. Trash Wheel and precipitation",
    "section": "",
    "text": "Photo by Nareeta Martin\n\n\n\n👋 Say hello to Mr. Trash Wheel and friends\nThis week’s #tidytuesday we’re looking into data related to Mr. Trash Wheel and friends. Mr. Trash Wheel is a semi-autonomous trash interceptor, who’s main purpose is to collect trash floating into the Baltimore Inner Harbor. Mr. Trash Wheel is a pretty neat invention. If you’re interested in how it works, check out the information found here.\nMy curiosity peaked when I came across the statement that most of the trash collected by Mr. Trash wheel is the result of water runoff, and not from people disposing trash directly into the habor. So, I wanted to explore the relationship between precipitation and the amount of trash being collected by Mr. Trash Wheel and friends for my contribution this week.\nIn this post, I created my visualizations using plotly and Tableau.\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(here)\nlibrary(janitor)\nlibrary(tidymodels)\ntidymodels_prefer()\n\n\ndata_trash &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel\", \n    \"trashwheel.csv\"\n  )\n)\n\nRows: 993 Columns: 16\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): ID, Name, Month, Date\ndbl (12): Dumpster, Year, Weight, Volume, PlasticBottles, Polystyrene, CigaretteButts, GlassBott...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_balt_precip &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel\", \n    \"balt_precip.csv\"\n  )\n)\n\nRows: 10 Columns: 13\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (13): year, january, february, march, april, may, june, july, august, september, october, no...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData description\nThe data contains observations related to trash collected from 2014 to 2023 by multiple trash wheels. The Baltimore precipitation data came from a tool found here. I simply just copy pasted this data into a Google sheet and saved it as a .csv file. Further wrangling steps for both data sets are included below.\nTo get a better sense of what’s in the data, I did a quick glimpse() and skim() of both the data_trash and data_balt_precip data sets.\n\nglimpse(data_trash)\n\nRows: 993\nColumns: 16\n$ ID             &lt;chr&gt; \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mist…\n$ Name           &lt;chr&gt; \"Mister Trash Wheel\", \"Mister Trash Wheel\", \"Mister Trash Wheel\", \"Mister T…\n$ Dumpster       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, …\n$ Month          &lt;chr&gt; \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"June\", \"June\", \"Ju…\n$ Year           &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 201…\n$ Date           &lt;chr&gt; \"5/16/2014\", \"5/16/2014\", \"5/16/2014\", \"5/17/2014\", \"5/17/2014\", \"5/20/2014…\n$ Weight         &lt;dbl&gt; 4.31, 2.74, 3.45, 3.10, 4.06, 2.71, 1.91, 3.70, 2.52, 3.76, 3.43, 4.17, 5.1…\n$ Volume         &lt;dbl&gt; 18, 13, 15, 15, 18, 13, 8, 16, 14, 18, 15, 19, 15, 15, 15, 15, 13, 15, 15, …\n$ PlasticBottles &lt;dbl&gt; 1450, 1120, 2450, 2380, 980, 1430, 910, 3580, 2400, 1340, 740, 950, 530, 84…\n$ Polystyrene    &lt;dbl&gt; 1820, 1030, 3100, 2730, 870, 2140, 1090, 4310, 2790, 1730, 869, 1140, 630, …\n$ CigaretteButts &lt;dbl&gt; 126000, 91000, 105000, 100000, 120000, 90000, 56000, 112000, 98000, 130000,…\n$ GlassBottles   &lt;dbl&gt; 72, 42, 50, 52, 72, 46, 32, 58, 49, 75, 38, 45, 58, 62, 64, 56, 47, 65, 63,…\n$ PlasticBags    &lt;dbl&gt; 584, 496, 1080, 896, 368, 672, 416, 1552, 984, 448, 344, 520, 224, 344, 432…\n$ Wrappers       &lt;dbl&gt; 1162, 874, 2032, 1971, 753, 1144, 692, 3015, 1988, 1066, 544, 727, 361, 631…\n$ SportsBalls    &lt;dbl&gt; 7, 5, 6, 6, 7, 5, 3, 6, 6, 7, 6, 8, 6, 6, 6, 6, 5, 6, 6, 7, 6, 6, 6, 5, 6, …\n$ HomesPowered   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\nglimpse(data_balt_precip)\n\nRows: 10\nColumns: 13\n$ year      &lt;dbl&gt; 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023\n$ january   &lt;dbl&gt; 2.71, 3.89, 3.50, 2.69, 1.00, 3.15, 3.11, 2.15, 4.27, 1.68\n$ february  &lt;dbl&gt; 4.58, 2.24, 5.70, 1.46, 5.30, 3.64, 2.98, 4.85, 2.31, 2.18\n$ march     &lt;dbl&gt; 4.38, 4.67, 2.10, 3.82, 2.25, 4.14, 3.05, 3.90, 3.13, 1.49\n$ april     &lt;dbl&gt; 8.60, 4.30, 1.31, 3.52, 3.20, 1.46, 5.52, 2.07, 3.92, 4.12\n$ may       &lt;dbl&gt; 3.35, 2.10, 5.24, 5.64, 8.17, 5.51, 1.76, 3.63, 5.39, 0.55\n$ june      &lt;dbl&gt; 3.95, 13.09, 3.20, 1.40, 4.77, 2.95, 5.95, 2.75, 2.95, 4.31\n$ july      &lt;dbl&gt; 2.80, 3.49, 6.09, 7.11, 16.73, 3.85, 3.43, 3.65, 6.25, 6.84\n$ august    &lt;dbl&gt; 7.90, 2.46, 3.96, 4.60, 3.84, 2.39, 11.81, 4.36, 3.71, 3.73\n$ september &lt;dbl&gt; 3.21, 3.25, 4.36, 1.95, 9.19, 0.16, 4.48, 6.04, 3.35, 6.27\n$ october   &lt;dbl&gt; 4.16, 3.40, 0.78, 2.99, 2.69, 6.21, 4.36, 5.24, 4.66, 1.13\n$ november  &lt;dbl&gt; 3.36, 2.42, 1.51, 2.15, 8.14, 1.10, 6.35, 1.33, 2.44, 2.80\n$ december  &lt;dbl&gt; 3.58, 5.85, 2.77, 0.95, 6.54, 3.57, 4.58, 0.82, 4.80, 7.16\n\n\n\nskim(data_trash)\n\n\nData summary\n\n\nName\ndata_trash\n\n\nNumber of rows\n993\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n6\n9\n0\n4\n0\n\n\nName\n0\n1\n18\n21\n0\n4\n0\n\n\nMonth\n0\n1\n3\n9\n0\n14\n0\n\n\nDate\n0\n1\n6\n10\n0\n623\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDumpster\n0\n1.00\n230.88\n185.82\n1.00\n73.00\n176.00\n381.00\n629.00\n▇▅▂▂▂\n\n\nYear\n0\n1.00\n2019.57\n2.75\n2014.00\n2018.00\n2020.00\n2022.00\n2023.00\n▃▃▅▆▇\n\n\nWeight\n0\n1.00\n2.97\n0.84\n0.61\n2.45\n3.04\n3.53\n5.62\n▁▅▇▃▁\n\n\nVolume\n0\n1.00\n14.92\n1.61\n5.00\n15.00\n15.00\n15.00\n20.00\n▁▁▁▇▁\n\n\nPlasticBottles\n1\n1.00\n2219.33\n1650.45\n0.00\n987.50\n1900.00\n2900.00\n9830.00\n▇▆▁▁▁\n\n\nPolystyrene\n1\n1.00\n1436.87\n1832.43\n0.00\n240.00\n750.00\n2130.00\n11528.00\n▇▂▁▁▁\n\n\nCigaretteButts\n1\n1.00\n13728.12\n24049.61\n0.00\n2900.00\n4900.00\n12000.00\n310000.00\n▇▁▁▁▁\n\n\nGlassBottles\n251\n0.75\n20.96\n15.26\n0.00\n10.00\n18.00\n28.00\n110.00\n▇▃▁▁▁\n\n\nPlasticBags\n1\n1.00\n984.00\n1412.34\n0.00\n240.00\n540.00\n1210.00\n13450.00\n▇▁▁▁▁\n\n\nWrappers\n144\n0.85\n2238.76\n2712.85\n0.00\n880.00\n1400.00\n2490.00\n20100.00\n▇▁▁▁▁\n\n\nSportsBalls\n364\n0.63\n13.59\n9.74\n0.00\n6.00\n12.00\n20.00\n56.00\n▇▆▂▁▁\n\n\nHomesPowered\n0\n1.00\n45.85\n18.23\n0.00\n38.00\n49.00\n58.00\n94.00\n▂▂▇▅▁\n\n\n\n\nskim(data_balt_precip)\n\n\nData summary\n\n\nName\ndata_balt_precip\n\n\nNumber of rows\n10\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2018.50\n3.03\n2014.00\n2016.25\n2018.50\n2020.75\n2023.00\n▇▇▇▇▇\n\n\njanuary\n0\n1\n2.82\n1.00\n1.00\n2.28\n2.91\n3.41\n4.27\n▂▅▅▇▅\n\n\nfebruary\n0\n1\n3.52\n1.50\n1.46\n2.26\n3.31\n4.78\n5.70\n▇▅▂▅▅\n\n\nmarch\n0\n1\n3.29\n1.07\n1.49\n2.45\n3.47\n4.08\n4.67\n▅▂▅▅▇\n\n\napril\n0\n1\n3.80\n2.15\n1.31\n2.35\n3.72\n4.26\n8.60\n▆▇▃▁▂\n\n\nmay\n0\n1\n4.13\n2.28\n0.55\n2.41\n4.44\n5.48\n8.17\n▃▃▂▇▂\n\n\njune\n0\n1\n4.53\n3.26\n1.40\n2.95\n3.58\n4.65\n13.09\n▇▆▁▁▂\n\n\njuly\n0\n1\n6.02\n4.09\n2.80\n3.53\n4.97\n6.69\n16.73\n▇▆▁▁▂\n\n\naugust\n0\n1\n4.88\n2.87\n2.39\n3.71\n3.90\n4.54\n11.81\n▇▂▁▁▁\n\n\nseptember\n0\n1\n4.23\n2.51\n0.16\n3.22\n3.86\n5.65\n9.19\n▅▇▅▅▂\n\n\noctober\n0\n1\n3.56\n1.73\n0.78\n2.77\n3.78\n4.58\n6.21\n▅▂▅▇▅\n\n\nnovember\n0\n1\n3.16\n2.30\n1.10\n1.67\n2.43\n3.22\n8.14\n▇▂▁▁▁\n\n\ndecember\n0\n1\n4.06\n2.16\n0.82\n2.97\n4.08\n5.59\n7.16\n▅▂▇▅▅\n\n\n\n\n\nLooking further into the data, I noticed a few things of note. Here’s some things to keep in mind:\n\nThere are missing data (e.g., NAs) within several variables: PlasticBottles, Polystyrene, CigaretteButts, GlassBottles, PlasticBags, Wrappers, and SportsBalls. The documentation didn’t reference why these were missing and since I wasn’t using these for my contribution, I didn’t dig any further.\nThe month has an issue with capitalization. Some string formatting should fix this issue, though I’m not using this column for my contribution.\nThe Date column needed to be transformed into a date. This can be addressed by using some functions from the lubridate package.\n\n\n\nData wrangling\nNow that we have a better sense of the data, let’s wrangle it. Below is the code to wrangle both the data_balt_precip and data_trash data sets. Since my precipitation data was aggregated by month, I decided to aggregate the trash data by month.\n\n\n\n\n\n\nNote\n\n\n\nWhile working on my contribution, I learned dplyr’s transmute function is superseded, and it’s now suggested to use mutate()’s .keep = \"none' argument.\n\n\n\ndata_balt_precip &lt;- data_balt_precip |&gt;\n  pivot_longer(cols = january:december, names_to = \"month\", values_to = \"precip\") |&gt;\n  mutate(\n    month = match(month, str_to_lower(month.name)),\n    day = 1,\n    month_date = ymd(str_c(year, month, day, sep = \"-\"))\n  ) |&gt;\n  select(\n    month_date, \n    precip\n  )\n\n\ndata_trash &lt;- data_trash |&gt;\n  clean_names() |&gt;\n  mutate(\n    id,\n    name,\n    date = mdy(date),\n    month_date = floor_date(date, \"month\"),\n    dumpster,\n    name = str_to_lower(name),\n    weight,\n    volume,\n    .keep = \"none\"\n  ) \n\n\ndata_trash_summ &lt;- data_trash |&gt;\n  group_by(month_date) |&gt;\n  summarise(\n    total_weight = sum(weight),\n    total_volume = sum(volume) \n  ) |&gt;\n  left_join(data_balt_precip)\n\nJoining with `by = join_by(month_date)`\n\nmin(data_trash_summ$month_date)\n\n[1] \"2014-05-01\"\n\nmax(data_trash_summ$month_date)\n\n[1] \"2023-12-01\"\n\n\n\n\nWhat is the relationship between rainfall and the weight and volume of trash processed by the trash wheels?\nTo explore this relationship, I created two scatter plots. The first plot included precipitation and total weight. The second included volume and precipitation. I did this because weight and volume represent different things. Here’s the code to create the two scatter plots using plotly:\n\nplot_ly(\n  data = data_trash_summ,\n  x = ~precip,\n  y = ~total_weight,\n  type = \"scatter\", \n  mode = \"markers\",\n  marker = list(\n    size = 10, \n    color = \"#6495ED\",\n    line = list(\n      color = \"#151B54\",\n      width = 2\n    )\n  ),\n  text = ~paste(\n    month_date,\n    \"&lt;br&gt;Precipitation (inches): \", precip, \n    \"&lt;br&gt;Weight (tons): \", total_weight\n  ),\n  hoverinfo = \"text\"\n) |&gt;\nplotly::layout(\n  title = list(\n    text = \"&lt;b&gt;More precipitation is related to heavier amounts of trash for Mr. Trash Wheel and friends to process &lt;/b&gt;\",\n    font = list(size = 18),\n    xanchor = \"center\"\n  ),\n  yaxis = list(\n    title = \"Total weight of trash (tons)/month\",\n    titlefont = list(size = 14)\n  ),\n  xaxis = list(\n    title = \"Total precipitation in Baltimore (inches)/month\",\n    titlefont = list(size = 14)\n  ),\n  font  = list(family = \"arial\", size = 18, face = \"bold\")\n)\n\n\n\n\n\n\nplot_ly(\n  data = data_trash_summ,\n  x = ~precip,\n  y = ~total_volume,\n  type = \"scatter\", \n  mode = \"markers\",\n  marker = list(\n    size = 10, \n    color = \"#FFAA33\",\n    line = list(\n      color = \"#151B54\",\n      width = 2\n    )\n  ),\n  text = ~paste(\n    month_date,\n    \"&lt;br&gt;Precipitation (inches): \", precip, \n    \"&lt;br&gt;Volume (cubic yards): \", total_volume \n  ),\n  hoverinfo = \"text\"\n) |&gt;\nplotly::layout(\n  title = list(\n    text = \"&lt;b&gt;More precipitation is related to a greater volume of trash for Mr. Trash Wheel and friends to process&lt;/b&gt;\",\n    font = list(size = 18),\n    xanchor = \"center\"\n  ),\n  yaxis = list(\n    title = \"Total volume of trash (cubic yards)/month\",\n    titlefont = list(size = 14)\n  ),\n  xaxis = list(\n    title = \"Total precipitation in Baltimore (inches)/month\",\n    titlefont = list(size = 14)\n  ),\n  font  = list(family = \"arial\", size = 18, face = \"bold\")\n)\n\n\n\n\n\nLooking at the individual observations, I had a hard time fathoming how much trash Mr. Trash Wheel and friends were processing. So, here’s a video giving you a sense of dimension of how much trash is really being collected–it’s a lot once you put it into perspective. I mean, in one month, the trash wheels processed nearly 25 of these 20 cubic yard dumpsters worth of trash. If you’ve ever seen these dumpters in real-life, they’re huge.\nAlthough upon visual inspection it seems a positive relationship is present for both weight and volume of trash, I wanted to further quantify this relationship using a linear model. To do this, I utilized tidymodels to create two simple linear models, one for volume and the other for weight of trash.\n\nlm_mdl &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n\n\nvolume_mdl &lt;- \n  lm_mdl |&gt;\n  fit(total_volume ~ precip, data = data_trash_summ)\n\ntidy(volume_mdl)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     65.4     16.8       3.90 0.000167 \n2 precip          16.0      3.59      4.47 0.0000186\n\n\n\nweight_mdl &lt;- \n  lm_mdl |&gt;\n  fit(total_weight ~ precip, data = data_trash_summ)\n\ntidy(weight_mdl)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    11.6      3.68       3.16 0.00200  \n2 precip          3.53     0.785      4.49 0.0000172\n\n\nBoth models indicate a statistically significant positive relationship between precipitation, volume, and weight of trash processed. In fact, for every additional inch of precipitation a month in Baltimore, the volume of trash processed increases by 16 cubic yards and the weight of trash increases by 3.53 tons.\nThe bottom line, throw your trash away properly. It has down stream effects, literally … no pun intended.\n\n\nAn attempt using Tableau\nTo further practice my data visualization tool skills, I recreated these plots using Tableau. You can view this version by clicking here.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring the Relationship Between Trash Processed by {Mr.}\n    {Trash} {Wheel} and Precipitation},\n  date = {2024-03-12},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Exploring the Relationship Between Trash\nProcessed by Mr. Trash Wheel and Precipitation.” March 12, 2024."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Workflow walkthrough: Interacting with Google BigQuery in R\n\n\n\n\n\n\nworkflow\n\n\ntutorial\n\n\nproductivity\n\n\nbigquery\n\n\nsql\n\n\n\nA tutorial on how to use the bigrquery package\n\n\n\n\n\nOct 5, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: k-means clustering of Google Analytics 4 data\n\n\n\n\n\n\nmachine learning\n\n\nunsupervised learning\n\n\nk-means clustering\n\n\n\nA tutorial on how to perform k-means clustering using Google Analytics data\n\n\n\n\n\nSep 14, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hex Update: June, 2024\n\n\n\n\n\n\nthe hex update\n\n\nmedia\n\n\n\nKey insights and what I learned about the media industry in June 2024\n\n\n\n\n\nJul 10, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: Market basket analysis of online merchandise store data\n\n\n\n\n\n\nmachine learning\n\n\nunsupervised learning\n\n\nassociation rules\n\n\nmarket basket analysis\n\n\n\nA tutorial on how to perform a market basket analysis using Google Analytics data\n\n\n\n\n\nJun 11, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring objects launched into space and gross domestic product\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nregression\n\n\n\nA contribution to the 2024-04-23 #tidytuesday social data project\n\n\n\n\n\nMay 3, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring data from the Fiscal Sponsor Directory\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-03-12 #tidytuesday social data project\n\n\n\n\n\nMar 22, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the relationship between trash processed by Mr. Trash Wheel and precipitation\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-03-05 #tidytuesday social data project\n\n\n\n\n\nMar 12, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the lifespans of historical figures born on a Leap Day\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-02-27 #tidytuesday social data project\n\n\n\n\n\nMar 5, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring R Consortium ISC Grants\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-02-20 #tidytuesday social data project\n\n\n\n\n\nFeb 26, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n30 day tidymodels recipes challenge\n\n\n\n\n\n\nmachine learning\n\n\nfeature engineering\n\n\ntidymodels\n\n\ndata wrangling\n\n\n\nLearning how to use the recipes package, one day at a time\n\n\n\n\n\nJan 1, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data\n\n\n\n\n\n\ntutorial\n\n\ntidymodels\n\n\nclassification\n\n\ndecision tree\n\n\nlogistic regression\n\n\n\nUsing tidymodels to predict wins and losses for volleyball matches\n\n\n\n\n\nDec 7, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n2023 data science rig: Set up and configuration\n\n\n\n\n\n\ntutorial\n\n\n\nOverviewing and reflecting on my current data science setup.\n\n\n\n\n\nJan 29, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nFlattening Google Analytics 4 data\n\n\n\n\n\n\nBigQuery\n\n\nsql\n\n\ndata wrangling\n\n\n\nLet’s deep dive into working with Google Analytics data stored in BigQuery.\n\n\n\n\n\nSep 20, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory analysis of Google Analytics 4 data for forecasting models\n\n\n\n\n\n\nforecasting\n\n\n\nExploring Google Analytics 4 data for forecasting models.\n\n\n\n\n\nMar 3, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nShiny summary tiles\n\n\n\n\n\n\nshiny\n\n\n\nBuilding custom metric summary tiles for Shiny.\n\n\n\n\n\nDec 30, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n2021 PBS TechCon: Your Data is Disgusting!\n\n\n\n\n\n\ntalks\n\n\n\nI was fortunate to be invited to present about topics I’m passionate about: tidy data and data pipelines.\n\n\n\n\n\nOct 19, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a next and back button in Shiny\n\n\n\n\n\n\nshiny\n\n\n\nTaking the time to understand a challenging question from Mastering Shiny.\n\n\n\n\n\nSep 12, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nIntro Post\n\n\n\n\n\n\npersonal\n\n\n\nHello World!, my name is Collin, and this is my blog.\n\n\n\n\n\nApr 2, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/posts/2024-02-10-til-r-dput-store-objects/index.html",
    "href": "til/posts/2024-02-10-til-r-dput-store-objects/index.html",
    "title": "Use base::dput() to easily create and save objects",
    "section": "",
    "text": "Image generated using the prompt ‘Robot manufacturing several widgets on a conveyor belt in a pop art style’ with the Bing Image Creator\n\n\n\nlibrary(tidyverse)\nlibrary(testthat)\n\n\nBackground\nLately, I’ve been doing a lot of data validation tests for a package I’m working on. Using testthat for the testing framework, some of the tests I’m writing verify dataset column names. For instance, these tests tend to look something like this:\n\ntest_that(\"column names are as expected\", {\n  expect_named(\n    mtcars, \n    c(\"mpg\", \"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\")\n  )\n})\n\nTest passed 🎉\n\n\nSince mtcars only has 11 columns, the character vector used for the column name test is pretty small. Creating this by hand isn’t too bad. However, what if we need to create a character vector for a dataset much larger than this. Say a dataset with 150+ columns–soul crushing. I don’t know about you, but I would hate to hand key a character vector this long (I’m sad to report I’ve done this more times than I would like to admit). Of course, there’s a better way. Use base::dput().\n\n\nTIL: Use dput()\nAccording to the docs, dput:\n\nWrites an ASCII text representation of an R object to a file, the R console, or a connection, or uses one to recreate the object.\n\nNow that we have a tool to make this easier, all we need to do is pass the data to names(), and then wrap dput() around the return value of names(). What results is a character vector that gets printed to the console. All we need to do now is copy and paste this output into our file. This is what this looks like:\n\ndput(names(mtcars))\n\nc(\"mpg\", \"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \n\"gear\", \"carb\")\n\n\nPretty neat.\ndput() also has a file argument, so you can pass along a file string the object will be written. Since I tend to save multiple objects in one file from time to time in a tests fixtures file, I rarely output to a file. Here’s the code to output the object to a file if your interested, though:\n\ndput(\n  names(mtcars), \n  here::here(\"til/posts/2024-02-10-til-r-dput-store-objects/mtcars-names.R\")\n)\n\n\n\nOne more tip, if you use vim or nvim\nI’m particular with how I style long character vectors within a file. If the objects can’t fit on one line, each will be placed on their own line. So you can output your object and use the following substitution command to place each object on it’s own line.\n:.,+1s/, /,\\r/g\nThis command will make our object look like this:\n\n\n\nUse substitution to finish cleaning up the character vector\n\n\nIndeed, it’s not perfect, but it’s close. We only needed to make some minor edits to finish it. But in the end, we’ve saved so much time, and we have a well formatted character vector.\n\n\nWrap up\nI wish I came across dput() much earlier. Not only is it one of those entire workflow changing tips, it’s one that would have saved me so much time. Hopefully if you’re reading this post, you avoid hand creating large character vectors and just use base::dput().\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Use `Base::dput()` to Easily Create and Save Objects},\n  date = {2024-02-10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Use `Base::dput()` to Easily Create and\nSave Objects.” February 10, 2024."
  },
  {
    "objectID": "til/posts/2023-10-24-temp-directories/index.html",
    "href": "til/posts/2023-10-24-temp-directories/index.html",
    "title": "Using base::tempdir() for temporary data storage",
    "section": "",
    "text": "Photo by Jesse Orrico\n\n\nToday I learned how to store data in R’s per-session temporary directory.\nRecently, I’ve been working on an R package for a project. This package contains some internal data, which is intended to be updated from time-to-time. As part of the data update process, I’m required to download a set of .zip files from cloud storage, unzip, wrangle, and make the data available in the package via the data folder.\nGiven the data I’m working with, I wanted to avoid storing pre-wrangled data in the data-raw directory of the package. My main concern was an accidental check-in of pre-proccessed data into version control. So, I sought out a means to solve this problem.\nThis post aims to overview an approach using R’s per-session temporary directory to store data temporarily. Specifically, this post will discuss the use of base::tempdir() and other system file management functions made available in R to store data in this directory.\n\n\n\n\n\n\nWarning\n\n\n\nUsing R’s per-session temporary directory may not be the right solution for your specific situation. If you’re working with sensitive data, make sure you follow your organization’s guidelines on where to store, access, and properly use your data.\nI am not a security expert.\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat are temporary directories?\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, I drafted this post on a computer running a Mac operating system. Some of what gets discussed here may not apply to Windows or Linux systems. The ideas and application should be similar, though I haven’t fully explored the differences.\n\n\nThe temporary directory, simply, is a location on your system. You can store files in this location just like any other directory. The difference is data stored within a temporary directory are not meant to be persistent, and your system will delete them automatically. File deletion either occurs when the system is shut down or after a set amount of time.\nIf you’re working on a Mac operating system, you can get the path to the temporary directory by running the following in your terminal:\necho $TMPDIR\nWhen I last ran this command on my system, echo returned the following path (later we’ll use base::tempdir() to get and use this path in R).\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/\nThis directory is located at the system level. The cd command can be used to navigate to it from the terminal. You may have to back up a few directories if your root starts at the user level, though. This is pretty standard, especially if you’re working on a Mac.\n\n\n\n\n\n\nNote\n\n\n\nSince I’m drafting this post on my personal machine, I’m not aware if you need admin privileges to access this folder. As such, you may run into issues if you’re not an admin on your machine.\n\n\nWith my curiosity peaked, I sought more information about what this directory was used for on a MacOS. Oddly enough, there is very little about this directory online. From what I can deduce, the /var directory is mainly a per-user cache for temporary files, and it provides security benefits beyond other cache locations on a Mac system (again, I’m not a security expert, so my previous statement may be inaccurate). Being that this location is temporary, this cache gets cleared every time the system restarts or every three days.\nAlthough there’s a lack of information about this directory online, I did come across a few blog posts and a Stack Overflow answer that were helpful in understanding this temporary directory in more depth: post 1; post 2; post 3. You might find these useful if you want to learn more. However, for me, the above is as far as I wanted to go to understand its purpose.\n\n\nAccess the temporary directory using base::tempdir()\nAt the start of every session, R creates a temporary per-session directory, and it removes this temporary directory when the session ends.. This temporary directory is stored in the system’s temporary directory location (e.g., /var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/). R also provides several functions to work with the temporary directory, create, and interact with files within it.\nbase::tempdir() can be used to print the file path of the temporary directory on your system. Let’s run it and take a look at what happens.\n\ntempdir()\n\nOutputted to the terminal is the path to the R session’s temporary directory. When I ran it, the returned path looked like this:\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T//RtmpaYxspA\nThe temporary directory R uses for the current session is labeled using the RtmpXXXXXX pattern. The final six characters of the path (i.e., the Xs) are determined by the system. Note, tempdir() doesn’t create this directory, it just prints the temporary directory’s path to the console. This directory is created every time a R session begins.\nSince the temporary directory is just like any location on your computer, you can navigate to it from your terminal during an active R session. With your terminal pointing to the temporary directory, you can use the following code to find R’s per-session temporary directory:\nla | grep \"Rtmp\"\nLet’s take a peak at what’s in this directory. R’s list.files() function can be helpful in this case.\n\nlist.files(tempdir())\n\ncharacter(0)\n\n\nMost R setups should start with an empty per-session directory. So the above should return character(0). Despite being empty now, list.files() will become handy again once we start to write files to this location.\n\n\nWriting files to the temporary directory\nNow that we know a little more about this temp directory and where it is located on our system, let’s write some data to it. We can do this by doing something like the following.\n\nwrite_csv(mtcars, file = paste0(tempdir(), \"/mtcars.csv\"))\n\nNow when we list the files in the temporary directory (e.g., list.files(tempdir())), you should see the mtcars.csv file.\nIf you’re looking to create files with unique names, you can pass the tempfile() function to the file argument. This looks something like this:\n\nwrite_csv(\n  mtcars, \n  file = tempfile(pattern = \"mtcars\", fileext = \".csv\")\n)\n\ntempfile() creates unique file names, which concatenates together the file path, the character vector passed to the pattern argument, a random string in hex, and the character vector inputed to the fileext argument. When you list the files in the temporary directory now, you’ll see the initial mtcars.csv file along with a file that looks something like this: mtcars7eb3503ac74c.csv. The random hex string ensures files remain unique.\nIndeed, the above is just one way to write files to the temporary directory. You can use other methods to read and write files at this location. However, you now know what is needed to interact with this directory, read and write files to and from it. At this point you can do any data wrangling steps your project requires. After which, we can go about deleting our files from this directory.\n\n\nDeleting files with file.remove()\nAlthough these files will eventually be removed by the system, we should be proactive and clean up after ourselves.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using this approach within functions, especially if their intended to be used by other users, you’ll want to be clear they will write data to and remove data from the user’s system.\nIndeed, it’s considered poor practice to change the R landscape on a user’s computer without good reason. So the least we can do here is clean up after ourselves.\n\n\nTo delete our files we wrote to the temporary directory, run the following in the console:\n\nfile.remove(list.files(tempdir(), full.names = TRUE, pattern = \".csv\"))\n\n[1] TRUE TRUE\n\n\nThe arguments of the list.files() function should be pretty straightforward. We want file paths to be full length (i.e., full.names = TRUE) and to list only files with the .csv extension (i.e., pattern = \".csv\"). Then, we use these full file paths within the file.remove() function, which will remove the files from R’s temporary directory.\n\n\nWrap-up\nToday I learned more about R’s per-session temporary directory, and how it can be used to write files not intended for persistent storage. I also learned how to use several base R functions to create files within this temporary directory by using tempfile() and tempdir(). I also demonstrated how the list.files() function can be used to list files within any directory on your system, specifically using it to list files in R’s temporary directory. Finally, I highlighted how files in the temporary directory can be deleted using the file.remove() function.\nHave fun using R’s per-session temporary directory. Cheers 🎉!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Using `Base::tempdir()` for Temporary Data Storage},\n  date = {2023-11-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Using `Base::tempdir()` for Temporary Data\nStorage.” November 3, 2023."
  },
  {
    "objectID": "til/posts/2023-10-14-edit-old-commit/index.html",
    "href": "til/posts/2023-10-14-edit-old-commit/index.html",
    "title": "Edit an older unpushed commit",
    "section": "",
    "text": "Photo by Yancy Min\n\n\nToday I learned how to edit older unpushed commit messages using git rebase.\nI’ve been attempting to be better about linking git commits to specific GitHub issues. Although I try to be disciplined, I forget to reference the issue in the commit message from time-to-time. Luckily, I researched and came upon a solution. The purpose of this post is to briefly document what I’ve learned.\nA quick note: I am not a Git Fu master. The approach I share here (which I learned from a Stack Overflow post) worked for a small project not intended to be in production. In fact, there may be better approaches to solve this problem given your specific situation. I for sure want to avoid receiving angry messages where someone applied what is discussed, and it took down a critical, in production system. Thus, make sure you are aware of what these commands will do to your commit history before applying them.\n\nThe problem\nLet’s take a look at a log from a practice repo I created. I’m using git’s --pretty=format flag here to simplify the printed output for this post; a simple git log will also return the same information but in a more verbose way.\ngit log --pretty=format:\"%h %s %n%b\"\nThis returns the following log information. Printed to the console is a log containing the various commit’s abbreviated SHA-1 values, subjects, and message bodies.\n31964b0 fix-found_bug\n- #1\n\nf8256d6 feat-you_get_the_point\n- #1\n\nb1b99e9 feat-another_awesome_new_feat\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nShoot! I forgot to tag the b1b99e9 commit as being related to issue #1. How can I edit before I push?\n\n\nThe solution\ngit rebase can be used here to edit the past commit message. Again, keep in mind these commits have not been pushed to the remote repository.\nFirst, we need to target the commit we want to edit. git rebase, with the --interactive flag, and the abbreviated SHA-1 value of the commit to be edited is used to do this:\ngit rebase --interactive b1b99e9~\nThis command will open our system’s default text editor. In it should be something like the following:\npick b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n\n# Rebase 5d9b87c..31964b0 onto 5d9b87c (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\nYou’ll notice the instructions and different options (formatted as comments) are plentiful. I have yet to explore what all these operations can do (maybe a future post). But here, we are focused on editing a past commit message.\nThe next step in the process was a little confusing. With a bit of reading of the Stack Overflow post and a little experimentation, I found out we need to manually change any pick to edit for any commit intended to be edited in the currently open file. Our file will look something like this:\nedit b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n...\nWe save the file and close our editor. Once back in the terminal, we’ll be on the commit targeted for edits. To make our edits, submit the following to the terminal:\ngit commit --amend\nOnce ran, the text editor will be opened to the commit message we targeted for edits. We’ll then make our changes, save them, and exit the text editor.\nNow, we need to return to the previous HEAD commit. To do this, we run the following command in our terminal:\ngit rebase --continue\n\n\nRewriting history\nLet’s look at the log and view our changes. We can do that again by submitting the following to our terminal:\ngit log --pretty=format:\"%h %s %n%b\"\nBelow is what gets printed.\n709c173 fix-found_bug\n- #1\n\ne0ed7ba feat-you_get_the_point\n- #1\n\n179be4a feat-another_awesome_new_feat\n- #1\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nSuccess! All our commits are now associated with issue #1. However, take a moment to compare the SHA-1 values from our previous log with the current log. Notice anything different? The SHA-1 values for both our edited commit message and all its children have been modified. We have just re-written part of our commit history.\nImportant point: You can break repos doing this if you’re not careful. This re-writing of history should only be applied in cases with unpushed commit messages and when you’re not collaborating on a branch with other people. If you make edits to your history using this approach, you’ll want to make sure to avoid using commands like git push --force. See the original Stack Overflow post for more detail.\n\n\nWrap-up\nSo there you have it. A little Git Fu magic to help edit past, unpushed commit messages.\nIf you know a better approach or if my Git Fu is way off, let me know. I have far from mastered git.\nHappy rebasing!\n\n\nResources to learn more\n\nHow do I modify a specific commit? Stack Overflow post submitted by Sam Liao and top answer from ZelluX\nGit Rebase Interactive :: A Practical Example YouTube tutorial from EdgeCoders\n7.6 Git Tools - Rewriting History from the git documentation\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Edit an Older Unpushed Commit},\n  date = {2023-10-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Edit an Older Unpushed Commit.”\nOctober 14, 2023."
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Here you’ll find posts related to things I’ve learned recently.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse base::dput() to easily create and save objects\n\n\n\n\n\n\nproductivity\n\n\nvim\n\n\ntesting\n\n\ndata wrangling\n\n\n\nNeed to create and store an object quickly, use this trick\n\n\n\n\n\nFeb 10, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nCombine plots using patchwork\n\n\n\n\n\n\ndata visualization\n\n\n\nNeed to add two or more plots together? Use the patchwork package\n\n\n\n\n\nDec 23, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing base::tempdir() for temporary data storage\n\n\n\n\n\n\ndata wrangling\n\n\nworkflow\n\n\nproductivity\n\n\n\nNeed to store data in a place that’s not persistent, use a temporary directory\n\n\n\n\n\nNov 3, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating correlations with corrr\n\n\n\n\n\n\ndata analysis\n\n\nexploratory analysis\n\n\ndata visualization\n\n\n\nUse the corrr package to calculate and visualize correlations\n\n\n\n\n\nOct 22, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nEdit an older unpushed commit\n\n\n\n\n\n\ngit\n\n\nGitHub\n\n\n\nUse git rebase to edit previous commit messages\n\n\n\n\n\nOct 14, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nFind and replace in Vim\n\n\n\n\n\n\nvim\n\n\nneovim\n\n\nproductivity\n\n\n\nImproving productivity by using Vim’s :substitute command\n\n\n\n\n\nFeb 24, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "I’m a media research analyst, data enthusiast, and news, sports, and podcast aficianado.\nProfessionally, I use data, audience measurement, and marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. I am particularly interested in the use and development of open-source statistical software (i.e. R) to achieve this goal, and gaining a broader understanding of the role these tools play in media, digital, and marketing analytics. I also adjunct university courses on the side.\nListening to NPR, watching PBS (especially NOVA), and college football and baseball are my jam.\n\n\nWant to know more about what I’m currently working on, reading, or mastering? Check out the now page.\n\n\n\n\nPh.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota\n\n\n\n\n\nDigital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#now",
    "href": "index.html#now",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Want to know more about what I’m currently working on, reading, or mastering? Check out the now page."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Ph.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Digital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "",
    "text": "Photo by Ricardo Gomez Angel"
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#composing-the-query",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#composing-the-query",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Composing the query",
    "text": "Composing the query\nThe first step is to compose a query. The query is a character string, which is assigned a variable name.\n\nquery_neb_county_pop &lt;- \"\n  with census_total_pop as (\n    select \n      geo_id,\n      total_pop\n    from `bigquery-public-data.census_bureau_acs.county_2020_5yr`\n    where regexp_contains(geo_id, r'^31')\n  ), \n  fips_codes as (\n    select \n      county_fips_code,\n      area_name\n    from `bigquery-public-data.census_utility.fips_codes_all`\n  )\n  \n  select \n    geo_id,\n    area_name,\n    total_pop\n  from census_total_pop left outer join fips_codes on census_total_pop.geo_id = fips_codes.county_fips_code\n\"\n\nUsing a left join, this query transposes the county names column from another dataset. The result of the join is the total population of each county according to the 2020 ACS survey. query_neb_county_pop is now available for the next few steps in the query process.\n\n\n\n\n\n\nNote\n\n\n\nThe query utilizes BigQuery’s convenient with clause to create temporary tables, which are then joined using a left join. Indeed, I can hear the SQL experts pointing out more clever ways to compose this query: it’s just a simple join. However, readability was the goal here."
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#submitting-the-query",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#submitting-the-query",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Submitting the query",
    "text": "Submitting the query\nWhen I first started using the bigrquery package, I struggled to understand what query function to use. I was also slightly confused why the package had a separate query and download table function (more on this in a bit). First, the type of data being queried (e.g., public vs. project data) dictates what query function to use. Second, the argument structure is slightly different between the two query functions. The nuance of these differences is subtle, so I suggest reading the package’s docs (?bq_query) to know what function to use and when.\nIf project data is queried, the bq_project_query() function is used. In cases where you’re not querying project data (e.g., public data), you’ll use bq_dataset_query(). The bq_dataset_query() is used in this post because public data is being queried. This function has parameters to associate the query with a Google Cloud billing account. In regard to the function’s other arguments, you’ll only need to pass a bq_dataset object (in our case a bq_dataset_acs) and a query string.\nGetting data into the R session involves two steps. First, you’ll submit the query to BigQuery using one of the functions highlighted above. BigQuery creates a temporary table in this initial step. Second, this temporary table is downloaded to the R session using the bq_table_download() function.\nThis intermediate, temporary table provides a couple conveniences:\n\nYou can use the bq_table_fields() function to check the temporary table’s fields before downloading it into your R session.\nThe table is essentially cached. As such, submitting the same exact query will return the data faster, and data processing costs will be reduced.\n\n\ntbl_temp_acs &lt;- bq_dataset_query(\n  bq_dataset_acs,\n  query = query_neb_county_pop,\n  billing = '&lt;project-name&gt;'\n)\n\n\nbq_table_fields(tbl_temp_acs)\n# &lt;bq_fields&gt;\n#   geo_id &lt;STRING&gt;\n#   area_name &lt;STRING&gt;\n#   total_pop &lt;FLOAT&gt;\n\n\ndata_nebpm_county &lt;- bq_table_download(tbl_temp_acs)\n\ndata_nebpm_county\n# # A tibble: 93 × 3\n#    geo_id area_name        total_pop\n#    &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n#  1 31005  Arthur County          439\n#  2 31085  Hayes County           889\n#  3 31087  Hitchcock County      2788\n#  4 31103  Keya Paha County       875\n#  5 31113  Logan County           896\n#  6 31115  Loup County            690\n#  7 31117  McPherson County       420\n#  8 31125  Nance County          3525\n#  9 31133  Pawnee County         2640\n# 10 31143  Polk County           5208\n# # ℹ 83 more rows\n# # ℹ Use `print(n = ...)` to see more rows\n\nNow the data is available in the R session. You can work with it like any other type imported via these methods."
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#create-and-write-disposition",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#create-and-write-disposition",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Create and write disposition",
    "text": "Create and write disposition\nbq_table_upload() has some additional arguments, which I’ve found are not clearly documented in the package’s documentation. These include the ability to pass a create_disposition and a write_disposition argument.\n\n\n\n\n\n\nWarning\n\n\n\nBe mindful of how you use these arguments, as the values you pass can overwrite data. Read more about the options by reviewing the linked docs below.\n\n\nMore about what these options do in BigQuery can be reviewed here. Here’s what the code would look like using the arguments listed above:\n\nbq_table_upload(\n  bq_neb_county_table,\n  values = fields_neb_county,\n  create_disposition = \"CREATE_NEVER\",\n  write_disposition = \"WRITE_TRUNCATE\"\n)\n\nThe create_disposition argument specifies how the table will be created, based on whether the table exists or not. A value of CREATE_NEVER requires the table to already exist, otherwise an error is pushed. CREATE_IF_NEEDED creates the table if it does not already exist. However, it’s best to use the bq_table_create() function rather than relying on the bq_table_upload() function to create the table for us. Nevertheless, it’s an option that’s available.\nThe write_disposition specifies what happens to values when they’re written to tables. There are three options: WRITE_TRUNCATE, WRITE_APPEND, and WRITE_EMPTY. Here’s what each of these options do:\n\nWRITE_TRUNCATE: If the table exists, overwrite the data using the schema of the newly inputted data (i.e., a destructive action).\nWRITE_APPEND: If the table exists, append the data to the table (i.e., add it to the bottom of the table).\nWRITE_EMPTY: If the table exists and it already contains data, push an error.\n\nWhen it comes to uploading data, you’ll most likely want to consider the write_disposition you use.\nOne last note about uploading data to your tables: BigQuery optimizes for speed. This optimization some times results in the data to be imported not in the order it is initially imported. Rather, the resulting data import may be shuffled in a way to speed up the process. Thus, you’ll likely need to arrange your data if you need to extract it again."
  }
]