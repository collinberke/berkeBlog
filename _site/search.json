[
  {
    "objectID": "data-media-marketing-resources.html",
    "href": "data-media-marketing-resources.html",
    "title": "Media and Marketing Research Resources",
    "section": "",
    "text": "This resource list was last updated on 2026-01-05. Want to add a resource? File an issue or edit the resources.csv file on GitHub and submit a Pull Request.\n\n\n\n\n\n\n\nNote\nThis list is provided for convenience and informational purposes only. Some resources may be open-source and freely available. Others may require a monetary payment, subscription, or account to access. Inclusion on this list does not represent an endorsement or guarantee of the accuracy, reliability, or content of any linked site. Content within these links may be invalid or inaccurate. We take no responsibility of what is contained within these links or what results from their review or use.\nThis list will be updated periodically. Resources will be added and removed during these updates.\n\n\nInspiration\nThis list and its structure was inspired by Nicola Rennie‚Äôs data visualization and data science resources list.\nCheck them out:\n\nData Visualization Resources\nData Science Resources"
  },
  {
    "objectID": "say-hi.html",
    "href": "say-hi.html",
    "title": "Say Hi!",
    "section": "",
    "text": "I want to meet you. Yeah, you!\nI‚Äôve set aside time on my calendar for 30-60 minute conversations over coffee, lunch, or other suggestions. These conversations can be virtual or in person if you‚Äôre in my area (Lincoln or Omaha, NE).\n\nWhat can our time together be used for?\nWhatever you want. Are you working as a data scientist, analyst, or any other type of professional in media or marketing? Let‚Äôs talk shop. Have an idea or blocker you want to discuss? I‚Äôm open to sharing objective, external feedback. Wrestling with a tricky piece of code? I might be able to help, or at the very least could be an extra hand in Googling or AI‚Äôing your problem. Have a presentation you want to practice? With my background in communication and public speaking, I‚Äôd be glad to listen and offer guidance. Want to spend 10 minutes screaming into the void of the internet and head out? I‚Äôm open to that to.\nTo be clear, I‚Äôm not selling anything. My intention with this time is connection and networking, and it‚Äôs not to push a sales pitch or demo.\nIf any of this sounds interesting, schedule a time and come say hi!\n\n üëã Say Hi! \n\n\n\nMy interests\nIf you need specifics, the topics I‚Äôd like to talk more about include:\n\nMedia, digital, and marketing data science and analytics.\nOpen-source statistical software: mostly R, but I‚Äôm open to discussing Python.\nR package development.\nNeovim and LazyVim.\nCloud computing.\nThe use of data and its relationship to organizational culture.\nData visualization.\n\n\n\nOther ways to connect\nNot interested in saying hi in person? That‚Äôs cool. Here are some other ways to connect with me:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nJan 9, 2026\n\n\nThe Hex Update: Issue 010\n\n\nAppointment viewing, is it back?; the future of the web; revenue models for streaming-first audiences\n\n\n\n\nJan 1, 2026\n\n\nThe Hex Update: Issue 009\n\n\nA weekly collection of what I recently read and learned while working in the media industry\n\n\n\n\nJan 1, 2026\n\n\nNotes: My first GitHub action workflow using RStats and Quarto\n\n\n\n\n\n\n\nDec 25, 2025\n\n\nThe Hex Update: Issue 008\n\n\nA weekly collection of what I recently read and learned while working in the media industry\n\n\n\n\nDec 19, 2025\n\n\nThe Hex Update: Issue 007\n\n\nA weekly collection of what I recently read and learned while working in the media industry\n\n\n\n\nDec 12, 2025\n\n\nThe Hex Update: Issue 006\n\n\nA weekly collection of what I recently read and learned while working in the media industry\n\n\n\n\nDec 5, 2025\n\n\nThe Hex Update: Issue 005\n\n\nA weekly collection of what I recently read and learned while working in the media industry\n\n\n\n\nNov 21, 2025\n\n\nThe Hex Update: Issue 004\n\n\nA weekly collection of what I recently read and learned about while working in the media industry\n\n\n\n\nNov 13, 2025\n\n\nThe Hex Update: Issue 003\n\n\nA weekly collection of what I recently read and learned about while working in the media industry\n\n\n\n\nNov 1, 2025\n\n\nTIL: Use a file to draft a GitHub issue or PR\n\n\nWhat I recently learned about GitHub cli tool‚Äôs --body-file command option\n\n\n\n\nOct 26, 2025\n\n\nNotes: The use of ... in R\n\n\nWhat I recently learned about this slippery construct\n\n\n\n\nOct 11, 2025\n\n\nTIR: The ghosts in the data by Vicki Boykis\n\n\nTakeaways from what I read recently\n\n\n\n\nSep 1, 2025\n\n\nNotes: Using GitHub search\n\n\nLearning more about GitHub‚Äôs search\n\n\n\n\nAug 23, 2025\n\n\nNotes: Using gganimate to animate plots\n\n\nLearning more about data visualization animation from the ‚ÄòGetting Started‚Äô vignette\n\n\n\n\nAug 10, 2025\n\n\nTidyTueday contribution: 2028-08-05\n\n\nIncome Inequality Before and After Taxes\n\n\n\n\nAug 3, 2025\n\n\nNotes: Rich Hickey‚Äôs talk Simple Made Easy\n\n\nLearning more about software and system design\n\n\n\n\nFeb 23, 2025\n\n\nNotes: Customizing Neovim‚Äôs key bindings\n\n\nLearning more about how to customize the R.nvim plugin while using LazyVim\n\n\n\n\nFeb 15, 2025\n\n\nNote: Date and time data types in PostgreSQL\n\n\nA scratchpad of concepts and code examples I collected while learning about the dates and times data types\n\n\n\n\nFeb 1, 2025\n\n\nTIL: Identifying explicit and implicit missing values\n\n\nHighlights from the DSLC book club discussion of Chapter 18: Missing values from R4DS\n\n\n\n\nJan 27, 2025\n\n\nNotes: The use and management of GitHub projects\n\n\nLearning how to take project management to the next level\n\n\n\n\nJan 26, 2025\n\n\nNotes: Link blogs and more frequent blog posting\n\n\nSome notes, quotes, and links about writing more and sharing your work\n\n\n\n\nJan 11, 2025\n\n\nWhat‚Äôs Worth Your Time: Read, Watch, and Listen\n\n\nA curated list of data science, analysis, coding, tech-related, and miscellaneous work I‚Äôve found to be useful and impactful\n\n\n\n\nDec 30, 2024\n\n\nTIL: Use list2env() or glue::glue_data() to use a set of elements from a tibble in a string\n\n\nNeed an easy way to access a set of elements from a tibble for string interpolation? Here‚Äôs two examples\n\n\n\n\nDec 27, 2024\n\n\nTIL: Separate character strings into rows and columns using tidyr functions\n\n\nNeed to separate strings? Use the separate_* family of functions\n\n\n\n\nDec 8, 2024\n\n\nTIL: Summarize logical vectors to calculate numeric summaries\n\n\nNeed proportion and count summaries from a logical vector? Use mean() and sum()\n\n\n\n\nNov 2, 2024\n\n\nThe Hex Update: Issue 002\n\n\nKey insights and what I learned about the media industry as of October 2024\n\n\n\n\nOct 5, 2024\n\n\nWorkflow walkthrough: Interacting with Google BigQuery in R\n\n\nA tutorial on how to use the bigrquery package\n\n\n\n\nSep 14, 2024\n\n\nMessing with models: k-means clustering of Google Analytics 4 data\n\n\nA tutorial on how to perform k-means clustering using Google Analytics data\n\n\n\n\nJul 10, 2024\n\n\nThe Hex Update: Issue 001\n\n\nKey insights and what I learned about the media industry in June 2024\n\n\n\n\nJun 11, 2024\n\n\nMessing with models: Market basket analysis of online merchandise store data\n\n\nA tutorial on how to perform a market basket analysis using Google Analytics data\n\n\n\n\nMay 3, 2024\n\n\nExploring objects launched into space and gross domestic product\n\n\nA contribution to the 2024-04-23 #tidytuesday social data project\n\n\n\n\nMar 22, 2024\n\n\nExploring data from the Fiscal Sponsor Directory\n\n\nA contribution to the 2024-03-12 #tidytuesday social data project\n\n\n\n\nMar 12, 2024\n\n\nExploring the relationship between trash processed by Mr.¬†Trash Wheel and precipitation\n\n\nA contribution to the 2024-03-05 #tidytuesday social data project\n\n\n\n\nMar 5, 2024\n\n\nExploring the lifespans of historical figures born on a Leap Day\n\n\nA contribution to the 2024-02-27 #tidytuesday social data project\n\n\n\n\nFeb 26, 2024\n\n\nExploring R Consortium ISC Grants\n\n\nA contribution to the 2024-02-20 #tidytuesday social data project\n\n\n\n\nFeb 10, 2024\n\n\nTIL: Use base::dput() to easily create and save objects\n\n\nNeed to create and store an object quickly, use this trick\n\n\n\n\nJan 1, 2024\n\n\n30 day tidymodels recipes challenge\n\n\nLearning how to use the recipes package, one day at a time\n\n\n\n\nDec 23, 2023\n\n\nTIL: Combine plots using patchwork\n\n\nNeed to add two or more plots together? Use the patchwork package\n\n\n\n\nDec 7, 2023\n\n\nMessing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data\n\n\nUsing tidymodels to predict wins and losses for volleyball matches\n\n\n\n\nNov 3, 2023\n\n\nTIL: Using base::tempdir() for temporary data storage\n\n\nNeed to store data in a place that‚Äôs not persistent, use a temporary directory\n\n\n\n\nOct 22, 2023\n\n\nTIL: Calculating correlations with corrr\n\n\nUse the corrr package to calculate and visualize correlations\n\n\n\n\nOct 14, 2023\n\n\nTIL: Edit an older unpushed commit\n\n\nUse git rebase to edit previous commit messages\n\n\n\n\nFeb 24, 2023\n\n\nTIL: Find and replace in Vim\n\n\nImproving productivity by using Vim‚Äôs :substitute command\n\n\n\n\nJan 29, 2023\n\n\n2023 data science rig: Set up and configuration\n\n\nOverviewing and reflecting on my current data science setup.\n\n\n\n\nSep 20, 2022\n\n\nFlattening Google Analytics 4 data\n\n\nLet‚Äôs deep dive into working with Google Analytics data stored in BigQuery.\n\n\n\n\nMar 3, 2022\n\n\nExploratory analysis of Google Analytics 4 data for forecasting models\n\n\nExploring Google Analytics 4 data for forecasting models.\n\n\n\n\nDec 30, 2021\n\n\nShiny summary tiles\n\n\nBuilding custom metric summary tiles for Shiny.\n\n\n\n\nOct 19, 2021\n\n\n2021 PBS TechCon: Your Data is Disgusting!\n\n\nI was fortunate to be invited to present about topics I‚Äôm passionate about: tidy data and data pipelines.\n\n\n\n\nSep 12, 2021\n\n\nImplementing a next and back button in Shiny\n\n\nTaking the time to understand a challenging question from Mastering Shiny.\n\n\n\n\nApr 2, 2021\n\n\nIntro Post\n\n\nHello World!, my name is Collin, and this is my blog.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-01-26-til-notes-quotes-links-credibility-blogs/index.html",
    "href": "blog/posts/2025-01-26-til-notes-quotes-links-credibility-blogs/index.html",
    "title": "Notes: Link blogs and more frequent blog posting",
    "section": "",
    "text": "Background\nI‚Äôve really been focusing on developing a daily writing habit. As a result, I‚Äôve been actively generating ideas on how to more frequently publish blog content. My biggest mental hurdle for turning posts around is a deeply embedded feeling that every piece of writing I produce needs to be polished prose, and everything included needs to be absent of error.\n\n\nLinks and thoughts on link blogging\nI recently came across this quote from an interview with Simon Willison on the Real Python podcast with Christopher Bailey (the quote is mentioned around 10m08s):\n\nCredibility is accumulated over time, and it doesn‚Äôt take much, like a link blog about a subject run that for six months and you will become one of the top 0.1% people on earth with credibility on that subject just from publishing a few notes and linking to a bunch of things about it.\n\nI found Willison‚Äôs statement motivating, inspiring, and a little freeing. It made me reconsider portions of my own blog and what‚Äôs their aim. This is my little corner the internet, and I set the expectations and rules of this site.\nCertainly, I believe there is room for more formal, well thought-out forms of writing, like you can read in the blog section of my site. However, I also see the Today I Learned (TIL) section of my site being more like a link blog, a scratch-pad of sorts, and at times a research notebook on topics I‚Äôm learning about. Learning is a messy endeavour some times, so why does every piece of writing from it need to be tidy? It doesn‚Äôt.\n\n\nMini-blogging and open-source ecosystems\nKelli Bodwin addressed these same ideas during her posit::conf(2024) talk. The talk contained a call to action for attendees to produce more content, even if they weren‚Äôt fully formed topics or ideas. Just sharing what you‚Äôve currently learned or are working on strengthens open-source ecosystems and makes them more sustainable. David Robinson‚Äôs Rstudio::conf(2019) keynote The unreasonable effectiveness of public work was also referenced, which I have queued up to watch later. Although I suggest listening to Bodwin‚Äôs entire talk, you can hear more about these specific ideas around 13m20s.\nWho knows where these thoughts may lead. I‚Äôm still learning, which is the most important.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes: {Link} Blogs and More Frequent Blog Posting},\n  date = {2025-01-26},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúNotes: Link Blogs and More Frequent Blog\nPosting.‚Äù January 26, 2025."
  },
  {
    "objectID": "blog/posts/2025-12-12-hex-update-006/index.html",
    "href": "blog/posts/2025-12-12-hex-update-006/index.html",
    "title": "The Hex Update: Issue 006",
    "section": "",
    "text": "Let‚Äôs catch up\nHowdy folks! Here‚Äôs Issue 006.\nThree topics caught my attention this week: media restoration, the use of AI for media management tasks, and data clean rooms. Rounding out the week with a little bit of fun, I look back to 1996, the year Space Jam was released. The first article explores media restoration for streaming platforms and the potential impact it may have on our collective culture. Then, I share a podcast highlighting AI usecases to automate some of the tedious tasks within the media management workflow. The third article is a report on Uber‚Äôs new insights platform, which utilizes a data clean room to make data and insights available to marketers in a privacy-consious way. Lastly, I share share two articles focused on using AI to recreate the infamous Space Jam website, which is still online today.\n\n\nThree things from this week\nHere are three things that caught my attention this week.\n\nBlog: The ‚ÄúMad Men‚Äù in 4K on HBO Max Debacle\nHere‚Äôs a cautionary tale of the impact media restoration for streaming services can have. It overviews the recent missteps of HBO Max‚Äôs remastering of the Mad Men series to 4K. It‚Äôs an interesting case study of the quality control needed to update archival content for streaming platforms. Not only is it about avoiding silly gaffes that reveal the behind the scenes work, it also emphasizes the impact this form of restoration has on the original storytelling in the content. The following quote from the piece does an excellent job summarizing this point, using the conversion of film to the 16:9 aspect ratio as an example:\n\nReframing old shows to fit a new aspect ratio is antithetical to the spirit of media restoration, and cheapens the future of our shared culture.\n\nSeveral examples of the impact restoration has on the original storytelling in Mad Men are highlighted in the piece.\n\nWhy does this matter?\nRestoration has the ability to impact our collective culture resulting from storytelling. Streaming is all about on-demand content access, which audiences want and have come to expect. As such, media organizations are turning to previously popular shows to fill their streaming libraries to meet their needs. However, the restoration of this content to fit the streaming environment has the potential to impact the original experience intended by the content and its creators. All of us have those TV moments we just remember. Moments where the line between making media and crafting an unforgetable cultural moment blends. Moments we talk about years after we finish a series. Modifying these experiences for future audiences just to fit the format of the viewing environment has the potential to alter these cultural moments. I believe this to be even more true as some of this restoration work will likely be offloaded to AI in the future, and these moments will inevitably be altered, impacting the shared culture created by the original content experience.\n\n\n\nPodcast: How Emmy Award‚Äìwinning filmmakers use AI to automate the tedious parts of documentaries\nQuoted from the episode:\n\nPost-production is like a technical mess of media management.\n\nHere‚Äôs an interesting podcast on how Tim McAleer, producer at Florentine Films, uses AI to develop solutions and automations to improve the post-production workflow.\nHere are some points in the episode that I found to be interesting and relevant:\n\n05M17S: Automating manual data entry for media management tasks.\n09M40S: Using file metadata to improve prompts and end results.\n13M40S: Scaling workflow to create media descriptions.\n20M08S: Improved media searchability with semantic discovery.\n30M24S: Loading context for AI using file metadata.\n40M14S: The future of AI for creative industries.\n\n\nWhy does this matter?\nCreative industries often emphasize generative AI, particularly its role in media creation. This is the first example where I‚Äôve seen AI solutions applied to improve some of the more tedious media management tasks. What I found interesting was this idea that AI can be leveraged to develop custom, bespoke software solutions and automations that no other company will create. As such, AI provides the capability for semi-technical folks to craft these solutions on their own to improve their specific workflows. This was a really interesting listen. I suggest you check it out.\n\n\n\nUber‚Äôs latest play for ad dollars: turning data about your trips and takeout into insights for marketers\nHere‚Äôs an interesting article from Business Insider. It reports on Uber Intelligence, the company‚Äôs recently rolled out marketing insights platform. The platform seeks to provide marketers with insights derived from the millions of data points Uber collects on rides and deliveries happening every day. To do this, Uber partnered with LiveRamp, which makes the data accessible via a data clean room: technology that makes data accessible to marketers in a privacy-safe environment. That is, an environment where insights and data points are made available, but raw or personally identifiable information remains inaccessible. This has several advantages. Aside from allowing businesses the ability to identify partnership opportunities through greater data access, this data will also be used to improve Uber‚Äôs advertising business. These improvements include improved capability for segmentation and targetting, where the aim is to improve the ads experience across Uber‚Äôs various platforms. As quoted in the article, Edwin Wong‚ÄîUber Advertising‚Äôs global head of measurement‚Äîwants marketers to say the following about their experience:\n\nOh, I‚Äôm not understanding Uber, I‚Äôm understanding Uber in my marketing context.\n\n\nWhy does this matter?\nThis reporting grabbed my attention because this is one of the first examples where I‚Äôve seen data clean rooms discussed in terms of data access in the marketing analytics space. Given audience consumption has turned to online, digital streaming services and platforms, there will likely be a time when user viewing data will be consolidated and made available via a data clean room solution. This will hopefully lead to improved privacy and identity protection for audiences, while also providing only the data marketers and media organizations need to create better experiences for audiences and consumers. Organizations who rely on this type of data will need to be aware of and be prepared for this adoption of such tools. Soon, they may be accessing their data via data clean room service providers. If you‚Äôre not familiar with the idea of a data clean room, check out Digiday‚Äôs YouTube video explaining it in more accessible terms here.\n\n\n\n\nJust for fun\nLet‚Äôs pause and look back to 1996, specfically around the time the movie Space Jam was released. A website was created as part of the release. It was one of the first examples a website was used for a film‚Äôs promotion. The website is still online today, and you can get a little nostalgic by viewing it here. There‚Äôs been some focus on this site recently, where people online have attempted to use AI to recreate the site. It‚Äôs a really interesting cross-section between the new and the old. It‚Äôs also fascinating to see others failed and successful attempts. Here‚Äôs the posts I bumped into:\n\nI failed to recreate the 1996 Space Jam Website with Claude\nI successfully recreated the 1996 Space Jam Website with Claude\n\nCongratulations on making it to end of the week.\nCheers üéâ!\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 006},\n  date = {2025-12-12},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúThe Hex Update: Issue 006.‚Äù\nDecember 12, 2025."
  },
  {
    "objectID": "blog/posts/2024-09-14-post-google-analytics-machine-learning-kmeans-clustering/index.html",
    "href": "blog/posts/2024-09-14-post-google-analytics-machine-learning-kmeans-clustering/index.html",
    "title": "Messing with models: k-means clustering of Google Analytics 4 data",
    "section": "",
    "text": "Marketing campaigns target specific audiences. Some form of customer segmentation is performed to identify these audiences. These segments can be identified using several approaches. This post describes one such approach: the use of data and machine learning to specify clustering models present within Google Analytics data for an e-commerce store.\nThe goal is simple: create customer segments a marketing team could use to craft and target specific marketing campaigns. To achieve this goal, this post overviews the use of a straightforward, useful machine learning algorithm called k-means clustering. Packages and functions from R, a statistical programming language, are used for the segmentation process.\n\nlibrary(tidyverse)\nlibrary(bigrquery)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(factoextra)\nlibrary(ids)\nlibrary(here)\nlibrary(psych)\nlibrary(gt)\n\n\nggplot_theme &lt;-\n  theme_bw() +\n  theme(\n    text = element_text(size = 12),\n    title = element_text(size = 16, face = \"bold\")\n  )\n# Override the default ggplot2 theme\ntheme_set(ggplot_theme)\n\n\nExtract and explore data\nFirst, we need some data. This section briefly discusses the data extraction process. I‚Äôve detailed the extraction of Google Analytics data in a previous post. For the sake of time, then, this topic won‚Äôt be discussed in-depth. If you‚Äôre already familiar with how to access this data, feel free to skip this section.\nThe data used here is obfuscated Google Analytics data for the Google Merchandise Store. This data is stored and accessed via BigQuery, a cloud-based data warehouse useful for analytics purposes.\n\n\n\n\n\n\nWarning\n\n\n\nThis data is useful for example tutorials, so the conclusions drawn here should not be used to infer anything about true purchasing behavior. The purpose of this post is to be a tutorial on how to perform k-means clustering, rather than about deriving true conclusions about Google Merchandise store customers.\n\n\nBefore data extraction, let‚Äôs do some exploration of the source data. The focus here is to get a sense of what‚Äôs in the data, while also creating an understanding of the source data‚Äôs structure. This mostly involves identifying the available fields and data types.\nThe bigrquery package provides the bq_tables_fields() function to retrieve this information. The following code example shows how to use this function to return field names within the dataset‚Äôs tables:\n\ntable_ecommerce &lt;-\n  bq_table(\n    \"bigquery-public-data\",\n    \"ga4_obfuscated_sample_ecommerce\",\n    \"events_20210101\"\n  )\n\nbq_table_fields(table_ecommerce)\n\nThen, the following code submits a SQL query to return the ga_obfuscated_sample_ecommerce data from BigQuery. It‚Äôs important to note, similar to what was done in past posts, I‚Äôm querying data associated with transactions occurring over the U.S. Christmas holiday season. Here‚Äôs the query string if you want specific details:\n\nquery &lt;- \"\n  select \n    event_date,\n    user_pseudo_id,\n    ecommerce.transaction_id,\n    items.item_category,\n    items.quantity,\n    ecommerce.purchase_revenue_in_usd\n  from `&lt;your-project-name&gt;.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(items) as items\n  where _table_suffix between '20201101' and '20201231' and \n  event_name = 'purchase'\n  order by user_pseudo_id, transaction_id\n\"\n\n\ndata_ga_transactions &lt;- bq_project_query(\n  \"&lt;your-project-name&gt;\",\n  query\n) |&gt;\n  bq_table_download()\n\nBefore moving forward, some data validation is in order. Columns containing missing values are the biggest concern.\n\n# Verify if there are any missing values\nmap(data_ga_transactions, \\(x) any(is.na(x)))\n\n$event_date\n[1] FALSE\n\n$user_pseudo_id\n[1] FALSE\n\n$transaction_id\n[1] TRUE\n\n$item_category\n[1] FALSE\n\n$quantity\n[1] TRUE\n\n$purchase_revenue_in_usd\n[1] FALSE\n\n\nNo surprise: some features contain missing values. transaction_id and quantity both contain missing values. Our data wrangling step will need to address these issues. Let‚Äôs look closer and explore why missing values might be present within the data. This code can be used to do this:\n\n# Examples with a missing `transaction_id`\ndata_ga_transactions |&gt; filter(is.na(transaction_id))\n\n# A tibble: 59 √ó 6\n   event_date user_pseudo_id transaction_id item_category quantity purchase_revenue_in_usd\n        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;                   &lt;dbl&gt;\n 1   20201101       1494019. &lt;NA&gt;           New                  1                      25\n 2   20201101       2422026. &lt;NA&gt;           New                  2                      72\n 3   20201101       2422026. &lt;NA&gt;           Fun                  1                      72\n 4   20201101      29640693. &lt;NA&gt;           Apparel              1                      55\n 5   20201101      29640693. &lt;NA&gt;           Shop by Brand        2                      59\n 6   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 7   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 8   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 9   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n10   20201101      33027284. &lt;NA&gt;           Accessories          1                      63\n# ‚Ñπ 49 more rows\n\n# Examples where a `quantity` is missing\ndata_ga_transactions |&gt; filter(is.na(quantity))\n\n# A tibble: 148 √ó 6\n   event_date user_pseudo_id transaction_id item_category quantity purchase_revenue_in_usd\n        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;                   &lt;dbl&gt;\n 1   20201217       1086663. (not set)      (not set)           NA                       0\n 2   20201120      11080045. (not set)      (not set)           NA                       0\n 3   20201204      12422651. (not set)      (not set)           NA                       0\n 4   20201213      13309292. (not set)      (not set)           NA                       0\n 5   20201126      13429550. (not set)      (not set)           NA                       0\n 6   20201213       1396855. (not set)      (not set)           NA                       0\n 7   20201127       1423291. (not set)      (not set)           NA                       0\n 8   20201210    1540308157. (not set)      (not set)           NA                       0\n 9   20201201      15915163. (not set)      (not set)           NA                       0\n10   20201120      15980073. (not set)      (not set)           NA                       0\n# ‚Ñπ 138 more rows\n\n\nFor the transaction_id column, 59 rows contain missing values. If you dig a little further, you‚Äôll notice the majority of these missing transaction_ids occur near the beginning of data collection (i.e., 2020-11-01). This was likely a measurement issue with the Google Analytics setup, which would warrant further exploration. Since access to information about how Google Analytics was set up for the Google Merchandise Store (i.e., I can‚Äôt speak with the developers), this issue can‚Äôt be further explored. The only option, then, is to simply drop these rows.\nMissing quantity values seem to be associated with examples where transaction_id and item_category both contain a (not set) character string. Again, this could be a measurement issue worth further exploration. Given the available information, these examples will also be dropped.\nAt this point, dplyr‚Äôs glimpse() and skimr‚Äôs skim() functions are used for some additional exploratory analysis. glimpse() is great to get info about the data‚Äôs structure. skim() provides summary statistics for each variable within the dataset, regardless of type.\n\nglimpse(data_ga_transactions)\n\nRows: 13,113\nColumns: 6\n$ event_date              &lt;dbl&gt; 20201210, 20201210, 20201210, 20201103, 20201103, 20201103, 202011‚Ä¶\n$ user_pseudo_id          &lt;dbl&gt; 10111056, 10111056, 10111056, 1014825, 1014825, 1014825, 1014825, ‚Ä¶\n$ transaction_id          &lt;chr&gt; \"741471\", \"741471\", \"741471\", \"(not set)\", \"(not set)\", \"(not set)‚Ä¶\n$ item_category           &lt;chr&gt; \"Apparel\", \"Apparel\", \"Apparel\", \"Shop by Brand\", \"Office\", \"Shop ‚Ä¶\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ purchase_revenue_in_usd &lt;dbl&gt; 94, 94, 94, 183, 183, 183, 183, 183, 183, 86, 86, 86, 86, 86, 86, ‚Ä¶\n\n\n\nskim(data_ga_transactions)\n\n\nData summary\n\n\nName\ndata_ga_transactions\n\n\nNumber of rows\n13113\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntransaction_id\n59\n1\n2\n9\n0\n3563\n0\n\n\nitem_category\n0\n1\n0\n23\n189\n22\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1.00\n20201167.86\n4.740000e+01\n20201101\n20201121\n20201202\n20201212\n20201231\n‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñá\n\n\nuser_pseudo_id\n0\n1.00\n289012023.38\n1.274012e+09\n1014825\n5706716\n25060616\n62921408\n9202815833\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nquantity\n148\n0.99\n1.49\n2.910000e+00\n1\n1\n1\n1\n160\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\npurchase_revenue_in_usd\n0\n1.00\n106.68\n1.171100e+02\n0\n40\n71\n137\n1530\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nThe data contains a total of 13,113 rows with six features. After reviewing the data types and comparing some of the example values outputted from glimpse(), it‚Äôs concerning that some variables are of type character rather than type numeric. Values of type integer would be best in this case. Let‚Äôs do some more digging.\n\n# Only printing the first 10 values for brevity\nunique(data_ga_transactions$transaction_id) |&gt;\n  head(n = 10)\n\n [1] \"741471\"    \"(not set)\" \"983645\"    \"406646\"    \"2105\"      \"886501\"    \"76937\"     \"29460\"    \n [9] \"339943\"    \"614046\"   \n\n\nAs expected, some transaction_ids are of type character, which is keeping the column from being a true numeric variable. This is due to some rows containing the (not set) character string. Since the transaction_id is critical for our analysis, these rows will need to be filtered out during data wrangling.\nInitial data exploration is complete. We now have enough information to begin data wrangling. The wrangling step will strive to format the data into a structure necessary for performing k-means clustering. The following code chunk contains the code to do this. After, there‚Äôs a step-by-step explanation of what this code is doing.\n\ndata_user_items &lt;-\n  data_ga_transactions |&gt;\n  filter(transaction_id != \"(not set)\") |&gt;\n  drop_na() |&gt;\n  mutate(\n    user_id = random_id(),\n    .by = user_pseudo_id,\n    .after = user_pseudo_id\n  ) |&gt;\n  select(-user_pseudo_id) |&gt;\n  summarise(\n    quantity = sum(quantity),\n    .by = c(user_id, item_category)\n  ) |&gt;\n  mutate(\n    item_category = case_when(\n      item_category == \"\" ~ \"Unknown\",\n      TRUE ~ as.character(item_category)\n    )\n  ) |&gt;\n  pivot_wider(\n    names_from = item_category,\n    values_from = quantity,\n    values_fill = 0\n  ) |&gt;\n  clean_names()\n\nIn the code above, the (not set) issue in the transaction_id feature is addressed first. Any row with the (not set) value is simply filtered from the data. drop_na() follows. This function drops any rows that contain a NA value. As a result of dropping NAs, we‚Äôre left with 11,615 rows: a loss of 1,498 examples (~11.4%).\n\n\n\n\n\n\nNote\n\n\n\nMerely dropping examples is convenient in this case, but it may not be ideal or even valid in every context. You‚Äôll want to explore what is appropriate for the data you‚Äôre working with before applying this strategy.\n\n\nSome mutating and summarization of the data is next. The mutation step applies a random id in place of the user_pseudo_id. This provides an extra layer of privacy for users, since the data is being used outside of its original source system.\n\n\n\n\n\n\nNote\n\n\n\nThis may be a little inordinate, but I argue it‚Äôs important. It‚Äôs best to do what we can to create another layer of privacy to the information we‚Äôre using outside of the original source system. Since this data is public, it‚Äôs not too much of a concern here. However, this may be an important consideration when you‚Äôre working with ‚Äòreal world‚Äô Google Analytics data.\nI‚Äôm not a privacy expert, so you‚Äôll want to identify best practice for the context you work in.\n\n\nWith the modelling goal top-of-mind, aggregation will sum values to individual users rather than transactions. This way, we‚Äôll be able to use k-means clustering to identify customer cohorts, rather than transaction cohorts. To do this, the data is grouped by user_id and item_category and the quantity feature is summed.\nPost aggregation, item_categorys that don‚Äôt contain any values need to be addressed. To address this, any missing string is replaced with ‚ÄòUnknown‚Äô using the case_when() function. Finally, tidyr‚Äôs pivot_wider() is used to transform the data from long format to wide format. When taken together, the transformation results in a data set where each feature, other than user_id, is a sum of the item categories purchased by each user during the period. Each example, then, could comprise of multiple transactions that occur over the period.\nPost wrangling, readr‚Äôs write_csv() function is used to save the data to disk. This way a request isn‚Äôt sent to BigQuery every time the post is built. You don‚Äôt have to do this, but it‚Äôs useful for limiting query costs associated with the service, though it‚Äôs pretty economical to query data in this way.\n\nwrite_csv(data_user_items, \"data_user_items.csv\")\n\n\ndata_user_items &lt;- read_csv(\"data_user_items.csv\")\n\nRows: 2941 Columns: 22\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (1): user_id\ndbl (21): apparel, bags, shop_by_brand, drinkware, new, clearance, accessories, campus_collectio...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData exploration for modeling\nBefore modeling, let‚Äôs do some data exploration. The goal during exploration is to ensure data meets the assumptions of k-means clustering. First, let‚Äôs get a general sense of the structure of our wrangled data. dplyr‚Äôs glimpse() is once again useful here.\n\nglimpse(data_user_items)\n\nRows: 2,941\nColumns: 22\n$ user_id                 &lt;chr&gt; \"vqnzdyytdx\", \"lhkctjeylp\", \"fkgfswdpur\", \"bycrrxogpw\", \"xdecczvgb‚Ä¶\n$ apparel                 &lt;dbl&gt; 3, 0, 3, 0, 0, 0, 4, 15, 1, 1, 0, 1, 2, 0, 2, 3, 0, 0, 0, 0, 1, 6,‚Ä¶\n$ bags                    &lt;dbl&gt; 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ shop_by_brand           &lt;dbl&gt; 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, ‚Ä¶\n$ drinkware               &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ‚Ä¶\n$ new                     &lt;dbl&gt; 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 6, 2, 0, 0, 0, 0, 1, 1, 0, 4, 0, 0, ‚Ä¶\n$ clearance               &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ accessories             &lt;dbl&gt; 0, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, ‚Ä¶\n$ campus_collection       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0,‚Ä¶\n$ office                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ lifestyle               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ small_goods             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ uncategorized_items     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ stationery              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ google                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ writing_instruments     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ fun                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ unknown                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ electronics_accessories &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ notebooks_journals      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ gift_cards              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ black_lives_matter      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n\n\nFor this time period, the data contains over 2,941 customers and 21 features available for the k-means clustering model. The user_id feature will be excluded from the modeling. Features represent counts of items purchased within each item category for each user during the period. skimr::skim() is again handy for getting a sense of the shape of the data.\n\nskim(data_user_items)\n\n\nData summary\n\n\nName\ndata_user_items\n\n\nNumber of rows\n2941\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nuser_id\n0\n1\n10\n10\n0\n2941\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\napparel\n0\n1\n1.42\n2.13\n0\n0\n1\n2\n28\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nbags\n0\n1\n0.28\n2.02\n0\n0\n0\n0\n75\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nshop_by_brand\n0\n1\n0.30\n1.32\n0\n0\n0\n0\n40\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ndrinkware\n0\n1\n0.30\n1.49\n0\n0\n0\n0\n30\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nnew\n0\n1\n0.56\n2.32\n0\n0\n0\n0\n87\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nclearance\n0\n1\n0.23\n1.03\n0\n0\n0\n0\n20\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\naccessories\n0\n1\n0.49\n2.23\n0\n0\n0\n0\n48\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ncampus_collection\n0\n1\n0.58\n3.40\n0\n0\n0\n0\n128\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\noffice\n0\n1\n0.56\n3.21\n0\n0\n0\n0\n80\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nlifestyle\n0\n1\n0.21\n0.97\n0\n0\n0\n0\n20\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nsmall_goods\n0\n1\n0.05\n0.28\n0\n0\n0\n0\n4\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nuncategorized_items\n0\n1\n0.21\n1.32\n0\n0\n0\n0\n48\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nstationery\n0\n1\n0.19\n2.16\n0\n0\n0\n0\n100\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ngoogle\n0\n1\n0.19\n3.39\n0\n0\n0\n0\n160\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nwriting_instruments\n0\n1\n0.10\n0.99\n0\n0\n0\n0\n40\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nfun\n0\n1\n0.04\n1.40\n0\n0\n0\n0\n75\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nunknown\n0\n1\n0.16\n1.08\n0\n0\n0\n0\n40\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nelectronics_accessories\n0\n1\n0.01\n0.24\n0\n0\n0\n0\n12\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nnotebooks_journals\n0\n1\n0.01\n0.30\n0\n0\n0\n0\n12\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ngift_cards\n0\n1\n0.02\n0.40\n0\n0\n0\n0\n15\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nblack_lives_matter\n0\n1\n0.00\n0.02\n0\n0\n0\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nUpon visual inspection, the shape of the distributions are concerning. Each histogram exhibits the presence of highly skewed data. Outliers are most likely present, and they will need to be addressed. Otherwise, they‚Äôll negatively impact the k-means clustering algorithm. The mean values for the features also indicate a likely issue: limited purchase frequency for certain item categories.\nBefore moving ahead with removing examples, dropping some features might be worth exploring. The objective here is to identify item categories with limited purchase frequency. Any item category with a limited purchase frequency could likely be dropped.\n\nsummarise(\n  data_user_items,\n  across(apparel:black_lives_matter, sum)\n) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 21\n$ apparel                 &lt;dbl&gt; 4169\n$ bags                    &lt;dbl&gt; 832\n$ shop_by_brand           &lt;dbl&gt; 889\n$ drinkware               &lt;dbl&gt; 893\n$ new                     &lt;dbl&gt; 1634\n$ clearance               &lt;dbl&gt; 684\n$ accessories             &lt;dbl&gt; 1438\n$ campus_collection       &lt;dbl&gt; 1700\n$ office                  &lt;dbl&gt; 1633\n$ lifestyle               &lt;dbl&gt; 607\n$ small_goods             &lt;dbl&gt; 143\n$ uncategorized_items     &lt;dbl&gt; 608\n$ stationery              &lt;dbl&gt; 554\n$ google                  &lt;dbl&gt; 572\n$ writing_instruments     &lt;dbl&gt; 291\n$ fun                     &lt;dbl&gt; 122\n$ unknown                 &lt;dbl&gt; 463\n$ electronics_accessories &lt;dbl&gt; 35\n$ notebooks_journals      &lt;dbl&gt; 43\n$ gift_cards              &lt;dbl&gt; 46\n$ black_lives_matter      &lt;dbl&gt; 1\n\n\nReviewing the output, small_goods; fun; electronics_accessories; notebooks_journals; gift_cards; and black_lives_matter have a small enough purcharse frequency to be dropped. Since we don‚Äôt have information on what is being purchased, the unknown feature is also another feature that could be dropped. Here‚Äôs the code to drop these features:\n\ndata_items &lt;-\n  data_user_items |&gt;\n  select(\n    -c(\n      small_goods,\n      fun,\n      electronics_accessories,\n      notebooks_journals,\n      gift_cards,\n      black_lives_matter,\n      unknown\n    )\n  )\n\nWhile the skimr::skim() output includes histograms, we‚Äôll use ggplot2 to examine the distributions in more detail.\n\ndata_item_hist &lt;- data_items |&gt;\n  pivot_longer(\n    cols = apparel:stationery,\n    values_to = \"items\"\n  )\n\nggplot(data_item_hist, aes(x = items)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~name, ncol = 4, scales = \"free\") +\n  labs(\n    title = \"Distribution of purchases by item category\",\n    y = \"\",\n    x = \"\"\n  )\n\n\n\n\n\n\n\n\nThe following histograms further confirm the presence of skewed data. It also further provides evidence of another characteristic of concern: low item purchase frequency. Both of these issues will need to be addressed before applying k-means clustering.\nLet‚Äôs first normalize the data, so we can get values across features to be within a similar range. To do this, we‚Äôll transform features into z-scores. The summary() function can be used to confirm the transformation was applied. This step can be done by utilizing the following code:\n\n\n\n\n\n\nNote\n\n\n\nscale() accepts a dataframe with only numeric features. So, I have to remove it, then add it back.\n\n\n\ndata_items_stnd &lt;-\n  as_tibble(scale(select(data_items, -user_id))) |&gt;\n  mutate(user_id = data_items$user_id, .before = 1)\n\n# Verify the standarization was applied\nsummary(data_items_stnd)\n\n   user_id             apparel             bags         shop_by_brand       drinkware      \n Length:2941        Min.   :-0.6655   Min.   :-0.1399   Min.   :-0.2297   Min.   :-0.2032  \n Class :character   1st Qu.:-0.6655   1st Qu.:-0.1399   1st Qu.:-0.2297   1st Qu.:-0.2032  \n Mode  :character   Median :-0.1960   Median :-0.1399   Median :-0.2297   Median :-0.2032  \n                    Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n                    3rd Qu.: 0.2734   3rd Qu.:-0.1399   3rd Qu.:-0.2297   3rd Qu.:-0.2032  \n                    Max.   :12.4797   Max.   :36.9390   Max.   :30.1695   Max.   :19.8776  \n      new            clearance        accessories     campus_collection     office       \n Min.   :-0.2391   Min.   :-0.2252   Min.   :-0.219   Min.   :-0.1701   Min.   :-0.1728  \n 1st Qu.:-0.2391   1st Qu.:-0.2252   1st Qu.:-0.219   1st Qu.:-0.1701   1st Qu.:-0.1728  \n Median :-0.2391   Median :-0.2252   Median :-0.219   Median :-0.1701   Median :-0.1728  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.:-0.2391   3rd Qu.:-0.2252   3rd Qu.:-0.219   3rd Qu.:-0.1701   3rd Qu.:-0.1728  \n Max.   :37.2019   Max.   :19.1369   Max.   :21.277   Max.   :37.4975   Max.   :24.7254  \n   lifestyle       uncategorized_items   stationery           google         writing_instruments\n Min.   :-0.2135   Min.   :-0.1566     Min.   :-0.08721   Min.   :-0.05736   Min.   :-0.1002    \n 1st Qu.:-0.2135   1st Qu.:-0.1566     1st Qu.:-0.08721   1st Qu.:-0.05736   1st Qu.:-0.1002    \n Median :-0.2135   Median :-0.1566     Median :-0.08721   Median :-0.05736   Median :-0.1002    \n Mean   : 0.0000   Mean   : 0.0000     Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000    \n 3rd Qu.:-0.2135   3rd Qu.:-0.1566     3rd Qu.:-0.08721   3rd Qu.:-0.05736   3rd Qu.:-0.1002    \n Max.   :20.4745   Max.   :36.2012     Max.   :46.21115   Max.   :47.13470   Max.   :40.4114    \n\n\nReviewing the summary() output post normalization, the maximum values are concerning. There are some users who purchased items within categories at a very significant rate. Although this is a nice problem to have for the merchandise store (i.e., big purchases are good for the bottom line), it may cause problems when specifying the clustering algorithm.\nIndeed, a couple of approaches could be taken here. For one, the outliers could be retained. This will likely highly affect the k-means algorithms‚Äô ability to effectively identify useful clusters, though. The second option is to drop examples that we consider to be outliers. Let‚Äôs think this through a bit.\nMore than likely, any example with the number of items purchased above 3 standard deviations beyond the mean should be dropped. The merchandise store is likely meant for business-to-consumer (B2C) sales, rather than business-to-business (B2B) sales. As such, the amount of items purchased during a typical customer transaction will likely be of a volume that is reasonable for consumer purchases (e.g., who needs more than 50 items of stationery?). Such large purchases are likely a B2B transaction, where large volumes of items are being bought. Given the intent of the store to be B2C, then examples exhibiting such large purchase volumes should be dropped.\nAdditional information could be used to further verify this assumption. We likely have a Customer Relationship Management (CRM) system with information about who the customers of these purchases are, and thus we could use this information to confirm if a purchase was for a business or individual. Since the ability to obtain this additional information is not possible, dropping these outliers before clustering is the best option. With all that said, here‚Äôs the code to further explore customers considered to be outliers. head() is used here to limit the output.\n\ndata_items_stnd |&gt;\n  filter(if_any(-c(user_id), ~ . &gt; 3)) |&gt;\n  head(n = 10)\n\n# A tibble: 10 √ó 15\n   user_id    apparel   bags shop_by_brand drinkware    new clearance accessories campus_collection\n   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 qifwtgogoz   6.38  -0.140        -0.230    -0.203 -0.239    -0.225      -0.219             7.78 \n 2 dootkphyen  -0.665 -0.140         4.33     -0.203  2.34     -0.225      -0.219            -0.170\n 3 qksqqiqfjm  -0.665 -0.140         1.29     -0.203  0.191     0.743       0.677             0.124\n 4 cdykzwveuk  -0.665 -0.140        -0.230    10.5   -0.239    -0.225      -0.219            -0.170\n 5 yigpconufy  -0.665 -0.140        -0.230    -0.203  6.65     -0.225      -0.219            -0.170\n 6 ubblgupnrd   4.97  -0.140        -0.230     5.82  -0.239    -0.225      -0.219             1.30 \n 7 shyjuzbuci  -0.665 36.9          -0.230    -0.203 -0.239    -0.225      -0.219            -0.170\n 8 cbpqtertvd  -0.665 -0.140         4.33     -0.203 -0.239    -0.225      -0.219            -0.170\n 9 djoibydgth   0.743 -0.140         0.530     2.47   1.05     -0.225      -0.219            -0.170\n10 zqgnutolrn   3.09  -0.140        -0.230    -0.203 12.7      -0.225      -0.219            -0.170\n# ‚Ñπ 6 more variables: office &lt;dbl&gt;, lifestyle &lt;dbl&gt;, uncategorized_items &lt;dbl&gt;, stationery &lt;dbl&gt;,\n#   google &lt;dbl&gt;, writing_instruments &lt;dbl&gt;\n\n\nHere‚Äôs the code to filter out customers considered to be outliers:\n\ndata_items_stnd &lt;- data_items_stnd |&gt;\n  select(-user_id) |&gt;\n  filter(!if_any(everything(), ~ . &gt; 3))\n\nsummary(data_items_stnd)\n\n    apparel              bags          shop_by_brand        drinkware             new          \n Min.   :-0.66549   Min.   :-0.13986   Min.   :-0.22973   Min.   :-0.20324   Min.   :-0.23910  \n 1st Qu.:-0.66549   1st Qu.:-0.13986   1st Qu.:-0.22973   1st Qu.:-0.20324   1st Qu.:-0.23910  \n Median :-0.19602   Median :-0.13986   Median :-0.22973   Median :-0.20324   Median :-0.23910  \n Mean   :-0.09975   Mean   :-0.05728   Mean   :-0.07586   Mean   :-0.08145   Mean   :-0.08105  \n 3rd Qu.: 0.27345   3rd Qu.:-0.13986   3rd Qu.:-0.22973   3rd Qu.:-0.20324   3rd Qu.:-0.23910  \n Max.   : 2.62080   Max.   : 2.82645   Max.   : 2.81020   Max.   : 2.47420   Max.   : 2.77339  \n   clearance         accessories       campus_collection      office           lifestyle       \n Min.   :-0.22516   Min.   :-0.21897   Min.   :-0.17010   Min.   :-0.17281   Min.   :-0.21349  \n 1st Qu.:-0.22516   1st Qu.:-0.21897   1st Qu.:-0.17010   1st Qu.:-0.17281   1st Qu.:-0.21349  \n Median :-0.22516   Median :-0.21897   Median :-0.17010   Median :-0.17281   Median :-0.21349  \n Mean   :-0.07933   Mean   :-0.08889   Mean   :-0.05533   Mean   :-0.09425   Mean   :-0.07426  \n 3rd Qu.:-0.22516   3rd Qu.:-0.21897   3rd Qu.:-0.17010   3rd Qu.:-0.17281   3rd Qu.:-0.21349  \n Max.   : 2.67916   Max.   : 2.91591   Max.   : 2.77268   Max.   : 2.93947   Max.   : 2.88970  \n uncategorized_items   stationery           google         writing_instruments\n Min.   :-0.15659    Min.   :-0.08721   Min.   :-0.05737   Min.   :-0.10021   \n 1st Qu.:-0.15659    1st Qu.:-0.08721   1st Qu.:-0.05737   1st Qu.:-0.10021   \n Median :-0.15659    Median :-0.08721   Median :-0.05737   Median :-0.10021   \n Mean   :-0.04984    Mean   :-0.05096   Mean   :-0.03856   Mean   :-0.05943   \n 3rd Qu.:-0.15659    3rd Qu.:-0.08721   3rd Qu.:-0.05737   3rd Qu.:-0.10021   \n Max.   : 2.87322    Max.   : 2.69069   Max.   : 1.71234   Max.   : 2.93816   \n\n\nLet‚Äôs take a look at the histograms again, just to get a sense if dropping outliers helped.\n\ndata_items_stnd |&gt;\n  pivot_longer(\n    cols = apparel:stationery,\n    values_to = \"items\"\n  ) |&gt;\n  ggplot(aes(x = items)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~name, ncol = 4, scales = \"free\") +\n  labs(\n    title = \"Distribution of purchases by item category post wrangling\",\n    y = \"\",\n    x = \"\"\n  )\n\n\n\n\n\n\n\n\nAlthough this helped with the issues caused by outliers, we still have to contend with the fact that some customers just don‚Äôt buy certain items. We‚Äôll want to keep this in mind when drawing conclusions from the final clusters.\n\n\nDetermine a k-value\nNow that the data is in a format acceptable for modeling, we need to explore a value for the number of cluster centers, the k-value. Various methods can be used to determine this value. I‚Äôll rely on three methods here: the elbow method; the average silhouette method; and using the realities imposed by the business case (Lantz 2023). Each will be discussed more in-depth in the following sections.\nThe first method is the elbow method, where we visually examine an elbow plot of the total within sum of squares based on the number of potential clusters used for the model. The fviz_nbclust() function from the factoextra package is useful here. We first pass in the data we intend to use for modeling, then base R‚Äôs stats kmeans() function. We‚Äôre also interested in creating the plot using the within sum of squares method, so we specifiy that using the method argument.\n\nfviz_nbclust(data_items_stnd, kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nGiven visual examination of the plot, six clusters seems to be a good starting point. The average silhouette method is another visulization useful for confirming the number of cluster groups for our cluster modelling.\n\nfviz_nbclust(data_items_stnd, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nJust as was expected, the silhouette method provides additional evidence for 6 clusters.\nConsidering the business case is another useful method for determining the k-value. For instance, say our goal is to cluster the data based on the number of campaigns our marketing team has capacity to manage. The number used here is completely arbitrary (i.e., I don‚Äôt have a marketing team to confirm capacity). Thus, for the sake of example, let‚Äôs say we have a marketing team capable of managing 3 campaigns over the holiday season.\nSo, based on the information above, let‚Äôs examine k-means clustering using 3 and 6 groups.\n\n\nSpecify the model\nNow the modeling step. We‚Äôll use base R‚Äôs kmeans() function from the stats package. Since a clustering model with 3 or 6 clusters is being explored here, purrr‚Äôs map() function is used to iterate the model specification. map() returns a list object, which each element of the list is a model output. One for the three and six cluster model. set.seed() is also used for the reproducibility of the results.\n\nset.seed(20240722)\nmdl_clusters &lt;-\n  map(c(3, 6), \\(x) kmeans(data_items_stnd, centers = x))\n\n\n\nEvaluating model performance\nTo assess model fit, we‚Äôll look to the cluster sizes for both clustering models. purrr‚Äôs map function makes this easy. Use the following code to return the size element of kmeans output:\n\nmap(mdl_clusters, \"size\")\n\n[[1]]\n[1] 1642  762  278\n\n[[2]]\n[1]  223  296 1284  660   80  139\n\n\nIdentifying imbalanced groups is priority here. Some imbalance is tolerable, but major imbalances might indicate the presence of model fit issues. The six cluster model looks fairly balanced, where only one group includes a small subset of customers (~80 customers). The three cluster model has one larger group followed by ever decreasing sized groups. Overall, the balance across the different groups seems to be acceptable here.\nVisualizing the clusters is also useful for model assessment. The factoextra package is once again helpful. The fviz_cluster() function from the package can be used to visualize the clusters. The function first takes our model output (mdl_clsuters[[1]]) and the initial data object (data_item_stnd) as arguments. Both the three and six cluster model are visualized using the example code below.\n\nfviz_cluster(mdl_clusters[[1]], data = data_items_stnd, pointsize = 3) +\n  labs(\n    title = \"Three-cluster model of Google Merchandise Store customers\",\n    subtitle = \"View of cluster groupings during the U.S. holiday season\"\n  ) +\n  theme_replace(ggplot_theme)\n\n\n\n\n\n\n\n\n\nfviz_cluster(mdl_clusters[[2]], data = data_items_stnd, pointsize = 3) +\n  labs(\n    title = \"Six-cluster model of Google Merchandise Store customers\",\n    subtitle = \"View of cluster groupings during the U.S. holiday season\"\n  ) +\n  theme_replace(ggplot_theme)\n\n\n\n\n\n\n\n\nfviz_cluster() uses dimension reduction methods to allow for plotting of a multi-dimensional dataset into a two-dimensional representation. The output is useful for identifying general clustering patterns within the data, which could provide additional information about the shape of the clusters. For instance, this type of visualization can be used to visually identify any outliers which may be influencing the shape clusters take.\nIndeed, the plot for both models shows some cluster overlap. This is an indication that the clusters for this data may not be as distinct as we would like. There might be some common products every customer buys, and a few peripheral products that other clusters purchase beyond these common products. These initial results indicate the presence of ‚Äòupsell‚Äô opportunities for the marketing team. You have your core items that most customers purchase, but some clusters seem to purchase items beyond the core products. Thus, some of the marketing campaigns might strategise ways to highlight the upselling of some products.\n\n\nDraw conclusions about clusters\nThe next step is to examine the cluster centers and factor loadings. The goal is to derive conclusions about each cluster from this information. Let‚Äôs first draw conclusions using our three-cluster model. We‚Äôll look to identify specific audience segments based on item category loadings.\nThe kmeans()‚Äôs output has an object labelled centers, which is a matrix of cluster centers. We then inspect these center values for each cluster, which are on the left, and the values associated within each column.\n\nmdl_clusters[[1]]$centers\n\n     apparel        bags shop_by_brand   drinkware         new  clearance  accessories\n1 -0.4476288 -0.06097525   -0.06495514 -0.08339455 -0.09966977 -0.2251558 -0.096510610\n2  0.7127262 -0.06395056   -0.09907281 -0.07850703 -0.07644876 -0.1933938 -0.103778685\n3 -0.2720182 -0.01715296   -0.07663600 -0.07803938  0.01632421  1.0946694 -0.003105749\n  campus_collection        office   lifestyle uncategorized_items  stationery      google\n1       -0.05701557 -0.1077973177 -0.09379899        -0.073095026 -0.06268202 -0.04012104\n2       -0.09325080 -0.0992918176 -0.05330947        -0.005497098 -0.05926368 -0.04110829\n3        0.05854479 -0.0004035005 -0.01628649        -0.033980814  0.04102364 -0.02235330\n  writing_instruments\n1        -0.062586565\n2        -0.072300004\n3        -0.005490095\n\n\n\nmdl_clusters[[1]]$centers |&gt;\n  as_tibble() |&gt;\n  mutate(\n    cluster = 1:3,\n    across(where(is.double), \\(x) round(x, digits = 4)),\n    .before = 1\n  ) |&gt;\n  gt() |&gt;\n  data_color(\n    columns = !(\"cluster\"),\n    rows = 1:3,\n    direction = \"row\",\n    method = \"numeric\",\n    palette = \"Blues\"\n  ) |&gt;\n  tab_header(\n    title = md(\n      \"**Three-cluster k-means model of Google Merchandise store customers factor loadings**\"\n    )\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"Source: Google Analytics data for the Google Merchandise Store\"\n    )\n  ) |&gt;\n  opt_interactive()\n\n\n\nTable¬†1: Three-cluster model factor loadings\n\n\n\n\n\n\nThree-cluster k-means model of Google Merchandise store customers factor loadings\n\n\n\n\n\n\nSource: Google Analytics data for the Google Merchandise Store\n\n\n\n\n\n\n\n\n\n\n\nAfter inspecting the three-cluster k-means model factor loadings, a few groups emerge. Cluster 1 (n = 1,642) seems to be the ‚Äòanything but apparel‚Äô customers. This customer segment really doesn‚Äôt purchase any specific item, but when they shop, they‚Äôre purchasing items other than apparel. Perhaps a campaign could be created to improve customer‚Äôs familiarity with products other than apparel that are available in the merchandise store.\nCluster 2 (n = 762) are the ‚Äòfashion fanatics‚Äô. In fact, it seems this group is mainly purchasing apparel. Maybe our product and marketing teams could consider releasing a new apparel line around the holiday season. A campaign focused on highlighting different apparel pieces could also be explored.\nCluster 3 (n = 278) are ‚Äòdiscount diggers‚Äô. Indeed, this data covers the holiday season, so maybe some customers around this time are trying to find a unique gift, but want to do so on a budget. Perhaps a campaign focused on ‚Äòholiday gift deals‚Äô might appeal to these types of customers.\nIf more nuance is required for the segmentation discussion, the factor loadings for the six-cluster model can be examined. Again, these results suggest the presence of ‚Äòanything but apparel customers‚Äô; ‚Äòfashion fanatics‚Äô; and ‚Äòdiscount diggers‚Äô. However, three additional groups emerge from the six cluster model.\n\nmdl_clusters[[2]]$centers |&gt;\n  as_tibble() |&gt;\n  mutate(\n    cluster = 1:6,\n    across(where(is.double), \\(x) round(x, digits = 4)),\n    .before = 1\n  ) |&gt;\n  gt() |&gt;\n  data_color(\n    columns = !(\"cluster\"),\n    rows = 1:6,\n    direction = \"row\",\n    method = \"numeric\",\n    palette = \"Blues\"\n  ) |&gt;\n  tab_header(\n    title = md(\n      \"**Six-cluster k-means model of Google Merchandise store customers factor loadings**\"\n    )\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"Source: Google Analytics data for the Google Merchandise Store\"\n    )\n  ) |&gt;\n  opt_interactive()\n\n\n\nTable¬†2: Six-cluster model factor loadings\n\n\n\n\n\n\nSix-cluster k-means model of Google Merchandise store customers factor loadings\n\n\n\n\n\n\nSource: Google Analytics data for the Google Merchandise Store\n\n\n\n\n\n\n\n\n\n\n\nThe first additional segment emerging from the six-cluster model includes ‚Äòlifestyle looters‚Äô (n = 223): the customers who purchase products that fit or enhance their lifestyle. Perhaps there‚Äôs room for a campaign focused on highlighting how the Google Merchandise Store‚Äôs products fit within the lives of its customers: most likely people who work in tech.\nThe second segment are the ‚Äòbrand buyers‚Äô (n = 296). These customers are mostly interested in purchasing branded items. Thus, a campaign highlighting the various branded items that are available might be explored.\nThe final group to emerge is our ‚Äòaccessory enthusiasts‚Äô (n = 139). These are customers most interested in purchasing accessories. Perhaps a focus on accessories could be another campaign our marketing team might look at to create.\nDepending on the model reviewed, clustering resulted in the identification of three customer segments campaigns could be targeted: ‚Äòanything but apparel‚Äô, ‚Äòfashion fanatics‚Äô, and ‚Äòdiscount diggers‚Äô. If an expanded list of segments was required, the six-cluster model provides additional information. This includes segments like ‚Äòlifestyle looters‚Äô, ‚Äòbrand buyers‚Äô, and ‚Äòaccessory enthusiasts‚Äô. Indeed, the segment names are up for debate. I would lean on my marketing team to workshop them some more. Analysts are poor at naming things.\n\n\nWrap up\nThis post was a tutorial on how to perform clustering using Google Analytics e-commerce data. The k-means algorithm was used to identify clusters within obfuscated analytics data from the Google Merchandise store. Information about the clusters was used to generate various customer segments. The intent was to use these clusters to better inform future marketing campaigns and targeting strategies.\nThe kmeans() function from the base R stats package was used to specify two clustering models. The elbow method, silhouette method, and information about the business case were used to determine the k-values for the clustering models. The fviz_nbclust() function from the factoextra package was useful for creating visualizations to further confirm the selected k-values. It was determined both a three and six cluster model would be effective to meet our modelling goal for this data. Lastly, the fviz_cluster() function from the factoextra package was used to create visualizations of each model‚Äôs clusters.\nIn truth, this dataset lacked the presence of any interesting cluster groups. I was hoping for some more cross-product segments that could be used for customer segmentation identification. Unfortunately, this wasn‚Äôt the case. Many of the Google Merchandise Store‚Äôs customers fell within a single item-category. This was likely due to low purchase frequency for item categories, which likely is due to customers only buying one or two products with each purchase. Nonetheless, we were able to still identify various customer segments useful for targeting purposes and provide some additional support for potential marketing campaigns.\nSo there you have it, another messing with models post. I hope you found something useful. If not, I hope it was somewhat informative and you found a few takeaways.\nUntil next time, keep messing with models.\n\n\n\n\n\nReferences\n\nLantz, Brett. 2023. Machine Learning with R. 4th ed. packt. https://www.oreilly.com/library/view/machine-learning-with/9781801071321/.\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Messing with Models: K-Means Clustering of {Google}\n    {Analytics} 4 Data},\n  date = {2024-09-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúMessing with Models: K-Means Clustering of\nGoogle Analytics 4 Data.‚Äù September 14, 2024."
  },
  {
    "objectID": "blog/posts/2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel/index.html",
    "href": "blog/posts/2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel/index.html",
    "title": "Exploring the relationship between trash processed by Mr.¬†Trash Wheel and precipitation",
    "section": "",
    "text": "üëã Say hello to Mr.¬†Trash Wheel and friends\nThis week‚Äôs #tidytuesday we‚Äôre looking into data related to Mr.¬†Trash Wheel and friends. Mr.¬†Trash Wheel is a semi-autonomous trash interceptor, who‚Äôs main purpose is to collect trash floating into the Baltimore Inner Harbor. Mr.¬†Trash Wheel is a pretty neat invention. If you‚Äôre interested in how it works, check out the information found here.\nMy curiosity peaked when I came across the statement that most of the trash collected by Mr.¬†Trash wheel is the result of water runoff, and not from people disposing trash directly into the habor. So, I wanted to explore the relationship between precipitation and the amount of trash being collected by Mr.¬†Trash Wheel and friends for my contribution this week.\nIn this post, I created my visualizations using plotly and Tableau.\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(here)\nlibrary(janitor)\nlibrary(tidymodels)\ntidymodels_prefer()\n\n\ndata_trash &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel\",\n    \"trashwheel.csv\"\n  )\n)\n\nRows: 993 Columns: 16\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (4): ID, Name, Month, Date\ndbl (12): Dumpster, Year, Weight, Volume, PlasticBottles, Polystyrene, CigaretteButts, GlassBott...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_balt_precip &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel\",\n    \"balt_precip.csv\"\n  )\n)\n\nRows: 10 Columns: 13\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (13): year, january, february, march, april, may, june, july, august, september, october, no...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData description\nThe data contains observations related to trash collected from 2014 to 2023 by multiple trash wheels. The Baltimore precipitation data came from a tool found here. I simply just copy pasted this data into a Google sheet and saved it as a .csv file. Further wrangling steps for both data sets are included below.\nTo get a better sense of what‚Äôs in the data, I did a quick glimpse() and skim() of both the data_trash and data_balt_precip data sets.\n\nglimpse(data_trash)\n\nRows: 993\nColumns: 16\n$ ID             &lt;chr&gt; \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mist‚Ä¶\n$ Name           &lt;chr&gt; \"Mister Trash Wheel\", \"Mister Trash Wheel\", \"Mister Trash Wheel\", \"Mister T‚Ä¶\n$ Dumpster       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, ‚Ä¶\n$ Month          &lt;chr&gt; \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"June\", \"June\", \"Ju‚Ä¶\n$ Year           &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 201‚Ä¶\n$ Date           &lt;chr&gt; \"5/16/2014\", \"5/16/2014\", \"5/16/2014\", \"5/17/2014\", \"5/17/2014\", \"5/20/2014‚Ä¶\n$ Weight         &lt;dbl&gt; 4.31, 2.74, 3.45, 3.10, 4.06, 2.71, 1.91, 3.70, 2.52, 3.76, 3.43, 4.17, 5.1‚Ä¶\n$ Volume         &lt;dbl&gt; 18, 13, 15, 15, 18, 13, 8, 16, 14, 18, 15, 19, 15, 15, 15, 15, 13, 15, 15, ‚Ä¶\n$ PlasticBottles &lt;dbl&gt; 1450, 1120, 2450, 2380, 980, 1430, 910, 3580, 2400, 1340, 740, 950, 530, 84‚Ä¶\n$ Polystyrene    &lt;dbl&gt; 1820, 1030, 3100, 2730, 870, 2140, 1090, 4310, 2790, 1730, 869, 1140, 630, ‚Ä¶\n$ CigaretteButts &lt;dbl&gt; 126000, 91000, 105000, 100000, 120000, 90000, 56000, 112000, 98000, 130000,‚Ä¶\n$ GlassBottles   &lt;dbl&gt; 72, 42, 50, 52, 72, 46, 32, 58, 49, 75, 38, 45, 58, 62, 64, 56, 47, 65, 63,‚Ä¶\n$ PlasticBags    &lt;dbl&gt; 584, 496, 1080, 896, 368, 672, 416, 1552, 984, 448, 344, 520, 224, 344, 432‚Ä¶\n$ Wrappers       &lt;dbl&gt; 1162, 874, 2032, 1971, 753, 1144, 692, 3015, 1988, 1066, 544, 727, 361, 631‚Ä¶\n$ SportsBalls    &lt;dbl&gt; 7, 5, 6, 6, 7, 5, 3, 6, 6, 7, 6, 8, 6, 6, 6, 6, 5, 6, 6, 7, 6, 6, 6, 5, 6, ‚Ä¶\n$ HomesPowered   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n\nglimpse(data_balt_precip)\n\nRows: 10\nColumns: 13\n$ year      &lt;dbl&gt; 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023\n$ january   &lt;dbl&gt; 2.71, 3.89, 3.50, 2.69, 1.00, 3.15, 3.11, 2.15, 4.27, 1.68\n$ february  &lt;dbl&gt; 4.58, 2.24, 5.70, 1.46, 5.30, 3.64, 2.98, 4.85, 2.31, 2.18\n$ march     &lt;dbl&gt; 4.38, 4.67, 2.10, 3.82, 2.25, 4.14, 3.05, 3.90, 3.13, 1.49\n$ april     &lt;dbl&gt; 8.60, 4.30, 1.31, 3.52, 3.20, 1.46, 5.52, 2.07, 3.92, 4.12\n$ may       &lt;dbl&gt; 3.35, 2.10, 5.24, 5.64, 8.17, 5.51, 1.76, 3.63, 5.39, 0.55\n$ june      &lt;dbl&gt; 3.95, 13.09, 3.20, 1.40, 4.77, 2.95, 5.95, 2.75, 2.95, 4.31\n$ july      &lt;dbl&gt; 2.80, 3.49, 6.09, 7.11, 16.73, 3.85, 3.43, 3.65, 6.25, 6.84\n$ august    &lt;dbl&gt; 7.90, 2.46, 3.96, 4.60, 3.84, 2.39, 11.81, 4.36, 3.71, 3.73\n$ september &lt;dbl&gt; 3.21, 3.25, 4.36, 1.95, 9.19, 0.16, 4.48, 6.04, 3.35, 6.27\n$ october   &lt;dbl&gt; 4.16, 3.40, 0.78, 2.99, 2.69, 6.21, 4.36, 5.24, 4.66, 1.13\n$ november  &lt;dbl&gt; 3.36, 2.42, 1.51, 2.15, 8.14, 1.10, 6.35, 1.33, 2.44, 2.80\n$ december  &lt;dbl&gt; 3.58, 5.85, 2.77, 0.95, 6.54, 3.57, 4.58, 0.82, 4.80, 7.16\n\n\n\nskim(data_trash)\n\n\nData summary\n\n\nName\ndata_trash\n\n\nNumber of rows\n993\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n6\n9\n0\n4\n0\n\n\nName\n0\n1\n18\n21\n0\n4\n0\n\n\nMonth\n0\n1\n3\n9\n0\n14\n0\n\n\nDate\n0\n1\n6\n10\n0\n623\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDumpster\n0\n1.00\n230.88\n185.82\n1.00\n73.00\n176.00\n381.00\n629.00\n‚ñá‚ñÖ‚ñÇ‚ñÇ‚ñÇ\n\n\nYear\n0\n1.00\n2019.57\n2.75\n2014.00\n2018.00\n2020.00\n2022.00\n2023.00\n‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá\n\n\nWeight\n0\n1.00\n2.97\n0.84\n0.61\n2.45\n3.04\n3.53\n5.62\n‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÅ\n\n\nVolume\n0\n1.00\n14.92\n1.61\n5.00\n15.00\n15.00\n15.00\n20.00\n‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ\n\n\nPlasticBottles\n1\n1.00\n2219.33\n1650.45\n0.00\n987.50\n1900.00\n2900.00\n9830.00\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÅ\n\n\nPolystyrene\n1\n1.00\n1436.87\n1832.43\n0.00\n240.00\n750.00\n2130.00\n11528.00\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nCigaretteButts\n1\n1.00\n13728.12\n24049.61\n0.00\n2900.00\n4900.00\n12000.00\n310000.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nGlassBottles\n251\n0.75\n20.96\n15.26\n0.00\n10.00\n18.00\n28.00\n110.00\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ\n\n\nPlasticBags\n1\n1.00\n984.00\n1412.34\n0.00\n240.00\n540.00\n1210.00\n13450.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nWrappers\n144\n0.85\n2238.76\n2712.85\n0.00\n880.00\n1400.00\n2490.00\n20100.00\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nSportsBalls\n364\n0.63\n13.59\n9.74\n0.00\n6.00\n12.00\n20.00\n56.00\n‚ñá‚ñÜ‚ñÇ‚ñÅ‚ñÅ\n\n\nHomesPowered\n0\n1.00\n45.85\n18.23\n0.00\n38.00\n49.00\n58.00\n94.00\n‚ñÇ‚ñÇ‚ñá‚ñÖ‚ñÅ\n\n\n\n\nskim(data_balt_precip)\n\n\nData summary\n\n\nName\ndata_balt_precip\n\n\nNumber of rows\n10\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2018.50\n3.03\n2014.00\n2016.25\n2018.50\n2020.75\n2023.00\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\njanuary\n0\n1\n2.82\n1.00\n1.00\n2.28\n2.91\n3.41\n4.27\n‚ñÇ‚ñÖ‚ñÖ‚ñá‚ñÖ\n\n\nfebruary\n0\n1\n3.52\n1.50\n1.46\n2.26\n3.31\n4.78\n5.70\n‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÖ\n\n\nmarch\n0\n1\n3.29\n1.07\n1.49\n2.45\n3.47\n4.08\n4.67\n‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñá\n\n\napril\n0\n1\n3.80\n2.15\n1.31\n2.35\n3.72\n4.26\n8.60\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÇ\n\n\nmay\n0\n1\n4.13\n2.28\n0.55\n2.41\n4.44\n5.48\n8.17\n‚ñÉ‚ñÉ‚ñÇ‚ñá‚ñÇ\n\n\njune\n0\n1\n4.53\n3.26\n1.40\n2.95\n3.58\n4.65\n13.09\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÇ\n\n\njuly\n0\n1\n6.02\n4.09\n2.80\n3.53\n4.97\n6.69\n16.73\n‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÇ\n\n\naugust\n0\n1\n4.88\n2.87\n2.39\n3.71\n3.90\n4.54\n11.81\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nseptember\n0\n1\n4.23\n2.51\n0.16\n3.22\n3.86\n5.65\n9.19\n‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÇ\n\n\noctober\n0\n1\n3.56\n1.73\n0.78\n2.77\n3.78\n4.58\n6.21\n‚ñÖ‚ñÇ‚ñÖ‚ñá‚ñÖ\n\n\nnovember\n0\n1\n3.16\n2.30\n1.10\n1.67\n2.43\n3.22\n8.14\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\ndecember\n0\n1\n4.06\n2.16\n0.82\n2.97\n4.08\n5.59\n7.16\n‚ñÖ‚ñÇ‚ñá‚ñÖ‚ñÖ\n\n\n\n\n\nLooking further into the data, I noticed a few things of note. Here‚Äôs some things to keep in mind:\n\nThere are missing data (e.g., NAs) within several variables: PlasticBottles, Polystyrene, CigaretteButts, GlassBottles, PlasticBags, Wrappers, and SportsBalls. The documentation didn‚Äôt reference why these were missing and since I wasn‚Äôt using these for my contribution, I didn‚Äôt dig any further.\nThe month has an issue with capitalization. Some string formatting should fix this issue, though I‚Äôm not using this column for my contribution.\nThe Date column needed to be transformed into a date. This can be addressed by using some functions from the lubridate package.\n\n\n\nData wrangling\nNow that we have a better sense of the data, let‚Äôs wrangle it. Below is the code to wrangle both the data_balt_precip and data_trash data sets. Since my precipitation data was aggregated by month, I decided to aggregate the trash data by month.\n\n\n\n\n\n\nNote\n\n\n\nWhile working on my contribution, I learned dplyr‚Äôs transmute function is superseded, and it‚Äôs now suggested to use mutate()‚Äôs .keep = \"none' argument.\n\n\n\ndata_balt_precip &lt;- data_balt_precip |&gt;\n  pivot_longer(\n    cols = january:december,\n    names_to = \"month\",\n    values_to = \"precip\"\n  ) |&gt;\n  mutate(\n    month = match(month, str_to_lower(month.name)),\n    day = 1,\n    month_date = ymd(str_c(year, month, day, sep = \"-\"))\n  ) |&gt;\n  select(\n    month_date,\n    precip\n  )\n\n\ndata_trash &lt;- data_trash |&gt;\n  clean_names() |&gt;\n  mutate(\n    id,\n    name,\n    date = mdy(date),\n    month_date = floor_date(date, \"month\"),\n    dumpster,\n    name = str_to_lower(name),\n    weight,\n    volume,\n    .keep = \"none\"\n  )\n\n\ndata_trash_summ &lt;- data_trash |&gt;\n  group_by(month_date) |&gt;\n  summarise(\n    total_weight = sum(weight),\n    total_volume = sum(volume)\n  ) |&gt;\n  left_join(data_balt_precip)\n\nJoining with `by = join_by(month_date)`\n\nmin(data_trash_summ$month_date)\n\n[1] \"2014-05-01\"\n\nmax(data_trash_summ$month_date)\n\n[1] \"2023-12-01\"\n\n\n\n\nWhat is the relationship between rainfall and the weight and volume of trash processed by the trash wheels?\nTo explore this relationship, I created two scatter plots. The first plot included precipitation and total weight. The second included volume and precipitation. I did this because weight and volume represent different things. Here‚Äôs the code to create the two scatter plots using plotly:\n\nplot_ly(\n  data = data_trash_summ,\n  x = ~precip,\n  y = ~total_weight,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(\n    size = 10,\n    color = \"#6495ED\",\n    line = list(\n      color = \"#151B54\",\n      width = 2\n    )\n  ),\n  text = ~ paste(\n    month_date,\n    \"&lt;br&gt;Precipitation (inches): \",\n    precip,\n    \"&lt;br&gt;Weight (tons): \",\n    total_weight\n  ),\n  hoverinfo = \"text\"\n) |&gt;\n  plotly::layout(\n    title = list(\n      text = \"&lt;b&gt;More precipitation is related to heavier amounts of trash for Mr. Trash Wheel and friends to process &lt;/b&gt;\",\n      font = list(size = 18),\n      xanchor = \"center\"\n    ),\n    yaxis = list(\n      title = \"Total weight of trash (tons)/month\",\n      titlefont = list(size = 14)\n    ),\n    xaxis = list(\n      title = \"Total precipitation in Baltimore (inches)/month\",\n      titlefont = list(size = 14)\n    ),\n    font = list(family = \"arial\", size = 18, face = \"bold\")\n  )\n\n\n\n\n\n\nplot_ly(\n  data = data_trash_summ,\n  x = ~precip,\n  y = ~total_volume,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(\n    size = 10,\n    color = \"#FFAA33\",\n    line = list(\n      color = \"#151B54\",\n      width = 2\n    )\n  ),\n  text = ~ paste(\n    month_date,\n    \"&lt;br&gt;Precipitation (inches): \",\n    precip,\n    \"&lt;br&gt;Volume (cubic yards): \",\n    total_volume\n  ),\n  hoverinfo = \"text\"\n) |&gt;\n  plotly::layout(\n    title = list(\n      text = \"&lt;b&gt;More precipitation is related to a greater volume of trash for Mr. Trash Wheel and friends to process&lt;/b&gt;\",\n      font = list(size = 18),\n      xanchor = \"center\"\n    ),\n    yaxis = list(\n      title = \"Total volume of trash (cubic yards)/month\",\n      titlefont = list(size = 14)\n    ),\n    xaxis = list(\n      title = \"Total precipitation in Baltimore (inches)/month\",\n      titlefont = list(size = 14)\n    ),\n    font = list(family = \"arial\", size = 18, face = \"bold\")\n  )\n\n\n\n\n\nLooking at the individual observations, I had a hard time fathoming how much trash Mr.¬†Trash Wheel and friends were processing. So, here‚Äôs a video giving you a sense of dimension of how much trash is really being collected‚Äìit‚Äôs a lot once you put it into perspective. I mean, in one month, the trash wheels processed nearly 25 of these 20 cubic yard dumpsters worth of trash. If you‚Äôve ever seen these dumpters in real-life, they‚Äôre huge.\nAlthough upon visual inspection it seems a positive relationship is present for both weight and volume of trash, I wanted to further quantify this relationship using a linear model. To do this, I utilized tidymodels to create two simple linear models, one for volume and the other for weight of trash.\n\nlm_mdl &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\nvolume_mdl &lt;-\n  lm_mdl |&gt;\n  fit(total_volume ~ precip, data = data_trash_summ)\n\ntidy(volume_mdl)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     65.4     16.8       3.90 0.000167 \n2 precip          16.0      3.59      4.47 0.0000186\n\n\n\nweight_mdl &lt;-\n  lm_mdl |&gt;\n  fit(total_weight ~ precip, data = data_trash_summ)\n\ntidy(weight_mdl)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    11.6      3.68       3.16 0.00200  \n2 precip          3.53     0.785      4.49 0.0000172\n\n\nBoth models indicate a statistically significant positive relationship between precipitation, volume, and weight of trash processed. In fact, for every additional inch of precipitation a month in Baltimore, the volume of trash processed increases by 16 cubic yards and the weight of trash increases by 3.53 tons.\nThe bottom line, throw your trash away properly. It has down stream effects, literally ‚Ä¶ no pun intended.\n\n\nAn attempt using Tableau\nTo further practice my data visualization tool skills, I recreated these plots using Tableau. You can view this version by clicking here.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring the Relationship Between Trash Processed by {Mr.}\n    {Trash} {Wheel} and Precipitation},\n  date = {2024-03-12},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúExploring the Relationship Between Trash\nProcessed by Mr. Trash Wheel and Precipitation.‚Äù March 12, 2024."
  },
  {
    "objectID": "blog/posts/2024-02-27-tidytuesday-2024-02-27-leap-day/index.html",
    "href": "blog/posts/2024-02-27-tidytuesday-2024-02-27-leap-day/index.html",
    "title": "Exploring the lifespans of historical figures born on a Leap Day",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(here)\n\n\nBackground\nHappy belated Leap Day! This week‚Äôs #tidytuesday is focused on significant historical events and people who were born or died on a Leap Day. The aim of this post is to contribute a couple data visualizations to this social data project. Specifically, I used plotly and Tableau to create my contributions.\n\ndata_births &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-02-27-tidytuesday-2024-02-27-leap-day\",\n    \"births.csv\"\n  )\n)\n\nLet‚Äôs do a quick glimpse() and skim() of our data, just so we get an idea of what we‚Äôre working with here.\n\nglimpse(data_births)\n\nRows: 121\nColumns: 4\n$ year_birth  &lt;dbl&gt; 1468, 1528, 1528, 1572, 1576, 1640, 1692, 1724, 1736, 1792, 1812, 1828, 1836, ‚Ä¶\n$ person      &lt;chr&gt; \"Pope Paul III\", \"Albert V\", \"Domingo B√°√±ez\", \"Edward Cecil\", \"Antonio Neri\", ‚Ä¶\n$ description &lt;chr&gt; NA, \"Duke of Bavaria\", \"Spanish theologian\", \"1st Viscount Wimbledon\", \"Floren‚Ä¶\n$ year_death  &lt;dbl&gt; 1549, 1579, 1604, 1638, 1614, 1704, 1763, 1822, 1784, 1868, 1880, 1921, 1908, ‚Ä¶\n\n\n\nskim(data_births)\n\n\nData summary\n\n\nName\ndata_births\n\n\nNumber of rows\n121\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nperson\n0\n1.00\n6\n29\n0\n121\n0\n\n\ndescription\n1\n0.99\n12\n95\n0\n107\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear_birth\n0\n1.00\n1919.90\n101.01\n1468\n1920\n1944.0\n1976\n2004\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nyear_death\n65\n0.46\n1933.61\n126.53\n1549\n1920\n1989.5\n2013\n2023\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\n\n\n\n\n\nData description\nThis week‚Äôs data comes from the February 29th Wikipedia page. Three data sets are made available, one focused on significant events, as well as births and deaths of historical figures that occurred on a Leap Day. Given what‚Äôs available, I was interested in exploring the age and lifespan of the historical figures born on a Leap Day. Here‚Äôs the wrangling code I created to explore the data.\n\ndata_age &lt;- data_births |&gt;\n  mutate(\n    is_alive = ifelse(is.na(year_death), 1, 0),\n    year_death = ifelse(is.na(year_death), 2024, year_death),\n    age = year_death - year_birth\n  ) |&gt;\n  arrange(desc(age)) |&gt;\n  relocate(person, description, year_birth, year_death, age)\n\ndata_age$person &lt;- factor(\n  data_age$person,\n  levels = data_age$person[order(data_age$year_birth)]\n)\n\n\n\nWhat are the lifespans of historical figures born on a leap day?\nTo explore this question, I decided to create a dumbbell chart. In the chart, the blue dots represent the person‚Äôs birth year. The black dot represents the year the person died. Absence of the black dot indicates a person is still alive, while the grey line represents the person‚Äôs lifespan. If you hover over the dots, a tool tip with information about each person is shown.\n\nnot_alive &lt;- data_age |&gt; filter(is_alive == 0)\n\nplot_ly(\n  data_age,\n  color = I(\"gray80\"),\n  text = ~ paste(\n    person,\n    \"&lt;br&gt;\",\n    \"Age: \",\n    age,\n    \"&lt;br&gt;\",\n    description\n  ),\n  hoverinfo = \"text\"\n) |&gt;\n  add_segments(\n    x = ~year_birth,\n    xend = ~year_death,\n    y = ~person,\n    yend = ~person,\n    showlegend = FALSE\n  ) |&gt;\n  add_markers(\n    x = ~year_birth,\n    y = ~person,\n    color = I(\"#0000FF\"),\n    name = \"Birth year\"\n  ) |&gt;\n  add_markers(\n    data = not_alive,\n    x = ~year_death,\n    y = ~person,\n    color = I(\"black\"),\n    name = \"Year passed\"\n  ) |&gt;\n  layout(\n    title = list(\n      text = \"&lt;b&gt;Lifespans of historical figures born on a Leap Day&lt;/b&gt;\",\n      xanchor = \"center\",\n      yanchor = \"top\",\n      font = list(family = \"arial\", size = 24)\n    ),\n    xaxis = list(\n      title = \"Year born | Year died\"\n    ),\n    yaxis = list(\n      title = \"\"\n    )\n  )\n\n\n\n\n\n\n\nAn attempt using Tableau\nI also created a version of this visualization using Tableau. You can view my attempt here. I was required to make a few concessions with this attempt, as I was unable to have as much fine control of the plot elements as I would have liked. However, I‚Äôm happy with what turned out.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring the Lifespans of Historical Figures Born on a\n    {Leap} {Day}},\n  date = {2024-03-05},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúExploring the Lifespans of Historical\nFigures Born on a Leap Day.‚Äù March 5, 2024."
  },
  {
    "objectID": "blog/posts/2026-01-01-hex-update-009/index.html",
    "href": "blog/posts/2026-01-01-hex-update-009/index.html",
    "title": "The Hex Update: Issue 009",
    "section": "",
    "text": "Let‚Äôs catch up\nHowdy folks!\nWelcome to Issue 009.\n2026! It‚Äôs a new year, and I‚Äôm looking forward to sharing some more interesting work and articles I‚Äôve found. Let‚Äôs get into it.\nThree topics caught my attention this week:\n\nAI and its impact on web traffic\nAn opinion on the impact AI will have on the advertising industry\nA piece highlighting the effects big streaming platforms‚Äô strategies will have on local broadcasters\n\nTo end the week with a little fun, I share an article highlighting the effect your TV settings might have on your viewing experience. I know‚Ä¶ TV settings, a fun topic.\n\n\nThree things\nHere‚Äôs what caught my attention this week:\n\nAs AI Eats Web Traffic, Don‚Äôt Panic‚ÄîEvolve\nPast Hex Updates pointed out the idea of Google Zero: changes in search resulting in significant drops in website referal traffic. This loss in website traffic has been posited, in part, to be caused by the rollout of AI summaries. As a result, digital industry professionals are assessing whether the SEO handbook still applies in the age of Generative Engine Optimization (GEO). This article, from KelloggInsight provides some direction and advice for practitioners. These include: focusing on engagement rather than count metrics; focusing strategy and budget on experimentation; and further developing personalized experiences.\n\nWhy does this matter?\nAlthough this is a moment of disruption, this is not a new or novel mode for the marketing and advertising industry. Search, since the beginning, has always gone through changes. Rather than viewing this as an industry killing event, it should be thought more as a realignment. This realignment involves a renewed focus on the purpose of websites: to connect people with information. This quote from the article puts it best:\n\nBut even if users don‚Äôt click through from AI summaries or personalized newsletters, Cutler thinks adapting to this new era of optimization might just mean getting back to the original purpose of websites: connecting people with information.\n\n\n\n\nI‚Äôve Watched Three ‚ÄúRevolutions‚Äù transform advertising. Here‚Äôs what actually changed\nThis post offers a perspective on how technological revolutions have affected the advertising industry. It highlights several changes: the introduction of the internet in the 1990‚Äôs; programatic ad sales in the 2000‚Äôs; and now AI. Despite these revolutions, the post reorients advertising professionals toward a focus on fundamentals. These include understanding your audience, development and execution of strategy, critical thinking, understanding human psychology, and asking better questions with our data.\n\nWhy does this matter?\nTechnology changes. Disruption will result, as observed with AI. The ways media professionals work will be impacted. But, the fundamentals remain the same. AI doesn‚Äôt overide these fundamentals, but rather they‚Äôre a tool to more effectively leverage the fundamentals. Audiences are human, and they still expect media to reflect some level of humaness. AI might assist in creating a better understanding of auidences, strategy, critical thinking, understanding human psychology, and lead to better question to be asked of our data, but in the end, these are the key components that bring humaness to the work. Humaness I believe can‚Äôt be replaced.\n\n\n\nScreencasting Is Gone. Bundles Are Breaking. The TV Ecosystem Is Locking Down\nThis article provides perspective on the impact large platform packaging and levels of access will have on local broadcaster‚Äôs hold on reach, visibility, and stability. The article posits the industry is moving toward a state of television lockdown. That is:\n\nTogether, they signal the television lock-down ‚Äî a shift toward platform-controlled devices, curated bundles and tightly managed viewing environments. Openness is no longer a feature.\n\nIn fact, the article highlights a larger point: platforms now decide how television is assembeled and promoted, not broadcasters. Such a switch has major implications for local broadcasters, who in the past had structural boundaries that provided protections from platform influence. These protections are quickly disappearing. This will only lead streaming platforms to further harden their control over discovery, content availability, the playback surface, and forms of content monetization that are more predictable to further erode these protections. There‚Äôs also some conversation about how these larger platform offerings influence audiences‚Äô expectations, which broadcaster‚Äôs may not be able to meet.\n\nWhy does this matter?\nAudience expectations matter. The platforms that best meet these expectations will drive reach, engagement, and revenue. Broadcasters, who in the past had protections due to infrastructure, may be losing ground to meet these expectations. This is especially pertinent to local broadcasters, as larger platforms now drive audience expectations due to their ability to deliver engaging experiences in the digital space. Is the promise of openess of broadcasting enough? Maybe? However, the more time audiences spend on platforms that produce amazing experiences, the more they‚Äôll come to expect other forms of delivery to meet the standard generated from large platforms. As such, content access may not be enough. Rather, the delivery experience might be a major component.\n\n\n\n\nJust for fun\n\nStranger Things Creator Says Turn Off ‚ÄúGarbage‚Äù Settings\nIt‚Äôs been almost 10 years since Stranger Things was released on Netflix. Now we‚Äôve come to the series‚Äô conclusion. The majority of audiences will likely watch the final season on their home TVs. Many of these TVs are likely configured to default viewing settings. Have you ever thought about how these settings might alter the original look the creators were striving for? Me neither. As such, one of the Duffer Brothers considered this to be a big enough problem that they needed to address via their socials. I, unforuntately, was too late to catch the message. In the future, though, I‚Äôll be sure to consider my TV‚Äôs settings. If you haven‚Äôt watched the final season yet, consider taking this advice. Also, if you‚Äôre a fan, enjoy the finale.\nThe first update of 2026 is now in the books. I look forward to sharing more.\nI hope you have a wonderful start to the new year, and I wish you all the best this coming year.\nCheers üéâ!\n\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2026,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 009},\n  date = {2026-01-01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2026. ‚ÄúThe Hex Update: Issue 009.‚Äù January\n1, 2026."
  },
  {
    "objectID": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html",
    "href": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html",
    "title": "Notes: The use of ... in R",
    "section": "",
    "text": "The ... construct can be a slippery thing to get a hold of until you know the trick.\n‚Äì R Inferno section 8.3.15\nThe first step toward improvement is admiting you have a problem. My problem? Understanding the dot-dot-dot (i.e., ...) when it comes to writing functions in R. I had an intuitive sense of how ...s worked, especially when using functions that had these as part of their implementation. I struggled, though, when applying them to my own self-defined functions. A few questions would constantly arise: Am I implementing these correctly? What am I missing? These notes are thus:"
  },
  {
    "objectID": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html#going-deeper-with-...-handling",
    "href": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html#going-deeper-with-...-handling",
    "title": "Notes: The use of ... in R",
    "section": "Going deeper with ... handling",
    "text": "Going deeper with ... handling\nOnce I realized there‚Äôs more to the three-dots than the simple forwarding of arguments, I came across other handling strategies. The R Inferno book overviews three strategies for handling arguments passed via the three-dots. The first was discussed above: use a list to capture the argument values passed via dots.\nAnother strategy is to use match.call(). That is,\n\nfn06 &lt;- function(...) {\n  extras &lt;- match.call(expand.dots = FALSE)$...\n  return(extras)\n}\n\nfn06(a = 1, b = 2, c = 3)\n\n$a\n[1] 1\n\n$b\n[1] 2\n\n$c\n[1] 3\n\n\nOr, in situations where your function processes the arguments, then you can use do.call()\n\n# I couldn't get this to work, and I didn't fully understand what was going on here\nfn07 &lt;- function(data, ...) {\n  dots &lt;- list(...)\n  ans &lt;- do.call(mean, dots[names(dots) %in% spec])\n}\n\nfn07(mtcars, trim = .01)\n\nWhile reviewing the do.call() strategy, it wasn‚Äôt immediately apparent to me when this would be useful. I also couldn‚Äôt get it to work using the example provided. Nonetheless, it‚Äôs an available strategy, so someone likely has a need for it and could likely get it to work."
  },
  {
    "objectID": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html#packages-extending-the-three-dots",
    "href": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html#packages-extending-the-three-dots",
    "title": "Notes: The use of ... in R",
    "section": "Packages extending the three-dots",
    "text": "Packages extending the three-dots\n\nrlang‚Äôs dynamic dots\nThe rlang package provides dynamic dots. Some of the package‚Äôs functions extend the functionality of .... Besides argument forwarding and collection, rlang‚Äôs dynamic dots provides additional features. Check out the package‚Äôs docs for a deeper explanation, as the following is just a summary.\nDynamic dots implements what‚Äôs known as injection: the process of modifying a piece of code before R processes it. Dynamic dots has two injection operators, !!! and {. As such, this extended functionality allows for:\n\nargument list splicing\ninjecting names with glue syntax\ntrailing commas, which are ignored\n\nAlthough examples are available in the package‚Äôs documentation, I decided to notate their use as a reminder of how they work.\n\nfn08 &lt;- function(...) {\n  out &lt;- rlang::list2(...)\n  return(out)\n}\n\n# list splicing\nx &lt;- list(a = \"one\", b = \"two\")\n\nfn09 &lt;- function(x) {\n  arguments &lt;- fn06(!!!x)\n  return(arguments)\n}\n\nfn08(x)\n\n[[1]]\n[[1]]$a\n[1] \"one\"\n\n[[1]]$b\n[1] \"two\"\n\n\n\n# name injections, glue syntax\nnm &lt;- \"values\"\nfn08(\"{nm}\" := x)\n\n$values\n$values$a\n[1] \"one\"\n\n$values$b\n[1] \"two\"\n\nfn08(\"prefix_{nm}\" := x)\n\n$prefix_values\n$prefix_values$a\n[1] \"one\"\n\n$prefix_values$b\n[1] \"two\"\n\n\n\n# ignoring trailing commas\nfn08(x = 6, )\n\n$x\n[1] 6\n\n\nIndeed, rlang provides some convenient extensions to ...‚Äôs functionality. Check it out.\nSafer use of ... with ellipsis\nAnother package useful when implementing dots is rlib‚Äôs ellipsis package. The goal of ellipsis is to make the use of ... safer, as some unintended side effects can arise from their use. The package provides three convenience functions to do this:\n\ncheck_dots_used()\ncheck_dots_unnamed()\ncheck_dots_empty()\n\nEach function performs some type of check on the arguments being passed with .... From the documentation, check_dots_used() throws an error if any ... are not evaluated. check_dots_unnamed() errors if any components of ... are named. check_dots_empty() errors if ... is used.\nOne concern of ... is it can ‚Äúsilently swallow‚Äù passed arguments. Say we want to ensure all the arguments passed to ... are evaluated. The check_dots_used() function can be helpful in this case. This function sets up a handler that evaluates when a function terminates, enforcing that all arguments have been evaluated. Otherwise, it will throw an error. For instance,\n\nfn09 &lt;- function(...) {\n  ellipsis::check_dots_used()\n  div_vals(...)\n}\n\ndiv_vals &lt;- function(x, y, ...) {\n  x / y\n}\n\n# works, yay!\nfn09(x = 10, y = 2)\n\n[1] 5\n\n# doesn't work, because we're trying to process more arguments then are available\ntry(fn09(x = 10, y = 2, z = 1))\n\nError in fn09(x = 10, y = 2, z = 1) : Arguments in `...` must be used.\n‚úñ Problematic argument:\n‚Ä¢ z = 1\n‚Ñπ Did you misspell an argument name?\n\n# also helpful when unevaluated unnamed arguments are passed\ntry(fn09(x = 10, y = 2, 1, 2, 3))\n\nError in fn09(x = 10, y = 2, 1, 2, 3) : Arguments in `...` must be used.\n‚úñ Problematic arguments:\n‚Ä¢ ..1 = 1\n‚Ä¢ ..2 = 2\n‚Ä¢ ..3 = 3\n‚Ñπ Did you misspell an argument name?\n\n\nNamed arguments passed with dots may be misspelled. As such, the check_dots_unnamed() function might be useful and a safer option for a function definition. For instance,\n\n# not very safe\nfn10 &lt;- function(..., val_extra = 10) {\n  c(...)\n}\n\n# who hasn't misspelled an argument name before?\nfn10(1, 2, 3, val = 4)\n\n            val \n  1   2   3   4 \n\nfn10(1, 2, 3, val_extra = 4)\n\n[1] 1 2 3\n\n# safer\nfn11 &lt;- function(..., val_extra = 10) {\n  rlang::check_dots_unnamed()\n  c(...)\n}\n\nfn11(1, 2, 3, val = 10)\n\nError in `fn11()`:\n! Arguments in `...` must be passed by position, not name.\n‚úñ Problematic argument:\n‚Ä¢ val = 10\n\nfn11(1, 2, 3, val_extra = 10)\n\n[1] 1 2 3\n\n\ncheck_dots_empty() is useful for when you want users to fully name the details arguments. While reviewing, I felt this strategy not only enforces this but it also allows for more informative errors to be pushed to the console. It also seems to better handle situations where partial argument matching happens.\n\nfn12 &lt;- function(x, ..., foofy = 8) {\n  x + foofy\n}\n\nfn12(3, foofy = 8)\n\n[1] 11\n\nfn12(3, foody = 8)\n\n[1] 11\n\nfn13 &lt;- function(x, ..., foofy = 8) {\n  rlang::check_dots_empty()\n  x + foofy\n}\n\nfn13(3, foofy = 8)\n\n[1] 11\n\nfn13(3, foody = 8)\n\nError in `fn13()`:\n! `...` must be empty.\n‚úñ Problematic argument:\n‚Ä¢ foody = 8\n\n\nThe ellipsis package provides some powerful, useful functionality for handling edge cases that come up when forwarding arguments via ...s. Check it out if you find yourself needing safer ... handling methods."
  },
  {
    "objectID": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html#other-concepts-to-be-aware-of-when-using-...",
    "href": "blog/posts/2025-10-26-til-notes-rstats-function-dots/index.html#other-concepts-to-be-aware-of-when-using-...",
    "title": "Notes: The use of ... in R",
    "section": "Other concepts to be aware of when using ...\n",
    "text": "Other concepts to be aware of when using ...\n\nThis R-bloggers‚Äô post provides some additional overview of ...‚Äôs behavior. I attempt to summarize some of the points shared in the post below.\nThe function receiving the ...s does not itself need ...s as an argument.\n\nfn14 &lt;- function(x, ...) {\n  fn15(...)\n}\n\nfn15 &lt;- function(y) {\n  print(y)\n}\n\nfn14(x = 1, y = 2)\n\n[1] 2\n\n\nThis is useful because if we pass anything other than y, we get an error.\n\nfn14(x = 1, y = 2, z = 3)\n\nError in fn15(...): unused argument (z = 3)\n\n\nUsing dots within both functions allows for the passing on an additional named argument without error. In this example, the z argument. This is not necessarily a utility of the dots, but rather a behavior to note. A behavior one would likely want to account for using functions from the ellipsis package, which was already discussed above.\n\nfn14 &lt;- function(x, ...) {\n  fn15(...)\n}\nfn15 &lt;- function(y, ...) {\n  print(y)\n}\n\nfn14(x = 1, y = 2)\n\n[1] 2\n\nfn14(x = 1, y = 2, z = 3)\n\n[1] 2\n\n\nlist(...) can be used to interpret the arguments passed using .... Why? This is helpful when you want to amend the arguments before forwarding them on. In other words, save the output of list(...) as a variable, amend this variable, then call the next function with the amended variable using do.call(). For instance, from the example from the original post:\n\nfn16 &lt;- function(x, ...) {\n  args &lt;- list(...)\n  if (\"y\" %in% names(args)) {\n    args$y &lt;- 2 * args$y\n  }\n  do.call(fn15, args)\n}\n\nfn15 &lt;- function(y) {\n  print(y)\n}\n\nfn16(x = 1, y = 2)\n\n[1] 4\n\n\nIn this case, if an argument y is included, then it will be captured, and subsequently doubled before being outputted. A neat additional strategy for handling forwarded arguments, which has some utility for conditionally modifying argument values based on what‚Äôs forwarded."
  },
  {
    "objectID": "blog/posts/2023-10-22-til-correlations-with-corrr/index.html",
    "href": "blog/posts/2023-10-22-til-correlations-with-corrr/index.html",
    "title": "TIL: Calculating correlations with corrr",
    "section": "",
    "text": "Today I learned calculating, visualising, and exploring correlations is easy with the corrr package.\nIn the past, I would rely on Base R‚Äôs stats::cor() for exploring correlations. This function is a powerful tool if you‚Äôre looking to do additional analysis beyond investigating correlation coefficients. stats::cor() has its pain points, though. Sometimes, I just want a package to explore correlations quickly and easily.\nI recently stumbled across the corrr package. It met all the needs I listed above. The purpose of this post is to highlight what I‚Äôve learned while using this package, and to demonstrate functionality I‚Äôve found useful. To get started, let‚Äôs attach some libraries and import some example data.\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\nlibrary(corrr)\nlibrary(here)"
  },
  {
    "objectID": "blog/posts/2023-10-22-til-correlations-with-corrr/index.html#footnotes",
    "href": "blog/posts/2023-10-22-til-correlations-with-corrr/index.html#footnotes",
    "title": "TIL: Calculating correlations with corrr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I was finishing this post, the Nebraska Women‚Äôs Volleyball team beat #1 Wisconsin in a five set thriller.‚Ü©Ô∏é\nThe Nebraska Women‚Äôs Volleyball team broke the World Record for a women‚Äôs sporting event on 2023-08-30. Official attendance was 92,003.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/posts/2025-09-01-til-notes-github-search/index.html",
    "href": "blog/posts/2025-09-01-til-notes-github-search/index.html",
    "title": "Notes: Using GitHub search",
    "section": "",
    "text": "I‚Äôve recently been experimenting more with GitHub search. Much of this has taken place using GitHub‚Äôs command line tool: gh. I wanted to learn more, since I began to see the usefulness of this tool. The goal, then, was to identify ways I could be more productive using search. This involved a review of GitHub‚Äôs docs. The following are notes from my review."
  },
  {
    "objectID": "blog/posts/2025-09-01-til-notes-github-search/index.html#sorting-results",
    "href": "blog/posts/2025-09-01-til-notes-github-search/index.html#sorting-results",
    "title": "Notes: Using GitHub search",
    "section": "Sorting results",
    "text": "Sorting results\nWe can also sort results in meaningful ways, like sorting by the number of comments. The --sort flag is used for sorting our results. Let‚Äôs apply this to our previous search.\ngh search issues release in:title repo:tidyverse/ggplot2 --state closed --sort \"comments\"\nSeveral values are available that can be passed to the --sort flag. Check out gh‚Äôs docs for additional info."
  },
  {
    "objectID": "blog/posts/2021-04-02-post-intro/index.html",
    "href": "blog/posts/2021-04-02-post-intro/index.html",
    "title": "Intro Post",
    "section": "",
    "text": "Hello World!\nI have decided to start writing a blog. I have never attempted anything like this before, but I felt it was time to start organizing some of my projects and analyses into one central location. For some time, I really wanted to develop a space where I could discuss topics I find interesting, both professionally and personally. So, Hello World!, my name is Collin, and this is my blog.\nI‚Äôm also on other platforms, however, I really don‚Äôt engage on them too often. Nevertheless, you can find more information about me and my projects in the following locations:\n\nGitHub is a great place to see the projects I am working on. Most are professional at this time.\nTwitter, I haven‚Äôt posted anything in a while, though I retweet and like stuff often.\nEmail, the best channel to get a hold of me: collin.berke@gmail.com.\n\n\n\nThe purpose of this blog\nThe purpose of this blog is to serve as a location for me to express my thoughts on topics I find interesting. To be honest, I don‚Äôt expect this to be a really niche blog with one focused, clear purpose. Most likely it will be data analysis and/or visualization focused, which I will apply to develop posts in areas I find interesting: open-source software, media/marketing analytics, data analysis, sports, media, etc. My purpose may become more refined once I find my voice.\n\n\nThe inspiration and motivation to do this blog\nI have spent countless hours reading, re-reading, bookmarking, and Googling multiple topics regarding the use of the statistical computing programming language called R. Much of this time has been spent accessing useful, open-source, and free content that has aided me professionally, and it has contributed to my deeper understanding of the topics I find interesting. I couldn‚Äôt even begin to describe how grateful I am for those who have spent time organizing and drafting content others find useful. In fact, it‚Äôs the #Rstats community that has motivated me to put this blog together, as I have seen how helpful and open it is to aiding in the development of others.\nI now feel I am in a place of not only being a consumer but a producer of this information. I will never be an expert in this area, as there is too much to learn for just one person. As one of my favorite podcasts (i.e., Make Me Smart With Kai and Molly) states in every episode, ‚ÄúNone of us is as smart as all of us.‚Äù Thus, I feel it is time to start organizing and drafting content others will hopefully find useful, at the very least amusing. Even if this blog helps one person, that will be enough motivation to keep me working on it.\n\n\nWhat‚Äôs up next?\nNot sure. I‚Äôm just excited I got this blog up and running. Most likely I‚Äôll do something sports related. Who knows‚Äìstay tuned.\n\n\nReferences & Acknowledgements\n\nI make every attempt to properly cite information from other sources. If I have failed to properly attribute credit to a source, please kindly let me know.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2021,\n  author = {Berke, Collin K},\n  title = {Intro {Post}},\n  date = {2021-04-02},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2021. ‚ÄúIntro Post.‚Äù April 2, 2021."
  },
  {
    "objectID": "blog/posts/2023-10-24-til-temp-directories/index.html",
    "href": "blog/posts/2023-10-24-til-temp-directories/index.html",
    "title": "TIL: Using base::tempdir() for temporary data storage",
    "section": "",
    "text": "Today I learned how to store data in R‚Äôs per-session temporary directory.\nRecently, I‚Äôve been working on an R package for a project. This package contains some internal data, which is intended to be updated from time-to-time. As part of the data update process, I‚Äôm required to download a set of .zip files from cloud storage, unzip, wrangle, and make the data available in the package via the data folder.\nGiven the data I‚Äôm working with, I wanted to avoid storing pre-wrangled data in the data-raw directory of the package. My main concern was an accidental check-in of pre-proccessed data into version control. So, I sought out a means to solve this problem.\nThis post aims to overview an approach using R‚Äôs per-session temporary directory to store data temporarily. Specifically, this post will discuss the use of base::tempdir() and other system file management functions made available in R to store data in this directory.\n\n\n\n\n\n\nWarning\n\n\n\nUsing R‚Äôs per-session temporary directory may not be the right solution for your specific situation. If you‚Äôre working with sensitive data, make sure you follow your organization‚Äôs guidelines on where to store, access, and properly use your data.\nI am not a security expert.\n\n\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.6\n‚úî forcats   1.0.1     ‚úî stringr   1.6.0\n‚úî ggplot2   4.0.1     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.2.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat are temporary directories?\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, I drafted this post on a computer running a Mac operating system. Some of what gets discussed here may not apply to Windows or Linux systems. The ideas and application should be similar, though I haven‚Äôt fully explored the differences.\n\n\nThe temporary directory, simply, is a location on your system. You can store files in this location just like any other directory. The difference is data stored within a temporary directory are not meant to be persistent, and your system will delete them automatically. File deletion either occurs when the system is shut down or after a set amount of time.\nIf you‚Äôre working on a Mac operating system, you can get the path to the temporary directory by running the following in your terminal:\necho $TMPDIR\nWhen I last ran this command on my system, echo returned the following path (later we‚Äôll use base::tempdir() to get and use this path in R).\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/\nThis directory is located at the system level. The cd command can be used to navigate to it from the terminal. You may have to back up a few directories if your root starts at the user level, though. This is pretty standard, especially if you‚Äôre working on a Mac.\n\n\n\n\n\n\nNote\n\n\n\nSince I‚Äôm drafting this post on my personal machine, I‚Äôm not aware if you need admin privileges to access this folder. As such, you may run into issues if you‚Äôre not an admin on your machine.\n\n\nWith my curiosity peaked, I sought more information about what this directory was used for on a MacOS. Oddly enough, there is very little about this directory online. From what I can deduce, the /var directory is mainly a per-user cache for temporary files, and it provides security benefits beyond other cache locations on a Mac system (again, I‚Äôm not a security expert, so my previous statement may be inaccurate). Being that this location is temporary, this cache gets cleared every time the system restarts or every three days.\nAlthough there‚Äôs a lack of information about this directory online, I did come across a few blog posts and a Stack Overflow answer that were helpful in understanding this temporary directory in more depth: post 1; post 2; post 3. You might find these useful if you want to learn more. However, for me, the above is as far as I wanted to go to understand its purpose.\n\n\nAccess the temporary directory using base::tempdir()\nAt the start of every session, R creates a temporary per-session directory, and it removes this temporary directory when the session ends.. This temporary directory is stored in the system‚Äôs temporary directory location (e.g., /var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/). R also provides several functions to work with the temporary directory, create, and interact with files within it.\nbase::tempdir() can be used to print the file path of the temporary directory on your system. Let‚Äôs run it and take a look at what happens.\n\ntempdir()\n\nOutputted to the terminal is the path to the R session‚Äôs temporary directory. When I ran it, the returned path looked like this:\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T//RtmpaYxspA\nThe temporary directory R uses for the current session is labeled using the RtmpXXXXXX pattern. The final six characters of the path (i.e., the Xs) are determined by the system. Note, tempdir() doesn‚Äôt create this directory, it just prints the temporary directory‚Äôs path to the console. This directory is created every time a R session begins.\nSince the temporary directory is just like any location on your computer, you can navigate to it from your terminal during an active R session. With your terminal pointing to the temporary directory, you can use the following code to find R‚Äôs per-session temporary directory:\nla | grep \"Rtmp\"\nLet‚Äôs take a peak at what‚Äôs in this directory. R‚Äôs list.files() function can be helpful in this case.\n\nlist.files(tempdir())\n\ncharacter(0)\n\n\nMost R setups should start with an empty per-session directory. So the above should return character(0). Despite being empty now, list.files() will become handy again once we start to write files to this location.\n\n\nWriting files to the temporary directory\nNow that we know a little more about this temp directory and where it is located on our system, let‚Äôs write some data to it. We can do this by doing something like the following.\n\nwrite_csv(mtcars, file = paste0(tempdir(), \"/mtcars.csv\"))\n\nNow when we list the files in the temporary directory (e.g., list.files(tempdir())), you should see the mtcars.csv file.\nIf you‚Äôre looking to create files with unique names, you can pass the tempfile() function to the file argument. This looks something like this:\n\nwrite_csv(\n  mtcars,\n  file = tempfile(pattern = \"mtcars\", fileext = \".csv\")\n)\n\ntempfile() creates unique file names, which concatenates together the file path, the character vector passed to the pattern argument, a random string in hex, and the character vector inputed to the fileext argument. When you list the files in the temporary directory now, you‚Äôll see the initial mtcars.csv file along with a file that looks something like this: mtcars7eb3503ac74c.csv. The random hex string ensures files remain unique.\nIndeed, the above is just one way to write files to the temporary directory. You can use other methods to read and write files at this location. However, you now know what is needed to interact with this directory, read and write files to and from it. At this point you can do any data wrangling steps your project requires. After which, we can go about deleting our files from this directory.\n\n\nDeleting files with file.remove()\nAlthough these files will eventually be removed by the system, we should be proactive and clean up after ourselves.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre using this approach within functions, especially if their intended to be used by other users, you‚Äôll want to be clear they will write data to and remove data from the user‚Äôs system.\nIndeed, it‚Äôs considered poor practice to change the R landscape on a user‚Äôs computer without good reason. So the least we can do here is clean up after ourselves.\n\n\nTo delete our files we wrote to the temporary directory, run the following in the console:\n\nfile.remove(list.files(tempdir(), full.names = TRUE, pattern = \".csv\"))\n\n[1] TRUE TRUE\n\n\nThe arguments of the list.files() function should be pretty straightforward. We want file paths to be full length (i.e., full.names = TRUE) and to list only files with the .csv extension (i.e., pattern = \".csv\"). Then, we use these full file paths within the file.remove() function, which will remove the files from R‚Äôs temporary directory.\n\n\nWrap-up\nToday I learned more about R‚Äôs per-session temporary directory, and how it can be used to write files not intended for persistent storage. I also learned how to use several base R functions to create files within this temporary directory by using tempfile() and tempdir(). I also demonstrated how the list.files() function can be used to list files within any directory on your system, specifically using it to list files in R‚Äôs temporary directory. Finally, I highlighted how files in the temporary directory can be deleted using the file.remove() function.\nHave fun using R‚Äôs per-session temporary directory. Cheers üéâ!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {TIL: {Using} `Base::tempdir()` for Temporary Data Storage},\n  date = {2023-11-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. ‚ÄúTIL: Using `Base::tempdir()` for Temporary\nData Storage.‚Äù November 3, 2023."
  },
  {
    "objectID": "blog/posts/2024-12-27-til-tidyr-separate-character-string/index.html",
    "href": "blog/posts/2024-12-27-til-tidyr-separate-character-string/index.html",
    "title": "TIL: Separate character strings into rows and columns using tidyr functions",
    "section": "",
    "text": "TIL, as of tidyr 1.3.0, there‚Äôs a new family of string separation functions:\n\nseparate_wider_delim()\nseparate_wider_position()\nseparate_wider_regex()\nseparate_longer_delim()\nseparate_longer_position()\n\nThese functions generally do two things:\n\nSeparate strings into individual rows or columns.\nSeparate strings by some character delimiter, position, or regular expression.\n\nIf you‚Äôve used tidyr in the past, you‚Äôre likely familiar with the separate() function. This function was useful in cases where character strings needed to be separated into different columns based on a pattern. While re-reading the 2nd edition of the R for Data Science book for the Online Data Science Learning Community (check us out here), I was reminded separate() was superseded by this family of functions. For myself, I decided writing a post was needed to better understand how to use these functions. In this post I‚Äôll describe each function and build on what‚Äôs discussed in the book by sharing some examples I might use these functions.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2"
  },
  {
    "objectID": "blog/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_many",
    "href": "blog/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_many",
    "title": "TIL: Separate character strings into rows and columns using tidyr functions",
    "section": "too_many",
    "text": "too_many\nSay we have a case where we don‚Äôt need the end of the path, specifically the -monthly_users portion. For example:\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\")\n)\n\nError in `separate_wider_delim()`:\n! Expected 3 pieces in each element of `path`.\n! 12 values were too long.\n‚Ñπ Use `too_many = \"debug\"` to diagnose the problem.\n‚Ñπ Use `too_many = \"drop\"/\"merge\"` to silence this message.\n\n\nWe get an error. The error is the result of having more data then there are columns to separate into. To address this, we need to pass different options to the too_many argument of the function. Let‚Äôs use too_many = \"debug\" to receive additional information on what needs to be fixed. Although the problem is pretty straightforward here, I wanted to show this option in case you‚Äôre confronted with a situation with a more complex separation.\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\"),\n  too_many = \"debug\"\n)\n\nWarning: Debug mode activated: adding variables `path_ok`, `path_pieces`, and `path_remainder`.\n\n\n# A tibble: 12 √ó 7\n   date       age_grp gender path                            path_ok path_pieces path_remainder\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;                           &lt;lgl&gt;         &lt;int&gt; &lt;chr&gt;         \n 1 2024_10_01 1925    f      2024_10_01-1925-f-monthly_users FALSE             4 -monthly_users\n 2 2024_11_01 1925    f      2024_11_01-1925-f-monthly_users FALSE             4 -monthly_users\n 3 2024_12_01 1925    f      2024_12_01-1925-f-monthly_users FALSE             4 -monthly_users\n 4 2024_10_01 1925    m      2024_10_01-1925-m-monthly_users FALSE             4 -monthly_users\n 5 2024_11_01 1925    m      2024_11_01-1925-m-monthly_users FALSE             4 -monthly_users\n 6 2024_12_01 1925    m      2024_12_01-1925-m-monthly_users FALSE             4 -monthly_users\n 7 2024_10_01 2635    f      2024_10_01-2635-f-monthly_users FALSE             4 -monthly_users\n 8 2024_11_01 2635    f      2024_11_01-2635-f-monthly_users FALSE             4 -monthly_users\n 9 2024_12_01 2635    f      2024_12_01-2635-f-monthly_users FALSE             4 -monthly_users\n10 2024_10_01 2635    m      2024_10_01-2635-m-monthly_users FALSE             4 -monthly_users\n11 2024_11_01 2635    m      2024_11_01-2635-m-monthly_users FALSE             4 -monthly_users\n12 2024_12_01 2635    m      2024_12_01-2635-m-monthly_users FALSE             4 -monthly_users\n\n\nThe too_many = \"debug\" outputs a tibble with some additional columns ( *_ok, *_pieces, *_remainder) of information. The * being the name of the column to be separated. These columns contain information to help us quickly diagnose the problem. Using our example data, we get the following:\nThis column is useful for identifying the presence of any variable length strings.\n\npath_ok provides a boolean to quickly identify cases where the separation failed.\npath_pieces represents the number of pieces resulting from separating the string.\npath_remainder shows what‚Äôs left after the separation is performed. This is useful for identifying if there‚Äôs any additional information you want to retain in additional columns.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough not applicable here, a neat trick to quickly identify the columns that didn‚Äôt separate as we expected is to use the following:\n\ndebug_path |&gt; filter(!x_ok)\n\n\n\nNow that we have additional information to help us figure out what‚Äôs going on, we can choose another option for the too_many argument to handle our specific case. We have two additional options beyond error and debug:\n\n\ndrop will drop the additional information that doesn‚Äôt fit into our newly specified columns.\n\nmerge will keep the additional information, but it will merge it with the data in the final column.\n\nLet‚Äôs observe both options:\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\"),\n  too_many = \"drop\"\n)\n\n# A tibble: 12 √ó 3\n   date       age_grp gender\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; \n 1 2024_10_01 1925    f     \n 2 2024_11_01 1925    f     \n 3 2024_12_01 1925    f     \n 4 2024_10_01 1925    m     \n 5 2024_11_01 1925    m     \n 6 2024_12_01 1925    m     \n 7 2024_10_01 2635    f     \n 8 2024_11_01 2635    f     \n 9 2024_12_01 2635    f     \n10 2024_10_01 2635    m     \n11 2024_11_01 2635    m     \n12 2024_12_01 2635    m     \n\n\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\"),\n  too_many = \"merge\"\n)\n\n# A tibble: 12 √ó 3\n   date       age_grp gender         \n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          \n 1 2024_10_01 1925    f-monthly_users\n 2 2024_11_01 1925    f-monthly_users\n 3 2024_12_01 1925    f-monthly_users\n 4 2024_10_01 1925    m-monthly_users\n 5 2024_11_01 1925    m-monthly_users\n 6 2024_12_01 1925    m-monthly_users\n 7 2024_10_01 2635    f-monthly_users\n 8 2024_11_01 2635    f-monthly_users\n 9 2024_12_01 2635    f-monthly_users\n10 2024_10_01 2635    m-monthly_users\n11 2024_11_01 2635    m-monthly_users\n12 2024_12_01 2635    m-monthly_users\n\n\nEither operation is pretty straightforward: drop the additional information or merge what‚Äôs left in the newly created column. Nonetheless, it‚Äôs likely best to debug first. Knowing what‚Äôs going on with your separation before applying a fix can be useful, and it will help you avoid parsing mistakes."
  },
  {
    "objectID": "blog/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_few",
    "href": "blog/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_few",
    "title": "TIL: Separate character strings into rows and columns using tidyr functions",
    "section": "too_few",
    "text": "too_few\nNext, I want to highlight options for when you have too few data. Let‚Äôs go back to some college sports examples, specifically college basketball game log data. Such data might look like this. Take note of the W/L column. Not only are wins and losses denoted, but the variable may also contain info if the win occurred during an overtime period. Let‚Äôs mimic this structure in some example data.\n\ndata_bball_wl &lt;- tibble(\n  game = c(1:6),\n  team = c(\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\"\n  ),\n  w_l = c(\n    \"W\",\n    \"W (1 OT)\",\n    \"L\",\n    \"W (3 OT)\",\n    \"L (2 OT)\",\n    \"W\"\n  )\n)\n\ndata_bball_wl\n\n# A tibble: 6 √ó 3\n   game team     w_l     \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   \n1     1 Nebraska W       \n2     2 Nebraska W (1 OT)\n3     3 Nebraska L       \n4     4 Nebraska W (3 OT)\n5     5 Nebraska L (2 OT)\n6     6 Nebraska W       \n\n\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ots\")\n)\n\nError in `separate_wider_delim()`:\n! Expected 2 pieces in each element of `w_l`.\n! 3 values were too short.\n‚Ñπ Use `too_few = \"debug\"` to diagnose the problem.\n‚Ñπ Use `too_few = \"align_start\"/\"align_end\"` to silence this message.\n\n\nError, so let‚Äôs debug what‚Äôs happening with our separation.\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ots\"),\n  too_few = \"debug\"\n)\n\nWarning: Debug mode activated: adding variables `w_l_ok`, `w_l_pieces`, and `w_l_remainder`.\n\n\n# A tibble: 6 √ó 7\n   game team     w_l      ots   w_l_ok w_l_pieces w_l_remainder\n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;lgl&gt;       &lt;int&gt; &lt;chr&gt;        \n1     1 Nebraska W        &lt;NA&gt;  FALSE           1 \"\"           \n2     2 Nebraska W (1 OT) 1 OT) TRUE            2 \"\"           \n3     3 Nebraska L        &lt;NA&gt;  FALSE           1 \"\"           \n4     4 Nebraska W (3 OT) 3 OT) TRUE            2 \"\"           \n5     5 Nebraska L (2 OT) 2 OT) TRUE            2 \"\"           \n6     6 Nebraska W        &lt;NA&gt;  FALSE           1 \"\"           \n\n\nJust what we thought, rows 1, 3, 5, and 6 don‚Äôt contain enough information to complete our operation of filling the ots variable. The separate_wider_delim() function has two options for the too_many argument to address this issue:\n\n\nalign_end adds NA at the start of short matches to pad to the correct length.\n\nalign_start adds NA at the end of the short matches to pad to the correct length.\n\nI‚Äôll start with align_end first, just to demonstrate what it does, though this operation isn‚Äôt what we‚Äôre looking to do here. Then, I‚Äôll show you align_start, the operation needed to complete our separation successfully.\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ot\"),\n  too_few = \"align_end\"\n)\n\n# A tibble: 6 √ó 4\n   game team     w_l   ot   \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n1     1 Nebraska &lt;NA&gt;  W    \n2     2 Nebraska W     1 OT)\n3     3 Nebraska &lt;NA&gt;  L    \n4     4 Nebraska W     3 OT)\n5     5 Nebraska L     2 OT)\n6     6 Nebraska &lt;NA&gt;  W    \n\n\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ot\"),\n  too_few = \"align_start\"\n)\n\n# A tibble: 6 √ó 4\n   game team     w_l   ot   \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n1     1 Nebraska W     &lt;NA&gt; \n2     2 Nebraska W     1 OT)\n3     3 Nebraska L     &lt;NA&gt; \n4     4 Nebraska W     3 OT)\n5     5 Nebraska L     2 OT)\n6     6 Nebraska W     &lt;NA&gt; \n\n\nIn short, all the align selection does is modify where the NA will be placed, essentially modifying the padding to create a correct length for the separation to be valid.\nGreat, now that we‚Äôve identified where to separate the columns, we just need to do some additional string manipulation to finish the wrangling of this data. Below are the wrangling steps I applied:\n\ndata_bball_wl |&gt;\n  separate_wider_delim(\n    cols = \"w_l\",\n    delim = \" (\",\n    names = c(\"w_l\", \"ot\"),\n    too_few = \"align_start\"\n  ) |&gt;\n  mutate(\n    n_ot = str_remove(ot, \" OT\\\\)\"),\n    ot = ifelse(is.na(ot), FALSE, TRUE),\n  )\n\n# A tibble: 6 √ó 5\n   game team     w_l   ot    n_ot \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;\n1     1 Nebraska W     FALSE &lt;NA&gt; \n2     2 Nebraska W     TRUE  1    \n3     3 Nebraska L     FALSE &lt;NA&gt; \n4     4 Nebraska W     TRUE  3    \n5     5 Nebraska L     TRUE  2    \n6     6 Nebraska W     FALSE &lt;NA&gt;"
  },
  {
    "objectID": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "href": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "",
    "text": "I had the great fortune of being a presenter at this year‚Äôs PBS TechCon conference. The focus of my talk was to introduce attendees to the principles of tidy data and discuss a data pipeline project my team has been working on at Nebraska Public Media. Here‚Äôs the session description:\nAs part of my talk, I mentioned having put together a curated list of resources others could use to learn more about the topics covered. This list can be found in the following section of this blog post. If you‚Äôre interested in discussing these topics further, please reach out."
  },
  {
    "objectID": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "href": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nTidy Data paper published in the Journal of Statistical Software written by Hadley Wickham\nTidy Data Chapter published in the open source R for Data Science book written by Hadley Wickham and Garrett Grolemun\nData Organization: Organizing Data in Spreadsheets post by Karl Broman\nData Organization: Organizing Data in Spreadsheets paper published in The American Statistician written by Karl Broman and Kara Woo\nTidy data section in Data Management in Large-Scale Education Research training modules written by Crystal Lewis\nTidy Data presented by Hadley Wickham\nTowards Data Science post published on Medium summarizing tidy data written by Benedict Neo"
  },
  {
    "objectID": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "href": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Join a Community",
    "text": "Join a Community\n\nR for Data Science Online Learning Community\n\nJoin the Slack workspace\n@Collin Berke to get a hold of me in the workspace"
  },
  {
    "objectID": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "href": "blog/posts/2021-10-19-talk-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Open source workflow management tool",
    "text": "Open source workflow management tool\n\nApache Airflow"
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html",
    "title": "The Hex Update: Issue 002",
    "section": "",
    "text": "New here? Check out this post to learn more.\nI‚Äôm little behind on posting. Nonetheless, here‚Äôs an update of what I‚Äôve found to be interesting in the media industry as of late. We‚Äôve got cookies, AI in journalism, the sea of media choices, another pivot to video for news, the importance of culture for a media organization, the political information ecosystem, and the share of internet traffic for the Chrome browser.\nIf you‚Äôre short on time, here‚Äôs some quick takeaways:\nWanna stay a little longer? Check out my summary below."
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html#google-forgoes-eliminating-cookies-for-now.",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html#google-forgoes-eliminating-cookies-for-now.",
    "title": "The Hex Update: Issue 002",
    "section": "Google forgoes eliminating cookies, for now.",
    "text": "Google forgoes eliminating cookies, for now.\nStarting in 2020, Google announced it would seek to depreciate third-party cookies from the Chrome browser. Following this announcement, advertisers and publishers scrambled to figure out the impact such a change would have on their tracking setups. Given industry pushback, Google scrapped their plans and instead will now give users the ability to make an informed choice. Soon, users will be given a prompt to decide how tracking will be applied across Google‚Äôs search products, and it will make the setting available to users if they change their minds.\nTalk of third-party cookie depreciation hasn‚Äôt been all bad. It‚Äôs prompted further innovation of more privacy-minded tracking methods, though there is little industry consensus on the best path forward. It has, however, motivated a greater emphasis on the collection of first-party data, which has the potential to create significant value. The collection of this type of data is not without issues, though. Privacy, transparency, and security are still a concern when collecting this type of data, and some solutions to address these concerns‚Äìlike data clean rooms‚Äìmay not be feasible or economical for every organization.\n\nHere are some links to go deeper\n\nGoogle gives up trying to eliminate cookies (Axios)\nGoogle‚Äôs official announcement (The Privacy Sandbox)\nWTF is a data clean room? (Digiday‚Äôs WTF series)\n\n\n\nWhat‚Äôs the takeaway?\nLet‚Äôs be clear: we‚Äôre talking about third-party cookies here, which are different from first-party cookies. First-party cookies are the technology analytics services like Google Analytics rely on for tracking events on a publisher‚Äôs platforms. These are not going away, at least for the time being. Third-party cookies are different; they allow for cross-domain tracking (e.g., retargeting).\nDoes this really matter? The majority of the internet has already moved away from third-party cookies. Browsers other than Chrome (e.g., Safari, Firefox) have already depreciated third-party cookies, and the majority of Chrome users have already disabled third-party cookies. I think the value here is not the actual depreciation of third-party cookies, but rather the conversations it has motivated throughout the industry.\nThe bottom line: users expect privacy. This should be the standard. Advertisers and publishers, however, rely on data to create better experiences for users and audiences. Knowing this, then, how can advertisers, publishers, and platforms come to a consensus around a solution that works for both? It may be initiatives like Google‚Äôs Privacy Sandbox, Universal IDs, or a greater reliance on the collection and use of first-party data."
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html#peoples-perceptions-of-the-use-of-ai-in-journalism",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html#peoples-perceptions-of-the-use-of-ai-in-journalism",
    "title": "The Hex Update: Issue 002",
    "section": "People‚Äôs perceptions of the use of AI in journalism",
    "text": "People‚Äôs perceptions of the use of AI in journalism\nThe Reuters Institute recently released a report focused on the public‚Äôs attitudes towards the use of artificial intelligence (AI) in journalism. Its aim was to better understand how audiences‚Äô perceive the use of AI‚Äìspecifically generative AI‚Äìin the creation of news. Using both survey and qualitative responses, the report highlighted some key, nuanced findings.\nSurprisingly, when viewed globally, the report found just under half of respondents have read a large or moderate amount about AI (45%). Results for the US were slightly higher, where 53% reported reading a large or moderate amount about AI. Some differences were also present across demographic groups, especially when looking at younger and older age cohorts (Under 35 = 56%; Over 35 = 42%).\nWhen it comes to perceptions on the use of news produced using AI, only 23% of US respondents mentioned feeling comfortable consuming news mostly produced by AI (Neither/nor = 18%; Uncomfortable = 52%; Don‚Äôt know = 7%). Although this is a pretty broad response towards the use of AI in news, the report went further by stating participant‚Äôs level of comfort is nuanced when different AI use cases were considered in the creation of news.\nThree use cases of AI were presented to participants: behind the scenes work; content delivery; and content creation. According to the qualitative responses, many were comfortable with ‚Äòbehind the scenes‚Äô uses of AI, but were less comfortable when it came to identifying ways to deliver news in new ways and formats, and they were least comfortable with the use of AI for content generation. In fact, many respondents believed a human should always be ‚Äòin the loop‚Äô during the content creation process. It‚Äôs also important to highlight that respondents‚Äô reported having varying comfort levels for different types of content generation AI would be used for. In fact, the report states most respondents were ‚Äòstrongly opposed‚Äô to the use of AI to create realistic-looking photographs or videos, even when disclosed. Finally, the article devotes a section to discussing people‚Äôs perceptions around the disclosure of the use of AI in news production.\n\nHere‚Äôs a link to go deeper\n\nPublic attitudes towards the use of AI in journalism | Reuters Institute for the Study of Journalism\n\n\n\nWhat‚Äôs the takeaway?\nAlthough AI affords many efficiencies and enhancements, news publishers must consider how audiences view the use of AI in news. Audiences are comfortable with the application of AI in cases of improved efficiency and the news consumption experience. However, this report makes it clear: audiences, at least for now, are not fully comfortable‚Äìeven strongly opposed‚Äìwith it being a part of specific types of news content production. This is especially relevant when AI is used to generate or edit realistic photographs and videos. There‚Äôs even negative attitudes towards using this type of content even when it‚Äôs clearly labelled by producers.\nIn sight of these results, it‚Äôs also important to consider consumers‚Äô perceptions around disclosure: what is needed, even ethical, to effectively disclose the use of AI within the creation of news? Moreover, it‚Äôs important to consider how the type of disclosure relates to users‚Äô trust in news. Certainly, there‚Äôs more open questions to explore than answers at this time."
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html#the-sea-of-media-choices-is-bundling-the-answer",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html#the-sea-of-media-choices-is-bundling-the-answer",
    "title": "The Hex Update: Issue 002",
    "section": "The sea of media choices, is bundling the answer?",
    "text": "The sea of media choices, is bundling the answer?\nIt‚Äôs obvious, audiences have a myriad of choices when it comes to consuming media. This isn‚Äôt a new or profound insight. But take a moment and count how many subscription services you have (really count). Now, consider the number of different sources that are accessible to consumers. On average, they have access to 13 different entertainment sources or subscriptions according to a survey performed by Hub Intel. It‚Äôs no surprise consumers have subscription fatigue, and media companies are seeking opportunities to address it. Bundling services is one such strategy, which at times has been dubbed ‚Äúcable 2.0‚Äù.\nKnowing the extent of choice, what would the ideal bundle look like for audiences? A staggering fact,\n\nTV is no longer the center of the entertainment universe.\n\nIndeed, according to the survey‚Äôs results, only about 50% of those 13 sources were premium video. This was even less for younger audiences. When given the opportunity to create their own ‚Äòpreferred bundle‚Äô, respondents‚Äô service bundles went beyond just video and entertainment sources. The most requested services to be included within a bundle were high-speed internet, Netflix, mobile phone service, streaming music subscription, and a MVPD/vMVPD network bundle with live TV.\n\nHere‚Äôs a link to go deeper\n\nBundles Are Back, Baby! (Hub Intel on Substack)\n\n\n\nWhat‚Äôs the takeaway?\nThere‚Äôs one main takeaway: the services audiences are willing to pay for and are prioritising in their media consumption diet are internet-based, rather than legacy type media services. We know this is the case because of how people responded to prompts about what they view as an ideal service bundle.\nIt‚Äôs also important to consider the power of bundling strategies. This includes non-traditional types of bundles. Think of one of the original successful bundles: TV, internet, and home phone. Not necessarily related services, but it was a bundle consumers found value in and were willing to pay for. It would be interesting to explore how bundling strategies could be implemented across various media organizations, even some non-traditional bundling might provide some interesting value to audiences."
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html#another-pivot-to-video",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html#another-pivot-to-video",
    "title": "The Hex Update: Issue 002",
    "section": "Another pivot to video?",
    "text": "Another pivot to video?\nAccording to a study published by the Reuters Institute for the Study of Journalism, video and video-led services are driving growth in news. Despite the lack of success of the first pivot to video for publishers, social media platforms are once again prioritizing video, especially short-form content. As suggested in an article from NiemanLab, many of the traditional social media platforms are modifying their platforms and products to prioritize short-form video content in a bid to compete with the massive growth of TikTok.\nNews consumption growth is not evenly spread across platforms, but rather it‚Äôs concentrated to a select few. Much of this growth, globally, is not coming from legacy social media networks, but it is coming from platforms like YouTube, TikTok, and Instagram. This growth is also being propelled by the younger audiences on these platforms. Zooming in on these groups, three motivations for using social video for news were identified:\n\nTrustworthiness and authenticity. The unfiltered nature of video makes the coverage more trustworthy and authentic.\nConvenience. They‚Äôre already on these platforms, and the algorithm acts as a filter to send content users find to be interesting.\nAccess to different perspectives. This includes perspectives from others that align with a user.\n\nDespite this growth, publishers creating content aimed to reach audiences on these platforms must confront specific issues. Capturing attention is first, much of which is going to influencers and celebrities and not necessarily journalists or news organizations. Even if news organizations capture user‚Äôs attention, they‚Äôre confronted with another big issue: the monetization of content on these platforms.\n\nHere are some links to go deeper\n\nIs the news industry ready for another pivot to video\nWhy Facebook And Mark Zuckerberg Went All In On Live Video\nDid Facebook‚Äôs faulty data push news publishers to make terrible decisions on video?\n\n\n\nWhat‚Äôs the takeaway?\nIt‚Äôs clear, video content is once again the priority for these platforms. Media organizations, especially news organizations, must confront this fact to remain relevant. Users are already on these platforms, and they receive certain gratifications from consuming video in these places. It‚Äôs a simple idea: be where your audience is and serve them with what they want. A simple thought in theory, but more challenging in terms of what‚Äôs needed to execute. Monetization strategies will also need to be considered if a third-party platform is relied on to reach and engage audiences."
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html#an-interview-with-netflixs-co-ceo-greg-peters",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html#an-interview-with-netflixs-co-ceo-greg-peters",
    "title": "The Hex Update: Issue 002",
    "section": "An interview with Netflix‚Äôs co-CEO Greg Peters",
    "text": "An interview with Netflix‚Äôs co-CEO Greg Peters\nThe Decoder Podcast held an interview with Netflix‚Äôs co-CEO Greg Peters. Nilay Patel, the host of Decoder, devoted some questions to Netflix‚Äôs culture and its view on where the company fits within the broader media and entertainment industry.\nWhile on the topic of culture, Reed Hastings‚Äô, co-founder of Netflix, 125-page powerpoint presentation was discussed. This document was the initial articulation of the culture Netflix strives to achieve. I highly suggest looking through it. Though it‚Äôs been refined since originally shared, many of the key tenets of the original document are present in the current version.\nA few sections of the document struck me. For one, the emphasis on Netflix‚Äôs culture being likened to a sports team rather than a family was attention grabbing. They strive to hire the best talent and expect their workforce to be high-performing based on several general values.\nOn a first read, some might view this type of culture as being overly competitive and ruthless. But upon a deeper read, you begin to notice this is not the case: it‚Äôs emphasis is about embracing an open, honest, growth mindset type of culture that focuses its talent on solving the right challenges that are meaningful to the business.\nThe weight culture takes within the organization was also an interesting listen. Greg Peters even mentioned that although culture, strategy, and execution are all needed for a successful organization, he would take a great culture over excellent strategy and execution. You can still have a successful company if you have a great culture and just mediocre strategy and execution.\nAlthough not directly addressed, the importance of understanding product market fit was an important theme that came up. Knowing where your organization and products fit within the market and the lives of your customers is critical. Additionally, product market fit was also connected back to the importance of culture. Although process is important, especially in some areas (e.g., finance and legal), it can create rigidity and brittleness. This limits media organization‚Äôs ability to move fast within the competitive entertainment industry and broader attention economy.\nProcess models were also addressed in the interview. It was posited that many of the models organizations use today were developed during the industrial revolution. These models certainly have their place in industries like manufacturing where you can iterate on a process to squeeze out as much efficiency as possible for a single product line. However, in the creative-inventive industry, things change quickly and pivots are required to stay relevant. These older models of process efficiency just don‚Äôt work in this environment. With this in mind, media organizations need to cultivate and embrace a culture that facilitates pivoting, so it can continue to provide products and experiences the market wants.\n\nHere‚Äôs a link to go deeper\n\nNetflix co-CEO Greg Peters on the future of streaming and where ads, AI, and games fit in -Netflix Culture‚ÄìThe Best Work of Our Lives\n\n\n\nWhat‚Äôs the takeaway\nBottom line: listen to the whole podcast episode. It covers too many interesting topics to give all the themes discussed fair coverage in this summary. If forced to choose a few topics to focus on while listening, I‚Äôd closely focus on the discussion around culture, understanding product market fit, and how these two relate for organizations operating in a creative-inventive industry.\nNetflix‚Äôs culture may not be the best fit for every organization. However, it‚Äôs still an interesting articulation of the importance of culture for the success of an organization. It‚Äôs also critical, when working in the ever-changing, fast-moving media industry to consider how culture influences a media‚Äôs organization‚Äôs ability to pivot quickly to meet the needs of the market."
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html#the-political-information-ecosystem",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html#the-political-information-ecosystem",
    "title": "The Hex Update: Issue 002",
    "section": "The political information ecosystem",
    "text": "The political information ecosystem\nThe Computational Social Science Lab at Penn (CSSLab) makes available some interesting interactive data visualizations mapping the political information ecosystem. Want to know more about the overall news consumption patterns for the US or your specific state? Check out this visualization here. Are you interested in exploring the level of partisan news echo chambers, the idea that media exposes people to overwhelmingly partisan and like-minded news content in your state? This data visualization is useful. Are you a news producer who is interested in seeing how news TV audiences‚Äô diets are changing? This figure might be helpful.\nMore here.\n\nWhat‚Äôs the takeaway\nThe trends highlighting the shift in TV news audiences is striking. Reviewing the overall news consumption trend, visually it seems audiences are spending less and less time consuming news on television. Where are these audiences going? In short, they‚Äôre moving more to no or minimal news viewing. Is this due to a shift in viewing to more digital spaces? Or, are audiences simply not watching news as much? A question worth further exploration."
  },
  {
    "objectID": "blog/posts/2024-11-01-hex-update-002/index.html#one-plot-to-ponder",
    "href": "blog/posts/2024-11-01-hex-update-002/index.html#one-plot-to-ponder",
    "title": "The Hex Update: Issue 002",
    "section": "One plot to ponder üìà",
    "text": "One plot to ponder üìà\nWith all the talk about the end of the end of cookies in the Chrome browser, I wanted to know more about how much Chrome was being used in the United States. Specifically, I had the following questions: How much is the Chrome browser being used? What other browsers, if any, compete with Chrome? So, I sought out some data and created a plot to ponder.\nThis publicly available data comes from Statcounter Global Stats. In short, Statcounter is a web analytics service which aggregates browser usage based on page view measurements of sites who have installed the service on their website. You can read more about how they calculate these values here.\nHave a look at and ponder about this trend:\n\n\n\n\n\nData made available by Statcounter Global Stats under a Creative Commons Attribution-Share Alike 3.0 Unported License."
  },
  {
    "objectID": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html",
    "href": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "",
    "text": "The purpose of this post is to document my workflow using the bigrquery R package. Although this package is essential to my analysis workflow, I mostly use it for data extraction tasks. Once data is extracted, I tend to put bigrquery to the side, and I fall out of practice until needing it for the next project. I‚Äôve also begun showing others how to use bigrquery in their own projects. As a result, I find myself repeating the same advice from time-to-time. According to David Robinson:\n\nWhen you‚Äôve written the same code 3 times, write a function\nWhen you‚Äôve given the same in-person advice 3 times, write a blog post\n‚Äî David Robinson\n\nI‚Äôm certain I‚Äôve searched for and given the same advice more than three times at this point (either to myself or to others). It‚Äôs time, then, to write a blog post."
  },
  {
    "objectID": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#composing-the-query",
    "href": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#composing-the-query",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Composing the query",
    "text": "Composing the query\nThe first step is to compose a query. The query is a character string, which is assigned a variable name.\n\nquery_neb_county_pop &lt;- \"\n  with census_total_pop as (\n    select \n      geo_id,\n      total_pop\n    from `bigquery-public-data.census_bureau_acs.county_2020_5yr`\n    where regexp_contains(geo_id, r'^31')\n  ), \n  fips_codes as (\n    select \n      county_fips_code,\n      area_name\n    from `bigquery-public-data.census_utility.fips_codes_all`\n  )\n  \n  select \n    geo_id,\n    area_name,\n    total_pop\n  from census_total_pop left outer join fips_codes on census_total_pop.geo_id = fips_codes.county_fips_code\n\"\n\nUsing a left join, this query transposes the county names column from another dataset. The result of the join is the total population of each county according to the 2020 ACS survey. query_neb_county_pop is now available for the next few steps in the query process.\n\n\n\n\n\n\nNote\n\n\n\nThe query utilizes BigQuery‚Äôs convenient with clause to create temporary tables, which are then joined using a left join. Indeed, I can hear the SQL experts pointing out more clever ways to compose this query: it‚Äôs just a simple join. However, readability was the goal here."
  },
  {
    "objectID": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#submitting-the-query",
    "href": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#submitting-the-query",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Submitting the query",
    "text": "Submitting the query\nWhen I first started using the bigrquery package, I struggled to understand what query function to use. I was also slightly confused why the package had a separate query and download table function (more on this in a bit). First, the type of data being queried (e.g., public vs.¬†project data) dictates what query function to use. Second, the argument structure is slightly different between the two query functions. The nuance of these differences is subtle, so I suggest reading the package‚Äôs docs (?bq_query) to know what function to use and when.\nIf project data is queried, the bq_project_query() function is used. In cases where you‚Äôre not querying project data (e.g., public data), you‚Äôll use bq_dataset_query(). The bq_dataset_query() is used in this post because public data is being queried. This function has parameters to associate the query with a Google Cloud billing account. In regard to the function‚Äôs other arguments, you‚Äôll only need to pass a bq_dataset object (in our case a bq_dataset_acs) and a query string.\nGetting data into the R session involves two steps. First, you‚Äôll submit the query to BigQuery using one of the functions highlighted above. BigQuery creates a temporary table in this initial step. Second, this temporary table is downloaded to the R session using the bq_table_download() function.\nThis intermediate, temporary table provides a couple conveniences:\n\nYou can use the bq_table_fields() function to check the temporary table‚Äôs fields before downloading it into your R session.\nThe table is essentially cached. As such, submitting the same exact query will return the data faster, and data processing costs will be reduced.\n\n\ntbl_temp_acs &lt;- bq_dataset_query(\n  bq_dataset_acs,\n  query = query_neb_county_pop,\n  billing = '&lt;project-name&gt;'\n)\n\n\nbq_table_fields(tbl_temp_acs)\n# &lt;bq_fields&gt;\n#   geo_id &lt;STRING&gt;\n#   area_name &lt;STRING&gt;\n#   total_pop &lt;FLOAT&gt;\n\n\ndata_nebpm_county &lt;- bq_table_download(tbl_temp_acs)\n\ndata_nebpm_county\n# # A tibble: 93 √ó 3\n#    geo_id area_name        total_pop\n#    &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n#  1 31005  Arthur County          439\n#  2 31085  Hayes County           889\n#  3 31087  Hitchcock County      2788\n#  4 31103  Keya Paha County       875\n#  5 31113  Logan County           896\n#  6 31115  Loup County            690\n#  7 31117  McPherson County       420\n#  8 31125  Nance County          3525\n#  9 31133  Pawnee County         2640\n# 10 31143  Polk County           5208\n# # ‚Ñπ 83 more rows\n# # ‚Ñπ Use `print(n = ...)` to see more rows\n\nNow the data is available in the R session. You can work with it like any other type imported via these methods."
  },
  {
    "objectID": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#create-and-write-disposition",
    "href": "blog/posts/2024-10-05-post-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#create-and-write-disposition",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Create and write disposition",
    "text": "Create and write disposition\nbq_table_upload() has some additional arguments, which I‚Äôve found are not clearly documented in the package‚Äôs documentation. These include the ability to pass a create_disposition and a write_disposition argument.\n\n\n\n\n\n\nWarning\n\n\n\nBe mindful of how you use these arguments, as the values you pass can overwrite data. Read more about the options by reviewing the linked docs below.\n\n\nMore about what these options do in BigQuery can be reviewed here. Here‚Äôs what the code would look like using the arguments listed above:\n\nbq_table_upload(\n  bq_neb_county_table,\n  values = fields_neb_county,\n  create_disposition = \"CREATE_NEVER\",\n  write_disposition = \"WRITE_TRUNCATE\"\n)\n\nThe create_disposition argument specifies how the table will be created, based on whether the table exists or not. A value of CREATE_NEVER requires the table to already exist, otherwise an error is pushed. CREATE_IF_NEEDED creates the table if it does not already exist. However, it‚Äôs best to use the bq_table_create() function rather than relying on the bq_table_upload() function to create the table for us. Nevertheless, it‚Äôs an option that‚Äôs available.\nThe write_disposition specifies what happens to values when they‚Äôre written to tables. There are three options: WRITE_TRUNCATE, WRITE_APPEND, and WRITE_EMPTY. Here‚Äôs what each of these options do:\n\nWRITE_TRUNCATE: If the table exists, overwrite the data using the schema of the newly inputted data (i.e., a destructive action).\nWRITE_APPEND: If the table exists, append the data to the table (i.e., add it to the bottom of the table).\nWRITE_EMPTY: If the table exists and it already contains data, push an error.\n\nWhen it comes to uploading data, you‚Äôll most likely want to consider the write_disposition you use.\nOne last note about uploading data to your tables: BigQuery optimizes for speed. This optimization some times results in the data to be imported not in the order it is initially imported. Rather, the resulting data import may be shuffled in a way to speed up the process. Thus, you‚Äôll likely need to arrange your data if you need to extract it again."
  },
  {
    "objectID": "blog/posts/2024-03-22-tidytuesday-2024-03-12-fiscal-sponsor-directory/index.html",
    "href": "blog/posts/2024-03-22-tidytuesday-2024-03-12-fiscal-sponsor-directory/index.html",
    "title": "Exploring data from the Fiscal Sponsor Directory",
    "section": "",
    "text": "Background\nI‚Äôm a little behind on this submission. My time to focus on #tidytuesday contributions has been limited recently. Nevertheless, here‚Äôs my submission for the 2024-03-12 data set.\nThis week‚Äôs data comes from the Fiscal Sponsor Directory. In short, this directory is a listing of groups supporting non-profits through the fiscal sponsorship of projects. I was unfamilar with this space, so I found the Fiscal Sponsor Directory‚Äôs About Us page helpful.\nWhy the Fiscal Sponsorship Directory this week? Well, the organizer of #tidytuesday is the R4DS Online Learning Community, a group I actively participate in. This group has been on the search for a new fiscal sponsor recently. The aim, thus, was to lean on the community to create data visualizations that may be helpful in identifying another fiscal sponsor for the group. So, below is what I came up with.\nBefore getting to my contribution, let‚Äôs take a moment to explore the data.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(skimr)\nlibrary(tidytext)\nlibrary(plotly)\n\n\ndata_sponsor_dir &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-03-22-tidytuesday-2024-03-12-fiscal-sponsor-directory\",\n    \"fiscal_sponsor_directory.csv\"\n  )\n)\n\nRows: 370 Columns: 12\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (9): details_url, name, website, fiscal_sponsorship_fee_description, eligibility_criteria, p...\ndbl (3): year_501c3, year_fiscal_sponsor, n_sponsored\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData description\nThis week was tough. The data was pretty dirty, and I relied heavily on string processing for data wrangling. I think, however, I‚Äôve come up with something that is a little more informative than just a listing of the different sponsorship groups.\n\nglimpse(data_sponsor_dir)\n\nRows: 370\nColumns: 12\n$ details_url                        &lt;chr&gt; \"https://fiscalsponsordirectory.org/?page_id=599\", \"htt‚Ä¶\n$ name                               &lt;chr&gt; \"1st Note Music Foundation\", \"50CAN, Inc.\", \"The Abunda‚Ä¶\n$ website                            &lt;chr&gt; \"www.1stnote.org\", \"50can.org\", \"abundancenc.org\", \"acc‚Ä¶\n$ year_501c3                         &lt;dbl&gt; 2012, 2011, 2006, 2014, 2007, 1992, 2008, 2002, 1989, 1‚Ä¶\n$ year_fiscal_sponsor                &lt;dbl&gt; 2012, 2016, 2007, 2017, 2013, 1997, 2009, 2018, 2004, 1‚Ä¶\n$ n_sponsored                        &lt;dbl&gt; 2, 10, 20, 6, 2, 1, NA, 7, 1, 15, 130, 60, 5, 13, 20, 1‚Ä¶\n$ fiscal_sponsorship_fee_description &lt;chr&gt; \"We charge a 7% administrative fee for most grants and ‚Ä¶\n$ eligibility_criteria               &lt;chr&gt; \"Type of service: Music related projects\", \"Aligned mis‚Ä¶\n$ project_types                      &lt;chr&gt; \"Arts and culture: Music Instruments to kids\", \"Educati‚Ä¶\n$ services                           &lt;chr&gt; \"Auditing: Grants\", \"Auditing|Bill paying|Bookkeeping/a‚Ä¶\n$ fiscal_sponsorship_model           &lt;chr&gt; \"Model C, Preapproved Grant Relationship\", \"Model A, Di‚Ä¶\n$ description                        &lt;chr&gt; \"1st Note Music Foundation Inc. is a nonprofit public s‚Ä¶\n\n\n\nskim(data_sponsor_dir)\n\n\nData summary\n\n\nName\ndata_sponsor_dir\n\n\nNumber of rows\n370\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndetails_url\n0\n1.00\n47\n49\n0\n370\n0\n\n\nname\n0\n1.00\n3\n64\n0\n368\n0\n\n\nwebsite\n7\n0.98\n7\n70\n0\n362\n0\n\n\nfiscal_sponsorship_fee_description\n20\n0.95\n2\n901\n0\n324\n0\n\n\neligibility_criteria\n7\n0.98\n21\n2039\n0\n303\n0\n\n\nproject_types\n9\n0.98\n9\n1244\n0\n324\n0\n\n\nservices\n14\n0.96\n9\n1852\n0\n307\n0\n\n\nfiscal_sponsorship_model\n86\n0.77\n7\n499\n0\n64\n0\n\n\ndescription\n31\n0.92\n70\n1665\n0\n337\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear_501c3\n6\n0.98\n1997.62\n17.18\n1903\n1985.75\n2001\n2012\n2022\n‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñá\n\n\nyear_fiscal_sponsor\n15\n0.96\n2005.15\n13.68\n1957\n1998.00\n2009\n2016\n2023\n‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñá\n\n\nn_sponsored\n13\n0.96\n42.68\n90.87\n0\n4.00\n12\n45\n850\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\nhead(data_sponsor_dir$project_types)\n\n[1] \"Arts and culture: Music Instruments to kids\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n[2] \"Education\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n[3] \"Children, youth and families|Economic development|Education|Environment/sustainable growth\"                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n[4] \"Arts and culture|Children, youth and families|Education|Festivals and events|Health|GBTQ|Mental health|Open space/Parks|People or communities of color/minorities| Social services|Other: Do you have a project idea that increases access to music and recreation? Our mission is to foster connection and understanding through pleasurable experiences like music and recreation to inspire the creation of safe, inclusive, equitable communities.\"                                                                                         \n[5] \"Arts and culture|Children, youth and families: Mentor Develop Programs, Youth Empowerment Programs, Grant-funded Programs. Family Empowerment and Educational Programs.|Education: Student Success Strategy Programs; Adult Re-Entry Into Education Projects.|Festivals and events: African Diaspora History Festivals and Events|People or communities of color/minorities: Diversity Impact Programs. African-American and African Diaspora Immigrant Projects|Youth development: Youth Empowerment, Job Readiness, Early Career Development,\"\n[6] \"Arts and culture|Children, youth and families|Disaster relief|Education|Festivals and events|People or communities of color/minorities|Women|Youth development\"                                                                                                                                                                                                                                                                                                                                                                                 \n\n\n\n\nData wrangling\nThe first step in the data wrangling process was to clean up the string data in the project_types column. I wanted to use this as a dimension to filter out fiscal sponsor potentially relevant to the the R4DS community. Take note, I used a regular expression to remove string values after the other in the column. These free text responses would have made it harder to filter data on this dimension.\n\n# Remove any string text after 'other'\ndata_sponsor_dir &lt;- data_sponsor_dir |&gt;\n  select(\n    details_url,\n    name,\n    year_fiscal_sponsor,\n    n_sponsored,\n    project_types,\n    website\n  ) |&gt;\n  mutate(\n    project_types = str_to_lower(project_types),\n    project_types = str_remove(project_types, \":.*\")\n  )\n\nThe next step was to tokenize project_types‚Äô categories into it‚Äôs own rows. I did this by using the unnest_tokens() function from the tidytext package.\n\n# What are the unique categories?\ndata_sponsor_cat &lt;- data_sponsor_dir |&gt;\n  unnest_tokens(cat, project_types, token = 'regex', pattern = \"\\\\|\")\n\nhead(unique(data_sponsor_cat$cat), n = 10)\n\n [1] \"arts and culture\"               \"education\"                     \n [3] \"children, youth and families\"   \"economic development\"          \n [5] \"environment/sustainable growth\" \"festivals and events\"          \n [7] \"health\"                         \"gbtq\"                          \n [9] \"mental health\"                  \"open space/parks\"              \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile wrangling this data, I learned you can pass a regex pattern to tokenize when using unnest_tokens(). I found this to be useful in this case.\n\n\nOutputted above are 10 example categories found in the project_types column. Looking through all of these categories, the following might be fiscal sponsors whose purpose aligns with that of the R4DS community. These include the following categories:\n\nconference/event management\neducation environment/sustainable growth\neducation\neconomic development\norganizational development\n\nHowever, before I go about creating a visualization using these categories, let‚Äôs see if there‚Äôs enough data within each category to make the visualization informative.\n\ncat_filter &lt;- c(\n  \"education\",\n  \"education environment/sustainable growth\",\n  \"economic development\",\n  \"conference/event management\",\n  \"organizational development\"\n)\n\ndata_sponsor_cat |&gt;\n  filter(cat %in% cat_filter) |&gt;\n  count(cat, sort = TRUE)\n\n# A tibble: 5 √ó 2\n  cat                                          n\n  &lt;chr&gt;                                    &lt;int&gt;\n1 education                                  186\n2 economic development                       144\n3 organizational development                   6\n4 conference/event management                  1\n5 education environment/sustainable growth     1\n\n\nIndeed, some categories don‚Äôt have enough data. Really, the only two categories worth plotting would be ‚Äòeconomic development‚Äô and ‚Äòeducation‚Äô. So, let‚Äôs filter for just these two categories. Let‚Äôs also drop NA values for simplicity sake.\n\ncat_filter &lt;- c(\n  \"education\",\n  \"economic development\"\n)\n\ndata_cat_filter &lt;- data_sponsor_cat |&gt;\n  filter(cat %in% cat_filter) |&gt;\n  mutate(cat = str_to_title(cat)) |&gt;\n  arrange(name, cat) |&gt;\n  drop_na() |&gt;\n  select(-details_url)\n\n\n\nCreating a Box and Whisker plot\nGiven I had a numeric variable, n_sponsored, I thought a Box and Whisker plot split by the two categories would be informative. It would certainly help identify fiscal sponsors who support many or very little projects based on the types of projects they support. Another thing I had to do was log the n_sponsored column. When I first plotted the untransformed variables, it was challenging to see the distribution of values. Logging n_sponsored made it easier to see the values. However, the hover tool provides the untransformed value for each fiscal sponsor in the data set.\n\n\n\n\n\n\nWarning\n\n\n\nThere will be duplicates in this visualization, as some sponsors will support both education and economic development focused projects.\n\n\n\nplot_ly(type = \"box\") |&gt;\n  add_boxplot(\n    data = data_cat_filter |&gt; filter(cat == \"Education\"),\n    x = ~ log(n_sponsored),\n    y = ~cat,\n    boxpoints = \"all\",\n    name = \"Education\",\n    color = I(\"#189AB4\"),\n    marker = list(color = \"#189AB4\"),\n    line = list(color = \"#000000\"),\n    text = ~ paste(\n      \"Sponsor: \",\n      name,\n      \"&lt;br&gt;Projects: \",\n      n_sponsored,\n      \"&lt;br&gt;Website: \",\n      website\n    ),\n    hoverinfo = \"text\"\n  ) |&gt;\n  add_boxplot(\n    data = data_cat_filter |&gt;\n      filter(cat == \"Economic Development\"),\n    x = ~ log(n_sponsored),\n    y = ~cat,\n    boxpoints = \"all\",\n    name = \"Economic development\",\n    color = I(\"#191970\"),\n    marker = list(color = \"#191970\"),\n    line = list(color = \"#000000\"),\n    text = ~ paste(\n      \"Sponsor: \",\n      name,\n      \"&lt;br&gt;Projects: \",\n      n_sponsored,\n      \"&lt;br&gt;Website: \",\n      website\n    ),\n    hoverinfo = \"text\"\n  ) |&gt;\n  layout(\n    title = \"&lt;b&gt;Distribution of the number of projects (logged) supported by fiscal sponsors\",\n    yaxis = list(title = \"\"),\n    xaxis = list(title = \"Projects sponsored on log scale\")\n  )\n\nWarning: Can't display both discrete & non-discrete data on same axis\n\n\n\n\n\n\nNot bad. The only thing I ran out of time on was related to the hover tool. I really wanted separate hovers, one for the five number summary in the box and whisker plot and one for the individual data points. Unfortunately, I wasn‚Äôt able to figure out how to do this with the time I had. Oh well, what resulted was still a useful data visualization, given where we started with the data.\nSo there you have it. Not the cleanest data to work with. Nonetheless, we came up with a visualization we could still learn something from.\n\n\nAn attempt using Tableau\nTo continue developing my skills and to practice using other data visualization tools, I created this same visualization using Tableau. You can check out this version of the visualization here.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring Data from the {Fiscal} {Sponsor} {Directory}},\n  date = {2024-03-22},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúExploring Data from the Fiscal Sponsor\nDirectory.‚Äù March 22, 2024."
  },
  {
    "objectID": "blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/index.html",
    "href": "blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/index.html",
    "title": "30 day tidymodels recipes challenge",
    "section": "",
    "text": "Background\nBefore the holidays, I came across Emil Hvitfeldt‚Äôs #adventofsteps LinkedIn posts. Following a model popularized by advent of code‚Äìan annual tradition of online programming puzzles based on the theme of an advent calendar‚Äìthese posts provided daily examples on the use of various step_* functions from the tidymodels‚Äô recipes package. This post, with a slight spin, is inspired by these posts.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(textrecipes)\nlibrary(themis)\nlibrary(timetk)\ntidymodels_prefer()\n\n\n\nMy spin on this\nOne of my personal goals this coming year is to learn and practice using the different tidymodels‚Äô packages. To complete this goal, I thought a 30 day recipes challenge would be a good start. Each day during this 30 day personal challenge, I will focus on learning and creating some daily notes about one functionality of the recipes package. First, I start with the basics (e.g., how to create a recipe object). Then, I‚Äôll focus on describing the various step_* functions.\nTo keep me on track, while also avoiding making this a chore, I‚Äôm going to place a 1-hour a day stopgap on studying, practicing, and documenting what I‚Äôve learned. Depending on my schedule and motivation, I may work ahead on some material, but I will strive to update this post once a day.\nGiven the time constraint I‚Äôm imposing on myself, some of my daily notes or examples may result in an incomplete description of functionality. In cases like this, I‚Äôll try to link to relevant documentation for you to follow up and learn more. Please be flexible with any grammar and spelling errors during this challenge, as I‚Äôll likely edit very little until the end of the 30 days, if at all.\nSince the aim of this post is to document what I‚Äôm learning, all errors are completely mine. I highly suggest following up with the recipes package‚Äôs documentation and the Tidy Modeling with R book following a review of these notes. Both do a more thorough job overviewing the package‚Äôs functionality.\n\n\nWhat I intend to get out of this challenge\nBy the end of this challenge, I hope to have pushed myself to learn more about how to use tidymodels‚Äôs recipe package, and to create several example use cases of different functionality.\n\n\nDay 01 - Create a recipe\nFirst off, what is a recipe? According to the docs:\n\nA recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.\n\nSo, I start this personal challenge by overviewing how to create a recipe object with the recipes package. The recipe() function is used to create a recipe object.\nWhen creating a recipe, we need to consider what roles variables take. In simple modeling tasks, you‚Äôll just have outcomes and predictors. However, variables may take on other roles (i.e., IDs). As such, the recipe() function provides multiple means for specifying the role of a variable:\n\nThe formula\nManually updating roles using the update_role() function.\n\nLet‚Äôs use the credit_data from tidymodels‚Äô modeldata package. You can get more information about this data by running ?credit_data in your console.\n\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good‚Ä¶\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2‚Ä¶\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own‚Ä¶\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, ‚Ä¶\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, ‚Ä¶\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr‚Ä¶\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no‚Ä¶\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti‚Ä¶\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, ‚Ä¶\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330‚Ä¶\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, ‚Ä¶\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0‚Ä¶\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400‚Ä¶\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, ‚Ä¶\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\n\n# Create splits for examples\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n\n# No outcome variable, `.` is a shortcut for **all** variables\ncredit_rec &lt;- recipe(~., data = credit_train)\n\n# Outcome with specific variables to be included within model\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)\n\n# Recipe uses `data` only as a template, all the data is not needed\n# Useful in cases when you're working with large data\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = head(credit_train)\n)\n\n\n# Use `update_role()` to specify variable roles\ncredit_rec_update &lt;- recipe(credit_train) |&gt;\n  update_role(Status, new_role = \"outcome\") |&gt;\n  update_role(\n    Seniority, Home, Time, Age, Marital, Records, \n    Job, Expenses, Income, Assets, Debt, Amount, \n    Price, new_role = \"predictor\"\n  )\n\ncredit_rec_update\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\n\n\nThe update_role() function is useful in cases where you might have an ID variable you don‚Äôt want to include within your model.\n\ncredit_data_id &lt;- credit_data |&gt;\n  mutate(id = 1:n(), .before = 1)\n\nset.seed(2)\ncredit_id_split &lt;- \n  initial_split(credit_data_id, prop = 0.8, strata = Status)\ncredit_id_train &lt;- training(credit_id_split)\ncredit_id_test &lt;- testing(credit_id_split)\n\n\n# Manually add an 'id' role to a variable\ncredit_id_rec &lt;- recipe(credit_id_train) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  update_role(Status, new_role = \"outcome\") |&gt;\n  update_role(\n    Seniority, Home, Time, Age, Marital, Records, \n    Job, Expenses, Income, Assets, Debt, Amount, \n    Price, new_role = \"predictor\"\n  ) \n\ncredit_id_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\nid:         1\n\n\nIn case you ever need to remove a role, you can use remove_role().\n\ncredit_no_id_rec &lt;- credit_id_rec |&gt;\n  remove_role(id, old_role = \"id\")\n\n# id will be assigned and 'undeclared' role\ncredit_no_id_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:          1\npredictor:       13\nundeclared role:  1\n\n\nEach recipe has its own summary method. We can wrap the recipe object within summary() to output more information about each variable and its assigned role.\n\n# Formula specified recipe\nsummary(credit_rec)\n\n# A tibble: 4 √ó 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 Debt     &lt;chr [2]&gt; predictor original\n2 Income   &lt;chr [2]&gt; predictor original\n3 Assets   &lt;chr [2]&gt; predictor original\n4 Status   &lt;chr [3]&gt; outcome   original\n\n# Manually specified using `update_role()`\nsummary(credit_rec_update)\n\n# A tibble: 14 √ó 4\n   variable  type      role      source  \n   &lt;chr&gt;     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Status    &lt;chr [3]&gt; outcome   original\n 2 Seniority &lt;chr [2]&gt; predictor original\n 3 Home      &lt;chr [3]&gt; predictor original\n 4 Time      &lt;chr [2]&gt; predictor original\n 5 Age       &lt;chr [2]&gt; predictor original\n 6 Marital   &lt;chr [3]&gt; predictor original\n 7 Records   &lt;chr [3]&gt; predictor original\n 8 Job       &lt;chr [3]&gt; predictor original\n 9 Expenses  &lt;chr [2]&gt; predictor original\n10 Income    &lt;chr [2]&gt; predictor original\n11 Assets    &lt;chr [2]&gt; predictor original\n12 Debt      &lt;chr [2]&gt; predictor original\n13 Amount    &lt;chr [2]&gt; predictor original\n14 Price     &lt;chr [2]&gt; predictor original\n\n# Recipe with a variable holding the 'id' role\nsummary(credit_id_rec)\n\n# A tibble: 15 √ó 4\n   variable  type      role      source  \n   &lt;chr&gt;     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 id        &lt;chr [2]&gt; id        original\n 2 Status    &lt;chr [3]&gt; outcome   original\n 3 Seniority &lt;chr [2]&gt; predictor original\n 4 Home      &lt;chr [3]&gt; predictor original\n 5 Time      &lt;chr [2]&gt; predictor original\n 6 Age       &lt;chr [2]&gt; predictor original\n 7 Marital   &lt;chr [3]&gt; predictor original\n 8 Records   &lt;chr [3]&gt; predictor original\n 9 Job       &lt;chr [3]&gt; predictor original\n10 Expenses  &lt;chr [2]&gt; predictor original\n11 Income    &lt;chr [2]&gt; predictor original\n12 Assets    &lt;chr [2]&gt; predictor original\n13 Debt      &lt;chr [2]&gt; predictor original\n14 Amount    &lt;chr [2]&gt; predictor original\n15 Price     &lt;chr [2]&gt; predictor original\n\n\n\n\nDay 02 - How to use prep() and bake()\nLet‚Äôs stick with the credit data for today‚Äôs examples.\n\n# Same code from day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good‚Ä¶\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2‚Ä¶\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own‚Ä¶\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, ‚Ä¶\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, ‚Ä¶\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr‚Ä¶\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no‚Ä¶\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti‚Ä¶\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, ‚Ä¶\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330‚Ä¶\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, ‚Ä¶\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0‚Ä¶\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400‚Ä¶\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, ‚Ä¶\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\n\n# Create splits for our day 2 examples\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\nWe‚Äôre going to continue to use the previously specified limited model from day 01 for our examples.\n\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)\n\nNow that we know how to specify a recipe, we need to learn how to use recipes‚Äô prep() and bake() functions. prep() calculates any intermediate values required for preprocessing. bake() applies the preprocessing steps‚Äìusing any intermediate values‚Äìto our testing and training data.\nprep() and bake() can be confusing at first. However, I like the following analogy from the R4DS learning community‚Äôs Q&A with the authors of the Tidy Modeling with R book:\n\nThey‚Äôre analogous to fit() and predict() ‚Ä¶ prep() is like fitting where you‚Äôre estimating stuff and bake() is like you‚Äôre applying it.\n- Max Kuhn\n\nFor a more formal treatment, the prep() docs state:\n\nFor a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets.\n\nThe bake() docs state:\n\nFor a recipe with at least one preprocessing operation that has been trained by prep(), apply the computations to new data.\n\nWhy two separate functions? Some preprocessing steps need an intermediate calculation step to be performed before applying the recipe to the data (e.g., step_normalize() and step_center(); more on this later). To better articulate this point, I‚Äôm going to fast-forward a bit in our challenge and apply the step_center() function to our recipe. step_center() is used to center variables.\nWhen centering a variable, we need to make an intermediate calculation (i.e., prep()) before applying the calculation to perform the centering to our data (i.e., bake()).\nFor our example, say we want to center the Debt variable. To do this, we can simply add step_center(Debt) to our recipe. When we pipe the recipe object to prep(), the mean is calculated in the background to perform the preprocessing step.\n\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n) |&gt;\n  step_center(Debt) |&gt;\n  prep() \n\nWe can see this calculated value by using the number argument in the tidy.recipe() method.\n\n# Print a summary of the recipe steps to be performed\ntidy(credit_rec)\n\n# A tibble: 1 √ó 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_lw98c\n\n# Print additional information about the first recipe step\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms value id          \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 Debt   337. center_lw98c\n\n\nTake note, though, the Debt variable has not been centered yet, and we are still working with a recipe object.\nWe then apply the centering transformation to the data by piping the prepped recipe to bake(). We can apply the preprocessing to the training data by passing the NULL to the new_data argument. bake() returns a tibble with our transformed variable using our training data.\n\ncredit_baked &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n) |&gt;\n  step_center(Debt) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_baked\n\n# A tibble: 3,563 √ó 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt; &lt;fct&gt; \n 1 -337.     80      0 bad   \n 2 -337.     50      0 bad   \n 3 -337.    107      0 bad   \n 4  163.    112   2000 bad   \n 5 -337.     85   5000 bad   \n 6   NA      NA     NA bad   \n 7 -337.     90      0 bad   \n 8 -337.     71   3000 bad   \n 9 -337.    128      0 bad   \n10 -337.    100      0 bad   \n# ‚Ñπ 3,553 more rows\n\n\nMost likely, you won‚Äôt use prep() and bake() for other modeling tasks. However, they‚Äôll be important as we continue exploring the recipes package in the coming days.\n\n\nDay 03 - Selector functions\nRemaining consistent, let‚Äôs continue using the credit_data data for some of today‚Äôs examples. We‚Äôll also use the Chicago data set for a couple additional examples. You can read more about this data by running ?Chicago in your console.\nHere we‚Äôll get our data and split it into training and testing for both data sets.\n\n# Same code from day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good‚Ä¶\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2‚Ä¶\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own‚Ä¶\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, ‚Ä¶\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, ‚Ä¶\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr‚Ä¶\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no‚Ä¶\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti‚Ä¶\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, ‚Ä¶\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330‚Ä¶\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, ‚Ä¶\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0‚Ä¶\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400‚Ä¶\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, ‚Ä¶\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n\ndata(Chicago, package = \"modeldata\")\nglimpse(Chicago)\n\nRows: 5,698\nColumns: 50\n$ ridership        &lt;dbl&gt; 15.732, 15.762, 15.872, 15.874, 15.423, 2.425, 1.467, 15.511, 15.927, 15.‚Ä¶\n$ Austin           &lt;dbl&gt; 1.463, 1.505, 1.519, 1.490, 1.496, 0.693, 0.408, 0.987, 1.551, 1.588, 1.5‚Ä¶\n$ Quincy_Wells     &lt;dbl&gt; 8.371, 8.351, 8.359, 7.852, 7.621, 0.911, 0.414, 4.807, 8.227, 8.246, 8.0‚Ä¶\n$ Belmont          &lt;dbl&gt; 4.599, 4.725, 4.684, 4.769, 4.720, 2.274, 1.631, 3.517, 4.707, 4.774, 4.8‚Ä¶\n$ Archer_35th      &lt;dbl&gt; 2.009, 2.088, 2.108, 2.166, 2.058, 0.624, 0.378, 1.339, 2.221, 2.227, 2.1‚Ä¶\n$ Oak_Park         &lt;dbl&gt; 1.421, 1.429, 1.488, 1.445, 1.415, 0.426, 0.225, 0.879, 1.457, 1.475, 1.4‚Ä¶\n$ Western          &lt;dbl&gt; 3.319, 3.344, 3.363, 3.359, 3.271, 1.111, 0.567, 1.937, 3.457, 3.511, 3.4‚Ä¶\n$ Clark_Lake       &lt;dbl&gt; 15.561, 15.720, 15.558, 15.745, 15.602, 2.413, 1.374, 9.017, 16.003, 15.8‚Ä¶\n$ Clinton          &lt;dbl&gt; 2.403, 2.402, 2.367, 2.415, 2.416, 0.814, 0.583, 1.501, 2.437, 2.457, 2.4‚Ä¶\n$ Merchandise_Mart &lt;dbl&gt; 6.481, 6.477, 6.405, 6.489, 5.798, 0.858, 0.268, 4.193, 6.378, 6.458, 6.2‚Ä¶\n$ Irving_Park      &lt;dbl&gt; 3.744, 3.853, 3.861, 3.843, 3.878, 1.735, 1.164, 2.903, 3.828, 3.869, 3.8‚Ä¶\n$ Washington_Wells &lt;dbl&gt; 7.560, 7.576, 7.620, 7.364, 7.089, 0.786, 0.298, 4.731, 7.479, 7.547, 7.2‚Ä¶\n$ Harlem           &lt;dbl&gt; 2.655, 2.760, 2.789, 2.812, 2.732, 1.034, 0.642, 1.958, 2.742, 2.753, 2.7‚Ä¶\n$ Monroe           &lt;dbl&gt; 5.672, 6.013, 5.786, 5.959, 5.769, 1.044, 0.530, 3.165, 5.935, 5.829, 5.9‚Ä¶\n$ Polk             &lt;dbl&gt; 2.481, 2.436, 2.526, 2.450, 2.573, 0.006, 0.000, 1.065, 2.533, 2.566, 2.4‚Ä¶\n$ Ashland          &lt;dbl&gt; 1.319, 1.314, 1.324, 1.350, 1.355, 0.566, 0.347, 0.852, 1.400, 1.358, 1.4‚Ä¶\n$ Kedzie           &lt;dbl&gt; 3.013, 3.020, 2.982, 3.013, 3.085, 1.130, 0.635, 1.969, 3.149, 3.099, 3.1‚Ä¶\n$ Addison          &lt;dbl&gt; 2.500, 2.570, 2.587, 2.528, 2.557, 0.800, 0.487, 1.560, 2.574, 2.618, 2.5‚Ä¶\n$ Jefferson_Park   &lt;dbl&gt; 6.595, 6.750, 6.967, 7.013, 6.922, 2.765, 1.856, 4.928, 6.817, 6.853, 6.8‚Ä¶\n$ Montrose         &lt;dbl&gt; 1.836, 1.915, 1.977, 1.979, 1.953, 0.772, 0.475, 1.325, 2.040, 2.038, 2.0‚Ä¶\n$ California       &lt;dbl&gt; 0.756, 0.781, 0.812, 0.776, 0.789, 0.370, 0.274, 0.473, 0.844, 0.835, 0.8‚Ä¶\n$ temp_min         &lt;dbl&gt; 15.1, 25.0, 19.0, 15.1, 21.0, 19.0, 15.1, 26.6, 34.0, 33.1, 23.0, 0.0, 10‚Ä¶\n$ temp             &lt;dbl&gt; 19.45, 30.45, 25.00, 22.45, 27.00, 24.80, 18.00, 32.00, 37.40, 34.00, 28.‚Ä¶\n$ temp_max         &lt;dbl&gt; 30.0, 36.0, 28.9, 27.0, 32.0, 30.0, 28.9, 41.0, 43.0, 36.0, 33.1, 21.2, 3‚Ä¶\n$ temp_change      &lt;dbl&gt; 14.9, 11.0, 9.9, 11.9, 11.0, 11.0, 13.8, 14.4, 9.0, 2.9, 10.1, 21.2, 20.0‚Ä¶\n$ dew              &lt;dbl&gt; 13.45, 25.00, 18.00, 10.90, 21.90, 15.10, 10.90, 30.20, 35.60, 30.90, 21.‚Ä¶\n$ humidity         &lt;dbl&gt; 78.0, 79.0, 81.0, 66.5, 84.0, 71.0, 74.0, 93.0, 93.0, 89.0, 80.0, 66.5, 7‚Ä¶\n$ pressure         &lt;dbl&gt; 30.430, 30.190, 30.160, 30.440, 29.910, 30.280, 30.330, 30.040, 29.400, 2‚Ä¶\n$ pressure_change  &lt;dbl&gt; 0.12, 0.18, 0.23, 0.16, 0.65, 0.49, 0.10, 0.78, 0.16, 0.48, 0.23, 0.28, 0‚Ä¶\n$ wind             &lt;dbl&gt; 5.20, 8.10, 10.40, 9.80, 12.70, 12.70, 8.10, 8.10, 9.20, 11.50, 11.50, 12‚Ä¶\n$ wind_max         &lt;dbl&gt; 10.4, 11.5, 19.6, 16.1, 19.6, 17.3, 13.8, 17.3, 23.0, 16.1, 16.1, 19.6, 1‚Ä¶\n$ gust             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ gust_max         &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 25.3, 26.5, 0.0, 26.5, 31.1, 0.0, 0.0, 23.0, 26.5, 0.‚Ä¶\n$ percip           &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0‚Ä¶\n$ percip_max       &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.11, 0.01, 0.00, 0.00, 0‚Ä¶\n$ weather_rain     &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0‚Ä¶\n$ weather_snow     &lt;dbl&gt; 0.00000000, 0.00000000, 0.21428571, 0.00000000, 0.51612903, 0.04000000, 0‚Ä¶\n$ weather_cloud    &lt;dbl&gt; 0.7083333, 1.0000000, 0.3571429, 0.2916667, 0.4516129, 0.6400000, 0.52000‚Ä¶\n$ weather_storm    &lt;dbl&gt; 0.00000000, 0.20833333, 0.07142857, 0.04166667, 0.45161290, 0.24000000, 0‚Ä¶\n$ Blackhawks_Away  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Blackhawks_Home  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Bulls_Away       &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Bulls_Home       &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0‚Ä¶\n$ Bears_Away       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Bears_Home       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ WhiteSox_Away    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ WhiteSox_Home    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Cubs_Away        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ Cubs_Home        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ date             &lt;date&gt; 2001-01-22, 2001-01-23, 2001-01-24, 2001-01-25, 2001-01-26, 2001-01-27, ‚Ä¶\n\n# For reproducibility\nset.seed(2)\nchicago_split &lt;- initial_split(Chicago, prop = 0.8)\nchicago_train &lt;- training(chicago_split)\nchicago_test &lt;- testing(chicago_split)\n\nWhen using recipes, we often need to select a group of variables (e.g., all predictors, all numeric variables, all categorical variables, etc.) to apply preprocessing steps. Indeed, we certainly could just explicitly specify each variable by name within our recipe. There‚Äôs a better way, though. Use selector functions.\nSelector functions can be used to choose variables based on:\n\nVariable names\nCurrent role\nData type\nAny combination of the above three\n\nThe first set of selectors comes from the tidyselect package, which allows you to make selections based on variable names. Some common ones include:\n\ntidyselect::starts_with()\ntidyselect::ends_with()\ntidyselect::contains()\ntidyselect::everything()\n\nCheck out recipes‚Äô ?selections and the tidyselect docs for a more exhaustive list of available selection functions. Included above are the ones I commonly use. Here are a few examples of how to use these selector functions to center variables.\n\n# Apply the centering to variables that start with the *weather* prefix\nchicago_rec &lt;- \n  recipe(ridership ~ ., data = chicago_train) |&gt;\n  step_center(starts_with(\"weather\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nchicago_rec |&gt; select(starts_with(\"weather\"))\n\n# A tibble: 4,558 √ó 4\n   weather_rain weather_snow weather_cloud weather_storm\n          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1      -0.0803      -0.0529       0.0973        -0.0518\n 2      -0.0386       0.572       -0.403          0.0315\n 3      -0.0803       0.147       -0.00272        0.0398\n 4      -0.0803      -0.0529       0.264          0.198 \n 5      -0.0803      -0.0529       0.193          0.0612\n 6      -0.0803      -0.0529       0.264          0.323 \n 7      -0.0803      -0.0529       0.264          0.201 \n 8      -0.0803       0.614       -0.403         -0.236 \n 9      -0.0803      -0.0529       0.144         -0.100 \n10       0.0678      -0.0529      -0.0694        -0.112 \n# ‚Ñπ 4,548 more rows\n\n\nSelections also allows us to use the - to exclude specific variables or groupings of variables while using selector functions.\n\nchicago_rec &lt;- \n  recipe(ridership ~ ., data = chicago_train) |&gt;\n  step_center(-date, -starts_with(\"weather\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nchicago_rec\n\n# A tibble: 4,558 √ó 50\n     Austin Quincy_Wells Belmont Archer_35th Oak_Park Western Clark_Lake Clinton Merchandise_Mart\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1  0.756          2.49    1.44        0.914   0.645    0.903       6.57   0.972            1.29 \n 2  0.279          0.222   1.54        0.577   0.316    1.35        3.80   1.34             0.379\n 3  0.579          2.96    1.64        0.949   0.537    1.20        6.12   1.45             3.52 \n 4  0.584          1.91    0.686       0.768   0.470    0.849       5.65   0.504            1.21 \n 5  0.616          2.41    1.47        0.964   0.569    0.831       6.15   0.995            1.92 \n 6  0.660          2.68    1.42        0.982   0.532    0.981       6.02   1.20             2.32 \n 7 -1.04          -5.56   -2.33       -1.61   -0.852   -2.25      -10.7   -1.55            -3.60 \n 8 -0.00927        0.699   0.483       0.217   0.0441   0.209       1.63   0.999            0.262\n 9 -1.06          -4.55   -2.45       -1.50   -1.05    -2.00      -10.9   -1.58            -4.26 \n10  0.536          2.03    0.500       0.372   0.560    0.345       5.71   0.557            1.36 \n# ‚Ñπ 4,548 more rows\n# ‚Ñπ 41 more variables: Irving_Park &lt;dbl&gt;, Washington_Wells &lt;dbl&gt;, Harlem &lt;dbl&gt;, Monroe &lt;dbl&gt;,\n#   Polk &lt;dbl&gt;, Ashland &lt;dbl&gt;, Kedzie &lt;dbl&gt;, Addison &lt;dbl&gt;, Jefferson_Park &lt;dbl&gt;, Montrose &lt;dbl&gt;,\n#   California &lt;dbl&gt;, temp_min &lt;dbl&gt;, temp &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_change &lt;dbl&gt;, dew &lt;dbl&gt;,\n#   humidity &lt;dbl&gt;, pressure &lt;dbl&gt;, pressure_change &lt;dbl&gt;, wind &lt;dbl&gt;, wind_max &lt;dbl&gt;, gust &lt;dbl&gt;,\n#   gust_max &lt;dbl&gt;, percip &lt;dbl&gt;, percip_max &lt;dbl&gt;, weather_rain &lt;dbl&gt;, weather_snow &lt;dbl&gt;,\n#   weather_cloud &lt;dbl&gt;, weather_storm &lt;dbl&gt;, Blackhawks_Away &lt;dbl&gt;, Blackhawks_Home &lt;dbl&gt;, ‚Ä¶\n\n# To show centering was not applied to variables with the *weather* prefix\nchicago_rec |&gt; select(starts_with(\"weather\"))\n\n# A tibble: 4,558 √ó 4\n   weather_rain weather_snow weather_cloud weather_storm\n          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1       0             0             0.833        0.208 \n 2       0.0417        0.625         0.333        0.292 \n 3       0             0.2           0.733        0.3   \n 4       0             0             1            0.458 \n 5       0             0             0.929        0.321 \n 6       0             0             1            0.583 \n 7       0             0             1            0.462 \n 8       0             0.667         0.333        0.0238\n 9       0             0             0.88         0.16  \n10       0.148         0             0.667        0.148 \n# ‚Ñπ 4,548 more rows\n\n\nrecipes provides functions to select variables based on role and type. This includes the has_role() and has_type() functions.\n\n# Simplified recipe, applying centering to variables with predictor role \ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)  |&gt;\n  step_center(has_role(\"predictor\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec\n\n# A tibble: 3,563 √ó 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ‚Ñπ 3,553 more rows\n\n\n\n# Applying centering to variables with type numeric\ncredit_rec_type &lt;- recipe(Status ~ ., data = credit_train) |&gt;\n  step_center(has_type(match = \"numeric\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_type\n\n# A tibble: 3,563 √ó 14\n   Seniority Home    Time      Age Marital Records Job   Expenses Income Assets  Debt Amount   Price\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     -7.91 pare‚Ä¶   1.54   3.94   married no      part‚Ä¶    34.6   -60.7 -5233. -337.  159.    -2.20\n 2     -7.91 other -28.5  -16.1    single  yes     part‚Ä¶   -20.4   -90.7 -5233. -337. -641.  -970.  \n 3     -5.91 rent   13.5  -12.1    single  no      fixed    -9.42  -33.7 -5233. -337.  459.   719.  \n 4     -6.91 owner  13.5    7.94   married no      part‚Ä¶    49.6   -28.7 -3233.  163. -441.  -138.  \n 5     -4.91 owner -22.5  -14.1    married no      fixed    19.6   -55.7  -233. -337. -441.   130.  \n 6     -7.91 &lt;NA&gt;    1.54  -0.0589 single  no      &lt;NA&gt;    -20.4    NA      NA    NA   459.   380.  \n 7     -2.91 rent    1.54  -6.06   single  no      fixed   -11.4   -50.7 -5233. -337.  259.   230.  \n 8     -5.91 owner  13.5    5.94   married no      part‚Ä¶    19.6   -69.7 -2233. -337.  459.    81.8 \n 9     -5.91 rent  -10.5  -10.1    separa‚Ä¶ no      fixed    -7.42  -12.7 -5233. -337. -591.  -925.  \n10     -6.91 rent    1.54  -8.06   married yes     free‚Ä¶    29.6   -40.7 -5233. -337.  -41.1 -125.  \n# ‚Ñπ 3,553 more rows\n# ‚Ñπ 1 more variable: Status &lt;fct&gt;\n\n\nAlthough has_role() and has_type() are available, you‚Äôll most likely rely on functions that are more specific. The docs state (?has_role):\n\nIn most cases, the right approach for users will be to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\nThese include functions to select variables based on type:\n\nall_numeric() - includes all numeric variables.\nall_nominal() - includes both character and factor variables.\n\n\n# Center **all** numeric variables\ncredit_rec_type &lt;- recipe(\n  Status ~ Debt + Income + Assets,\n  data = credit_train\n) |&gt;\n  step_center(all_numeric()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_type \n\n# A tibble: 3,563 √ó 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ‚Ñπ 3,553 more rows\n\n\nFunctions to select by role:\n\nall_predictors()\nall_outcomes()\n\n\n# Center all predictors\ncredit_rec_role &lt;- \n  recipe(\n    Status ~ Debt + Income + Assets, \n    data = credit_train\n  ) |&gt;\n  step_center(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_role\n\n# A tibble: 3,563 √ó 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ‚Ñπ 3,553 more rows\n\n\nFunctions to select variables that intersect by role and type:\n\nall_numeric_predictors()\nall_nominal_predictors()\n\n\ncredit_rec_num_pred &lt;- \n  recipe(Status ~ ., data = credit_train) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_num_pred\n\n# A tibble: 3,563 √ó 14\n   Seniority Home    Time      Age Marital Records Job   Expenses Income Assets  Debt Amount   Price\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     -7.91 pare‚Ä¶   1.54   3.94   married no      part‚Ä¶    34.6   -60.7 -5233. -337.  159.    -2.20\n 2     -7.91 other -28.5  -16.1    single  yes     part‚Ä¶   -20.4   -90.7 -5233. -337. -641.  -970.  \n 3     -5.91 rent   13.5  -12.1    single  no      fixed    -9.42  -33.7 -5233. -337.  459.   719.  \n 4     -6.91 owner  13.5    7.94   married no      part‚Ä¶    49.6   -28.7 -3233.  163. -441.  -138.  \n 5     -4.91 owner -22.5  -14.1    married no      fixed    19.6   -55.7  -233. -337. -441.   130.  \n 6     -7.91 &lt;NA&gt;    1.54  -0.0589 single  no      &lt;NA&gt;    -20.4    NA      NA    NA   459.   380.  \n 7     -2.91 rent    1.54  -6.06   single  no      fixed   -11.4   -50.7 -5233. -337.  259.   230.  \n 8     -5.91 owner  13.5    5.94   married no      part‚Ä¶    19.6   -69.7 -2233. -337.  459.    81.8 \n 9     -5.91 rent  -10.5  -10.1    separa‚Ä¶ no      fixed    -7.42  -12.7 -5233. -337. -591.  -925.  \n10     -6.91 rent    1.54  -8.06   married yes     free‚Ä¶    29.6   -40.7 -5233. -337.  -41.1 -125.  \n# ‚Ñπ 3,553 more rows\n# ‚Ñπ 1 more variable: Status &lt;fct&gt;\n\n\nSelector functions will become useful as we continue to explore the step_* functions within the recipes package.\n\n\nDay 04 - Create dummy variables using step_dummy()\nBefore starting our overview of recipes‚Äô step_* functions, we need a bit of direction on what preprocessing steps might be required or beneficial to apply. The type of data preprocessing is determined by the model being fit. As a starting point, the Tidy Modeling with R book provides an appendix with a table of preprocessing recommendations based on the types of models being used. This table is separate from the types of feature engineering that may be applied, but it‚Äôs a good baseline for determining the initial step_* functions to be included within a recipe.\nDummy variables is the first preprocessing method highlighted in this appendix. That is, the encoding of qualitative predictors into numeric predictors. Closely related is one-hot encoding. When dummy variables are created, most commonly, nominal variable columns are converted into separate columns of 1‚Äôs and 0‚Äôs. recipes‚Äô step_dummy() function performs these preprocessing operations.\nLet‚Äôs continue using the credit_data for today‚Äôs examples. Take note, this data contains some NA‚Äòs. To address this issue, I‚Äôm just going to drop any cases with a missing value using dplyr‚Äôs drop_na() function. Indeed, this issue could be addressed with imputation through the use of recipes‚Äô step_impute_* functions (more on this in the coming days).\n\n# Same code as day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good‚Ä¶\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2‚Ä¶\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own‚Ä¶\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, ‚Ä¶\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, ‚Ä¶\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr‚Ä¶\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no‚Ä¶\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti‚Ä¶\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, ‚Ä¶\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330‚Ä¶\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, ‚Ä¶\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0‚Ä¶\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400‚Ä¶\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, ‚Ä¶\n\ncredit_data &lt;- credit_data |&gt;\n  drop_na()\n\n\n# Create the split, training and testing data\nset.seed(20230104)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\nHere‚Äôs the recipe we‚Äôll use. I‚Äôm gonna keep it simple, so it‚Äôs easier to observe the results of adding step_dummy() to our recipe.\n\ncredit_rec &lt;- \n  recipe(\n    Status ~ Job + Home + Marital, \n    data = credit_train\n  ) \n\nLet‚Äôs create dummy variables from the Job column. But first, let‚Äôs take a look at how many different variable levels there are.\n\nunique(credit_data$Job)\n\n[1] freelance fixed     partime   others   \nLevels: fixed freelance others partime\n\n\nSince we have four levels (freelance, fixed, partime, others), the step_dummy() function will create three columns. The fixed Job level will be the reference group, since it‚Äôs the first level specified for the factor.\n\ncredit_rec |&gt;\n  step_dummy(Job) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 √ó 6\n   Home    Marital Status Job_freelance Job_others Job_partime\n   &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 owner   married good               0          0           0\n 2 other   married bad                0          1           0\n 3 owner   married good               0          0           0\n 4 owner   married good               1          0           0\n 5 parents single  good               0          0           0\n 6 rent    single  good               0          0           0\n 7 parents single  good               0          0           0\n 8 other   widow   good               0          0           0\n 9 priv    single  good               1          0           0\n10 owner   married bad                0          0           0\n# ‚Ñπ 3,221 more rows\n\n\nTake note of the naming conventions applied to the new dummy columns. step_dummy() uses the following naming convention variable-name_variable-level. This makes it easier to know what variable the dummy variables originated.\nSay you don‚Äôt want to drop the original column when the dummy variables are created. We can pass TRUE to the keep_original_cols argument. This will retain the original column, while also creating the dummy variables.\n\ncredit_rec |&gt;\n  step_dummy(Job, keep_original_cols = TRUE) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 √ó 7\n   Job       Home    Marital Status Job_freelance Job_others Job_partime\n   &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 fixed     owner   married good               0          0           0\n 2 others    other   married bad                0          1           0\n 3 fixed     owner   married good               0          0           0\n 4 freelance owner   married good               1          0           0\n 5 fixed     parents single  good               0          0           0\n 6 fixed     rent    single  good               0          0           0\n 7 fixed     parents single  good               0          0           0\n 8 fixed     other   widow   good               0          0           0\n 9 freelance priv    single  good               1          0           0\n10 fixed     owner   married bad                0          0           0\n# ‚Ñπ 3,221 more rows\n\n\nWhat about one-hot encoding? To apply one-hot encoding we specify FALSE to the one_hot argument within the function. The preprocessed, baked data will now contain four columns. One column for each level of the source column.\n\ncredit_rec |&gt;\n  step_dummy(Job, one_hot = TRUE) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 √ó 7\n   Home    Marital Status Job_fixed Job_freelance Job_others Job_partime\n   &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 owner   married good           1             0          0           0\n 2 other   married bad            0             0          1           0\n 3 owner   married good           1             0          0           0\n 4 owner   married good           0             1          0           0\n 5 parents single  good           1             0          0           0\n 6 rent    single  good           1             0          0           0\n 7 parents single  good           1             0          0           0\n 8 other   widow   good           1             0          0           0\n 9 priv    single  good           0             1          0           0\n10 owner   married bad            1             0          0           0\n# ‚Ñπ 3,221 more rows\n\n\nWe can scale this preprocessing to all nominal predictors by using, you guessed it, selector functions.\n\ncredit_rec |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 √ó 13\n   Status Job_freelance Job_others Job_partime Home_other Home_owner Home_parents Home_priv\n   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 good               0          0           0          0          1            0         0\n 2 bad                0          1           0          1          0            0         0\n 3 good               0          0           0          0          1            0         0\n 4 good               1          0           0          0          1            0         0\n 5 good               0          0           0          0          0            1         0\n 6 good               0          0           0          0          0            0         0\n 7 good               0          0           0          0          0            1         0\n 8 good               0          0           0          1          0            0         0\n 9 good               1          0           0          0          0            0         1\n10 bad                0          0           0          0          1            0         0\n# ‚Ñπ 3,221 more rows\n# ‚Ñπ 5 more variables: Home_rent &lt;dbl&gt;, Marital_married &lt;dbl&gt;, Marital_separated &lt;dbl&gt;,\n#   Marital_single &lt;dbl&gt;, Marital_widow &lt;dbl&gt;\n\n\nThat‚Äôs a lot of additional columns. How can we keep track of all these additional columns and how they were preprocessed? We can summary and tidy our prepped recipe. Summarizing the prepped recipe is useful because of the source column that gets outputted. In our example, the source column of the returned tibble contains two values: original (i.e., the column was an original column in the data set) and derived (i.e., a column created from the preprocessing step). When we tidy() the recipe object returned from step_dummy(), a tibble with two columns is returned: terms and columns. terms represents the original variable the dummy variables were created from. columns represents the newly preprocessed dummy variable.\n\n# Prep our dummy variables\ncredit_rec &lt;- \n  credit_rec |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep()\n\nsummary(credit_rec)\n\n# A tibble: 13 √ó 4\n   variable          type      role      source  \n   &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Status            &lt;chr [3]&gt; outcome   original\n 2 Job_freelance     &lt;chr [2]&gt; predictor derived \n 3 Job_others        &lt;chr [2]&gt; predictor derived \n 4 Job_partime       &lt;chr [2]&gt; predictor derived \n 5 Home_other        &lt;chr [2]&gt; predictor derived \n 6 Home_owner        &lt;chr [2]&gt; predictor derived \n 7 Home_parents      &lt;chr [2]&gt; predictor derived \n 8 Home_priv         &lt;chr [2]&gt; predictor derived \n 9 Home_rent         &lt;chr [2]&gt; predictor derived \n10 Marital_married   &lt;chr [2]&gt; predictor derived \n11 Marital_separated &lt;chr [2]&gt; predictor derived \n12 Marital_single    &lt;chr [2]&gt; predictor derived \n13 Marital_widow     &lt;chr [2]&gt; predictor derived \n\n# View what preprocessing steps are applied\ntidy(credit_rec)\n\n# A tibble: 1 √ó 6\n  number operation type  trained skip  id         \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;      \n1      1 step      dummy TRUE    FALSE dummy_9a72e\n\n# Drill down and view what was done in during this specific step \ntidy(credit_rec, number = 1)\n\n# A tibble: 12 √ó 3\n   terms   columns   id         \n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;      \n 1 Job     freelance dummy_9a72e\n 2 Job     others    dummy_9a72e\n 3 Job     partime   dummy_9a72e\n 4 Home    other     dummy_9a72e\n 5 Home    owner     dummy_9a72e\n 6 Home    parents   dummy_9a72e\n 7 Home    priv      dummy_9a72e\n 8 Home    rent      dummy_9a72e\n 9 Marital married   dummy_9a72e\n10 Marital separated dummy_9a72e\n11 Marital single    dummy_9a72e\n12 Marital widow     dummy_9a72e\n\n\nWhen it comes to specifying interactions within a model, there are some special considerations when using dummy variables. I don‚Äôt have much time to discuss this today, but I hope to address it on a future day of this challenge. I suggest reviewing the ‚ÄòInteractions with Dummy Variables‚Äô section from the ‚ÄòDummies‚Äô vignette (vignettes(\"Dummies\", package = \"recipes\")) for more information.\nOne more thing, step_dummy() is useful for straight forward dummy variable creation. However, recipes also has some other closely related step_* functions. Here is a list of a few from the ‚ÄòDummies‚Äô vignette:\n\nstep_other() - collapses infrequently occurring levels into an ‚Äòother‚Äô category.\nstep_holiday() - creates dummy variables from dates to capture holidays. Useful when working with time series data.\nstep_zv() - removes dummy variables that are zero-variance.\n\nI look to highlight the use of some of these step_* functions in the coming days.\n\n\nDay 05 - Create a binary indicator variable for holidays using step_holiday()\nStaying on the topic of dummy variables, I wanted to take a day to focus on the use of recipes‚Äô step_holiday() function. It seems to be pretty useful when working with time series data.\nFor today‚Äôs example, I‚Äôm going to use some obfuscated, simulated Google Analytics ecommerce data. This emulates data closely related to what would be collected for the Google Merchandise Store. You can learn more about this data by clicking on the previously linked docs. Let‚Äôs do some data wrangling.\nSome notes about what wrangling was done:\n\nParse the event_date column into a date variable.\nCalculate the revenue generated from the purchase of items based on quantity.\nRetain only relevant columns.\n\nFor simplicity, I‚Äôm not going to create a testing training split for this data.\n\ndata_ga &lt;-\n  read_csv(\n    here(\"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  ) |&gt;\n  mutate(\n    event_date = ymd(event_date),\n    revenue = price_in_usd * quantity\n  ) |&gt;\n  select(event_date, transaction_id, item_category, revenue)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet‚Äôs start on our recipe. Since we have an id variable, transaction_id, let‚Äôs update the recipe to change it‚Äôs role to id. Once we do that, we can pass the event_date to the step_holiday() function. Before we bake our recipe, I wanna prep() and summarise the preprocessing to see what columns will get added.\n\nga_rec &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  update_role(transaction_id, new_role = \"id\") |&gt;\n  step_holiday(event_date) |&gt;\n  prep() \n\nsummary(ga_rec)\n\n# A tibble: 7 √ó 4\n  variable                type      role      source  \n  &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 event_date              &lt;chr [1]&gt; predictor original\n2 transaction_id          &lt;chr [2]&gt; id        original\n3 item_category           &lt;chr [3]&gt; predictor original\n4 revenue                 &lt;chr [2]&gt; outcome   original\n5 event_date_LaborDay     &lt;chr [2]&gt; predictor derived \n6 event_date_NewYearsDay  &lt;chr [2]&gt; predictor derived \n7 event_date_ChristmasDay &lt;chr [2]&gt; predictor derived \n\n\nNote, three new columns will be added once the recipe is baked. This includes:\n\nevent_date_LaborDay - a dummy variable to represent an item purchases on Labor Day.\nevent_date_NewYearsDay - a dummy variable to represent item purchases on New Years Day.\nevent_date_ChristmasDay - a dummy variable to represent item purchases made on Christmas Day.\n\nYou can see the variables that get added by baking the recipe.\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 9,365 √ó 7\n   event_date transaction_id item_category       revenue event_date_LaborDay event_date_NewYearsDay\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt;               &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance                12                   0                      0\n 2 2020-12-01          10648 Accessories               2                   0                      0\n 3 2020-12-01          10648 Drinkware                 4                   0                      0\n 4 2020-12-01          10648 Small Goods               2                   0                      0\n 5 2020-12-01          10648 Office                    3                   0                      0\n 6 2020-12-01          10648 Accessories               3                   0                      0\n 7 2020-12-01          10648 Apparel                  14                   0                      0\n 8 2020-12-01         171491 Apparel                  48                   0                      0\n 9 2020-12-01         171491 Drinkware                14                   0                      0\n10 2020-12-01         174748 Uncategorized Items      44                   0                      0\n# ‚Ñπ 9,355 more rows\n# ‚Ñπ 1 more variable: event_date_ChristmasDay &lt;int&gt;\n\n\nLabor Day, New Years Day, and Christmas Day are the default holidays preprocessed by the function. You can modify this by passing a character vector of holidays to step_holiday()‚Äôs holidays argument. For instance, say we wanted to create dummy variables for Boxing Day and the United State‚Äôs Thanksgiving Day holiday, while excluding Labor Day. The following code will specify this preprocessing step for us:\n\nga_rec_holidays &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  update_role(transaction_id, new_role = \"transaction_id\") |&gt;\n  step_holiday(\n    event_date, \n    holidays = c(\"USThanksgivingDay\", \"ChristmasDay\", \"BoxingDay\", \"NewYearsDay\")\n  ) |&gt;\n  prep()\n\nsummary(ga_rec_holidays)\n\n# A tibble: 8 √ó 4\n  variable                     type      role           source  \n  &lt;chr&gt;                        &lt;list&gt;    &lt;chr&gt;          &lt;chr&gt;   \n1 event_date                   &lt;chr [1]&gt; predictor      original\n2 transaction_id               &lt;chr [2]&gt; transaction_id original\n3 item_category                &lt;chr [3]&gt; predictor      original\n4 revenue                      &lt;chr [2]&gt; outcome        original\n5 event_date_USThanksgivingDay &lt;chr [2]&gt; predictor      derived \n6 event_date_ChristmasDay      &lt;chr [2]&gt; predictor      derived \n7 event_date_BoxingDay         &lt;chr [2]&gt; predictor      derived \n8 event_date_NewYearsDay       &lt;chr [2]&gt; predictor      derived \n\n\nNow we have a dummy variable for all four of these holidays. Let‚Äôs bake our recipe and see the final result.\n\nbake(ga_rec_holidays, new_data = NULL)\n\n# A tibble: 9,365 √ó 8\n   event_date transaction_id item_category     revenue event_date_USThanksg‚Ä¶¬π event_date_Christmas‚Ä¶¬≤\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;                  &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance              12                      0                      0\n 2 2020-12-01          10648 Accessories             2                      0                      0\n 3 2020-12-01          10648 Drinkware               4                      0                      0\n 4 2020-12-01          10648 Small Goods             2                      0                      0\n 5 2020-12-01          10648 Office                  3                      0                      0\n 6 2020-12-01          10648 Accessories             3                      0                      0\n 7 2020-12-01          10648 Apparel                14                      0                      0\n 8 2020-12-01         171491 Apparel                48                      0                      0\n 9 2020-12-01         171491 Drinkware              14                      0                      0\n10 2020-12-01         174748 Uncategorized It‚Ä¶      44                      0                      0\n# ‚Ñπ 9,355 more rows\n# ‚Ñπ abbreviated names: ¬π‚Äãevent_date_USThanksgivingDay, ¬≤‚Äãevent_date_ChristmasDay\n# ‚Ñπ 2 more variables: event_date_BoxingDay &lt;int&gt;, event_date_NewYearsDay &lt;int&gt;\n\n\nIndeed, there are many holidays that could be specified for dummy variable creation. All the available holidays can be seen by running timeDate::listHolidays() in your console. Last time I checked, there were 118 available holidays.\n\n\nDay 06 - Use step_zv() to drop variables with one value\nFor today, I‚Äôm focusing on recipes‚Äô step_zv() function. This function is a filter function, which drops variables that only contain one value.\nAt first, I didn‚Äôt really understand why step_zv() was made available. Why would you want a step to drop variables within a recipe? Then it clicked working on yesterday‚Äôs example using the obfuscated Google Analytics data for the Google Merchandise store.\nBut first, let‚Äôs get our data again and specify our recipe. I‚Äôm going to keep things simple here. First, I‚Äôm just going to use data_ga, which was previously wrangled in yesterday‚Äôs post (check it out if you want more info). Second, I‚Äôm going to skip creating a testing and training split. Lastly, I‚Äôm going to create dummy variables using step_holiday(), just to show how step_zv() can be useful.\n\nga_rec &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  step_holiday(event_date)\n\nsummary(ga_rec)\n\n# A tibble: 4 √ó 4\n  variable       type      role      source  \n  &lt;chr&gt;          &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 event_date     &lt;chr [1]&gt; predictor original\n2 transaction_id &lt;chr [2]&gt; predictor original\n3 item_category  &lt;chr [3]&gt; predictor original\n4 revenue        &lt;chr [2]&gt; outcome   original\n\n\nLet‚Äôs take a closer look at our data. You‚Äôll notice the range of the event_date is a subset of data. data_ga‚Äôs event_date ranges between the US holiday season. It starts right before Christmas and moves into the first month of the new year.\n\nc(\n  min_date = min(data_ga$event_date), \n  max_date = max(data_ga$event_date)\n)\n\n    min_date     max_date \n\"2020-12-01\" \"2021-01-30\" \n\n\nIf you remember from yesterday‚Äôs post, one of the default holidays for step_holiday() is Labor Day. As such, a dummy variable with all 0‚Äôs will be created for the Labor Day holiday. Purchases made on these dates were not included within this data.\n\nga_prep &lt;- prep(ga_rec)\n\ntidy(ga_prep, number = 1)\n\n# A tibble: 3 √ó 3\n  terms      holiday      id           \n  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;        \n1 event_date LaborDay     holiday_cClNx\n2 event_date NewYearsDay  holiday_cClNx\n3 event_date ChristmasDay holiday_cClNx\n\n# Check the unique values\nbake(ga_prep, new_data = NULL) |&gt; \n  select(event_date_LaborDay) |&gt;\n  distinct(event_date_LaborDay)\n\n# A tibble: 1 √ó 1\n  event_date_LaborDay\n                &lt;int&gt;\n1                   0\n\n\nAs such, this variable is not very useful and should be dropped before being applied within our model. This is why step_zv() can be handy, especially in situations where you have a lot of variables that could only have one value. step_zv() makes it easy to drop all unnecessary variables in one step, while allowing you to continue working with a recipe object.\nIndeed, keen observers might note this step could be mitigated by modifying the holiday argument in step_holiday(). However, the function‚Äôs utility extends beyond just step_holiday(). You might even consider useful as a final step you apply to every recipe.\n\nga_rec_drop &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  step_holiday(event_date) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep()\n\nga_rec_drop\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 9365 data points and 164 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Holiday features from: event_date | Trained\n\n\n‚Ä¢ Zero variance filter removed: event_date_LaborDay | Trained\n\ntidy(ga_rec_drop)\n\n# A tibble: 2 √ó 6\n  number operation type    trained skip  id           \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;        \n1      1 step      holiday TRUE    FALSE holiday_FpWeG\n2      2 step      zv      TRUE    FALSE zv_3cDmc     \n\n\nTake note, the prep() output informs us of the variables that were dropped when the step was applied. This is something to keep an eye on, just in case you need to explore situations where many variables are dropped, and you need to explore what your recipe is actually doing.\nFor completeness, lets bake() our final recipe.\n\nga_rec &lt;- recipe(revenue ~., data = data_ga) |&gt;\n  step_holiday(event_date) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec\n\n# A tibble: 9,365 √ó 6\n   event_date transaction_id item_category     revenue event_date_NewYearsDay event_date_Christmas‚Ä¶¬π\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;                  &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance              12                      0                      0\n 2 2020-12-01          10648 Accessories             2                      0                      0\n 3 2020-12-01          10648 Drinkware               4                      0                      0\n 4 2020-12-01          10648 Small Goods             2                      0                      0\n 5 2020-12-01          10648 Office                  3                      0                      0\n 6 2020-12-01          10648 Accessories             3                      0                      0\n 7 2020-12-01          10648 Apparel                14                      0                      0\n 8 2020-12-01         171491 Apparel                48                      0                      0\n 9 2020-12-01         171491 Drinkware              14                      0                      0\n10 2020-12-01         174748 Uncategorized It‚Ä¶      44                      0                      0\n# ‚Ñπ 9,355 more rows\n# ‚Ñπ abbreviated name: ¬π‚Äãevent_date_ChristmasDay\n\nglimpse(ga_rec)\n\nRows: 9,365\nColumns: 6\n$ event_date              &lt;date&gt; 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-‚Ä¶\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1‚Ä¶\n$ item_category           &lt;fct&gt; Clearance, Accessories, Drinkware, Small Goods, Office, Accessorie‚Ä¶\n$ revenue                 &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, ‚Ä¶\n$ event_date_NewYearsDay  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ event_date_ChristmasDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n\n\n\n\nDay 07 - Use step_impute_*() functions for imputation\nThe recipes package makes it easy to perform imputation tasks. As of this writing, recipes had the following functions to perform different methods of imputation:\n\nstep_impute_bag()\nstep_impute_knn()\nstep_impute_linear()\nstep_impute_lower()\nstep_impute_mean()\nstep_impute_median()\nstep_impute_mode()\nstep_impute_roll()\n\nFor today‚Äôs examples, I‚Äôm going to highlight the use of step_impute_mean(), step_input_median(), and step_input_mode(). First, though, we need some data with missing values. Let‚Äôs switch it up a bit and use the Palmer Station penguin data (run ?penguins in your console to get more information about the data). In brief, these data represent different measurements of various penguin species in Antarctica.\n\ndata(penguins, package = \"modeldata\") \n\n# Add an id column\npenguins &lt;- \n  penguins |&gt; mutate(id = 1:n(), .before = everything())\n\nThis data set contains some missing values that could be addressed using imputation methods. Let‚Äôs take a moment and explore the data a little further.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2‚Ä¶\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, ‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe‚Ä¶\n\n# What columns have missing data?\nmap_df(penguins, \\(x) any(is.na(x)))\n\n# A tibble: 1 √ó 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  TRUE           TRUE          TRUE              TRUE        TRUE \n\n# What percentage of data is missing in each column?\nmap(penguins, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$species\n[1] 0\n\n$island\n[1] 0\n\n$bill_length_mm\n[1] 0.005813953\n\n$bill_depth_mm\n[1] 0.005813953\n\n$flipper_length_mm\n[1] 0.005813953\n\n$body_mass_g\n[1] 0.005813953\n\n$sex\n[1] 0.03197674\n\n# Missing data examples\nmissing_examples &lt;- c(4, 12, 69, 272)\npenguins |&gt; slice(missing_examples)\n\n# A tibble: 4 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nThe following columns contain missing data (included are the variable types):\n\nbill_length_mm - double\nbill_depth_mm - double\nflipper_length_mm - integer\nbody_mass_g - integer\nsex - factor\n\nYou‚Äôll also notice some of these variables are of various types (i.e.¬†factor, double, or integer). Indeed, the variable type will determine the method of imputation applied.\n\nset.seed(20240107)\npenguins_split &lt;- initial_split(penguins, prop = .8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nLet‚Äôs start by highlighting how to apply mean substitution as our imputation method. Specifically, let‚Äôs apply this step to our first numeric variable with missing values, bill_length_mm.\n\n\n\n\n\n\nNote\n\n\n\nTake note of the importance of the use of prep() here. Remember, some recipe steps need to calculate an intermediate value before applying it to the final baked data. This is highlighted with the tidy(penquin_rec, number = 1) in the code below.\n\n\n\npenguins_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_mean(bill_length_mm) |&gt;\n  prep()\n\nsummary(penguins_rec)\n\n# A tibble: 8 √ó 4\n  variable          type      role      source  \n  &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 id                &lt;chr [2]&gt; id        original\n2 species           &lt;chr [3]&gt; predictor original\n3 island            &lt;chr [3]&gt; predictor original\n4 bill_length_mm    &lt;chr [2]&gt; predictor original\n5 bill_depth_mm     &lt;chr [2]&gt; predictor original\n6 flipper_length_mm &lt;chr [2]&gt; predictor original\n7 body_mass_g       &lt;chr [2]&gt; predictor original\n8 sex               &lt;chr [3]&gt; predictor original\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm  43.7 impute_mean_yJPI2\n\n\n\npenguins_baked &lt;- bake(penguins_rec, new_data = NULL)\n\npenguins_baked\n\n# A tibble: 275 √ó 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ‚Ñπ 265 more rows\n\n# Imputation should result in a complete column of data\nany(is.na(penguins_baked$bill_length_mm))\n\n[1] FALSE\n\n# The missing values have now been substituted\npenguins_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n1   272 Gentoo  Biscoe              43.7          NA                  NA          NA &lt;NA&gt; \n2     4 Adelie  Torgersen           43.7          NA                  NA          NA &lt;NA&gt; \n3    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt; \n\n\nstep_impute_mean() also includes a trim argument, which trims observations from the end of the variable before the mean is computed. This is also a tuning parameter, which can be used in any hyperparameter tuning applied within your modeling. I would like to explore this more, but it‚Äôs outside the scope of this post. Just to highlight the use of the trim argument, here‚Äôs some example code:\n\npenguin_mean_trim_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  step_impute_mean(bill_length_mm, trim = .5) |&gt;\n  prep()\n\n# Notice how the intermediate calculation changed because\n# we trimmed the observations used to make the mean calculation\ntidy(penguin_mean_trim_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm    44 impute_mean_sGprM\n\n\nLet‚Äôs bake this recipe for completeness.\n\npenguins_mean_trim_baked &lt;- \n  bake(penguin_mean_trim_rec, new_data = NULL)\n\npenguins_mean_trim_baked\n\n# A tibble: 275 √ó 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ‚Ñπ 265 more rows\n\n# The missing values have now been imputed\npenguins_mean_trim_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n1   272 Gentoo  Biscoe              44            NA                  NA          NA &lt;NA&gt; \n2     4 Adelie  Torgersen           44            NA                  NA          NA &lt;NA&gt; \n3    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt; \n\n\nMean substitution is just one imputation step. The recipes package also includes the step_impute_median() and step_impute_mode(). These step functions have similar syntax, just a different calculated metric is applied in the background. Let‚Äôs apply step_impute_median() to bill_depth_mm, flipper_length_mm, and body_mass_g.\nIn addition, we‚Äôll apply step_impute_mode() to impute values for the missing data within the sex variable. Take note, the docs for this function state:\n\nImpute nominal data using the most common value.\n\nSo, it only seems step_impute_mode() can only be used to impute missing values for nominal variables.\n\npenguin_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  step_impute_mean(bill_length_mm) |&gt;\n  step_impute_median(\n    bill_depth_mm, \n    flipper_length_mm, \n    body_mass_g\n  ) |&gt;\n  step_impute_mode(sex) |&gt;\n  prep()\n\npenguin_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 8\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 275 data points and 11 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Mean imputation for: bill_length_mm | Trained\n\n\n‚Ä¢ Median imputation for: bill_depth_mm, flipper_length_mm, body_mass_g | Trained\n\n\n‚Ä¢ Mode imputation for: sex | Trained\n\ntidy(penguin_rec)\n\n# A tibble: 3 √ó 6\n  number operation type          trained skip  id                 \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;              \n1      1 step      impute_mean   TRUE    FALSE impute_mean_fOycb  \n2      2 step      impute_median TRUE    FALSE impute_median_zocWX\n3      3 step      impute_mode   TRUE    FALSE impute_mode_PGohX  \n\n\nLet‚Äôs take a look at the calculated values for all these steps.\n\nmap(1:3, \\(x) tidy(penguin_rec, number = x))\n\n[[1]]\n# A tibble: 1 √ó 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm  43.7 impute_mean_fOycb\n\n[[2]]\n# A tibble: 3 √ó 3\n  terms              value id                 \n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;              \n1 bill_depth_mm       17.3 impute_median_zocWX\n2 flipper_length_mm  196   impute_median_zocWX\n3 body_mass_g       4050   impute_median_zocWX\n\n[[3]]\n# A tibble: 1 √ó 3\n  terms value  id               \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;            \n1 sex   female impute_mode_PGohX\n\n\nAs always, let‚Äôs bake this recipe and look at the final data, which should now contain no missing data.\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 √ó 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ‚Ñπ 265 more rows\n\nmap_df(baked_penguin, \\(x) any(is.na(x)))\n\n# A tibble: 1 √ó 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  FALSE          FALSE         FALSE             FALSE       FALSE\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              43.7          17.3               196        4050 female\n2     4 Adelie  Torgersen           43.7          17.3               196        4050 female\n3    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n\n\nThat‚Äôs all the time I have for today. Tomorrow I‚Äôll pick up exploring some more of the other step_impute_* functions.\n\n\nDay 08 - Use bagged tree models to impute missing data with step_impute_bag()\nTo start, I wanted to highlight a really good, simplified definition of imputation from the Feature Engineering and Selection: A Practical Approach for Predictive Models book by Max Kuhn and Kjell Johnson.\n\nImputation uses information and relationships among the non-missing predictors to provide an estimate to fill in the missing values.\n\nYesterday we used the step_impute_mean(), step_impute_median(), and step_impute_mode() functions to calculate missing values. However, we can also use tree-based methods, which uses information from different variables rather than just values in rows, to perform our imputation step.\nTo be honest, this imputation method was beyond my current knowledge set. Thus, my explanation of what is happening on the backend may be quite general. However, check out the ‚ÄòTrees‚Äô section from the Feature Engineering and Selection: A Practical Approach for Predictive Models book for a good starting point to learn more. The book does suggest using bagged models can produce reasonable outputs, which results in values to be produced within the range of the training data. Such methods also retains all predictors, unlike when case-wise deletion is used to manage missing data.\nrecipes‚Äô step_impute_bag() function is used to impute missing data using bagged tree models. To highlight the use of this step, let‚Äôs go back to using the penguins data from yesterday.\n\ndata(penguins, package = \"modeldata\")\nmissing_examples &lt;- c(4, 12, 69, 272)\n\n# Create an id variable\npenguins &lt;- \n  penguins |&gt; \n  mutate(id = 1:n(), .before = everything())\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2‚Ä¶\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, ‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe‚Ä¶\n\n# Print the missing examples\npenguins |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nWe‚Äôll now create our training and testing split.\n\nset.seed(20240108)\npenguins_split &lt;- initial_split(penguins, prop = 0.8)\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nTo start small, let‚Äôs use a bagged tree model to impute values for the missing data in the bill_length_mm variable. The syntax is pretty straightforward:\n\npenguins_rec &lt;- recipe (~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(bill_length_mm) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 275 data points and 9 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Bagged tree imputation for: bill_length_mm | Trained\n\n\nBefore we bake() our recipe, let‚Äôs tidy() our prepped recipe a bit to see what‚Äôs happening under the hood.\n\ntidy(penguins_rec)\n\n# A tibble: 1 √ó 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_bag TRUE    FALSE impute_bag_OQalP\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms          model     id              \n  &lt;chr&gt;          &lt;list&gt;    &lt;chr&gt;           \n1 bill_length_mm &lt;regbagg&gt; impute_bag_OQalP\n\n\nTidying down to the bagging step, you‚Äôll see this step outputs a tibble with three columns:\n\nterms - the selectors or variables selected.\nmodel - the bagged tree model object.\nid - a unique id for the step being applied in the recipe.\n\nLet‚Äôs bake the recipe and see the result of our imputation step.\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 √ó 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ‚Ñπ 265 more rows\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              43.7          17.3               196        4050 female\n2     4 Adelie  Torgersen           43.7          17.3               196        4050 female\n3    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n\n\nstep_impute_bagged() also has several options to modify the imputation method. First, it has an impute_with argument that allows you to be selective about what variables are used as predictors in the bagged tree model. We‚Äôll specify these variables by passing them into the imp_vars() function to the argument.\nThis argument accepts the various selector functions as well. For instance, the default for the argument is the all_predictors() function. The following code uses this argument to limit the imputation to the bill_depth_mm and sex variables (I‚Äôm not a biologist, so I have no idea if this is actually a good approach).\nI did come across a cryptic warning when first doing this, though. This warning also resulted in the imputation step to not be applied.\n\npenguin_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    impute_with = imp_vars(bill_depth_mm, sex)\n  ) |&gt;\n  prep()\n\nWarning: The `impute_with` variables for `bill_length_mm` only contains missing values for row: 26 and 210.\nCannot impute for those rows.\n\n\nI assumed this was because all the predictors used to create the model for imputation had missing values. So, I applied some imputation to these first before applying the step_impute_bag() and the warning went away. However, I‚Äôm unsure if this was the initial problem. I might submit an issue to the recipes GitHub repo to which I‚Äôll link later. Nevertheless, I got the example to work. Here‚Äôs the code:\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_mean(bill_depth_mm) |&gt;\n  step_impute_mode(sex) |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    impute_with = imp_vars(bill_depth_mm)\n  ) |&gt;\n  prep() \n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           41.0          17.2                NA          NA male  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 male  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              41.0          17.2                NA          NA male  \n\n\nGiven we‚Äôre using a bagged tree model to perform imputation, we can modify the number of bagged trees used in each model in the step_impute_bag() function. To do this, we just pass a value to the trees argument. Indeed, its suggested to keep this value between 25 - 50 trees.\n\n# The default is 25 trees\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(bill_length_mm, trees = 50) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           38.3          NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              47.8          NA                  NA          NA &lt;NA&gt;  \n\n\nThe last step_impute_bag() argument I‚Äôll highlight is options. ipred::ipredbagg() implements the bagged model used for this imputation step. Thus, the options argument is used to pass arguments to this function. For example, if we want to speed up execution, we can lower the nbagg argument, the number of bootstrap replications applied, and indicate we don‚Äôt want to return a data frame of predictors by setting keepX = FALSE.\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    options = list(nbagg = 2, keepX = FALSE)\n  ) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 √ó 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   280 Chinstrap Dream               45.4          18.7               188        3525 female\n 2   160 Gentoo    Biscoe              46.7          15.3               219        5200 male  \n 3    27 Adelie    Biscoe              40.6          18.6               183        3550 male  \n 4   274 Gentoo    Biscoe              50.4          15.7               222        5750 male  \n 5   288 Chinstrap Dream               51.7          20.3               194        3775 male  \n 6    46 Adelie    Dream               39.6          18.8               190        4600 male  \n 7   316 Chinstrap Dream               53.5          19.9               205        4500 male  \n 8   286 Chinstrap Dream               51.3          19.9               198        3700 male  \n 9   164 Gentoo    Biscoe              49            16.1               216        5550 male  \n10    79 Adelie    Torgersen           36.2          16.1               187        3550 female\n# ‚Ñπ 265 more rows\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           38.1          NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              48.0          NA                  NA          NA &lt;NA&gt;  \n\n\nJust for the heck of it, let‚Äôs apply step_impute_bag() to all predictor variables in our recipe.\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(all_predictors()) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 √ó 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   280 Chinstrap Dream               45.4          18.7               188        3525 female\n 2   160 Gentoo    Biscoe              46.7          15.3               219        5200 male  \n 3    27 Adelie    Biscoe              40.6          18.6               183        3550 male  \n 4   274 Gentoo    Biscoe              50.4          15.7               222        5750 male  \n 5   288 Chinstrap Dream               51.7          20.3               194        3775 male  \n 6    46 Adelie    Dream               39.6          18.8               190        4600 male  \n 7   316 Chinstrap Dream               53.5          19.9               205        4500 male  \n 8   286 Chinstrap Dream               51.3          19.9               198        3700 male  \n 9   164 Gentoo    Biscoe              49            16.1               216        5550 male  \n10    79 Adelie    Torgersen           36.2          16.1               187        3550 female\n# ‚Ñπ 265 more rows\n\n# There should now be no missing data\nmap_df(baked_penguin, \\(x) any(is.na(x)))\n\n# A tibble: 1 √ó 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  FALSE          FALSE         FALSE             FALSE       FALSE\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           37.9          17.8               188        3546 male  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              47.9          14.8               214        5028 female\n\n\nThat‚Äôs it for today. Tomorrow I‚Äôll focus on the use of step_impute_knn().\n\n\nDay 09 - Impute missing values using step_impute_knn()\nToday, we‚Äôre focusing on imputing missing data using recipes‚Äô step_impute_knn() function. In short, this function uses a k-nearest neighbors approach to impute missing values.\nFor today‚Äôs examples, I‚Äôm going to stick with the penguins data we‚Äôve been using the past few days. Given this data is relatively small (n = 344), it‚Äôs a good candidate for using a k-nearest neighbor approach to imputation.\n\ndata(penguins, package = \"modeldata\")\n\n# Create a row id\npenguins &lt;- penguins |&gt;\n  mutate(id = 1:n(), .before = everything())\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2‚Ä¶\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, ‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe‚Ä¶\n\n# Percent missing for each column\nmap(penguins, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$species\n[1] 0\n\n$island\n[1] 0\n\n$bill_length_mm\n[1] 0.005813953\n\n$bill_depth_mm\n[1] 0.005813953\n\n$flipper_length_mm\n[1] 0.005813953\n\n$body_mass_g\n[1] 0.005813953\n\n$sex\n[1] 0.03197674\n\n\nJust for a refresher, let‚Äôs peek at a few of the missing values.\n\nmissing_examples &lt;- c(4, 12, 69, 272)\n\npenguins |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nLet‚Äôs create our training and testing split.\n\nset.seed(20240109)\npenguins_split &lt;- initial_split(penguins, prop = 0.8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nSince imputation can be applied to either numeric or nominal data, step_impute_knn() uses Gower‚Äôs distance for calculating nearest neighbors (you can learn more by running ?step_impute_knn in your console). Once the neighbors are calculated, nominal variables are predicted using the mean, and numeric data is predicted using the mode. The number of neighbors can be set by specifying the neighbors argument of the function, which can also be used for hyperparameter tuning.\nLet‚Äôs start by imputing values for our missing data in the sex column. Here‚Äôs the code for this initial recipe.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(sex) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ K-nearest neighbor imputation for: sex | Trained\n\n\nBefore we bake and examine what the imputation step does, let‚Äôs drill down and see what‚Äôs occurring at the prep stage. tidy() will be used to do this. Similar to step_impute_bag(), a tibble of terms, predictors, neighbors (specific to k-nearest neighbors), and an id is returned.\n\ntidy(penguins_rec)\n\n# A tibble: 1 √ó 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_knn TRUE    FALSE impute_knn_dK6OX\n\n# Drill down into the specific impute_knn step\ntidy(penguins_rec, number = 1)\n\n# A tibble: 6 √ó 4\n  terms predictors        neighbors id              \n  &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;           \n1 sex   species                   5 impute_knn_dK6OX\n2 sex   island                    5 impute_knn_dK6OX\n3 sex   bill_length_mm            5 impute_knn_dK6OX\n4 sex   bill_depth_mm             5 impute_knn_dK6OX\n5 sex   flipper_length_mm         5 impute_knn_dK6OX\n6 sex   body_mass_g               5 impute_knn_dK6OX\n\n\nLet‚Äôs bake our recipe and examine the result of our imputation step.\n\nbaked_penguins &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguins\n\n# A tibble: 275 √ó 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   168 Gentoo    Biscoe              49.3          15.7               217        5850 male  \n 2   196 Gentoo    Biscoe              49.6          15                 216        4750 male  \n 3   117 Adelie    Torgersen           38.6          17                 188        2900 female\n 4    39 Adelie    Dream               37.6          19.3               181        3300 female\n 5   299 Chinstrap Dream               43.2          16.6               187        2900 female\n 6   207 Gentoo    Biscoe              46.5          14.4               217        4900 female\n 7   167 Gentoo    Biscoe              45.8          14.6               210        4200 female\n 8   101 Adelie    Biscoe              35            17.9               192        3725 female\n 9   339 Chinstrap Dream               45.7          17                 195        3650 female\n10   267 Gentoo    Biscoe              46.2          14.1               217        4375 female\n# ‚Ñπ 265 more rows\n\nbaked_penguins |&gt; \n  filter(id %in% missing_examples) |&gt; \n  relocate(sex, .after = 1) \n\n# A tibble: 3 √ó 8\n     id sex    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1   272 male   Gentoo  Biscoe              NA            NA                  NA          NA\n2    69 female Adelie  Torgersen           35.9          16.6               190        3050\n3     4 male   Adelie  Torgersen           NA            NA                  NA          NA\n\n\nAs mentioned before, neighbors is an argument to set the number of neighbors to use in our estimation. The function defaults to five, but we can modify this to any integer value. It is suggested that 5 - 10 neighbors is a sensible default. However, this is dependent on the data you are working with. For our next example, let‚Äôs constrain this parameter to 3, while also applying our imputation step to all numeric predictors.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(all_numeric_predictors(), neighbors = 3) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ K-nearest neighbor imputation for: bill_length_mm bill_depth_mm, ... | Trained\n\n\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 √ó 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              47.2          15.4               214        5217 &lt;NA&gt;  \n2    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n3     4 Adelie  Torgersen           36.6          18.3               184        3658 &lt;NA&gt;  \n\n\nJust like step_impute_bag(), step_impute_knn() provides both an impute_with and options argument. We can be explicit about the variables to use with our knn calculations by passing a comma-separated list of names to the imp_vars() function to the impute_with arugment. options accepts a list of arguments. These get passed along to the underlying gower::gower_topn() function running under the hood, which performs the k-nearest neighbors calculation using Gower‚Äôs distance. According to the docs, the only two options accepted are:\n\nnthread - specify the number of threads to use for parallelization.\neps - optional option for variable ranges (I‚Äôm not quite sure what this does).\n\nMy assumption is these options can be used to optimize the run-time for our calculations. However, I would consult the documentation for the gower::gower_topn() function to verify. The key takeaway here is that step_impute_knn() provides an interface to configure options for the function it wraps.\nHere‚Äôs an example constraining our sex imputation to the bill_length and bill_depth_mm variables.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(\n    sex, \n    impute_with = imp_vars(bill_depth_mm, bill_length_mm)\n  ) |&gt;\n  prep()\n\nWarning: The `impute_with` variables for `sex` only contains missing values for row: 64 and 95. Cannot\nimpute for those rows.\n\npenguins_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ K-nearest neighbor imputation for: sex | Trained\n\n\nLet‚Äôs bake our final example and examine what happened.\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin |&gt; \n  filter(id %in% missing_examples) |&gt;\n  relocate(sex, .after = 1)\n\n# A tibble: 3 √ó 8\n     id sex    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1   272 &lt;NA&gt;   Gentoo  Biscoe              NA            NA                  NA          NA\n2    69 female Adelie  Torgersen           35.9          16.6               190        3050\n3     4 &lt;NA&gt;   Adelie  Torgersen           NA            NA                  NA          NA\n\n\nSo there you have it, another example of a step_impute_* function. Tomorrow I‚Äôll continue exploring imputation steps by highlighting the use of the step_impute_linear() function.\n\n\nDay 10 - Impute missing values using a linear model with step_impute_linear()\nSo here we are, day 10. We continue our overview of recipes‚Äô imputation steps. Specifically, I‚Äôm going to highlight the use of step_impute_linear() for today. step_impute_linear() uses linear regression models to impute missing data. Indeed, when there is a strong, linear relationship between a complete predictor variable and one that requires imputation (i.e., contains missing data), linear methods for imputation may be a good approach. Such a method is also really quick and requires few computational resources to calculate.\nFor today‚Äôs examples, we‚Äôre going back to our credit_data data. You can read more about this data by running ?credit_data in your console.\n\ndata(credit_data, package = \"modeldata\")\n\ncredit_data &lt;- credit_data |&gt;\n  mutate(id = seq_len(nrow(credit_data)), .before = everything()) |&gt;\n  as_tibble()\n\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 15\n$ id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2‚Ä¶\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good‚Ä¶\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2‚Ä¶\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own‚Ä¶\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, ‚Ä¶\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, ‚Ä¶\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr‚Ä¶\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no‚Ä¶\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti‚Ä¶\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, ‚Ä¶\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330‚Ä¶\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, ‚Ä¶\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0‚Ä¶\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400‚Ä¶\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, ‚Ä¶\n\nmap(credit_data, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$Status\n[1] 0\n\n$Seniority\n[1] 0\n\n$Home\n[1] 0.001347104\n\n$Time\n[1] 0\n\n$Age\n[1] 0\n\n$Marital\n[1] 0.0002245173\n\n$Records\n[1] 0\n\n$Job\n[1] 0.0004490346\n\n$Expenses\n[1] 0\n\n$Income\n[1] 0.08554109\n\n$Assets\n[1] 0.01055231\n\n$Debt\n[1] 0.004041311\n\n$Amount\n[1] 0\n\n$Price\n[1] 0\n\n\nLet‚Äôs peek at some missing data examples.\n\nmissing_examples &lt;- c(114, 195, 206, 242, 496)\n\ncredit_data |&gt; filter(id %in% missing_examples) \n\n# A tibble: 5 √ó 15\n     id Status Seniority Home   Time   Age Marital Records Job   Expenses Income Assets  Debt Amount\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1   114 bad            0 owner    36    39 single  no      free‚Ä¶       35     NA   4000     0   1000\n2   195 bad            0 other    36    48 married yes     free‚Ä¶       45     NA      0     0   1600\n3   206 good          10 owner    36    45 married yes     free‚Ä¶       60     NA   9500   250    750\n4   242 bad           10 rent     60    43 married no      free‚Ä¶       90     NA      0     0   1350\n5   496 bad            3 owner    60    33 separa‚Ä¶ no      free‚Ä¶       35     NA   6000     0    950\n# ‚Ñπ 1 more variable: Price &lt;int&gt;\n\n\nAs mentioned before, linear imputation methods are useful in cases where you have a strong, linear relationship between a complete variable (i.e., contains no missing data) and one where imputation is to be applied. For our example, let‚Äôs use linear methods to impute values for missing data in the Income variable. The Senority variable is complete, so we‚Äôll use it for our imputation step. Although the relationship between these two variables isn‚Äôt a strong, linear one, let‚Äôs use it for example sake.\n\n# Use corrr::correlate() to examine correlation among numeric variables\ncorrelate(credit_data)\n\nNon-numeric variables removed from input: `Status`, `Home`, `Marital`, `Records`, and `Job`\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 10 √ó 11\n   term            id Seniority     Time     Age Expenses  Income   Assets     Debt   Amount   Price\n   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 id        NA        -0.00362  8.19e-3 -0.0165 -2.71e-1 -0.116  -0.00533 -0.00578  0.0264   0.0266\n 2 Seniority -0.00362  NA       -2.14e-2  0.506   1.26e-1  0.122   0.127   -0.0191  -0.00791  0.0409\n 3 Time       0.00819  -0.0214  NA       -0.0517 -8.40e-4 -0.0430 -0.0848   0.0577   0.431    0.130 \n 4 Age       -0.0165    0.506   -5.17e-2 NA       2.48e-1  0.147   0.185   -0.0459   0.0292   0.0489\n 5 Expenses  -0.271     0.126   -8.40e-4  0.248  NA        0.258   0.0184   0.0148   0.0492   0.0403\n 6 Income    -0.116     0.122   -4.30e-2  0.147   2.58e-1 NA       0.237    0.151    0.192    0.227 \n 7 Assets    -0.00533   0.127   -8.48e-2  0.185   1.84e-2  0.237  NA        0.191    0.147    0.200 \n 8 Debt      -0.00578  -0.0191   5.77e-2 -0.0459  1.48e-2  0.151   0.191   NA        0.0525   0.0456\n 9 Amount     0.0264   -0.00791  4.31e-1  0.0292  4.92e-2  0.192   0.147    0.0525  NA        0.725 \n10 Price      0.0266    0.0409   1.30e-1  0.0489  4.03e-2  0.227   0.200    0.0456   0.725   NA     \n\n\nWe start off by creating our testing and training split.\n\nset.seed(20240110)\ncredit_split &lt;- initial_split(credit_data, prop = .80)\n\ncredit_tr &lt;- training(credit_split)\ncredit_te &lt;- testing(credit_split)\n\n\n\n\n\n\n\nNote\n\n\n\nThe docs mention imputed variables must be of type numeric. Also, this method requires predictors to be complete cases. As such, the imputation model will only use training set predictors that don‚Äôt have any missing values.\n\n\nJust like we did with other imputation methods that use a modeling approach, we can be specific about what variables to use as predictors. We do this by passing them to the impute_with argument, which are wrapped in the imp_vars() function. Here‚Äôs what this looks like:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_linear(Income, impute_with = imp_vars(Seniority)) |&gt;\n  prep(credit_tr)\n\ncredit_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\nid:         1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 3563 data points and 319 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Linear regression imputation for: Income | Trained\n\n\nBefore we bake our recipe, let‚Äôs take a look at what‚Äôs happening under the hood with tidy().\n\ntidy(credit_rec)\n\n# A tibble: 1 √ó 6\n  number operation type          trained skip  id                 \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;              \n1      1 step      impute_linear TRUE    FALSE impute_linear_5l9Ot\n\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms  model  id                 \n  &lt;chr&gt;  &lt;list&gt; &lt;chr&gt;              \n1 Income &lt;lm&gt;   impute_linear_5l9Ot\n\n\nA tibble is outputted with three columns:\n\nterms - the variable we‚Äôre seeking to replace missing values with imputed values.\nmodel - the model object used to calculate the imputed value. Note, the model object is lm.\nid - a unique id for the step being performed.\n\nReady, get set, bake.\n\ncredit_baked &lt;- bake(credit_rec, new_data = NULL)\n\n# View some of the imputed values\ncredit_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 √ó 15\n     id Status Seniority Home   Time   Age Marital Records Job   Expenses Income Assets  Debt Amount\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1   496 bad            3 owner    60    33 separa‚Ä¶ no      free‚Ä¶       35    135   6000     0    950\n2   242 bad           10 rent     60    43 married no      free‚Ä¶       90    143      0     0   1350\n3   206 good          10 owner    36    45 married yes     free‚Ä¶       60    143   9500   250    750\n# ‚Ñπ 1 more variable: Price &lt;int&gt;\n\n\nThe recipes‚Äô step_impute_linear() documentation also has a really good example of how to visualize the imputed data, using a dataset with complete values. It‚Äôs purpose is to show a comparison between the original values and the newly imputed values.\nThe following code is adapted from this example. Here I show the regression line used for the imputed Income values based on Seniority. It‚Äôs just a linear regression. If you‚Äôre interested in seeing the full example, run ?step_impute_linear() in your console, and scroll down to the examples section.\n\nggplot(credit_baked, aes(x = Seniority, y = Income)) +\n  geom_abline(col = \"green\") +\n  geom_point(alpha = .3) +\n  labs(title = \"Imputed Values\")\n\n\n\n\n\n\n\n\nAgain, this relationship is not a strong, linear one. Therefore, a linear imputation method of estimation may not be the best approach for this data. Nevertheless, it serves as an example of how to do it using the recipes package.\nDay 10, check. Another 20 to go. Tomorrow I‚Äôll continue my exploration of step_impute_*() functions by highlighting the use of the step_impute_lower() function.\n\n\nDay 11 - Impute values using step_impute_lower()\nstep_impute_lower() is our focus for today. According to the docs, step_impute_lower() calculates a variable‚Äôs minimum, simulates a value between zero and the minimum, and imputes this value for any cases at the minimum value. This imputation method is useful when we have non-negative numeric data, where values cannot be measured below a known value.\nFor today‚Äôs examples, I‚Äôm going to use the modeldata package‚Äôs crickets data. This data comes from a study examining the relationship between chirp rates and temperature for two different cricket species (run ?crickets in your console to learn more).\n\ndata(crickets, package = \"modeldata\")\n\nglimpse(crickets)\n\nRows: 31\nColumns: 3\n$ species &lt;fct&gt; O. exclamationis, O. exclamationis, O. exclamationis, O. exclamationis, O. exclama‚Ä¶\n$ temp    &lt;dbl&gt; 20.8, 20.8, 24.0, 24.0, 24.0, 24.0, 26.2, 26.2, 26.2, 26.2, 28.4, 29.0, 30.4, 30.4‚Ä¶\n$ rate    &lt;dbl&gt; 67.9, 65.1, 77.3, 78.7, 79.4, 80.4, 85.8, 86.6, 87.5, 89.1, 98.6, 100.8, 99.3, 101‚Ä¶\n\nskim(crickets)\n\n\nData summary\n\n\nName\ncrickets\n\n\nNumber of rows\n31\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1\nFALSE\n2\nO. : 17, O. : 14\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntemp\n0\n1\n23.76\n3.82\n17.2\n20.80\n24.0\n26.35\n30.4\n‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ\n\n\nrate\n0\n1\n72.89\n16.91\n44.3\n59.45\n76.2\n85.25\n101.7\n‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñÉ\n\n\n\n\n\nWe‚Äôre also going to modify the data a bit to better highlight what step_impute_lower() is doing. Here I‚Äôm just truncating all temp values less than or equal to 21 to 21.\n\n# Create a floor at temp = 21\ncrickets &lt;- crickets |&gt; \n  mutate(temp = case_when(\n    temp &lt;= 21 ~ 21,\n    TRUE ~ as.double(temp)\n  ))\n\nprint(crickets, n = 50)\n\n# A tibble: 31 √ó 3\n   species           temp  rate\n   &lt;fct&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 O. exclamationis  21    67.9\n 2 O. exclamationis  21    65.1\n 3 O. exclamationis  24    77.3\n 4 O. exclamationis  24    78.7\n 5 O. exclamationis  24    79.4\n 6 O. exclamationis  24    80.4\n 7 O. exclamationis  26.2  85.8\n 8 O. exclamationis  26.2  86.6\n 9 O. exclamationis  26.2  87.5\n10 O. exclamationis  26.2  89.1\n11 O. exclamationis  28.4  98.6\n12 O. exclamationis  29   101. \n13 O. exclamationis  30.4  99.3\n14 O. exclamationis  30.4 102. \n15 O. niveus         21    44.3\n16 O. niveus         21    47.2\n17 O. niveus         21    47.6\n18 O. niveus         21    49.6\n19 O. niveus         21    50.3\n20 O. niveus         21    51.8\n21 O. niveus         21    60  \n22 O. niveus         21    58.5\n23 O. niveus         21    58.9\n24 O. niveus         22.1  60.7\n25 O. niveus         23.5  69.8\n26 O. niveus         24.2  70.9\n27 O. niveus         25.9  76.2\n28 O. niveus         26.5  76.1\n29 O. niveus         26.5  77  \n30 O. niveus         26.5  77.7\n31 O. niveus         28.6  84.7\n\n\nFor simplicity, I‚Äôm not going to create a testing and training split and will use the full data for our example. Let‚Äôs start with our recipe.\n\ncrickets_rec &lt;- recipe(~., data = crickets) |&gt;\n  step_impute_lower(temp) |&gt;\n  prep()\n\ncrickets_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 31 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Lower bound imputation for: temp | Trained\n\n\nLet‚Äôs take a quick peek at what‚Äôs happening under the hood by tidying our recipe object.\n\ntidy(crickets_rec)\n\n# A tibble: 1 √ó 6\n  number operation type         trained skip  id                \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;             \n1      1 step      impute_lower TRUE    FALSE impute_lower_h7llz\n\ntidy(crickets_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms value id                \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;             \n1 temp     21 impute_lower_h7llz\n\n\nYou‚Äôll notice the value column in the tibble represents the minimum value within the column. Using this value, step_impute_lower() will replace these values with any value between 0 and 21. Let‚Äôs bake this recipe and verify this is the case.\n\nbaked_crickets &lt;- bake(crickets_rec, new_data = NULL)\n\nprint(baked_crickets, n = 50)\n\n# A tibble: 31 √ó 3\n   species             temp  rate\n   &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 O. exclamationis  7.84    67.9\n 2 O. exclamationis 20.1     65.1\n 3 O. exclamationis 24       77.3\n 4 O. exclamationis 24       78.7\n 5 O. exclamationis 24       79.4\n 6 O. exclamationis 24       80.4\n 7 O. exclamationis 26.2     85.8\n 8 O. exclamationis 26.2     86.6\n 9 O. exclamationis 26.2     87.5\n10 O. exclamationis 26.2     89.1\n11 O. exclamationis 28.4     98.6\n12 O. exclamationis 29      101. \n13 O. exclamationis 30.4     99.3\n14 O. exclamationis 30.4    102. \n15 O. niveus         5.51    44.3\n16 O. niveus         7.75    47.2\n17 O. niveus         6.99    47.6\n18 O. niveus         8.92    49.6\n19 O. niveus        15.2     50.3\n20 O. niveus         0.0919  51.8\n21 O. niveus        13.6     60  \n22 O. niveus        12.0     58.5\n23 O. niveus        10.0     58.9\n24 O. niveus        22.1     60.7\n25 O. niveus        23.5     69.8\n26 O. niveus        24.2     70.9\n27 O. niveus        25.9     76.2\n28 O. niveus        26.5     76.1\n29 O. niveus        26.5     77  \n30 O. niveus        26.5     77.7\n31 O. niveus        28.6     84.7\n\n\nGreat! All values of 21 have now been imputed with a value between 0 and the minimum value for the column, 21.\nWe can also create a plot to highlight what step_impute_lower() is doing. This approach is adapted from the example in the docs (?step_impute_lower()).\n\nplot(baked_crickets$temp, crickets$temp,\n  ylab = \"pre-imputation\", xlab = \"imputed\"\n)\n\n\n\n\n\n\n\n\nstep_impute_lower() is pretty straightforward in my opinion. Give it a try. That‚Äôs it for today. A short one. Tomorrow is our final step_impute_*() function, step_impute_roll().\n\n\nDay 12 - Impute values using a rolling window via step_impute_roll()\nWe‚Äôre going to round out our discussion of step_impute_*() functions by highlighting the use of step_impute_roll(). step_impute_roll() utilizes window functions to calculate missing values. At first, I had trouble understanding how window functions are utilized. However, seeing a simplified example helped me better understand what was occurring.\nLet‚Äôs get some data for today‚Äôs examples. I‚Äôm going back to our obfuscated Google Analytics data from the Google Merchandise store for today‚Äôs examples.\n\ndata_ga &lt;- \n  read_csv(\n    here(\"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  ) |&gt;\n  mutate(\n    event_date = ymd(event_date),\n    revenue = price_in_usd * quantity\n  ) \n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data_ga)\n\nRows: 9,365\nColumns: 15\n$ event_date              &lt;date&gt; 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-‚Ä¶\n$ purchase_revenue_in_usd &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 62, 62, 44, 28, 28, 36, 36, 36, 36, 92‚Ä¶\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1‚Ä¶\n$ item_name               &lt;chr&gt; \"Google Hemp Tote\", \"Android SM S/F18 Sticker Sheet\", \"Android Buo‚Ä¶\n$ item_category           &lt;chr&gt; \"Clearance\", \"Accessories\", \"Drinkware\", \"Small Goods\", \"Office\", ‚Ä¶\n$ price_in_usd            &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 7, 92, 7, 14,‚Ä¶\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 53, 1, 1, 1, 1,‚Ä¶\n$ item_revenue_in_usd     &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, ‚Ä¶\n$ shipping_tier           &lt;chr&gt; \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"F‚Ä¶\n$ payment_type            &lt;chr&gt; \"Pay with credit card\", \"Pay with credit card\", \"Pay with credit c‚Ä¶\n$ category                &lt;chr&gt; \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobil‚Ä¶\n$ country                 &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United States\"‚Ä¶\n$ region                  &lt;chr&gt; \"California\", \"California\", \"California\", \"California\", \"Californi‚Ä¶\n$ city                    &lt;chr&gt; \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San J‚Ä¶\n$ revenue                 &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, ‚Ä¶\n\nskim(data_ga)\n\n\nData summary\n\n\nName\ndata_ga\n\n\nNumber of rows\n9365\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nDate\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n385\n0\n\n\nitem_category\n164\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n109\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n96\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n289\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n434\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nevent_date\n0\n1\n2020-12-01\n2021-01-30\n2020-12-16\n61\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npurchase_revenue_in_usd\n0\n1\n101.84\n118.06\n2\n39\n72\n125\n1530\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ntransaction_id\n0\n1\n487977.44\n282873.71\n546\n249662\n479506\n724658\n999850\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nprice_in_usd\n0\n1\n19.52\n18.74\n1\n7\n14\n24\n120\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nquantity\n0\n1\n1.45\n2.77\n1\n1\n1\n1\n160\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nitem_revenue_in_usd\n0\n1\n23.26\n28.61\n1\n8\n15\n30\n704\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nrevenue\n0\n1\n23.26\n28.58\n1\n8\n15\n30\n704\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nTo make it clear what step_impute_roll() is doing, I‚Äôm going to wrangle the data to only look at total revenue for Clearance item purchases for a specific date range (2 weeks). Since this data is complete, I will introduce some missing values into the data.\n\ndata_ga &lt;- data_ga |&gt;\n  select(event_date, transaction_id, item_category, revenue) |&gt;\n  filter(item_category == \"Clearance\") |&gt;\n  group_by(event_date) |&gt;\n  summarise(total_rev = sum(revenue))  |&gt;\n  filter(\n    event_date &gt;= as_date('2020-12-14') & \n    event_date &lt;= as_date('2020-12-27')\n  )\n\n# Introduce some NAs for the example\ndata_ga$total_rev[c(1, 6, 7, 8, 14)] &lt;- NA\n\nLet‚Äôs get our recipe set up to the point of prepping it.\n\nga_rec &lt;- recipe(~., data = data_ga) |&gt;\n  update_role(event_date, new_role = \"data_ref\") |&gt;\n  step_impute_roll(total_rev, window = 3) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\ndata_ref:  1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 14 data points and 5 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Rolling imputation for: total_rev | Trained\n\ntidy(ga_rec)\n\n# A tibble: 1 √ó 6\n  number operation type        trained skip  id               \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;            \n1      1 step      impute_roll TRUE    FALSE impute_roll_6HJg3\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms     window id               \n  &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;            \n1 total_rev      3 impute_roll_6HJg3\n\n\nNot too much useful information here. Let‚Äôs move forward with bake and see the imputed values. Take note, we will still have a missing value (more on this later).\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 14 √ó 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14      324.\n 2 2020-12-15      323 \n 3 2020-12-16      324 \n 4 2020-12-17      203 \n 5 2020-12-18      215 \n 6 2020-12-19      215 \n 7 2020-12-20       NA \n 8 2020-12-21       55 \n 9 2020-12-22       55 \n10 2020-12-23      256 \n11 2020-12-24       32 \n12 2020-12-25       73 \n13 2020-12-26       19 \n14 2020-12-27       46 \n\n\nWe now have a complete data set. But, how were these imputed values calculated? If you peek at step_impute_roll()‚Äôs arguments, you‚Äôll notice it contains a statistic argument set to median. It should be pretty intuitive that we are calculating the median here, but the median of what? It‚Äôs the median of our window we set in the function, which was window = 3.\n\nargs(step_impute_roll)\n\nfunction (recipe, ..., role = NA, trained = FALSE, columns = NULL, \n    statistic = median, window = 5L, skip = FALSE, id = rand_id(\"impute_roll\")) \nNULL\n\n\nWe need to be aware of some important notes regarding our calcuation of the median here. Let‚Äôs put our baked data side-by-side with our old data, just so it‚Äôs easier to see what was imputed and how it was calculated.\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL) |&gt;\n  rename(new_rev = total_rev) |&gt;\n  left_join(\n    data_ga |&gt; rename(old_rev = total_rev)\n  )\n\nJoining with `by = join_by(event_date)`\n\n\nLet‚Äôs start with the tails, the missing values at row 1 and 14. Since these values lack a value above and below, they default to using the series 1:3 and 12:14 for imputation. When making the calculation for the window, only complete values are passed to the calculation. Take for example the missing value at row 1. The 324 imputed value comes from calculating the median between 323 and 324.\n\n# We're rounding up here for the imputed value \nmedian(c(323, 324))\n\n[1] 323.5\n\n\nThe same calculation is being done for the 14th value, which looks like this:\n\nmedian(c(73, 19))\n\n[1] 46\n\n\nThis brings up the interesting case for the imputed NA value at row 7, or the lack there of. Since all the values within the window are NA, an NA is imputed (i.e., you can‚Äôt calculate a median with no known values). To fix this, we would need to expand our window to include more values. We do this by setting window = 5 within the function.\n\nbaked_ga &lt;- recipe(~., data = data_ga) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_impute_roll(total_rev, window = 5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# Now we have complete values, since we expanded the window\nbaked_ga\n\n# A tibble: 14 √ó 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14     269  \n 2 2020-12-15     323  \n 3 2020-12-16     324  \n 4 2020-12-17     203  \n 5 2020-12-18     215  \n 6 2020-12-19     209  \n 7 2020-12-20     135  \n 8 2020-12-21     156. \n 9 2020-12-22      55  \n10 2020-12-23     256  \n11 2020-12-24      32  \n12 2020-12-25      73  \n13 2020-12-26      19  \n14 2020-12-27      52.5\n\n\nWhat about other functions to calculate values? To do this, step_impute_roll() has the statistic argument. Say instead of the median we want to calculate our imputed value using the mean. We just do the following:\n\nga_mean_rec &lt;- recipe(~., data_ga) |&gt;\n  update_role(event_date, new_role = \"ref_date\") |&gt;\n  step_impute_roll(total_rev, window = 5, statistic = mean) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_mean_rec\n\n# A tibble: 14 √ó 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14      266.\n 2 2020-12-15      323 \n 3 2020-12-16      324 \n 4 2020-12-17      203 \n 5 2020-12-18      215 \n 6 2020-12-19      209 \n 7 2020-12-20      135 \n 8 2020-12-21      156.\n 9 2020-12-22       55 \n10 2020-12-23      256 \n11 2020-12-24       32 \n12 2020-12-25       73 \n13 2020-12-26       19 \n14 2020-12-27       95 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAccording to the docs, the statistic function you use should:\n\ncontain a single argument for the data to compute the imputed value.\nreturn a double precision value.\n\nThey also note only complete values will be passed to the functions specified with the statistic argument.\n\n\nThat‚Äôs a wrap for step_imputation_*() functions. Next we‚Äôre going to focus on step functions to decorrelate predictors.\n\n\nDay 13 - Use step_corr() to remove highly correlated predictors\nStarting today, I‚Äôm going to begin highlighting step_*() functions useful for performing decorrelation preprocessing steps. These include steps to filter out highly correlated predictors, using principal component analysis, or other model-based methods.\nI‚Äôm starting out simple here by focusing on the use of step_corr(). According to the docs, step_corr() will\n\npotentially remove variables that have large absolute correations with other variables.\n\nUsing some threshold value, step_corr() will identify column combinations where a minimum number of columns are removed and all the absolute correlations between columns are below a specified threshold.\n\n\n\n\n\n\nImportant\n\n\n\nstep_corr() will potentially remove columns.\n\n\nBefore using step_corr() let‚Äôs highlight some of the important arguments of the function.\n\nthreshold - used as the absolute correlation cut off value. The function defaults to .9. This is the only tuning parameter for the function.\nmethod - the method used to make correlation calculations, defaults to pearson.\n\nLet‚Äôs get our data together. For today‚Äôs example, I‚Äôm going back to our penguins data. You can learn more about this data by running ?penguins in your console or by reviewing my past examples using this data.\n\ndata(penguins, package = \"modeldata\")\n\npenguins\n\n# A tibble: 344 √ó 7\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n 2 Adelie  Torgersen           39.5          17.4               186        3800 female\n 3 Adelie  Torgersen           40.3          18                 195        3250 female\n 4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193        3450 female\n 6 Adelie  Torgersen           39.3          20.6               190        3650 male  \n 7 Adelie  Torgersen           38.9          17.8               181        3625 female\n 8 Adelie  Torgersen           39.2          19.6               195        4675 male  \n 9 Adelie  Torgersen           34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190        4250 &lt;NA&gt;  \n# ‚Ñπ 334 more rows\n\nglimpse(penguins)\n\nRows: 344\nColumns: 7\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, ‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe‚Ä¶\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n‚ñÉ‚ñá‚ñá‚ñÜ‚ñÅ\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n‚ñÖ‚ñÖ‚ñá‚ñá‚ñÇ\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÇ\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÇ\n\n\n\n\n\nNow we create our testing training split.\n\nset.seed(20240113)\npenguins_split &lt;- initial_split(penguins, prop = .8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nQuickly, let‚Äôs use corrr::correlate() to explore correlations between variables in our training data. The code looks like this:\n\ncorrelate(penguins_tr)\n\nNon-numeric variables removed from input: `species`, `island`, and `sex`\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 4 √ó 5\n  term              bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 bill_length_mm            NA            -0.229             0.646       0.591\n2 bill_depth_mm             -0.229        NA                -0.582      -0.477\n3 flipper_length_mm          0.646        -0.582            NA           0.872\n4 body_mass_g                0.591        -0.477             0.872      NA    \n\n\nYou‚Äôll notice a highly positive correlation between flipper_length_mm and body_mass_g (.872). Although I‚Äôm not making a causal argument here, it would seem feasible to assume that as a penguin‚Äôs flippers get longer, their body mass would increase. The question now is, if we were using this data to train a model, would the inclusion of both these variables improve our model? Or, could we simplify model estimation by eliminating one of these variables? Perhaps we can achieve the same amount of accuracy while also specifying the most parsimonious model.\nHere we‚Äôll create our recipe to address these two, highly correlated variables within our data.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_corr(all_numeric_predictors(), threshold = .8) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 275 data points and 11 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Correlation filter on: flipper_length_mm | Trained\n\n\nThe prepped recipe now tells us the flipper_length_mm variable will be removed once we bake the data. This is a good thing to keep an eye on, verifying the step removed expected columns.\nWhen you drill down into the step using tidy(), you‚Äôll notice a tibble that highlights what column will be removed once we bake() the recipe.\n\ntidy(penguins_rec)\n\n# A tibble: 1 √ó 6\n  number operation type  trained skip  id        \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;     \n1      1 step      corr  TRUE    FALSE corr_q9cC1\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 √ó 2\n  terms             id        \n  &lt;chr&gt;             &lt;chr&gt;     \n1 flipper_length_mm corr_q9cC1\n\n\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 √ó 6\n   species island    bill_length_mm bill_depth_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Gentoo  Biscoe              50.5          15.9        5400 male  \n 2 Gentoo  Biscoe              NA            NA            NA &lt;NA&gt;  \n 3 Gentoo  Biscoe              50            15.3        5550 male  \n 4 Adelie  Torgersen           40.2          17          3450 female\n 5 Adelie  Biscoe              40.5          17.9        3200 female\n 6 Adelie  Torgersen           37.8          17.1        3300 &lt;NA&gt;  \n 7 Gentoo  Biscoe              48.2          14.3        4600 female\n 8 Gentoo  Biscoe              55.1          16          5850 male  \n 9 Adelie  Dream               37.2          18.1        3900 male  \n10 Gentoo  Biscoe              48.7          15.7        5350 male  \n# ‚Ñπ 265 more rows\n\n\nYou can verify this highly correlated variable is mitigated by checking out the correlations between variables in our baked data set:\n\ncorrelate(baked_penguin)\n\nNon-numeric variables removed from input: `species`, `island`, and `sex`\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 √ó 4\n  term           bill_length_mm bill_depth_mm body_mass_g\n  &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 bill_length_mm         NA            -0.229       0.591\n2 bill_depth_mm          -0.229        NA          -0.477\n3 body_mass_g             0.591        -0.477      NA    \n\n\nSo there you have it, our first step_*() function to perform decorrelation preprocessing. Tomorrow I‚Äôll be focusing on step_pca(), which will use principal components analysis to manage correlations among predictors.\n\n\nDay 14 - Perform dimension reduction with step_pca()\nFor today, I‚Äôm overviewing step_pca(). step_pca() performs dimension reduction using principal components analysis. Dimension reduction seeks to combine predictors into latent predictors, while also retaining as much information from the full data set as possible. Principal component analysis can seem like an advanced topic, but the @statquest YouTube channel has a good step-by-step video on how it‚Äôs performed, in general terms.\nWe‚Äôll use the mlc_churn data from the ‚Äòmodeldata‚Äô package for today‚Äôs examples. This data is an artificial data set representing customer churn (i.e., the loss of customers to a service). You can learn more about this data by running ?mlc_churn within your console. But here‚Äôs some basic data exploration code to get a sense of what‚Äôs included within the data.\n\ndata(mlc_churn, package = \"modeldata\")\n\nglimpse(mlc_churn)\n\nRows: 5,000\nColumns: 20\n$ state                         &lt;fct&gt; KS, OH, NJ, OH, OK, AL, MA, MO, LA, WV, IN, RI, IA, MT, IA, ‚Ä¶\n$ account_length                &lt;int&gt; 128, 107, 137, 84, 75, 118, 121, 147, 117, 141, 65, 74, 168,‚Ä¶\n$ area_code                     &lt;fct&gt; area_code_415, area_code_415, area_code_415, area_code_408, ‚Ä¶\n$ international_plan            &lt;fct&gt; no, no, no, yes, yes, yes, no, yes, no, yes, no, no, no, no,‚Ä¶\n$ voice_mail_plan               &lt;fct&gt; yes, yes, no, no, no, no, yes, no, no, yes, no, no, no, no, ‚Ä¶\n$ number_vmail_messages         &lt;int&gt; 25, 26, 0, 0, 0, 0, 24, 0, 0, 37, 0, 0, 0, 0, 0, 0, 27, 0, 3‚Ä¶\n$ total_day_minutes             &lt;dbl&gt; 265.1, 161.6, 243.4, 299.4, 166.7, 223.4, 218.2, 157.0, 184.‚Ä¶\n$ total_day_calls               &lt;int&gt; 110, 123, 114, 71, 113, 98, 88, 79, 97, 84, 137, 127, 96, 88‚Ä¶\n$ total_day_charge              &lt;dbl&gt; 45.07, 27.47, 41.38, 50.90, 28.34, 37.98, 37.09, 26.69, 31.3‚Ä¶\n$ total_eve_minutes             &lt;dbl&gt; 197.4, 195.5, 121.2, 61.9, 148.3, 220.6, 348.5, 103.1, 351.6‚Ä¶\n$ total_eve_calls               &lt;int&gt; 99, 103, 110, 88, 122, 101, 108, 94, 80, 111, 83, 148, 71, 7‚Ä¶\n$ total_eve_charge              &lt;dbl&gt; 16.78, 16.62, 10.30, 5.26, 12.61, 18.75, 29.62, 8.76, 29.89,‚Ä¶\n$ total_night_minutes           &lt;dbl&gt; 244.7, 254.4, 162.6, 196.9, 186.9, 203.9, 212.6, 211.8, 215.‚Ä¶\n$ total_night_calls             &lt;int&gt; 91, 103, 104, 89, 121, 118, 118, 96, 90, 97, 111, 94, 128, 1‚Ä¶\n$ total_night_charge            &lt;dbl&gt; 11.01, 11.45, 7.32, 8.86, 8.41, 9.18, 9.57, 9.53, 9.71, 14.6‚Ä¶\n$ total_intl_minutes            &lt;dbl&gt; 10.0, 13.7, 12.2, 6.6, 10.1, 6.3, 7.5, 7.1, 8.7, 11.2, 12.7,‚Ä¶\n$ total_intl_calls              &lt;int&gt; 3, 3, 5, 7, 3, 6, 7, 6, 4, 5, 6, 5, 2, 5, 6, 9, 4, 3, 5, 2, ‚Ä¶\n$ total_intl_charge             &lt;dbl&gt; 2.70, 3.70, 3.29, 1.78, 2.73, 1.70, 2.03, 1.92, 2.35, 3.02, ‚Ä¶\n$ number_customer_service_calls &lt;int&gt; 1, 1, 0, 2, 3, 0, 3, 0, 1, 0, 4, 0, 1, 3, 4, 4, 1, 3, 1, 1, ‚Ä¶\n$ churn                         &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no,‚Ä¶\n\nskim(mlc_churn)\n\n\nData summary\n\n\nName\nmlc_churn\n\n\nNumber of rows\n5000\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nstate\n0\n1\nFALSE\n51\nWV: 158, MN: 125, AL: 124, ID: 119\n\n\narea_code\n0\n1\nFALSE\n3\nare: 2495, are: 1259, are: 1246\n\n\ninternational_plan\n0\n1\nFALSE\n2\nno: 4527, yes: 473\n\n\nvoice_mail_plan\n0\n1\nFALSE\n2\nno: 3677, yes: 1323\n\n\nchurn\n0\n1\nFALSE\n2\nno: 4293, yes: 707\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naccount_length\n0\n1\n100.26\n39.69\n1\n73.00\n100.00\n127.00\n243.00\n‚ñÇ‚ñá‚ñá‚ñÇ‚ñÅ\n\n\nnumber_vmail_messages\n0\n1\n7.76\n13.55\n0\n0.00\n0.00\n17.00\n52.00\n‚ñá‚ñÅ‚ñÇ‚ñÅ‚ñÅ\n\n\ntotal_day_minutes\n0\n1\n180.29\n53.89\n0\n143.70\n180.10\n216.20\n351.50\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÅ\n\n\ntotal_day_calls\n0\n1\n100.03\n19.83\n0\n87.00\n100.00\n113.00\n165.00\n‚ñÅ‚ñÅ‚ñá‚ñá‚ñÅ\n\n\ntotal_day_charge\n0\n1\n30.65\n9.16\n0\n24.43\n30.62\n36.75\n59.76\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÅ\n\n\ntotal_eve_minutes\n0\n1\n200.64\n50.55\n0\n166.38\n201.00\n234.10\n363.70\n‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñÅ\n\n\ntotal_eve_calls\n0\n1\n100.19\n19.83\n0\n87.00\n100.00\n114.00\n170.00\n‚ñÅ‚ñÅ‚ñá‚ñá‚ñÅ\n\n\ntotal_eve_charge\n0\n1\n17.05\n4.30\n0\n14.14\n17.09\n19.90\n30.91\n‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñÅ\n\n\ntotal_night_minutes\n0\n1\n200.39\n50.53\n0\n166.90\n200.40\n234.70\n395.00\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\n\ntotal_night_calls\n0\n1\n99.92\n19.96\n0\n87.00\n100.00\n113.00\n175.00\n‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÅ\n\n\ntotal_night_charge\n0\n1\n9.02\n2.27\n0\n7.51\n9.02\n10.56\n17.77\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\n\ntotal_intl_minutes\n0\n1\n10.26\n2.76\n0\n8.50\n10.30\n12.00\n20.00\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\n\ntotal_intl_calls\n0\n1\n4.44\n2.46\n0\n3.00\n4.00\n6.00\n20.00\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\n\n\ntotal_intl_charge\n0\n1\n2.77\n0.75\n0\n2.30\n2.78\n3.24\n5.40\n‚ñÅ‚ñÉ‚ñá‚ñÉ‚ñÅ\n\n\nnumber_customer_service_calls\n0\n1\n1.57\n1.31\n0\n1.00\n1.00\n2.00\n9.00\n‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nLet‚Äôs create our training and testing split.\n\nset.seed(20240114)\nchurn_split &lt;- initial_split(mlc_churn, prop = .8)\n\nchurn_tr &lt;- training(churn_split)\nchurn_te &lt;- testing(churn_split)\n\nBefore we apply the step_pca() function, we need to first center and scale our data. To do this, we‚Äôll first use step_normalize() on all numeric variables within the data set. Normalizing will place all numeric data on the same scale, which is required step to complete before performing principal components analysis.\nPost center and scaling of our variables, we‚Äôll use step_pca() on all_numeric() predictors. We use the num_comp = 3 argument to specify how many components will be outputted from our principal components analysis.\nLet‚Äôs prep() and tidy() our recipe to peek under the hood to get a better sense of what‚Äôs happening with this recipe step.\n\nchurn_rec &lt;- recipe(churn ~ ., data = churn_tr) |&gt;\n  update_role(state, area_code, new_role = \"ref_var\") |&gt;\n  update_role(ends_with(\"plan\"), new_role = \"plan_var\") |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric_predictors(), num_comp = 3) |&gt;\n  prep()\n\nchurn_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 15\nplan_var:   2\nref_var:    2\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 4000 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Centering and scaling for: account_length number_vmail_messages, ... | Trained\n\n\n‚Ä¢ PCA extraction with: account_length, number_vmail_messages, total_day_minutes, ... | Trained\n\n\nIt‚Äôs also important to point out that the tidy() method for step_pca() has two type options:\n\ntype = \"coef\"\ntype = \"variance\"\n\nEach of these options modify what gets printed to the console. I‚Äôve added some comments below on what gets outputted for each.\n\ntidy(churn_rec)\n\n# A tibble: 2 √ó 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_7hokR\n2      2 step      pca       TRUE    FALSE pca_BXk23      \n\n# Output the variable loadings for each component\ntidy(churn_rec, number = 2, type = \"coef\")\n\n# A tibble: 225 √ó 4\n   terms                     value component id       \n   &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 account_length         0.0192   PC1       pca_BXk23\n 2 number_vmail_messages -0.00296  PC1       pca_BXk23\n 3 total_day_minutes      0.312    PC1       pca_BXk23\n 4 total_day_calls       -0.000110 PC1       pca_BXk23\n 5 total_day_charge       0.312    PC1       pca_BXk23\n 6 total_eve_minutes     -0.462    PC1       pca_BXk23\n 7 total_eve_calls        0.0230   PC1       pca_BXk23\n 8 total_eve_charge      -0.462    PC1       pca_BXk23\n 9 total_night_minutes    0.426    PC1       pca_BXk23\n10 total_night_calls      0.00724  PC1       pca_BXk23\n# ‚Ñπ 215 more rows\n\n# Output the variance each component accounts for\ntidy(churn_rec, number = 2, type = \"variance\")\n\n# A tibble: 60 √ó 4\n   terms    value component id       \n   &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 2.07          1 pca_BXk23\n 2 variance 2.02          2 pca_BXk23\n 3 variance 1.98          3 pca_BXk23\n 4 variance 1.94          4 pca_BXk23\n 5 variance 1.06          5 pca_BXk23\n 6 variance 1.04          6 pca_BXk23\n 7 variance 1.01          7 pca_BXk23\n 8 variance 0.988         8 pca_BXk23\n 9 variance 0.977         9 pca_BXk23\n10 variance 0.965        10 pca_BXk23\n# ‚Ñπ 50 more rows\n\n\nGreat, let‚Äôs bake our recipe. You‚Äôll notice step_pca() reduced all numeric data into our three components, PC1, PC2, and PC3.\n\nbake(churn_rec, new_data = NULL)\n\n# A tibble: 4,000 √ó 8\n   state area_code     international_plan voice_mail_plan churn    PC1     PC2    PC3\n   &lt;fct&gt; &lt;fct&gt;         &lt;fct&gt;              &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 IN    area_code_408 no                 no              no    -0.358  1.03   -1.30 \n 2 AZ    area_code_415 no                 yes             no     3.04   2.20   -1.28 \n 3 IL    area_code_415 no                 no              no     1.17  -2.36    0.697\n 4 IN    area_code_415 no                 no              no    -0.127  0.811   1.14 \n 5 FL    area_code_415 no                 no              no     0.248  2.06   -0.311\n 6 NM    area_code_510 yes                no              no    -1.28   0.0720  1.02 \n 7 DE    area_code_415 no                 no              no    -1.95   0.250  -0.553\n 8 SD    area_code_408 no                 no              no     0.624  1.36   -0.344\n 9 DE    area_code_510 no                 no              no    -1.13  -0.663  -0.804\n10 CT    area_code_415 no                 yes             no    -2.60   0.817   0.939\n# ‚Ñπ 3,990 more rows\n\n\nConstraining by component works, but step_pca() also provides a threshold argument. In other words, we can specify the total variance that should be covered by components, and step_pca() will create the required number of components based on this threshold. Let‚Äôs say we want our PCA to create a number of components to cover 70% of the variability in our variables. We can do this by using the following recipe code:\n\nchurn_rec_thresh &lt;- recipe(churn ~ ., data = churn_tr) |&gt;\n  update_role(state, area_code, new_role = \"ref_var\") |&gt;\n  update_role(ends_with(\"plan\"), new_role = \"plan_var\") |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric_predictors(), threshold = .7) |&gt;\n  prep()\n\nchurn_rec_thresh\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 15\nplan_var:   2\nref_var:    2\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 4000 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Centering and scaling for: account_length number_vmail_messages, ... | Trained\n\n\n‚Ä¢ PCA extraction with: account_length, number_vmail_messages, total_day_minutes, ... | Trained\n\n\nLet‚Äôs tidy() our recipe to see what‚Äôs happening here.\n\ntidy(churn_rec_thresh)\n\n# A tibble: 2 √ó 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_8WvPy\n2      2 step      pca       TRUE    FALSE pca_Aj4UJ      \n\n# variable loadings\ntidy(churn_rec_thresh, number = 2, type = \"coef\")\n\n# A tibble: 225 √ó 4\n   terms                     value component id       \n   &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 account_length         0.0192   PC1       pca_Aj4UJ\n 2 number_vmail_messages -0.00296  PC1       pca_Aj4UJ\n 3 total_day_minutes      0.312    PC1       pca_Aj4UJ\n 4 total_day_calls       -0.000110 PC1       pca_Aj4UJ\n 5 total_day_charge       0.312    PC1       pca_Aj4UJ\n 6 total_eve_minutes     -0.462    PC1       pca_Aj4UJ\n 7 total_eve_calls        0.0230   PC1       pca_Aj4UJ\n 8 total_eve_charge      -0.462    PC1       pca_Aj4UJ\n 9 total_night_minutes    0.426    PC1       pca_Aj4UJ\n10 total_night_calls      0.00724  PC1       pca_Aj4UJ\n# ‚Ñπ 215 more rows\n\n# variance accounted for\ntidy(churn_rec_thresh, number = 2, type = \"variance\")\n\n# A tibble: 60 √ó 4\n   terms    value component id       \n   &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 2.07          1 pca_Aj4UJ\n 2 variance 2.02          2 pca_Aj4UJ\n 3 variance 1.98          3 pca_Aj4UJ\n 4 variance 1.94          4 pca_Aj4UJ\n 5 variance 1.06          5 pca_Aj4UJ\n 6 variance 1.04          6 pca_Aj4UJ\n 7 variance 1.01          7 pca_Aj4UJ\n 8 variance 0.988         8 pca_Aj4UJ\n 9 variance 0.977         9 pca_Aj4UJ\n10 variance 0.965        10 pca_Aj4UJ\n# ‚Ñπ 50 more rows\n\n\nNow, we‚Äôll bake our churn_rec_thresh recipe.\n\nbake(churn_rec_thresh, new_data = NULL)\n\n# A tibble: 4,000 √ó 12\n   state area_code     international_plan voice_mail_plan churn    PC1     PC2    PC3    PC4    PC5\n   &lt;fct&gt; &lt;fct&gt;         &lt;fct&gt;              &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 IN    area_code_408 no                 no              no    -0.358  1.03   -1.30   0.613  0.402\n 2 AZ    area_code_415 no                 yes             no     3.04   2.20   -1.28   1.06   1.04 \n 3 IL    area_code_415 no                 no              no     1.17  -2.36    0.697  1.05   0.218\n 4 IN    area_code_415 no                 no              no    -0.127  0.811   1.14   1.14   0.177\n 5 FL    area_code_415 no                 no              no     0.248  2.06   -0.311 -2.28  -1.20 \n 6 NM    area_code_510 yes                no              no    -1.28   0.0720  1.02   1.05  -1.12 \n 7 DE    area_code_415 no                 no              no    -1.95   0.250  -0.553  1.80  -0.712\n 8 SD    area_code_408 no                 no              no     0.624  1.36   -0.344 -0.717 -1.42 \n 9 DE    area_code_510 no                 no              no    -1.13  -0.663  -0.804 -0.993 -0.159\n10 CT    area_code_415 no                 yes             no    -2.60   0.817   0.939 -1.09   2.53 \n# ‚Ñπ 3,990 more rows\n# ‚Ñπ 2 more variables: PC6 &lt;dbl&gt;, PC7 &lt;dbl&gt;\n\n\nTo meet our threshold, the step_pca() recipes step calculated and returned seven principal components, PC1 - PC7.\nstep_pca() makes it pretty easy to do dimension reduction utilizing principal components analysis. Give it a try.\nAnother day down. See you tomorrow.\n\n\nDay 15 - Use step_ica() for signal extraction\nHere we are, the halfway point üéâ.\nToday, I‚Äôm overviewing the use of step_ica(). This step is used to make transformations in signal processing. In general terms, independent component analysis (ICA) is a dimensionality reduction technique that attempts to isolate signal from noise.\nBefore reviewing this step, I had to do some research on what ICA is and how it is used. Here are a few sources I found useful to gain an intuitive sense of this step:\n\nMaking sense of independent component analysis Cross Validated post.\nIntroduction to ICA: Independent Component Analysis by Jonas Dieckmann\nIndependent Component Analysis (ICA) and Automated Component Labeling ‚Äî EEG Example by Bartek Kulas\n\nAcross many of these sources, two examples are commonly presented to more clearly explain the purpose of ICA. First, the cocktail party problem, where we can isolate individual conversations among many during a party. The second comes from audio recording. Take for example a situation where you have multiple microphones. These microphones are being used to capture the sound of several audio sources (e.g., various instruments), and you‚Äôre attempting to isolate the sound from a single source. Both are situations where you‚Äôre trying to isolate the signal from surrounding noise. Indeed, these are simplified examples. The linked sources above provide more sophisticated explanations.\nBefore we use step_ica(), we‚Äôll need some data. I‚Äôm using a portion of the data from the Independent Component Analysis (ICA) and Automated Component Labeling ‚Äî EEG Example. This data comes from the use of Electroencephalography (EEG). It was collected to examine EEG correlates of a genetic predisposition to alcoholism, and it was made available via Kaggle.\n\n\n\n\n\n\nWarning\n\n\n\nI am not an EEG professional, and I rarely work with this type of data. The approach I highlight here may not be best practice.\n\n\nThis data represents EEG sensor measurements of electrical activity during an experimental session for one participant. There are multiple sensors, which collect data intended to observe six known brain wave patterns (again, I‚Äôm not an expert here, so this is certainly a more complex topic then I‚Äôm making it out to be here). Let‚Äôs use step_ica() to see if we can transform this data into six different signals.\n\ndata_eeg &lt;-\n  read_csv(\n    here(\n      \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_eeg.csv\"\n    )\n  ) |&gt;\n  clean_names() |&gt;\n  rename(id = x1)\n\nNew names:\nRows: 16384 Columns: 10\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" chr\n(4): sensor position, subject identifier, matching condition, name dbl (6): ...1, trial number,\nsample num, sensor value, channel, time\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ Specify the column types\nor set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...1`\n\nglimpse(data_eeg)\n\nRows: 16,384\nColumns: 10\n$ id                 &lt;dbl&gt; 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, ‚Ä¶\n$ trial_number       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ sensor_position    &lt;chr&gt; \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"‚Ä¶\n$ sample_num         &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2‚Ä¶\n$ sensor_value       &lt;dbl&gt; -8.921, -8.433, -2.574, 5.239, 11.587, 14.028, 11.587, 6.704, 1.821, -1‚Ä¶\n$ subject_identifier &lt;chr&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"‚Ä¶\n$ matching_condition &lt;chr&gt; \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"‚Ä¶\n$ channel            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ name               &lt;chr&gt; \"co2a0000364\", \"co2a0000364\", \"co2a0000364\", \"co2a0000364\", \"co2a000036‚Ä¶\n$ time               &lt;dbl&gt; 0.00000000, 0.00390625, 0.00781250, 0.01171875, 0.01562500, 0.01953125,‚Ä¶\n\nskim(data_eeg)\n\n\nData summary\n\n\nName\ndata_eeg\n\n\nNumber of rows\n16384\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsensor_position\n0\n1\n1\n3\n0\n64\n0\n\n\nsubject_identifier\n0\n1\n1\n1\n0\n1\n0\n\n\nmatching_condition\n0\n1\n6\n6\n0\n1\n0\n\n\nname\n0\n1\n11\n11\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n8228.00\n4748.27\n5.00\n4116.50\n8228.00\n12339.50\n16451.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\ntrial_number\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.0\n‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ\n\n\nsample_num\n0\n1\n127.50\n73.90\n0.00\n63.75\n127.50\n191.25\n255.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nsensor_value\n0\n1\n1.99\n7.51\n-39.83\n-2.23\n1.22\n5.40\n51.9\n‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñÅ\n\n\nchannel\n0\n1\n31.50\n18.47\n0.00\n15.75\n31.50\n47.25\n63.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\ntime\n0\n1\n0.50\n0.29\n0.00\n0.25\n0.50\n0.75\n1.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\n\n\n\nI‚Äôll have to do some data wrangling here to work with the data first, though. Specifically, I need to go from long to wide data. This will give each EEG sensor measurement its own column.\n\ndata_eeg &lt;- data_eeg |&gt;\n  select(sensor_position, sensor_value, time) |&gt;\n  pivot_wider(\n    names_from = sensor_position,\n    values_from = sensor_value\n  )\n\ndata_eeg\n\n# A tibble: 256 √ó 65\n      time   FP1    FP2      F7     F8    AF1    AF2     FZ    F4     F3   FC6    FC5    FC2    FC1\n     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 0       -8.92  0.834 -19.8    8.15  -2.15   1.13  -0.071 3.41  -0.092 4.83  -2.43   0.488  0.824\n 2 0.00391 -8.43  3.28  -12.5    1.80  -2.15   0.641 -0.559 1.46   0.397 6.30  -4.38  -0.977  0.824\n 3 0.00781 -2.57  5.72    1.15  -2.59  -1.66  -0.336 -1.05  0.478 -1.07  5.81  -5.36  -1.46   0.336\n 4 0.0117   5.24  7.67   14.8   -4.55  -0.682 -0.824 -0.559 0.966 -3.51  3.37  -5.36   0     -0.641\n 5 0.0156  11.6   9.62   20.7   -5.04   2.25   0.641  0.905 1.94  -5.46  1.41  -0.966  1.95  -1.13 \n 6 0.0195  14.0   9.62   17.3   -5.52   5.18   3.57   2.37  2.92  -4.49  0.437  6.85   3.42  -1.62 \n 7 0.0234  11.6   8.65    8.96  -4.55   6.64   6.01   3.84  1.94  -1.07  0.926 13.7    3.42  -1.13 \n 8 0.0273   6.70  5.23    0.173 -0.641  5.18   6.99   4.32  0.478  3.33  1.90  15.6    1.95   0.824\n 9 0.0312   1.82  1.32   -3.73   5.71   1.76   5.52   3.35  0.478  6.74  1.90  11.7    0.977  3.75 \n10 0.0352  -1.11 -2.10   -2.27  10.6   -1.66   2.59   1.88  1.94   7.23  1.90   3.92   0.977  5.22 \n# ‚Ñπ 246 more rows\n# ‚Ñπ 51 more variables: T8 &lt;dbl&gt;, T7 &lt;dbl&gt;, CZ &lt;dbl&gt;, C3 &lt;dbl&gt;, C4 &lt;dbl&gt;, CP5 &lt;dbl&gt;, CP6 &lt;dbl&gt;,\n#   CP1 &lt;dbl&gt;, CP2 &lt;dbl&gt;, P3 &lt;dbl&gt;, P4 &lt;dbl&gt;, PZ &lt;dbl&gt;, P8 &lt;dbl&gt;, P7 &lt;dbl&gt;, PO2 &lt;dbl&gt;, PO1 &lt;dbl&gt;,\n#   O2 &lt;dbl&gt;, O1 &lt;dbl&gt;, X &lt;dbl&gt;, AF7 &lt;dbl&gt;, AF8 &lt;dbl&gt;, F5 &lt;dbl&gt;, F6 &lt;dbl&gt;, FT7 &lt;dbl&gt;, FT8 &lt;dbl&gt;,\n#   FPZ &lt;dbl&gt;, FC4 &lt;dbl&gt;, FC3 &lt;dbl&gt;, C6 &lt;dbl&gt;, C5 &lt;dbl&gt;, F2 &lt;dbl&gt;, F1 &lt;dbl&gt;, TP8 &lt;dbl&gt;, TP7 &lt;dbl&gt;,\n#   AFZ &lt;dbl&gt;, CP3 &lt;dbl&gt;, CP4 &lt;dbl&gt;, P5 &lt;dbl&gt;, P6 &lt;dbl&gt;, C1 &lt;dbl&gt;, C2 &lt;dbl&gt;, PO7 &lt;dbl&gt;, PO8 &lt;dbl&gt;,\n#   FCZ &lt;dbl&gt;, POZ &lt;dbl&gt;, OZ &lt;dbl&gt;, P2 &lt;dbl&gt;, P1 &lt;dbl&gt;, CPZ &lt;dbl&gt;, nd &lt;dbl&gt;, Y &lt;dbl&gt;\n\n\nIn addition, step_ica() requires a few packages to be installed. This includes dimRed and fastICA. If these are not installed beforehand, an error will be returned.\nNow we create our recipe. Just like principal components analysis, we need to center and scale our data. We achieve this by applying step_center() and step_scale() to all_numeric() variables. Since we‚Äôre attempting to create columns to represent the six brain waves, we‚Äôll set num_comp to 6.\n\neeg_rec &lt;- recipe(~., data = data_eeg) |&gt;\n  update_role(time, new_role = \"time_ref\") |&gt;\n  step_center(all_numeric()) |&gt;\n  step_scale(all_numeric()) |&gt;\n  step_ica(all_numeric(), num_comp = 6) |&gt;\n  prep()\n\nLet‚Äôs tidy() our recipe to get a better sense of what‚Äôs happening here. You‚Äôll notice it‚Äôs very similar to principal components analysis.\n\ntidy(eeg_rec, number = 3)\n\n# A tibble: 390 √ó 4\n   terms component     value id       \n   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    \n 1 AF1   IC1       -0.0446   ica_xymfX\n 2 AF1   IC2        0.0123   ica_xymfX\n 3 AF1   IC3        0.00928  ica_xymfX\n 4 AF1   IC4        0.000524 ica_xymfX\n 5 AF1   IC5       -0.0280   ica_xymfX\n 6 AF1   IC6       -0.0415   ica_xymfX\n 7 AF2   IC1       -0.0489   ica_xymfX\n 8 AF2   IC2        0.0667   ica_xymfX\n 9 AF2   IC3       -0.00168  ica_xymfX\n10 AF2   IC4       -0.0385   ica_xymfX\n# ‚Ñπ 380 more rows\n\n\nNow let‚Äôs bake our data and see what the result of the step_ica() function.\n\nbaked_eeg &lt;- bake(eeg_rec, new_data = NULL) |&gt;\n  bind_cols(data_eeg |&gt; select(time))\n\nFor the heck of it, let‚Äôs plot our baked data. Do you see any of the common brain wave types in these plots? Remember, this is only one participant, so if additional data were included, the patterns might become more apparent.\n\nwalk(\n  baked_eeg |&gt; select(-time),\n  ~ plot(baked_eeg$time, .x, type = \"line\")\n)\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nThere you have it, another step_*() function to try out. See you tomorrow.\n\n\nDay 16 - Use step_other() to collapse low occurring categorical levels into other\nFor today, I‚Äôm going to highlight the use of step_other(). step_other() is used to collapse infrequent categorical values into an other category. To do this, the function has a threshold argument to modify the cutoff for determining values within this category. Let‚Äôs highlight its use with an example.\nFor data, I‚Äôm going back to our Google Analytics data. I‚Äôve used this data for several examples already, so I‚Äôm not going to detail it too much here. However, here is some quick data exploration code for you to review.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data_ga)\n\nRows: 9,365\nColumns: 14\n$ event_date              &lt;dbl&gt; 20201201, 20201201, 20201201, 20201201, 20201201, 20201201, 202012‚Ä¶\n$ purchase_revenue_in_usd &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 62, 62, 44, 28, 28, 36, 36, 36, 36, 92‚Ä¶\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1‚Ä¶\n$ item_name               &lt;chr&gt; \"Google Hemp Tote\", \"Android SM S/F18 Sticker Sheet\", \"Android Buo‚Ä¶\n$ item_category           &lt;chr&gt; \"Clearance\", \"Accessories\", \"Drinkware\", \"Small Goods\", \"Office\", ‚Ä¶\n$ price_in_usd            &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 7, 92, 7, 14,‚Ä¶\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 53, 1, 1, 1, 1,‚Ä¶\n$ item_revenue_in_usd     &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, ‚Ä¶\n$ shipping_tier           &lt;chr&gt; \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"F‚Ä¶\n$ payment_type            &lt;chr&gt; \"Pay with credit card\", \"Pay with credit card\", \"Pay with credit c‚Ä¶\n$ category                &lt;chr&gt; \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobil‚Ä¶\n$ country                 &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United States\"‚Ä¶\n$ region                  &lt;chr&gt; \"California\", \"California\", \"California\", \"California\", \"Californi‚Ä¶\n$ city                    &lt;chr&gt; \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San J‚Ä¶\n\nskim(data_ga)\n\n\nData summary\n\n\nName\ndata_ga\n\n\nNumber of rows\n9365\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n385\n0\n\n\nitem_category\n164\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n109\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n96\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n289\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n434\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1\n20203675.03\n3983.09\n20201201\n20201209\n20201216\n20210106\n20210130\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\npurchase_revenue_in_usd\n0\n1\n101.84\n118.06\n2\n39\n72\n125\n1530\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ntransaction_id\n0\n1\n487977.44\n282873.71\n546\n249662\n479506\n724658\n999850\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nprice_in_usd\n0\n1\n19.52\n18.74\n1\n7\n14\n24\n120\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nquantity\n0\n1\n1.45\n2.77\n1\n1\n1\n1\n160\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nitem_revenue_in_usd\n0\n1\n23.26\n28.61\n1\n8\n15\n30\n704\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nFirst, let‚Äôs create a split.\n\nsplit_ga &lt;- initial_split(data_ga, prop = .8)\n\ndata_ga_tr &lt;- training(split_ga)\ndata_ga_te &lt;- testing(split_ga)\n\nI‚Äôm interested in the item_category variable here. Let‚Äôs get a sense of the unique values and counts of each value within the data.\n\ndata_ga_tr |&gt;\n  count(item_category, sort = TRUE) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  print(n = 25)\n\n# A tibble: 21 √ó 3\n   item_category               n     prop\n   &lt;chr&gt;                   &lt;int&gt;    &lt;dbl&gt;\n 1 Apparel                  2199 0.294   \n 2 Campus Collection         755 0.101   \n 3 New                       724 0.0966  \n 4 Accessories               632 0.0844  \n 5 Shop by Brand             463 0.0618  \n 6 Office                    408 0.0545  \n 7 Bags                      372 0.0497  \n 8 Drinkware                 343 0.0458  \n 9 Clearance                 336 0.0448  \n10 Lifestyle                 263 0.0351  \n11 Uncategorized Items       242 0.0323  \n12 Google                    148 0.0198  \n13 Stationery                146 0.0195  \n14 &lt;NA&gt;                      140 0.0187  \n15 Writing Instruments       121 0.0162  \n16 Small Goods               106 0.0141  \n17 Gift Cards                 48 0.00641 \n18 Electronics Accessories    20 0.00267 \n19 Notebooks & Journals       16 0.00214 \n20 Fun                         9 0.00120 \n21 Black Lives Matter          1 0.000133\n\n\nIndeed, there are certainly some item categories that are purchased less than 5% of the time in our training data. Let‚Äôs create our recipe, where we focus on using step_other() to create an other category.\n\n# threshold defaults to .05\nga_rec &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 230 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Collapsing factor levels for: item_category | Trained\n\n\nWe‚Äôll now tidy() our recipe to drill down and get more information on what‚Äôs happening. When we look deeper into the first step, a tibble describing what columns will be affected by the step and what variables will not be converted into an other category is returned. In our case, Accessories, Apparel, Bags, Campus Collection, New, Office, and Shop by Brand will remain as factor levels. All others will be converted to the other category.\n\ntidy(ga_rec)\n\n# A tibble: 1 √ó 6\n  number operation type  trained skip  id         \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;      \n1      1 step      other TRUE    FALSE other_MsyPy\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 7 √ó 3\n  terms         retained          id         \n  &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt;      \n1 item_category Accessories       other_MsyPy\n2 item_category Apparel           other_MsyPy\n3 item_category Bags              other_MsyPy\n4 item_category Campus Collection other_MsyPy\n5 item_category New               other_MsyPy\n6 item_category Office            other_MsyPy\n7 item_category Shop by Brand     other_MsyPy\n\n\nWe can bake to see what happens as a result.\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 7,492 √ó 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic‚Ä¶ New                      4        1\n 2   20201208                      63         776080 Android SM‚Ä¶ Accessories              2        2\n 3   20201209                     163         517829 Womens Goo‚Ä¶ Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An‚Ä¶ Accessories             13       15\n 5   20201210                     275         104071 Gift Card ‚Ä¶ other                   25        1\n 6   20210113                      18         865692 Google Met‚Ä¶ Office                   6        1\n 7   20201203                     311         652984 Android Bu‚Ä¶ other                    4        5\n 8   20210120                      84         482111 Google See‚Ä¶ other                   10        1\n 9   20201202                      45         541131 Google Chr‚Ä¶ Accessories             30        1\n10   20201222                      54         125445 Google Tot‚Ä¶ New                     19        1\n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nbaked_ga |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 9 √ó 2\n  item_category         n\n  &lt;fct&gt;             &lt;int&gt;\n1 Apparel            2199\n2 other              1799\n3 Campus Collection   755\n4 New                 724\n5 Accessories         632\n6 Shop by Brand       463\n7 Office              408\n8 Bags                372\n9 &lt;NA&gt;                140\n\n\nSay we want to modify the threshold to be 10%. All we need to do is pass this value to the function via the threshold argument in step_other(). This looks like this:\n\nga_rec_thresh &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category, threshold = .1) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec_thresh\n\n# A tibble: 7,492 √ó 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic‚Ä¶ other                    4        1\n 2   20201208                      63         776080 Android SM‚Ä¶ other                    2        2\n 3   20201209                     163         517829 Womens Goo‚Ä¶ Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An‚Ä¶ other                   13       15\n 5   20201210                     275         104071 Gift Card ‚Ä¶ other                   25        1\n 6   20210113                      18         865692 Google Met‚Ä¶ other                    6        1\n 7   20201203                     311         652984 Android Bu‚Ä¶ other                    4        5\n 8   20210120                      84         482111 Google See‚Ä¶ other                   10        1\n 9   20201202                      45         541131 Google Chr‚Ä¶ other                   30        1\n10   20201222                      54         125445 Google Tot‚Ä¶ other                   19        1\n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nga_rec_thresh |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 4 √ó 2\n  item_category         n\n  &lt;fct&gt;             &lt;int&gt;\n1 other              4398\n2 Apparel            2199\n3 Campus Collection   755\n4 &lt;NA&gt;                140\n\n\nThis might be too restrictive. You can also pass an integer to threshold to use a frequency as a cutoff. This looks like:\n\nga_rec_thresh &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category, threshold = 300) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec_thresh\n\n# A tibble: 7,492 √ó 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic‚Ä¶ New                      4        1\n 2   20201208                      63         776080 Android SM‚Ä¶ Accessories              2        2\n 3   20201209                     163         517829 Womens Goo‚Ä¶ Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An‚Ä¶ Accessories             13       15\n 5   20201210                     275         104071 Gift Card ‚Ä¶ other                   25        1\n 6   20210113                      18         865692 Google Met‚Ä¶ Office                   6        1\n 7   20201203                     311         652984 Android Bu‚Ä¶ Drinkware                4        5\n 8   20210120                      84         482111 Google See‚Ä¶ other                   10        1\n 9   20201202                      45         541131 Google Chr‚Ä¶ Accessories             30        1\n10   20201222                      54         125445 Google Tot‚Ä¶ New                     19        1\n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nga_rec_thresh |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 11 √ó 2\n   item_category         n\n   &lt;fct&gt;             &lt;int&gt;\n 1 Apparel            2199\n 2 other              1120\n 3 Campus Collection   755\n 4 New                 724\n 5 Accessories         632\n 6 Shop by Brand       463\n 7 Office              408\n 8 Bags                372\n 9 Drinkware           343\n10 Clearance           336\n11 &lt;NA&gt;                140\n\n\nIf you don‚Äôt like the other category label, you can modify it by passing a string to the otherargument.\n\nga_rec_thresh &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category, threshold = 300, other = \"Other items\") |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec_thresh\n\n# A tibble: 7,492 √ó 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic‚Ä¶ New                      4        1\n 2   20201208                      63         776080 Android SM‚Ä¶ Accessories              2        2\n 3   20201209                     163         517829 Womens Goo‚Ä¶ Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An‚Ä¶ Accessories             13       15\n 5   20201210                     275         104071 Gift Card ‚Ä¶ Other items             25        1\n 6   20210113                      18         865692 Google Met‚Ä¶ Office                   6        1\n 7   20201203                     311         652984 Android Bu‚Ä¶ Drinkware                4        5\n 8   20210120                      84         482111 Google See‚Ä¶ Other items             10        1\n 9   20201202                      45         541131 Google Chr‚Ä¶ Accessories             30        1\n10   20201222                      54         125445 Google Tot‚Ä¶ New                     19        1\n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nga_rec_thresh |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 11 √ó 2\n   item_category         n\n   &lt;fct&gt;             &lt;int&gt;\n 1 Apparel            2199\n 2 Other items        1120\n 3 Campus Collection   755\n 4 New                 724\n 5 Accessories         632\n 6 Shop by Brand       463\n 7 Office              408\n 8 Bags                372\n 9 Drinkware           343\n10 Clearance           336\n11 &lt;NA&gt;                140\n\n\nCheck mark next to another day. step_other() is pretty useful in cases where you have many factor levels in a categorical variable. Take it for a test drive. See you tomorrow.\n\n\nDay 17 - Convert date data into factor or numeric variables with step_date()\nDo you ever work with date variables? Do these tasks often require you to parse date variables into various components (e.g., month, year, day of the week, etc.)? recipes‚Äô step_date() simplifies these operations.\nLet‚Äôs continue using our Google Analytics ecommerce data from yesterday. I‚Äôve overviewed this data a few times now, so I‚Äôm not going to discuss it in depth here. Rather, I‚Äôm going to jump right into today‚Äôs examples.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240117)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nWithin this data is an event_date column. Let‚Äôs say I want to create two new columns from this data: month and year. First, we need to utilize step_mutate() to convert event_date to type date (i.e., 20201202 to 2020-12-02). lubridate‚Äôs ymd() function is used to do this. We do this by doing the following:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_date(event_date, features = c(\"month\", \"year\")) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: ~ymd(event_date) | Trained\n\n\n‚Ä¢ Date features from: event_date | Trained\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms      value           id          \n  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;       \n1 event_date ymd(event_date) mutate_1gITK\n\n\nLet‚Äôs bake our recipe and take a look at what‚Äôs happening. You should notice two new variables are created, each prefixed with the original variable name event_date_. The data now contains event_date_month and event_date_year.\n\nbake(ga_rec, new_data = NULL) |&gt;\n  relocate(starts_with(\"event_\"), .after = 1)\n\n# A tibble: 7,492 √ó 16\n   event_date event_date_month event_date_year purchase_revenue_in_usd transaction_id item_name     \n   &lt;date&gt;     &lt;fct&gt;                      &lt;int&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;         \n 1 2020-12-02 Dec                         2020                      41          11395 Google Navy S‚Ä¶\n 2 2020-12-07 Dec                         2020                      22         190059 Google Super ‚Ä¶\n 3 2020-12-08 Dec                         2020                      29         147010 Google SF Cam‚Ä¶\n 4 2021-01-20 Jan                         2021                      86         101328 Google Speckl‚Ä¶\n 5 2020-12-04 Dec                         2020                     142         469624 Google Men's ‚Ä¶\n 6 2020-12-28 Dec                         2020                      79         797936 Google Kirkla‚Ä¶\n 7 2020-12-28 Dec                         2020                      79         797936 Google Kirkla‚Ä¶\n 8 2020-12-17 Dec                         2020                     139         439292 Google Speckl‚Ä¶\n 9 2021-01-24 Jan                         2021                      55         172750 Google Black ‚Ä¶\n10 2020-12-18 Dec                         2020                     128         878199 #IamRemarkabl‚Ä¶\n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 10 more variables: item_category &lt;fct&gt;, price_in_usd &lt;dbl&gt;, quantity &lt;dbl&gt;,\n#   item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;, category &lt;fct&gt;,\n#   country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\nSay we don‚Äôt like the use of the abbreviation for event_date_month, we can change this by setting the abbr argument to FALSE in our recipe.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_date(event_date, features = c(\"month\", \"year\"), abbr = FALSE) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: ~ymd(event_date) | Trained\n\n\n‚Ä¢ Date features from: event_date | Trained\n\nbake(ga_rec, new_data = NULL) |&gt;\n  relocate(starts_with(\"event_\"), .after = 1)\n\n# A tibble: 7,492 √ó 16\n   event_date event_date_month event_date_year purchase_revenue_in_usd transaction_id item_name     \n   &lt;date&gt;     &lt;fct&gt;                      &lt;int&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;         \n 1 2020-12-02 December                    2020                      41          11395 Google Navy S‚Ä¶\n 2 2020-12-07 December                    2020                      22         190059 Google Super ‚Ä¶\n 3 2020-12-08 December                    2020                      29         147010 Google SF Cam‚Ä¶\n 4 2021-01-20 January                     2021                      86         101328 Google Speckl‚Ä¶\n 5 2020-12-04 December                    2020                     142         469624 Google Men's ‚Ä¶\n 6 2020-12-28 December                    2020                      79         797936 Google Kirkla‚Ä¶\n 7 2020-12-28 December                    2020                      79         797936 Google Kirkla‚Ä¶\n 8 2020-12-17 December                    2020                     139         439292 Google Speckl‚Ä¶\n 9 2021-01-24 January                     2021                      55         172750 Google Black ‚Ä¶\n10 2020-12-18 December                    2020                     128         878199 #IamRemarkabl‚Ä¶\n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 10 more variables: item_category &lt;fct&gt;, price_in_usd &lt;dbl&gt;, quantity &lt;dbl&gt;,\n#   item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;, category &lt;fct&gt;,\n#   country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\nAlso of note is step_date() converted the event_date_month column into an ordered factor for us.\n\nbake(ga_rec, new_data = NULL) |&gt;\n  pull(event_date_month) |&gt;\n  levels()\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"      \"July\"      \"August\"   \n [9] \"September\" \"October\"   \"November\"  \"December\" \n\n\nstep_date()‚Äôs feature argument has many different options. This includes:\n\nmonth\ndow (day of week)\ndoy (day of year)\nweek\nmonth\ndecimal (decimal date)\nquarter\nsemester\nyear\n\nJust for the heck of it, let‚Äôs create features using all of the available options.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_date(\n    event_date,\n    features = c(\n      \"month\",\n      \"dow\",\n      \"doy\",\n      \"week\",\n      \"decimal\",\n      \"quarter\",\n      \"semester\",\n      \"year\"\n    )\n  ) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: ~ymd(event_date) | Trained\n\n\n‚Ä¢ Date features from: event_date | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL) |&gt;\n  relocate(starts_with(\"event_\"), .after = 1)\n\nbaked_ga\n\n# A tibble: 7,492 √ó 22\n   event_date event_date_month event_date_dow event_date_doy event_date_week event_date_decimal\n   &lt;date&gt;     &lt;fct&gt;            &lt;fct&gt;                   &lt;int&gt;           &lt;int&gt;              &lt;dbl&gt;\n 1 2020-12-02 Dec              Wed                       337              49              2021.\n 2 2020-12-07 Dec              Mon                       342              49              2021.\n 3 2020-12-08 Dec              Tue                       343              49              2021.\n 4 2021-01-20 Jan              Wed                        20               3              2021.\n 5 2020-12-04 Dec              Fri                       339              49              2021.\n 6 2020-12-28 Dec              Mon                       363              52              2021.\n 7 2020-12-28 Dec              Mon                       363              52              2021.\n 8 2020-12-17 Dec              Thu                       352              51              2021.\n 9 2021-01-24 Jan              Sun                        24               4              2021.\n10 2020-12-18 Dec              Fri                       353              51              2021.\n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 16 more variables: event_date_quarter &lt;int&gt;, event_date_semester &lt;int&gt;, event_date_year &lt;int&gt;,\n#   purchase_revenue_in_usd &lt;dbl&gt;, transaction_id &lt;dbl&gt;, item_name &lt;fct&gt;, item_category &lt;fct&gt;,\n#   price_in_usd &lt;dbl&gt;, quantity &lt;dbl&gt;, item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;,\n#   payment_type &lt;fct&gt;, category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike other step_*() functions, step_date does not remove the original column. If needed you can modify this with the keep_original_columns argument.\n\n\n\nglimpse(baked_ga)\n\nRows: 7,492\nColumns: 22\n$ event_date              &lt;date&gt; 2020-12-02, 2020-12-07, 2020-12-08, 2021-01-20, 2020-12-04, 2020-‚Ä¶\n$ event_date_month        &lt;fct&gt; Dec, Dec, Dec, Jan, Dec, Dec, Dec, Dec, Jan, Dec, Dec, Dec, Jan, D‚Ä¶\n$ event_date_dow          &lt;fct&gt; Wed, Mon, Tue, Wed, Fri, Mon, Mon, Thu, Sun, Fri, Thu, Thu, Fri, T‚Ä¶\n$ event_date_doy          &lt;int&gt; 337, 342, 343, 20, 339, 363, 363, 352, 24, 353, 345, 352, 22, 352,‚Ä¶\n$ event_date_week         &lt;int&gt; 49, 49, 49, 3, 49, 52, 52, 51, 4, 51, 50, 51, 4, 51, 50, 49, 51, 5‚Ä¶\n$ event_date_decimal      &lt;dbl&gt; 2020.918, 2020.932, 2020.934, 2021.052, 2020.923, 2020.989, 2020.9‚Ä¶\n$ event_date_quarter      &lt;int&gt; 4, 4, 4, 1, 4, 4, 4, 4, 1, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 1, ‚Ä¶\n$ event_date_semester     &lt;int&gt; 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, ‚Ä¶\n$ event_date_year         &lt;int&gt; 2020, 2020, 2020, 2021, 2020, 2020, 2020, 2020, 2021, 2020, 2020, ‚Ä¶\n$ purchase_revenue_in_usd &lt;dbl&gt; 41, 22, 29, 86, 142, 79, 79, 139, 55, 128, 23, 41, 20, 15, 223, 18‚Ä¶\n$ transaction_id          &lt;dbl&gt; 11395, 190059, 147010, 101328, 469624, 797936, 797936, 439292, 172‚Ä¶\n$ item_name               &lt;fct&gt; Google Navy Speckled Tee, Google Super G Tumbler (Red Lid), Google‚Ä¶\n$ item_category           &lt;fct&gt; Apparel, Lifestyle, Campus Collection, New, Apparel, Clearance, Cl‚Ä¶\n$ price_in_usd            &lt;dbl&gt; 24, 22, 6, 16, 71, 14, 14, 16, 55, 10, 16, 11, 10, 14, 3, 71, 11, ‚Ä¶\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, ‚Ä¶\n$ item_revenue_in_usd     &lt;dbl&gt; 24, 22, 6, 16, 71, 14, 14, 16, 55, 19, 16, 11, 10, 14, 6, 71, 11, ‚Ä¶\n$ shipping_tier           &lt;fct&gt; FedEx Ground, FedEx Ground, UPS Ground, FedEx Ground, FedEx Ground‚Ä¶\n$ payment_type            &lt;fct&gt; Pay with credit card, Pay with credit card, Pay with credit card, ‚Ä¶\n$ category                &lt;fct&gt; desktop, desktop, mobile, desktop, desktop, desktop, desktop, mobi‚Ä¶\n$ country                 &lt;fct&gt; France, United States, Thailand, United Kingdom, United States, Un‚Ä¶\n$ region                  &lt;fct&gt; Nouvelle-Aquitaine, Missouri, Bangkok, England, Washington, Califo‚Ä¶\n$ city                    &lt;fct&gt; (not set), (not set), Bangkok, (not set), (not set), (not set), (n‚Ä¶\n\n\nPretty neat! step_date() provides some pretty good utility, especially if you tend to work with a lot of dates needing to be transformed into different representations. We‚Äôll see you tomorrow everybody.\n\n\nDay 18 - Create a lagged variable using step_lag()\nWelcome back, fellow readers! Another day, another step_*() function.\nCurrently, I tend to work with timeseries data. As such, I need to create lagged variables from time-to-time. recipes‚Äô step_lag() function is handy in these situations. Let‚Äôs highlight an example using this function. But first, we need some data.\nOnce again, I‚Äôm going to use the obfuscated Google Analytics ecommerce data. You can read more about this data in my previous posts. I‚Äôm not going to detail it much here, other than importing it, doing a little wrangling to calculate total revenue and total number of items purchased, and creating a training testing split. Here‚Äôs all the code to do this:\n\ndata_ga &lt;-\n  read_csv(here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  ))\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    items_purchased = sum(quantity),\n    revenue = sum(item_revenue_in_usd)\n  )\n\n\nset.seed(20240118)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nLet‚Äôs create our recipe, prep, and bake it. The tidy() method doesn‚Äôt provide too much useful information about our step, other than what columns lagged columns will be created from, so no use in looking at it here. The important argument is lag. This argument is where we specify how much we want to lag our variable by.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1) |&gt;\n  prep()\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 48 √ó 5\n   event_date items_purchased revenue lag_1_items_purchased lag_1_revenue\n   &lt;date&gt;               &lt;dbl&gt;   &lt;dbl&gt;                 &lt;dbl&gt;         &lt;dbl&gt;\n 1 2020-12-28             187    2097                    NA            NA\n 2 2021-01-27              51     457                   187          2097\n 3 2020-12-30             317    1606                    51           457\n 4 2020-12-18             398    6617                   317          1606\n 5 2021-01-09              68    1362                   398          6617\n 6 2021-01-24             133    2406                    68          1362\n 7 2020-12-06             140    2328                   133          2406\n 8 2020-12-14             527    8498                   140          2328\n 9 2021-01-04              63     835                   527          8498\n10 2020-12-01             356    6260                    63           835\n# ‚Ñπ 38 more rows\n\n\nThe lag argument also makes it convenient to create multiple lagged variables in one step. We just need to pass along a vector of positive vectors (e.g., 1:3) to the lag argument. This looks like this:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1:3) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: ~ymd(event_date) | Trained\n\n\n‚Ä¢ Lagging: items_purchased revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date            &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-01‚Ä¶\n$ items_purchased       &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156, ‚Ä¶\n$ revenue               &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551,‚Ä¶\n$ lag_1_items_purchased &lt;dbl&gt; NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 1‚Ä¶\n$ lag_2_items_purchased &lt;dbl&gt; NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 6‚Ä¶\n$ lag_3_items_purchased &lt;dbl&gt; NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 2‚Ä¶\n$ lag_1_revenue         &lt;dbl&gt; NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, ‚Ä¶\n$ lag_2_revenue         &lt;dbl&gt; NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 12‚Ä¶\n$ lag_3_revenue         &lt;dbl&gt; NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260‚Ä¶\n\n\nSay we want a variable lagged by 1, 3, and 5 values. We can do this by doing the following:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = c(1, 3, 5)) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: ~ymd(event_date) | Trained\n\n\n‚Ä¢ Lagging: items_purchased revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date            &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-01‚Ä¶\n$ items_purchased       &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156, ‚Ä¶\n$ revenue               &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551,‚Ä¶\n$ lag_1_items_purchased &lt;dbl&gt; NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 1‚Ä¶\n$ lag_3_items_purchased &lt;dbl&gt; NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 2‚Ä¶\n$ lag_5_items_purchased &lt;dbl&gt; NA, NA, NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 9‚Ä¶\n$ lag_1_revenue         &lt;dbl&gt; NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, ‚Ä¶\n$ lag_3_revenue         &lt;dbl&gt; NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260‚Ä¶\n$ lag_5_revenue         &lt;dbl&gt; NA, NA, NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 8‚Ä¶\n\n\nNot happy with the default NA value. We can change that by passing a value to the default argument.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1:3, default = 0) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: ~ymd(event_date) | Trained\n\n\n‚Ä¢ Lagging: items_purchased revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date            &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-01‚Ä¶\n$ items_purchased       &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156, ‚Ä¶\n$ revenue               &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551,‚Ä¶\n$ lag_1_items_purchased &lt;dbl&gt; 0, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 15‚Ä¶\n$ lag_2_items_purchased &lt;dbl&gt; 0, 0, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64,‚Ä¶\n$ lag_3_items_purchased &lt;dbl&gt; 0, 0, 0, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, ‚Ä¶\n$ lag_1_revenue         &lt;dbl&gt; 0, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 5‚Ä¶\n$ lag_2_revenue         &lt;dbl&gt; 0, 0, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224‚Ä¶\n$ lag_3_revenue         &lt;dbl&gt; 0, 0, 0, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1‚Ä¶\n\n\nThe prefix argument also makes it easy to modify the prefix of the outputted column names.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1:3, prefix = \"diff_\") |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Variable mutation for: ~ymd(event_date) | Trained\n\n\n‚Ä¢ Lagging: items_purchased revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date             &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-0‚Ä¶\n$ items_purchased        &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156,‚Ä¶\n$ revenue                &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551‚Ä¶\n$ diff_1_items_purchased &lt;dbl&gt; NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, ‚Ä¶\n$ diff_2_items_purchased &lt;dbl&gt; NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, ‚Ä¶\n$ diff_3_items_purchased &lt;dbl&gt; NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, ‚Ä¶\n$ diff_1_revenue         &lt;dbl&gt; NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224,‚Ä¶\n$ diff_2_revenue         &lt;dbl&gt; NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1‚Ä¶\n$ diff_3_revenue         &lt;dbl&gt; NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 626‚Ä¶\n\n\nThere you have it, another useful step_*() function. step_lag() is pretty straightforward, but the lag argument makes it easy to create many different lagged variables. See you tomorrow.\n\n\nDay 19 - Use step_log() to log transform variables\nLet‚Äôs pivot topics at this point in the post. Another type of preprocessing step recipes can perform is data transformations. For today‚Äôs example, I‚Äôll spend some time learning about step_log(). step_log() creates a recipe step that will log transform data.\nBefore highlighting the use of this step function, I wanted to revisit this topic from my Stats 101 course. To refresh my memory, I went to find explanations on what a log transformation is, while also looking for views on why you might employ it as a data preprocessing step in machine learning contexts.\nWhat is a log transformation? In simple terms, it‚Äôs using a mathematical function (i.e., a logarithim) to transform variables from one representation to another. The @statquest YouTube channel has a pretty good video explaining what a log transformation does when applied to a variable.\nWhy use a log transformation? In most of what I‚Äôve read, a log transformation is used to address skewed distributions, where it seeks to make the distribution more normal. This normality is important because some models (e.g., linear regression) assume variables are normally distributed. In addition, when employed in various modeling contexts, its application may lead to more accurate model predictions. Several of the articles I read stated log transformations are useful when working in domains that tend to have variables with skewed distributions, like financial data (i.e., salaries, housing prices, etc.). This Stack Exchange post provides a pretty good explanation on the use of a log transformation.\nTo highlight the use of this step function, let‚Äôs continue using our obfuscated Google Analytics data. Here‚Äôs the code to get started with this data.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240117)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nskim(ga_tr)\n\n\nData summary\n\n\nName\nga_tr\n\n\nNumber of rows\n7492\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n384\n0\n\n\nitem_category\n132\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n85\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n94\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n285\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n420\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1\n20203701.15\n3996.13\n20201201\n20201209\n20201216\n20210106.0\n20210130\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\npurchase_revenue_in_usd\n0\n1\n101.74\n119.22\n2\n39\n72\n124.0\n1530\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ntransaction_id\n0\n1\n487314.88\n283530.55\n546\n246420\n479575\n724942.5\n999850\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nprice_in_usd\n0\n1\n19.57\n18.85\n1\n7\n14\n24.0\n120\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nquantity\n0\n1\n1.41\n2.13\n1\n1\n1\n1.0\n53\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nitem_revenue_in_usd\n0\n1\n23.26\n28.89\n1\n8\n15\n30.0\n704\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nLet‚Äôs take a quick look at item_revenue_in_usd. Using skim(), this variable ranges from $1 to $704. Now let‚Äôs peek at the distribution of the item_revenue_in_usd using base R plotting.\n\nhist(ga_tr$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nIndeed, this variable is skewed to the right. A log transformation would be appropriate here. Let‚Äôs do this by using step_log().\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\nNow that we transformed the variable, let‚Äôs explore the histogram of our baked variable and see its effect on the distribution.\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nNot bad. The transformed item_revenue_in_usd now exhibits a more normal distribution.\nstep_log() also has a few useful arguments. The first useful argument is base. When taking the log, R defaults to using the natural log, or exp(1). Say we want to log using base 10? This is certainly useful when dealing with money, since money makes more since in multiples of 10 (e.g., $100, $1,000, $10,000, etc.). Lets change the base of our recipe and see what happens.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd, base = 10) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nThe second useful argument is the offset argument. This argument accepts a value to offset the data prior to logging, which helps us avoid log(0). Here‚Äôs some example code:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd, offset = 5) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nThe final useful argument is signed, which accepts a boolean. Setting this argument to TRUE will make step_log() calculate the signed log. This is useful in cases where you have negative values, since a standard log transformation can‚Äôt be used to transform negative values. I found this article useful for getting an intuitive sense of what the signed log is and how it can useful for modeling. Here‚Äôs the code to serve as an example, even though we don‚Äôt have any negative values in our data:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd, signed = TRUE) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Signed log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nThat‚Äôs it for today. step_log() is a great first step in learning all the transformations recipes can do. I‚Äôm looking forward to tomorrow.\n\n\nDay 20 - Use step_sqrt() to apply a square root transformation\nWelcome back fellow learners. I had to take a couple days off because my schedule didn‚Äôt allow me the time to focus on this post. Nevertheless, let‚Äôs get back to it.\nToday I‚Äôm focusing on another transformation function, step_sqrt(). step_sqrt() applies a square root transformation to variables. This function is similair to step_log()‚Äìit‚Äôs just applying a different function to the variable.\nThe square root transformation is pretty straightforward, but I did some digging to more deeply understand what it is and why it‚Äôs useful. Here‚Äôs what I came up with:\n\nWhat could be the reason for using square root transformation on data? - Cross Validated post\nWhy is the square root transformation recommended for count data? - Cross Validated post\n\nLet‚Äôs get our data. I‚Äôm going to continue using the obfuscated Google Analytics ecommerce data we‚Äôve been working with.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240122)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nskim(ga_tr)\n\n\nData summary\n\n\nName\nga_tr\n\n\nNumber of rows\n7492\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n382\n0\n\n\nitem_category\n138\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n84\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n93\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n282\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n419\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1\n20203682.14\n3986.72\n20201201\n20201209\n20201216\n20210106.0\n20210130\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\npurchase_revenue_in_usd\n0\n1\n101.68\n118.25\n3\n39\n72\n124.0\n1530\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ntransaction_id\n0\n1\n487567.11\n282529.44\n546\n250363\n477087\n723799.8\n999850\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nprice_in_usd\n0\n1\n19.55\n18.80\n1\n7\n14\n24.0\n120\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nquantity\n0\n1\n1.45\n2.89\n1\n1\n1\n1.0\n160\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nitem_revenue_in_usd\n0\n1\n23.40\n29.67\n1\n8\n15\n30.0\n704\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nWe do have some count data, quantity. This is the number of times a specific item was purchased. Let‚Äôs take a look at its distribution before we apply our transformation.\n\nhist(ga_tr$quantity)\n\n\n\n\n\n\n\n\nIndeed, this data seems to follow a Poisson Distribution. Let‚Äôs see if we can transform it to be more normal (Gaussian) with a square root transformation.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_sqrt(quantity) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 7492 data points and 221 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Square root transformation on: quantity | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 7,492 √ó 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201211                      60         411613 Supernatur‚Ä¶ Bags                    46     1   \n 2   20210126                      68         315802 Google Sea‚Ä¶ Campus Colle‚Ä¶            7     1   \n 3   20201217                      75         940161 Google Inc‚Ä¶ Bags                    75     1   \n 4   20210114                      83          22807 Google Pen‚Ä¶ Office                   1     1.41\n 5   20201215                      95         892183 Google Fel‚Ä¶ Clearance               16     1.41\n 6   20210102                     183         548270 Google Wom‚Ä¶ Apparel                 50     1   \n 7   20201217                      16         199350 Google Per‚Ä¶ Lifestyle               16     1   \n 8   20201228                      44         302288 Google Lap‚Ä¶ New                      8     1   \n 9   20210124                     104          19596 Google Key‚Ä¶ New                      6     1.41\n10   20201212                      74         539684 Google Bou‚Ä¶ Clearance               14     1   \n# ‚Ñπ 7,482 more rows\n# ‚Ñπ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\nNow we‚Äôll check out the effect of the transformation by looking at a histogram of the baked data‚Äôs quantity variable.\n\nhist(baked_ga$quantity)\n\n\n\n\n\n\n\n\nWe can further explore the effect of this transformation by plotting the un-transformed variable on a scatter plot with the baked variable.\n\nplot(ga_tr$quantity, baked_ga$quantity)\n\n\n\n\n\n\n\n\nEh, looking at the histogram it seems this transformation didn‚Äôt really help very much. My assumption is this is due to many items only being purchased once. Thus, no transformation would likely be helpful in this case. Nonetheless, step_sqrt() is a pretty simple transformation function. You might find it useful when working with your data.\n\n\nDay 21 - Apply the Box-Cox transformation using step_BoxCox()\nHere we are, another day of transformations. To get us started, I turned to ChatGPT for a joke about data transformations.\nHey ChatGPT, tell me a joke about data transformations (prompt).\n\nWhy do data transformations never get invited to parties?\nBecause they always skew the conversation and standard deviations can be quite awkward!\n\nNot bad ‚Ä¶ Okay, let‚Äôs talk about step_BoxCox(). This function applies a Box Cox Transformation to our variables. I haven‚Äôt used this transformation in some time, so I scoured the internet to get back up to speed. I found these sources helpful in reminding me what this transformation is and how it can be useful:\n\nTransforming the response(Y) in regression: Box Cox transformation (7 mins) from the @PhilChanstats YouTube channel\nBox Cox transformation formula in regression analysis from the @PhilChanstats YouTube channel\nBox-Cox Transformation + R Demo from the @mathetal YouTube channel\n\nThough the following is a simplified explanation, many of these sources mention the Box Cox transformation attempts to make our distribution more normal. It does this by focusing on the tails of the distribution. Box Cox can be applied in situations where variance is not equal (i.e., heteroscedasticity). This transformation also allows us to better meet the assumptions of our models, and its application can result in a better predictive model. Lastly, many of these sources suggest this transformation is applied to the outcome variable of our model.\nLet‚Äôs go back to our Google Analytics obfuscated ecommerce data for today‚Äôs examples.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    items_purchased = sum(quantity),\n    revenue = sum(item_revenue_in_usd)\n  )\n\nset.seed(20240122)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nSince we can use the Box Cox transformation to rescale a variable to be more normal, let‚Äôs see if the revenue variable will be a good candidate. You‚Äôll notice the application of the Box Cox transformation is pretty straightforward (like most of recipes‚Äô transformation functions) with step_BoxCox().\n\nhist(ga_tr$revenue)\n\n\n\n\n\n\n\n\nIndeed, this varible exhibits a slight right skew. Let‚Äôs use the step_BoxCox() function here to perform the transformation.\n\nga_rec &lt;- recipe(revenue ~ ., data = ga_tr) |&gt;\n  step_BoxCox(revenue) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Box-Cox transformation on: revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 48 √ó 3\n   event_date items_purchased revenue\n        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1   20210101              97    14.2\n 2   20210126              61    15.2\n 3   20201225              40    12.2\n 4   20210125             199    19.4\n 5   20201211             617    25.3\n 6   20210115             119    17.2\n 7   20210120             400    22.8\n 8   20201201             356    22.5\n 9   20201205             333    22.6\n10   20201224              69    15.0\n# ‚Ñπ 38 more rows\n\n\nNot too bad. The distribution has been rescaled to resemble a slightly more normal distribution. The following histogram shows the effect of this transformation on the distribution.\n\nhist(baked_ga$revenue)\n\n\n\n\n\n\n\n\nIf for some reason you wanted to inspect the lambda value used by the transformation, you can inspect it by tidying the recipe. The outputted table shows lambda in the value column.\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 √ó 3\n  terms   value id          \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 revenue 0.190 BoxCox_GMKqI\n\n\nstep_BoxCox() is pretty simple, but useful. You might give it a try if you have some data of positive values exihibiting skewness. Until tomorrow, keep having fun with recipes.\n\n\nDay 22 - Apply an inverse transformation using step_inverse()\nI‚Äôm keeping things simple today; step_inverse() is my focus. This step function will inverse transform our data. Just like the other transformation functions, recipes makes it pretty easy to perform.\nLet‚Äôs continue using our Google Analytics ecommerce data for today‚Äôs example.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    items_purchased = sum(quantity),\n    revenue = sum(item_revenue_in_usd)\n  )\n\nset.seed(20240122)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nJust as a quick reminder, let‚Äôs take a look at the histogram of items_purchased.\n\nhist(ga_tr$items_purchased)\n\n\n\n\n\n\n\n\nTo apply an inverse transformation to our variable, we do the following in our recipe.\n\nga_rec &lt;- recipe(~., data = ga_te) |&gt;\n  step_inverse(items_purchased) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 13 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Inverse transformation on: items_purchased | Trained\n\n\nNow bake.\n\nga_baked &lt;- bake(ga_rec, new_data = NULL)\n\nga_baked\n\n# A tibble: 13 √ó 3\n   event_date items_purchased revenue\n        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1   20201202         0.00366    5202\n 2   20201203         0.00319    4313\n 3   20201207         0.00215    8466\n 4   20201208         0.00161    8691\n 5   20201214         0.00190    8498\n 6   20201219         0.00714    2778\n 7   20201222         0.00474    3226\n 8   20201230         0.00315    1606\n 9   20201231         0.0106     2595\n10   20210102         0.00990    1025\n11   20210113         0.00990    1859\n12   20210116         0.0139     1462\n13   20210123         0.00524    2744\n\n\nHere‚Äôs the effect of the transformation on our variable in two plots.\n\nhist(ga_baked$items_purchased)\n\n\n\n\n\n\n\nplot(ga_te$items_purchased, ga_baked$items_purchased)\n\n\n\n\n\n\n\n\nThe inverse did a pretty good job of making this distribution more normal.\nLike I said, keeping things simple today with a straight forward step_*() function. recipes makes it pretty easy to perform the inverse transformation with the step_inverse() function.\n\n\nDay 23 - Use extension packages to get even more steps\nRecently, I‚Äôve felt these posts have become a little stale and templated. So today, I‚Äôm switching it up a bit. I‚Äôm highlighting recipes extension packages. I‚Äôm also using this post for a bit of inspiration; I‚Äôm exploring other step_* functions to highlight. As such, I most likely won‚Äôt describe specific examples from each package in today‚Äôs post (there‚Äôs too many to cover). However, I‚Äôm aiming to list them here and highlight some packages that would be useful for the work I do.\nSeveral packages are available that provide additional step_*() functions, which are outside the core functionality of the recipes package. At the time of this writing, these include but are not limited to:\n\nactxps\nbestNormalize\ncustomsteps\nembed\nextrasteps\ntfhub\nhealthcareai\nhealthyR.ai\nhealthyR.ts\nhydrorecipes\nMachineShop\nmeasure\nnestedmodels\nsparseR\ntextrecipes\nthemis\ntimetk\n\nThe tidymodels site provides a table of a good majority of step_*() functions made available via these extension packages. Shout out to @jonthegeek from the R4DS Online Learning Community for pointing me to this table, and much praise to @Emil Hvitfeldt for pointing me in the direction of additional recipes extension packages I wasn‚Äôt aware. Keep in mind, not all of these extension packages are made available on CRAN.\nTaking a moment to read the descriptions of each extension package, I found textrecipes, themis, and timetk to be the most applicable to my work. Here‚Äôs a brief description of what each does. I also included some functions that could be interesting to explore in future posts. Note, some of these packages are meant to be applied in a specific domain, so their purpose may not solely be to be an extension of recipes (e.g., timetk).\n\nThe textrecipes package provides extra step_*() functions to preprocess text data. Looking through the package‚Äôs reference section, the step_tokenize() family of functions look useful. I also find the token modification (e.g., step_stem() and step_stopwords()) and text cleaning functions (e.g.¬†step_clean_levels()) to be intriguing and worth further exploration.\nThe themis package provides additional step_*() functions to assist in the preprocessing of unbalanced data. Specifically, this package provides functions to apply over-sampling or under-sampling methods to unbalance the data used for modeling. There‚Äôs several functions in here that seem to be useful when dealing with unbalanced data. I highly suggest checking out its reference page to get an idea of what all is available.\nThe timetk package does more than extend recipes. It‚Äôs main focus is to assist in the analysis of timeseries data. The package contains some similar functions we‚Äôve highlighted before (e.g., step_holiday_signature() and step_box_cox()). However, there‚Äôs some additional functions of interest. step_ts_pad() seems useful in cases where you need to add rows to fill in gaps. step_ts_clean() helps to clean outliers and missing data for timeseries analysis. step_diff() also looks helpful if you need to create a differenced predictor. If you‚Äôre working with timeseries, you might want to explore this package some more.\n\nIndeed, the tidymodels ecosystem is quite vast. This is especially true in the area of preprocessing steps achieved by extension packages for the recipes package.\nI wanted to shake it up a little bit. Today‚Äôs post was a little different, but in a good, refreshing way. See you tomorrow.\n\n\nDay 24 - Use step_tokenize() from textrecipes to convert a character predictor into a token variable\nBuilding on yesterday‚Äôs post, I‚Äôll begin exploring recipes extension packages‚Äô functions. step_tokenize() from textrecipes is first up. This function is useful when we need to convert a character variable into a token variable. If you‚Äôre unfamiliar with the concept of tokenization, the Text Mining with R: A Tidy Approach provides an overview.\nI‚Äôll keep things simple here by using a pared down data set. This will make it easier to see what this step_function() is doing. How about some quotes about statistics?\n\nquotes &lt;- tibble(\n  text = c(\n    \"All models are wrong, but some are useful.\",\n    \"Statisticians, like artists, have the bad habit of falling in love with their models.\",\n    \"If you torture the data enough, nature will always confess.\",\n    \"All generalizations are false, including this one.\"\n  )\n)\n\nLet‚Äôs create our recipe to tokenize by word.\n\nquotes_rec &lt;- recipe(~text, data = quotes) |&gt;\n  step_tokenize(text) |&gt;\n  prep()\n\nquotes_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 4 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Tokenization for: text | Trained\n\nbaked_quotes &lt;- bake(quotes_rec, new_data = NULL)\n\nbaked_quotes\n\n# A tibble: 4 √ó 1\n         text\n    &lt;tknlist&gt;\n1  [8 tokens]\n2 [14 tokens]\n3 [10 tokens]\n4  [7 tokens]\n\n\nOne thing I didn‚Äôt expect, but learned, was step_tokenize() creates a tknlist column. Indeed, I expected the column to still be a character where every token was placed on it‚Äôs own row, like what happens when you use tidytext functions. This certainly makes the object more compact and easier to work with, an excellent design decision in my opinion. To see the tokens, though, we need to use recipes show_tokens() function.\n\nrecipe(~text, data = quotes) |&gt;\n  step_tokenize(text) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all\"    \"models\" \"are\"    \"wrong\"  \"but\"    \"some\"   \"are\"    \"useful\"\n\n[[2]]\n [1] \"statisticians\" \"like\"          \"artists\"       \"have\"          \"the\"           \"bad\"          \n [7] \"habit\"         \"of\"            \"falling\"       \"in\"            \"love\"          \"with\"         \n[13] \"their\"         \"models\"       \n\n[[3]]\n [1] \"if\"      \"you\"     \"torture\" \"the\"     \"data\"    \"enough\"  \"nature\"  \"will\"    \"always\" \n[10] \"confess\"\n\n[[4]]\n[1] \"all\"             \"generalizations\" \"are\"             \"false\"           \"including\"      \n[6] \"this\"            \"one\"            \n\n\nstep_tokenize() defaults to tokenizing by word. However, there are several ways we can tokenize by. To do this, we modify the tokenizers argument. For example, say we want to tokenize by character instead of word. We do the following:\n\nrecipe(~text, data = quotes) |&gt;\n  step_tokenize(text, token = \"characters\") |&gt;\n  show_tokens(text)\n\n[[1]]\n [1] \"a\" \"l\" \"l\" \"m\" \"o\" \"d\" \"e\" \"l\" \"s\" \"a\" \"r\" \"e\" \"w\" \"r\" \"o\" \"n\" \"g\" \"b\" \"u\" \"t\" \"s\" \"o\" \"m\" \"e\"\n[25] \"a\" \"r\" \"e\" \"u\" \"s\" \"e\" \"f\" \"u\" \"l\"\n\n[[2]]\n [1] \"s\" \"t\" \"a\" \"t\" \"i\" \"s\" \"t\" \"i\" \"c\" \"i\" \"a\" \"n\" \"s\" \"l\" \"i\" \"k\" \"e\" \"a\" \"r\" \"t\" \"i\" \"s\" \"t\" \"s\"\n[25] \"h\" \"a\" \"v\" \"e\" \"t\" \"h\" \"e\" \"b\" \"a\" \"d\" \"h\" \"a\" \"b\" \"i\" \"t\" \"o\" \"f\" \"f\" \"a\" \"l\" \"l\" \"i\" \"n\" \"g\"\n[49] \"i\" \"n\" \"l\" \"o\" \"v\" \"e\" \"w\" \"i\" \"t\" \"h\" \"t\" \"h\" \"e\" \"i\" \"r\" \"m\" \"o\" \"d\" \"e\" \"l\" \"s\"\n\n[[3]]\n [1] \"i\" \"f\" \"y\" \"o\" \"u\" \"t\" \"o\" \"r\" \"t\" \"u\" \"r\" \"e\" \"t\" \"h\" \"e\" \"d\" \"a\" \"t\" \"a\" \"e\" \"n\" \"o\" \"u\" \"g\"\n[25] \"h\" \"n\" \"a\" \"t\" \"u\" \"r\" \"e\" \"w\" \"i\" \"l\" \"l\" \"a\" \"l\" \"w\" \"a\" \"y\" \"s\" \"c\" \"o\" \"n\" \"f\" \"e\" \"s\" \"s\"\n\n[[4]]\n [1] \"a\" \"l\" \"l\" \"g\" \"e\" \"n\" \"e\" \"r\" \"a\" \"l\" \"i\" \"z\" \"a\" \"t\" \"i\" \"o\" \"n\" \"s\" \"a\" \"r\" \"e\" \"f\" \"a\" \"l\"\n[25] \"s\" \"e\" \"i\" \"n\" \"c\" \"l\" \"u\" \"d\" \"i\" \"n\" \"g\" \"t\" \"h\" \"i\" \"s\" \"o\" \"n\" \"e\"\n\n\nHow about by word stems?\n\nrecipe(~text, data = quotes) |&gt;\n  step_tokenize(text, token = \"word_stems\") |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all\"   \"model\" \"are\"   \"wrong\" \"but\"   \"some\"  \"are\"   \"use\"  \n\n[[2]]\n [1] \"statistician\" \"like\"         \"artist\"       \"have\"         \"the\"          \"bad\"         \n [7] \"habit\"        \"of\"           \"fall\"         \"in\"           \"love\"         \"with\"        \n[13] \"their\"        \"model\"       \n\n[[3]]\n [1] \"if\"      \"you\"     \"tortur\"  \"the\"     \"data\"    \"enough\"  \"natur\"   \"will\"    \"alway\"  \n[10] \"confess\"\n\n[[4]]\n[1] \"all\"     \"general\" \"are\"     \"fals\"    \"includ\"  \"this\"    \"one\"    \n\n\nngrams?\n\nrecipe(~text, data = quotes) |&gt;\n  step_tokenize(text, token = \"ngrams\") |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all models are\"   \"models are wrong\" \"are wrong but\"    \"wrong but some\"   \"but some are\"    \n[6] \"some are useful\" \n\n[[2]]\n [1] \"statisticians like artists\" \"like artists have\"          \"artists have the\"          \n [4] \"have the bad\"               \"the bad habit\"              \"bad habit of\"              \n [7] \"habit of falling\"           \"of falling in\"              \"falling in love\"           \n[10] \"in love with\"               \"love with their\"            \"with their models\"         \n\n[[3]]\n[1] \"if you torture\"      \"you torture the\"     \"torture the data\"    \"the data enough\"    \n[5] \"data enough nature\"  \"enough nature will\"  \"nature will always\"  \"will always confess\"\n\n[[4]]\n[1] \"all generalizations are\"   \"generalizations are false\" \"are false including\"      \n[4] \"false including this\"      \"including this one\"       \n\n\nSay we only want 2 words in our ngram tokens. In this case, we need to pass argument values to the tokenizers::tokenize_ngrams() function using step_tokenize()‚Äôs options argument. Here we pass all the argument values in a list.\n\nrecipe(~text, data = quotes) |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2)\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all models\" \"models are\" \"are wrong\"  \"wrong but\"  \"but some\"   \"some are\"   \"are useful\"\n\n[[2]]\n [1] \"statisticians like\" \"like artists\"       \"artists have\"       \"have the\"          \n [5] \"the bad\"            \"bad habit\"          \"habit of\"           \"of falling\"        \n [9] \"falling in\"         \"in love\"            \"love with\"          \"with their\"        \n[13] \"their models\"      \n\n[[3]]\n[1] \"if you\"         \"you torture\"    \"torture the\"    \"the data\"       \"data enough\"   \n[6] \"enough nature\"  \"nature will\"    \"will always\"    \"always confess\"\n\n[[4]]\n[1] \"all generalizations\" \"generalizations are\" \"are false\"           \"false including\"    \n[5] \"including this\"      \"this one\"           \n\n\nGoing further, say we want to also exclude stop words (e.g., the, is, and) from being included within our ngrams. We just pass this option along in our list of options.\n\nrecipe(~text, data = quotes) |&gt;\n  step_tokenize(\n    text,\n    token = \"ngrams\",\n    options = list(n = 2, stopwords = c(\"the\", \"is\", \"and\"))\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all models\" \"models are\" \"are wrong\"  \"wrong but\"  \"but some\"   \"some are\"   \"are useful\"\n\n[[2]]\n [1] \"statisticians like\" \"like artists\"       \"artists have\"       \"have bad\"          \n [5] \"bad habit\"          \"habit of\"           \"of falling\"         \"falling in\"        \n [9] \"in love\"            \"love with\"          \"with their\"         \"their models\"      \n\n[[3]]\n[1] \"if you\"         \"you torture\"    \"torture data\"   \"data enough\"    \"enough nature\" \n[6] \"nature will\"    \"will always\"    \"always confess\"\n\n[[4]]\n[1] \"all generalizations\" \"generalizations are\" \"are false\"           \"false including\"    \n[5] \"including this\"      \"this one\"           \n\n\nThat‚Äôs all the time I have for today. There‚Äôs some other things step_tokenize() can do. I highly suggest checking out it‚Äôs documentation to see all of its functionality.\n\n\nDay 25 - Use step_clean_level() to clean categorical levels\nKeeping our focus on textrecipes, I‚Äôm going to highlight the use of step_clean_levels(). This function is useful for cleaning up the text used within character or factor variables. Specifically, this function will clean text values in our variable to only include:\n\nletters\nnumbers\nunderscores\n\nLet‚Äôs apply this text cleaning recipe step to the item_name variable in our obfuscated Google Analytics data. In fact, to make it easier to see what‚Äôs happening, I‚Äôll select() just the item_name within our example data set. I‚Äôll also go ahead and just create our training and testing split here as well.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n) |&gt;\n  select(item_name)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240127)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nIf you scan through the items, you‚Äôll begin to notice some of the text could be cleaned. For example, the data contains item names like:\n\nGoogle Medium Pet Collar (Red/Yellow)\n#IamRemarkable Unisex T-Shirt\nAndroid SM S/F18 Sticker Sheet\n\nIf you‚Äôve ever worked with ecommerce or website data, these strings are actually not too bad. However, we can use step_clean_levels() to make them easier to compute on.\nHere‚Äôs our recipe.\n\nga_rec &lt;- recipe(~., data = ga_te) |&gt;\n  step_clean_levels(item_name) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 1873 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Cleaning factor levels for: item_name | Trained\n\n\nstep_clean_levels()‚Äô tidy method is pretty informative. It provides one column with the original value and a second column with the cleaned value. Take a look at what it will do to our values once baked.\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 332 √ó 4\n   terms     original                      value                                id                \n   &lt;chr&gt;     &lt;chr&gt;                         &lt;chr&gt;                                &lt;chr&gt;             \n 1 item_name #IamRemarkable Journal        number_iam_remarkable_journal        clean_levels_JONrr\n 2 item_name #IamRemarkable Ladies T-Shirt number_iam_remarkable_ladies_t_shirt clean_levels_JONrr\n 3 item_name #IamRemarkable Lapel Pin      number_iam_remarkable_lapel_pin      clean_levels_JONrr\n 4 item_name #IamRemarkable Pen            number_iam_remarkable_pen            clean_levels_JONrr\n 5 item_name #IamRemarkable Unisex Hoodie  number_iam_remarkable_unisex_hoodie  clean_levels_JONrr\n 6 item_name #IamRemarkable Unisex T-Shirt number_iam_remarkable_unisex_t_shirt clean_levels_JONrr\n 7 item_name #IamRemarkable Water Bottle   number_iam_remarkable_water_bottle   clean_levels_JONrr\n 8 item_name Android Buoy Bottle           android_buoy_bottle                  clean_levels_JONrr\n 9 item_name Android Garden Tee Orange     android_garden_tee_orange            clean_levels_JONrr\n10 item_name Android Geek Pin              android_geek_pin                     clean_levels_JONrr\n# ‚Ñπ 322 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs interesting to see that the hashtag (i.e., #) was converted to the value number. I couldn‚Äôt find any options in step_clean_levels()to modify this, so you may need to first do some pre-cleaning before applying this function.\n\n\nNow, we bake.\n\n\n\n\n\n\nNote\n\n\n\nstep_clean_levels() will convert character vectors to factors for us.\n\n\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 1,873 √ó 1\n   item_name                           \n   &lt;fct&gt;                               \n 1 google_thermal_tumbler_navy         \n 2 noogler_android_figure              \n 3 google_leather_strap_hat_blue       \n 4 you_tube_jotter_task_pad            \n 5 google_nyc_campus_mug               \n 6 google_nyc_campus_zip_hoodie        \n 7 number_iam_remarkable_ladies_t_shirt\n 8 number_iam_remarkable_lapel_pin     \n 9 google_leather_strap_hat_blue       \n10 google_f_c_longsleeve_ash           \n# ‚Ñπ 1,863 more rows\n\n\nAll in all, a pretty useful step function from textrecipes. Take some time to apply the step_clean_levels() to your data. It might save you a great deal of time if you‚Äôre working with some messy text data.\n\n\nDay 26 - Use over-sampling and under-sampling methods with the themis package\nToday I‚Äôm going to highlight some basic uses of the themis package. themis makes over- and under-sampling methods available. These methods are useful to address imbalanced class data. You can read more about these methods here and here. In short, these methods allow us to extract more accurate information from imbalanced data.\nTo highlight this, let‚Äôs use our obfuscated Google Analytics ecommerce data. Specifically, let‚Äôs check out our shipping_tier variable for each transaction. Here‚Äôs the code we‚Äôll need to wrangle the data for our example:\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(transaction_id) |&gt;\n  summarise(\n    revenue = sum(item_revenue_in_usd),\n    shipping_tier = max(shipping_tier)\n  ) |&gt;\n  mutate(shipping_tier = factor(shipping_tier))\n\n\nset.seed(20240128)\nga_split &lt;- initial_split(data_ga, prop = 0.8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nLet‚Äôs take a moment to check for the presence of imbalanced data. We can do this with a simple count() and a quick bar chart using ggplot2.\n\nga_tr |&gt;\n  count(shipping_tier, sort = TRUE)\n\n# A tibble: 14 √ó 2\n   shipping_tier              n\n   &lt;fct&gt;                  &lt;int&gt;\n 1 FedEx Ground            2048\n 2 UPS Ground               270\n 3 FedEx 2Day               125\n 4 International Shipping    34\n 5 FedEx Overnight           25\n 6 &lt;NA&gt;                      25\n 7 FedEx Ground-T            12\n 8 FedEx Ground-V             7\n 9 UPS 2nd Day Air            5\n10 UPS 3 Day Select           5\n11 FedEx Ground-GI            2\n12 FedEx 2Day-V               1\n13 UPS 2nd Day Air-F-V        1\n14 UPS Next Day Air           1\n\n\n\nggplot(\n  count(ga_tr, shipping_tier, sort = TRUE),\n  aes(reorder(shipping_tier, n), y = n)\n) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Shipping Tier\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou‚Äôll notice the presence of a severe imbalance in the shipping_tier variable, as most transactions selected FedEx ground as the shipping tier (most likely the cheapest and default shipping tier). We can use step_upsample() and step_downsample() to create synthetic samples to address this imbalance with our recipe.\nWhen over- or under-sampling, we need to select an over_ratio or under_raio. These values specify the ratio of the majority-to-minority or minority-to-majority frequencies, which are used to determine how many of the minority samples are added or how many of the majority class are removed.\nFor instance, if we are upsampling, we can set the over_ratio to 1. This will synthetically add points to the minority class so the minority and majority classes are equal.\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôm omitting NAs here with step_naomit(), just to make things easier.\n\n\n\nga_rec &lt;- recipe(~., data = data_ga) |&gt;\n  step_naomit(shipping_tier) |&gt;\n  step_upsample(shipping_tier, over_ratio = 1) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 3202 data points and 27 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Removing rows with NA values in: shipping_tier | Trained\n\n\n‚Ä¢ Up-sampling based on: shipping_tier | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 33,436 √ó 3\n   transaction_id revenue shipping_tier\n            &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;        \n 1         677613     113 FedEx 2Day   \n 2         383365      76 FedEx 2Day   \n 3         852510      50 FedEx 2Day   \n 4         311346      24 FedEx 2Day   \n 5         283625     152 FedEx 2Day   \n 6         516528      61 FedEx 2Day   \n 7         101047      76 FedEx 2Day   \n 8         892183      94 FedEx 2Day   \n 9         433754      65 FedEx 2Day   \n10         970795      81 FedEx 2Day   \n# ‚Ñπ 33,426 more rows\n\n\n\nggplot(\n  count(baked_ga, shipping_tier, sort = TRUE),\n  aes(reorder(shipping_tier, n), y = n)\n) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Shipping Tier\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSay we only want the minority levels to have about half the amount of values of the majority class. We can do this by passing .5 to the over_ratio argument.\n\nrecipe(~., data = data_ga) |&gt;\n  step_naomit(shipping_tier) |&gt;\n  step_upsample(shipping_tier, over_ratio = .5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  count(shipping_tier, sort = TRUE) |&gt;\n  ggplot(\n    aes(reorder(shipping_tier, n), y = n)\n  ) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Shipping Tier\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUnder-sampling is pretty much the same, but we use step_downsample() and the under_ratio argument. Here‚Äôs some example code to get you started.\n\nrecipe(~., data = data_ga) |&gt;\n  step_naomit(shipping_tier) |&gt;\n  step_downsample(shipping_tier, under_ratio = 100) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  count(shipping_tier, sort = TRUE) |&gt;\n  ggplot(\n    aes(reorder(shipping_tier, n), y = n)\n  ) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Shipping Tier\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nthemis makes it pretty easy to utilize over- and under-sampling methods. Indeed, this was just a highlight of some of the package‚Äôs basics. themis also has implemented some more advanced methods to address imbalanced data (e.g., SMOTE). If you work with imbalanced data, I suggest checking out the themis package.\n\n\nDay 27 - Use step_ts_pad() from timetk to pad timeseries data\nIt‚Äôs been a few of days. Work was getting busy, so I didn‚Äôt have time to focus on this post as much as I would have liked. So, let‚Äôs pick up where we left off, focusing on some step_*() functions from extension packages. Today, I‚Äôm focusing on the step_ts_pad() from the timetk package.\ntimetk is it‚Äôs own package. It serves as a toolkit for working with timeseries data, so it‚Äôs not just an extension package to recipes. However, it contains some useful step_*() functions when modeling timeseries data. step_ts_pad() is highlighted in today‚Äôs post.\nstep_ts_pad() fills in the gaps of timeseries data with a specified value. Let‚Äôs take a look at some example data. Specifically, we‚Äôre returning to our obfuscated Google Analytics ecommerce data. To make it more clear what this function is doing, I‚Äôll remove some values.\n\ndata_ga &lt;- read_csv(\n  here(\n    \"blog/posts/2024-01-01-post-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"\n  )\n)\n\nRows: 9365 Columns: 14\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  mutate(event_date = ymd(event_date)) |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    revenue = sum(item_revenue_in_usd),\n  )\n\n# Remove some values to better highlight the example\ndata_ga &lt;- data_ga[1:61 %% 2 == 0, ]\n\ndata_ga\n\n# A tibble: 30 √ó 2\n   event_date revenue\n   &lt;date&gt;       &lt;dbl&gt;\n 1 2020-12-02    5202\n 2 2020-12-04    6454\n 3 2020-12-06    2328\n 4 2020-12-08    8691\n 5 2020-12-10   10760\n 6 2020-12-12    6280\n 7 2020-12-14    8498\n 8 2020-12-16   11505\n 9 2020-12-18    6617\n10 2020-12-20    1401\n# ‚Ñπ 20 more rows\n\n\nWe‚Äôre working with timeseries data, so the training and testing split is created using rsamples‚Äô initital_time_split() function. This split function performs the same action as initial_split(), however, it takes the first prop samples for training, instead of a random selection (checkout ?initial_time_split for more info).\n\nset.seed(20240202)\nga_split &lt;- initial_time_split(data_ga)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nWe don‚Äôt have a complete timeseries here, since this data is aggregated by day and we removed some days in the wrangling step. However, we can use step_ts_pad() to fill in values for missing days. This can be as simple as filling in missing values with NA. Here‚Äôs the code to do this:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_ts_pad(event_date, by = \"day\", pad_value = NA) |&gt;\n  prep()\n\ntidy(ga_rec)\n\n# A tibble: 1 √ó 6\n  number operation type   trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      ts_pad TRUE    FALSE ts_padding_KR6BU\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 √ó 4\n  terms      by    pad_value id              \n  &lt;chr&gt;      &lt;chr&gt; &lt;lgl&gt;     &lt;chr&gt;           \n1 event_date day   NA        ts_padding_KR6BU\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 43 √ó 2\n   event_date revenue\n   &lt;date&gt;       &lt;dbl&gt;\n 1 2020-12-02    5202\n 2 2020-12-03      NA\n 3 2020-12-04    6454\n 4 2020-12-05      NA\n 5 2020-12-06    2328\n 6 2020-12-07      NA\n 7 2020-12-08    8691\n 8 2020-12-09      NA\n 9 2020-12-10   10760\n10 2020-12-11      NA\n# ‚Ñπ 33 more rows\n\n\nNow we have a complete series. However, say we want to fill in this padded value with another value, say the mean of revenue from our training data.\n\nga_rec &lt;- recipe(~., data = data_ga) |&gt;\n  step_ts_pad(event_date, by = \"day\", pad_value = mean(ga_tr$revenue)) |&gt;\n  prep()\n\ntidy(ga_rec)\n\n# A tibble: 1 √ó 6\n  number operation type   trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      ts_pad TRUE    FALSE ts_padding_wvkNb\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 √ó 4\n  terms      by    pad_value id              \n  &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;           \n1 event_date day       3883. ts_padding_wvkNb\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 59 √ó 2\n   event_date revenue\n   &lt;date&gt;       &lt;dbl&gt;\n 1 2020-12-02   5202 \n 2 2020-12-03   3883.\n 3 2020-12-04   6454 \n 4 2020-12-05   3883.\n 5 2020-12-06   2328 \n 6 2020-12-07   3883.\n 7 2020-12-08   8691 \n 8 2020-12-09   3883.\n 9 2020-12-10  10760 \n10 2020-12-11   3883.\n# ‚Ñπ 49 more rows\n\n\nThe by argument can be used to modify the padding interval. This argument can take values like:\n\n‚Äúauto‚Äù\n‚Äúyear‚Äù\n‚Äúmonth‚Äù\n‚Äúday‚Äù\n‚Äúhour‚Äù\n‚Äú7 days‚Äù\n\nFor example, let‚Äôs use the drinks timeseries data from the modeldata package. This data is a monthly timeseries of drink sales from 1992-01-01 to 2017-09-01. You can get more information about this data by running ?drinks in your console. I‚Äôm going to remove some rows again to make it more clear on what this function is doing to our data.\n\ndata(drinks, package = \"modeldata\")\n\n# Remove some values to better highlight the example\ndata_drinks &lt;- drinks[1:nrow(drinks) %% 2 == 0, ]\n\nset.seed(20240202)\ndrinks_split &lt;- initial_time_split(data_drinks)\ndrinks_tr &lt;- training(drinks_split)\ndrinks_te &lt;- testing(drinks_split)\n\nTo pad by month, all we do is pass ‚Äúmonth‚Äù to the by argument of the function.\n\ndrinks_rec &lt;- recipe(~., data = drinks_tr) |&gt;\n  step_ts_pad(date, by = \"month\") |&gt;\n  prep()\n\nbake(drinks_rec, new_data = NULL)\n\n# A tibble: 229 √ó 2\n   date       S4248SM144NCEN\n   &lt;date&gt;              &lt;dbl&gt;\n 1 1992-02-01           3458\n 2 1992-03-01             NA\n 3 1992-04-01           4564\n 4 1992-05-01             NA\n 5 1992-06-01           4529\n 6 1992-07-01             NA\n 7 1992-08-01           4137\n 8 1992-09-01             NA\n 9 1992-10-01           4259\n10 1992-11-01             NA\n# ‚Ñπ 219 more rows\n\n\nWe can also pad with an alternative value, just like we did above. We just do the following:\n\ndrinks_rec &lt;- recipe(~., data = drinks_tr) |&gt;\n  step_ts_pad(date, by = \"month\", pad_value = mean(drinks_tr$S4248SM144NCEN)) |&gt;\n  prep()\n\nbake(drinks_rec, new_data = NULL)\n\n# A tibble: 229 √ó 2\n   date       S4248SM144NCEN\n   &lt;date&gt;              &lt;dbl&gt;\n 1 1992-02-01          3458 \n 2 1992-03-01          6628.\n 3 1992-04-01          4564 \n 4 1992-05-01          6628.\n 5 1992-06-01          4529 \n 6 1992-07-01          6628.\n 7 1992-08-01          4137 \n 8 1992-09-01          6628.\n 9 1992-10-01          4259 \n10 1992-11-01          6628.\n# ‚Ñπ 219 more rows\n\n\ntimetk‚Äôs step_ts_pad() function is pretty useful. If you need to pad values in a set of timeseries data, then check it out.\n\n\nDay 28 - Specify an interaction variable using step_interact()\nToday, I‚Äôm taking a step back. I forgot to cover a very important step_*() function. Interaction terms are an essential component to modeling, and so I need to highlight the use of recipes‚Äô step_interact() function.\nrecipes‚Äô step_interact() function specifies new columns of interaction terms between two or more variables within our recipe. Let‚Äôs apply this step function to some example data. Going back to a previous data set, let‚Äôs use credit_data from the modeldata package for our examples. I‚Äôve used this data several times in previous posts, so I just provide the code to wrangle and create the testing and training split without much exploration or explanation.\n\ndata(\"credit_data\", package = \"modeldata\")\n\n# Remove NAs to make it easier to work with data\ncredit_data &lt;- credit_data |&gt; na.omit()\n\nset.seed(20240203)\ncredit_split &lt;- initial_split(credit_data, prop = .8)\n\ncredit_tr &lt;- training(credit_split)\ncredit_te &lt;- testing(credit_split)\n\nHow about we add a simple interaction between Expenses and Income to our recipe. We can do this by doing the following:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(terms = ~ Expenses:Income) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\ncredit_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 3231 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Interactions with: Expenses:Income | Trained\n\ntidy(credit_rec)\n\n# A tibble: 1 √ó 6\n  number operation type     trained skip  id            \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n1      1 step      interact TRUE    FALSE interact_xYaaz\n\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 √ó 2\n  terms           id            \n  &lt;chr&gt;           &lt;chr&gt;         \n1 Expenses:Income interact_xYaaz\n\n# Select just the interaction term, just to make it easier to see\nbake(credit_rec, new_data = NULL) |&gt;\n  select(Expenses_x_Income)\n\n# A tibble: 3,231 √ó 1\n   Expenses_x_Income\n               &lt;dbl&gt;\n 1              7200\n 2              4125\n 3              2835\n 4              8100\n 5              2345\n 6              7500\n 7              4500\n 8              7200\n 9              4500\n10              6525\n# ‚Ñπ 3,221 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nstep_interact()‚Äôs docs mention this function is intended for numeric data, and categorical variables should be converted to dummy variables first. This can be done using step_dummy() (see my previous day 04 post).\n\n\nYou‚Äôll notice step_interact() modifies the interaction variable‚Äôs name by using _x_ as the separator. If for some reason you want to change this, you just pass a character string to step_interact()‚Äôs sep argument.\nThis naming convention is useful, especially if you intend to specify higher order interaction variables within your model, like a 3-way interaction.\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(terms = ~ Expenses:Income:Assets) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\nbake(credit_rec, new_data = NULL) |&gt;\n  select(Expenses_x_Income_x_Assets)\n\n# A tibble: 3,231 √ó 1\n   Expenses_x_Income_x_Assets\n                        &lt;dbl&gt;\n 1                          0\n 2                   16500000\n 3                          0\n 4                  243000000\n 5                          0\n 6                   60000000\n 7                   15750000\n 8                   21600000\n 9                   24750000\n10                   45675000\n# ‚Ñπ 3,221 more rows\n\n\nstep_interact() also allows the use of traditional R model formula when specifying interaction variables. This is really convenient. Say we want to include all the two-way interactions along with our three way interaction, we could do something like this:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(\n    terms = ~ Expenses:Income +\n      Expenses:Assets +\n      Income:Assets +\n      Expenses:Income:Assets\n  ) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\nbake(credit_rec, new_data = NULL) |&gt;\n  select(starts_with(\"Expenses\"), starts_with(\"Income\"))\n\n# A tibble: 3,231 √ó 6\n   Expenses Expenses_x_Income Expenses_x_Assets Expenses_x_Income_x_Assets Income Income_x_Assets\n      &lt;int&gt;             &lt;dbl&gt;             &lt;dbl&gt;                      &lt;dbl&gt;  &lt;int&gt;           &lt;dbl&gt;\n 1       60              7200                 0                          0    120               0\n 2       75              4125            300000                   16500000     55          220000\n 3       35              2835                 0                          0     81               0\n 4       90              8100           2700000                  243000000     90         2700000\n 5       35              2345                 0                          0     67               0\n 6       75              7500            600000                   60000000    100          800000\n 7       45              4500            157500                   15750000    100          350000\n 8       60              7200            180000                   21600000    120          360000\n 9       60              4500            330000                   24750000     75          412500\n10       45              6525            315000                   45675000    145         1015000\n# ‚Ñπ 3,221 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdd additional interactions using the +\n\n\nHowever, a short-cut would be to do something like this:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(\n    terms = ~ (Expenses + Income + Assets)^3\n  ) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\ncredit_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 3231 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Interactions with: Expenses:Income, Expenses:Assets, Income:Assets, ... | Trained\n\nbake(credit_rec, new_data = NULL) |&gt;\n  select(starts_with(\"Expenses\"), starts_with(\"Income\"))\n\n# A tibble: 3,231 √ó 6\n   Expenses Expenses_x_Income Expenses_x_Assets Expenses_x_Income_x_Assets Income Income_x_Assets\n      &lt;int&gt;             &lt;dbl&gt;             &lt;dbl&gt;                      &lt;dbl&gt;  &lt;int&gt;           &lt;dbl&gt;\n 1       60              7200                 0                          0    120               0\n 2       75              4125            300000                   16500000     55          220000\n 3       35              2835                 0                          0     81               0\n 4       90              8100           2700000                  243000000     90         2700000\n 5       35              2345                 0                          0     67               0\n 6       75              7500            600000                   60000000    100          800000\n 7       45              4500            157500                   15750000    100          350000\n 8       60              7200            180000                   21600000    120          360000\n 9       60              4500            330000                   24750000     75          412500\n10       45              6525            315000                   45675000    145         1015000\n# ‚Ñπ 3,221 more rows\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAlthough traditional model formula syntax can be used, step_interact()‚Äôs docs warn that inline functions (e.g., log) should not be used.\n\n\nIt‚Äôs also important to mention, again, categorical variables should be converted into numeric variables before being used within an interaction. Let‚Äôs say we want to specify an interaction between Debt and Home variables within our credit_data recipe. We would use step_dummy() first on Home, then specify all the interactions between the dummy variables and Debt. step_interact() makes this very easy with the use of dplyr-like selection functions.\n\ncredit_rec &lt;- recipe(~., data = credit_data) |&gt;\n  step_dummy(Home) |&gt;\n  step_interact(~ starts_with(\"Home\"):Debt) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\ncredit_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 4039 data points and no incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Dummy variables from: Home | Trained\n\n\n‚Ä¢ Interactions with: Home_other:Debt, Home_owner:Debt, Home_parents:Debt, ... | Trained\n\ntidy(credit_rec, number = 1)\n\n# A tibble: 5 √ó 3\n  terms columns id         \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      \n1 Home  other   dummy_asve2\n2 Home  owner   dummy_asve2\n3 Home  parents dummy_asve2\n4 Home  priv    dummy_asve2\n5 Home  rent    dummy_asve2\n\ntidy(credit_rec, number = 2)\n\n# A tibble: 5 √ó 2\n  terms             id            \n  &lt;chr&gt;             &lt;chr&gt;         \n1 Home_other:Debt   interact_hgBQM\n2 Home_owner:Debt   interact_hgBQM\n3 Home_parents:Debt interact_hgBQM\n4 Home_priv:Debt    interact_hgBQM\n5 Home_rent:Debt    interact_hgBQM\n\nbake(credit_rec, new_data = NULL)\n\n# A tibble: 4,039 √ó 23\n   Status Seniority  Time   Age Marital Records Job       Expenses Income Assets  Debt Amount Price\n   &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n 1 good           9    60    30 married no      freelance       73    129      0     0    800   846\n 2 good          17    60    58 widow   no      fixed           48    131      0     0   1000  1658\n 3 bad           10    36    46 married yes     freelance       90    200   3000     0   2000  2985\n 4 good           0    60    24 single  no      fixed           63    182   2500     0    900  1325\n 5 good           0    36    26 single  no      fixed           46    107      0     0    310   910\n 6 good           1    60    36 married no      fixed           75    214   3500     0    650  1645\n 7 good          29    60    44 married no      fixed           75    125  10000     0   1600  1800\n 8 good           9    12    27 single  no      fixed           35     80      0     0    200  1093\n 9 good           0    60    32 married no      freelance       90    107  15000     0   1200  1957\n10 bad            0    48    41 married no      partime         90     80      0     0   1200  1468\n# ‚Ñπ 4,029 more rows\n# ‚Ñπ 10 more variables: Home_other &lt;dbl&gt;, Home_owner &lt;dbl&gt;, Home_parents &lt;dbl&gt;, Home_priv &lt;dbl&gt;,\n#   Home_rent &lt;dbl&gt;, Home_other_x_Debt &lt;dbl&gt;, Home_owner_x_Debt &lt;dbl&gt;, Home_parents_x_Debt &lt;dbl&gt;,\n#   Home_priv_x_Debt &lt;dbl&gt;, Home_rent_x_Debt &lt;dbl&gt;\n\n\nThe tidy methods are useful here, especially if you want to know what variables will be created as a result of the recipe.\nTo sum up today‚Äôs post, step_interact() has a nice, intuitive interface. It allows for traditional interaction variable specification, while also adding some convenient functionality. A really great step_*() function.\n\n\nDay 29 - Use recipes‚Äô check_*() functions to validate steps\nrecipes has several check_*() functions. These functions are useful for validating data returned from recipe steps. Specifically, recipes includes the following check_*() functions:\n\ncheck_class() - checks variable classes\ncheck_cols() - checks if all columns are present\ncheck_missing() - checks for any missing values\ncheck_new_values() - checks for the presence of any new values\ncheck_range() - checks for whether a variable‚Äôs range changes\n\nToday, I‚Äôm going to highlight a couple of these check_* functions: check_class(), check_cols(), and check_missing(). Let‚Äôs start with check_class().\nWe‚Äôll continue to use our credit_data for some of today‚Äôs examples. Towards the end, I‚Äôll use the penguins data for the final example.\n\ndata(\"credit_data\", package = \"modeldata\")\n\ncredit_data &lt;- credit_data |&gt; na.omit()\n\nset.seed(20240204)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8)\ncredit_tr &lt;- training(credit_split)\ncredit_te &lt;- testing(credit_split)\n\ncheck_class() is useful for verifying variables are expected classes. Two methods are used to check variable classes. First, this check function uses values provided to the class argument. If NULL, the check will then learn the classes from the prep. A variable can have multiple classes, which will be used within the check.\nHere‚Äôs how to use the training set to learn variable classes for the check:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_class(everything()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec\n\n# A tibble: 3,231 √ó 14\n   Status Seniority Home     Time   Age Marital   Records Job    Expenses Income Assets  Debt Amount\n   &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 good           1 other      30    26 single    no      fixed        35    110      0     0    700\n 2 good           1 parents    48    22 married   no      fixed        45    117      0     0    768\n 3 good          25 priv       48    55 married   no      fixed        60    169   4000     0   1000\n 4 good          34 owner      60    50 married   yes     fixed        60    150   9000     0   1300\n 5 good           0 parents    60    21 single    no      parti‚Ä¶       45    312  10000     0   1033\n 6 good          15 owner      60    35 single    yes     fixed        35    150   5000  2800   1300\n 7 good           3 other      12    30 single    no      freel‚Ä¶       35    150      0     0    920\n 8 good          23 other      54    38 separated no      fixed        60    178      0     0    740\n 9 bad            2 owner      60    38 married   yes     fixed        75     74   3500   500   1700\n10 good           0 parents    36    22 single    no      parti‚Ä¶       35    105   3000     0    750\n# ‚Ñπ 3,221 more rows\n# ‚Ñπ 1 more variable: Price &lt;int&gt;\n\n\nIndeed, nothing happens because our recipe resulted in variables to not change their class. If any unexpected changes did occur, then the bake would be broken with an error.\nWe can manually specify our variable class expectations using the class_nm argument:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_class(\n    Status,\n    Home,\n    Marital,\n    Records,\n    Job,\n    class_nm = \"factor\"\n  ) |&gt;\n  check_class(\n    Seniority,\n    Time,\n    Age,\n    Expenses,\n    Income,\n    Assets,\n    Debt,\n    Amount,\n    Price,\n    class_nm = \"integer\"\n  ) |&gt;\n  prep(strings_as_factors = FALSE) |&gt;\n  bake(new_data = NULL)\n\nWarning: The `strings_as_factors` argument of `prep.recipe()` is deprecated as of recipes 1.3.0.\n‚Ñπ Please use the `strings_as_factors` argument of `recipe()` instead.\n\ncredit_rec\n\n# A tibble: 3,231 √ó 14\n   Status Seniority Home     Time   Age Marital   Records Job    Expenses Income Assets  Debt Amount\n   &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 good           1 other      30    26 single    no      fixed        35    110      0     0    700\n 2 good           1 parents    48    22 married   no      fixed        45    117      0     0    768\n 3 good          25 priv       48    55 married   no      fixed        60    169   4000     0   1000\n 4 good          34 owner      60    50 married   yes     fixed        60    150   9000     0   1300\n 5 good           0 parents    60    21 single    no      parti‚Ä¶       45    312  10000     0   1033\n 6 good          15 owner      60    35 single    yes     fixed        35    150   5000  2800   1300\n 7 good           3 other      12    30 single    no      freel‚Ä¶       35    150      0     0    920\n 8 good          23 other      54    38 separated no      fixed        60    178      0     0    740\n 9 bad            2 owner      60    38 married   yes     fixed        75     74   3500   500   1700\n10 good           0 parents    36    22 single    no      parti‚Ä¶       35    105   3000     0    750\n# ‚Ñπ 3,221 more rows\n# ‚Ñπ 1 more variable: Price &lt;int&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you intend to have a variable with multiple classes, you can specify this with allow_additional = TRUE in the check_class() function. Check out the function‚Äôs examples section for more details (run ?check_class in your console).\n\n\nJust to highlight what happens when the check fails, let‚Äôs set the second check to expect a numeric rather than an integer class for the variable.\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_class(\n    Status,\n    Home,\n    Marital,\n    Records,\n    Job,\n    class_nm = \"factor\"\n  ) |&gt;\n  check_class(\n    Seniority,\n    Time,\n    Age,\n    Expenses,\n    Income,\n    Assets,\n    Debt,\n    Amount,\n    Price,\n    class_nm = \"numeric\"\n  ) |&gt;\n  prep(strings_as_factors = FALSE) |&gt;\n  bake(new_data = NULL)\n\nError in `check_class()`:\nCaused by error:\n! `Seniority` should have the class &lt;numeric&gt; but has the class &lt;integer&gt;.\n\ncredit_rec\n\n# A tibble: 3,231 √ó 14\n   Status Seniority Home     Time   Age Marital   Records Job    Expenses Income Assets  Debt Amount\n   &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 good           1 other      30    26 single    no      fixed        35    110      0     0    700\n 2 good           1 parents    48    22 married   no      fixed        45    117      0     0    768\n 3 good          25 priv       48    55 married   no      fixed        60    169   4000     0   1000\n 4 good          34 owner      60    50 married   yes     fixed        60    150   9000     0   1300\n 5 good           0 parents    60    21 single    no      parti‚Ä¶       45    312  10000     0   1033\n 6 good          15 owner      60    35 single    yes     fixed        35    150   5000  2800   1300\n 7 good           3 other      12    30 single    no      freel‚Ä¶       35    150      0     0    920\n 8 good          23 other      54    38 separated no      fixed        60    178      0     0    740\n 9 bad            2 owner      60    38 married   yes     fixed        75     74   3500   500   1700\n10 good           0 parents    36    22 single    no      parti‚Ä¶       35    105   3000     0    750\n# ‚Ñπ 3,221 more rows\n# ‚Ñπ 1 more variable: Price &lt;int&gt;\n\n\nAnother useful check is check_cols(). This function checks if all the columns from the training frame are also present within the new data. Here‚Äôs what this looks like:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_cols(everything()) |&gt;\n  step_dummy(Job) |&gt;\n  prep()\n\n# Omit Job variable in the new data to show the error\nbake(credit_rec, new_data = credit_te[, -8])\n\nError in `bake()`:\n‚úñ The following required columns are missing from `new_data`: `Job`.\n‚Ñπ These columns have one of the following roles, which are required at `bake()` time: `predictor`.\n\n\nYou‚Äôll notice an error stops our bake. This is because the variable used to make our dummy variables, Job, was omitted from the new data. Indeed, this is a very useful check, especially to verify if variables needed in our new data are available when the recipe is baked.\nOkay, I‚Äôll highlight one more useful check_* function for today, check_missing(). This function‚Äôs purpose is simple‚Äìcheck if variables contain any missing values. check_missing() can fail either on prep or bake. Both help catch if missing values are present in the training or new data.\nThe check_missing() documentation (run ?check_missing in your console), uses the credit_data data for its examples. So, let‚Äôs switch it up a bit here. Instead, I‚Äôll use the penguins data for this final example. Here‚Äôs the code to get us up to the specification of the recipe.\n\ndata(\"penguins\", package = \"modeldata\")\n\nset.seed(20240204)\npenguins_split &lt;- initial_split(penguins, prop = .8)\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\n\nrecipe(~., data = penguins_tr) |&gt;\n  check_missing(everything()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nError in `check_missing()`:\nCaused by error in `bake()`:\n! The following columns contains missing values: bill_length_mm, bill_depth_mm,\n  flipper_length_mm, body_mass_g, and sex.\n\n\nYou‚Äôll notice the above code errors because all the columns listed in the error contain a missing value(s).\n\nis.na(penguins_tr) |&gt; colSums()\n\n          species            island    bill_length_mm     bill_depth_mm flipper_length_mm \n                0                 0                 1                 1                 1 \n      body_mass_g               sex \n                1                 9 \n\n\nIf for some reason your training data doesn‚Äôt contain missing values, but your new data does, then the check will push an error during the bake.\nHere I‚Äôll use step_naomit() to remove missing values from the training set to show how check_missing() throws an error during the bake.\n\n# No error\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_naomit(everything()) |&gt;\n  check_missing(everything()) |&gt;\n  prep()\n\n\n# Error\nbake(penguin_rec, new_data = penguins_te)\n\nError in `bake()`:\n! The following columns contains missing values: bill_length_mm, bill_depth_mm,\n  flipper_length_mm, body_mass_g, and sex.\n\n\nA pretty useful check_*() function, especially if you‚Äôre concerned data might contain missing values.\nIndeed, recipes provides some other check_*() functions. I highly suggest looking over recipes‚Äô reference page to see the other functions that are provided.\nThat‚Äôs all for day 29. Tomorrow is day 30 of this challenge. I‚Äôm excited to wrap this post up.\n\n\nDay 30 - Use step_cut() to turn a numeric variable into a factor\nHere we are, day 30. We made it! üéâ\nstep_cut() is my focus for our final day. This step function creates factor variables from numeric variables. If you‚Äôre familiar with base::cut(), you‚Äôll have a pretty good idea of what step_cut() does within a recipe.\nLet‚Äôs use our penguins data for our final day of examples. Here‚Äôs the code to split the data into our training and testing sets.\n\ndata(penguins, package = \"modeldata\")\n\nset.seed(20240206)\npenguins_split &lt;- initial_split(penguins, prop = .8)\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nI want to split our penguins into three categories: small, medium, and large. We‚Äôll do this by cutting the body_mass_g column into three categories. Let‚Äôs first obtain some summary statistics to inform us on the cut values we should use.\n\nskim(penguins_tr, body_mass_g)\n\n\nData summary\n\n\nName\npenguins_tr\n\n\nNumber of rows\n275\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbody_mass_g\n2\n0.99\n4185.07\n789.06\n2850\n3550\n4000\n4750\n6300\n‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÅ\n\n\n\n\n\nGreat, let‚Äôs use body_mass_g‚Äôs 25th and 75th percentiles as the cut offs in our recipe. You‚Äôll notice I use step_naomit() here to remove rows with missing values. If I didn‚Äôt do this, step_cut() would error.\n\n\n\n\n\n\nNote\n\n\n\nstep_naomit()‚Äôs skip argument is set to TRUE by default. This will cause the bake step to skip this step, and NA values will still be present within our data.\n\n\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_naomit(body_mass_g) |&gt;\n  step_cut(body_mass_g, breaks = c(3550, 4750)) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n‚îÄ‚îÄ Recipe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n\n\n\n\n‚îÄ‚îÄ Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\n\n\n\n\n\n‚îÄ‚îÄ Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n‚îÄ‚îÄ Operations \n\n\n‚Ä¢ Removing rows with NA values in: body_mass_g | Trained\n\n\n‚Ä¢ Cut numeric for: body_mass_g | Trained\n\nbake(penguins_rec, penguins_tr)\n\n# A tibble: 275 √ó 7\n   species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g         sex   \n   &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;fct&gt;               &lt;fct&gt; \n 1 Adelie    Biscoe              43.2          19                 197 (4.75e+03,6.3e+03]  male  \n 2 Chinstrap Dream               45.7          17.3               193 (3.55e+03,4.75e+03] female\n 3 Gentoo    Biscoe              48.4          16.3               220 (4.75e+03,6.3e+03]  male  \n 4 Adelie    Torgersen           34.6          21.1               198 (3.55e+03,4.75e+03] male  \n 5 Gentoo    Biscoe              45.2          15.8               215 (4.75e+03,6.3e+03]  male  \n 6 Gentoo    Biscoe              49.2          15.2               221 (4.75e+03,6.3e+03]  male  \n 7 Adelie    Torgersen           38.5          17.9               190 [2.85e+03,3.55e+03] female\n 8 Gentoo    Biscoe              45.1          14.5               215 (4.75e+03,6.3e+03]  female\n 9 Adelie    Torgersen           46            21.5               194 (3.55e+03,4.75e+03] male  \n10 Chinstrap Dream               46.4          17.8               191 (3.55e+03,4.75e+03] female\n# ‚Ñπ 265 more rows\n\n\nSay we wanted to keep the original variable along with the newly created factor variable. We need to first use step_mutate() to retain the original before cutting. Here‚Äôs the code to do this:\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_naomit(body_mass_g) |&gt;\n  step_mutate(body_mass_g_orig = body_mass_g) |&gt;\n  step_cut(body_mass_g, breaks = c(3550, 4750)) |&gt;\n  prep()\n\nbake(penguins_rec, penguins_tr) |&gt;\n  select(starts_with(\"body\"))\n\n# A tibble: 275 √ó 2\n   body_mass_g         body_mass_g_orig\n   &lt;fct&gt;                          &lt;int&gt;\n 1 (4.75e+03,6.3e+03]              4775\n 2 (3.55e+03,4.75e+03]             3600\n 3 (4.75e+03,6.3e+03]              5400\n 4 (3.55e+03,4.75e+03]             4400\n 5 (4.75e+03,6.3e+03]              5300\n 6 (4.75e+03,6.3e+03]              6300\n 7 [2.85e+03,3.55e+03]             3325\n 8 (4.75e+03,6.3e+03]              5000\n 9 (3.55e+03,4.75e+03]             4200\n10 (3.55e+03,4.75e+03]             3700\n# ‚Ñπ 265 more rows\n\n\nOne other note about step_cut(), it also has an include_outside_range argument. This argument accepts a boolean value (TRUE / FALSE), which specifies what you want to do with ranges outside of the cut values learned during the prep phase. TRUE will include values outside of the range. FALSE will exclude them and apply an NA as the value.\nI‚Äôm ending today‚Äôs post on an easy one. step_cut() is pretty straightforward, but it is very useful for creating factor variables out of numeric variables within our recipe.\nAlthough this is the end of the 30ish days, I‚Äôm going to take some time‚Äìhopefully tomorrow‚Äìto draft up a quick summary about what I‚Äôve learned during this process. Until then, keep working on those recipes.\n\n\nDay 31 and beyond\nFor the past 30ish days (I surely wasn‚Äôt perfect and missed some days), I devoted time to learning more about tidymodels‚Äô recipes package. Being the end of this challenge, I thought a post reflecting on what I‚Äôve learned would be valuable. Here‚Äôs a grab bag of things I‚Äôve learned throughout this process. Some are related to the specific focus of this post, the use of the recipes package. Others are things I learned while taking on this personal challenge.\nrecipes makes it really easy to perform most data preprocessing steps for modelling tasks. Most step_function()s I came across made intuitive sense, and when a function had various options and functionality, the interfaces made them easy to work with. Most of the time I recognized you just needed to pass a variable name to perform the intended step.\nrecipes‚Äô summary() and tidy() methods are great for understanding what‚Äôs happening under the hood. At times, I wasn‚Äôt exactly sure what was occurring in the background with each step. However, taking the prep()ped recipe and throwing it into a summary() or tidy() function helped provide more information on what was taking place in the background.\nI certainly got some things wrong, but that‚Äôs okay. This post was intended to be a form of ‚Äòlearning out loud‚Äô. Indeed, getting things wrong was great for multiple reasons. First, it challenged me to go deeper. If I didn‚Äôt know how something worked or what concepts I needed to know to understand what each step_*() function was doing, I read further and wider. Doing this additional reading introduced me to many other topics, some I‚Äôve been introduced to, and others I didn‚Äôt know existed. Second, it allowed me to experiment with how functions worked. As a result, allowing me to better understand how a function worked. If a recipe errored or pushed a warning, I was forced to ask: ‚ÄòWhy is this failing?‚Äô, ‚ÄòWhat is wrong with my recipe?‚Äô, and ‚ÄòAm I specifying the arguments correctly?‚Äô. Answering these questions made me really ‚Äòread the docs‚Äô to understand what is happening.\nReading the docs closely also led me to identify areas I could contribute to open-source software. Indeed, I wasn‚Äôt adding features or fixing critical bugs, but I was able to provide little fixes, hopefully making the documentation more clear for the next person. Even though my contributions were small, I was elated to see my name was added to the tidyverse‚Äôs, Q4 2023 tidymodels digest blog post. Having my name highlighted has been a catalyst to find other areas to contribute.\nWriting‚Äôs tough. This especially becomes apparent when you‚Äôre forced to get something on the page every day. Some days I just wasn‚Äôt into it. Other days I didn‚Äôt know what to cover. From time-to-time, I was just busy and didn‚Äôt have the time to write. During these times, I found taking a break to be best. Taking breaks usually resulted in the reset I needed, allowing me to be even more focused the next day.\nAside from learning the above, here‚Äôs a brief list of additional things I learned that were helpful in getting me to finish writing this post (future-self, take note):\n\nHave a plan for what you‚Äôre going to write next.\nSet a timer and just write. You‚Äôll be amazed how much you can output in 50 - 60 minutes.\nDon‚Äôt care about quality at first, you can always revise later (there are certainly revisions to be made here).\n\n\n\nWrap up\nSo there you have it. 30ish days of learning how to use tidymodels‚Äô recipes package. I started off by discussing what a recipe is and how to specify it. I highlighted various step_*() functions that get added to recipes. Then, I covered some recipes‚Äô extension packages I‚Äôve found to be useful. Finally, I highlighted the use of some of the recipes‚Äô check functions.\nThere‚Äôs certainly more to explore with this package and others in the tidymodels ecosystem. I‚Äôm excited to explore what its other packages have to offer.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {30 Day Tidymodels `Recipes` Challenge},\n  date = {2024-01-01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚Äú30 Day Tidymodels `Recipes`\nChallenge.‚Äù January 1, 2024."
  },
  {
    "objectID": "blog/posts/2025-12-25-hex-update-008/index.html",
    "href": "blog/posts/2025-12-25-hex-update-008/index.html",
    "title": "The Hex Update: Issue 008",
    "section": "",
    "text": "Let‚Äôs catch up\nHowdy folks!\nHappy holidays to those that celebrate. Given the momentum I‚Äôve generated from the past several posts, I wanted to publish an issue over the holiday. So, here‚Äôs Issue 008.\nThree topics caught my attention this week: podcasts taking over the living room, Netflix‚Äôs deal with Barstool Sports, and the fading faith younger folks have in the internet. To end the week with a little fun, I share an article looking back at the internet in 2023.\nLet‚Äôs get into it.\n\n\nThree things from this week\nHere are three things that caught my attention:\n\nBlog: How podcasts took over the living room in 2025\nAccording to YouTube‚Äôs official blog, podcast listening via TVs dominated this past year. In October 2025 alone, YouTube users listened to 700 million hours of podcasts via a living room device. This was up from last year‚Äôs 450 million hours. That amounts to a year-over-year increase of 650%, a signal of massive growth. As such, YouTube sought to meet this demand by making changes to their TV UI and through the rollout of a top podcasts chart, which both are aimed at making it easier for users to find their favorite podcasts.\n\nWhy does this matter?\nPodcast listening has continued to grow steadily, according to research and reporting from Edison Research. Noting this growth can be attributed to listening on TVs is something to be aware of. In fact, could podcasts soon become the new late night TV for some audiences? If so, YouTube might have found some additional leverage to further disrupt and compete with linear broadcast offerings in the television space. This is a trend worth continuous monitoring.\n\n\n\nArticle: Netflix Strikes Video Podcast Deal With Barstool Sports\nThis article from the The Hollywood Reporter covers the recent partnerhsip between Netflix and Barstool Sports. The deal includes exclusive rights to some of Barstool‚Äôs most popular podcasts. This also involves rights to some of the pocast‚Äôs video library content. In addition to the deal struck with Barstool Sports, the article highlights additional deals Netflix has struck up with other podcast production companies, including iHeartPodcasts and SpotifyStudios.\n\nWhy does this matter?\nThis just feels like another inflection point for podcasts. Indeed, many of these deals are likely part and parcel of the trend that audiences, more and more, are consuming podcasts via their televisions. But even more than this, it just feels like additional leverage for streaming platforms to compete with linear broadcast for the time spent audiences give when using their TVs. YouTube and Netflix seem to be fully aware of this trend, and they are striking deals to remain competitive. Watch this space, as this might be another opening for podcasts from independent creators and smaller media organizations to break into and disrupt spaces typically dominated by traditional media brands.\n\n\n\nArticle: Faith in the internet is fading among young Brits\nHere‚Äôs some interesting reporting from The Register. It highlights recent survey results on young British persons‚Äô (18 to 34) view of the internet. The data, collected by OfCom, the United Kingdom‚Äôs regulatory group for the broadcasting, internet, telecommunications, and postal industries, shows many young adults find the internet to be negative for society and unhelpful for their mental health. A question is then posed: why are younger individuals losing faith in the internet? This quote from the article summarizes an answer:\n\nOne reason may be that their online experiences differ significantly from those of their elders, including more material chosen by algorithms than actively selected by users.\n\n\nWhy does this matter?\nThese findings are not surprising. Many younger folks are beginning to feel more and more negative toward their online experiences, and some will continue to believe it has a neative impact on society. Many experiences and platforms suffer from what Cory Doctrow claims to be the enshitification of the internet. These negative feelings are likely related. Why should media organizations care, then? Although it may be profitable in some cases to degrade experiences while locking users into your platforms, this will only catch up to those that do so. There‚Äôs already negativity towards the internet, so this will only exacerbate users‚Äô negative feelings even more. Maybe there‚Äôs space and opportunity to address this negativity in some way via platforms and the expericences created by media organizations.\n\n\n\n\nJust for fun\nArticle: What the Internet Was Like in 2003\nHere‚Äôs another interesting look back at 2003. Specifically, it‚Äôs an entertaining look back what the internet was like post the dot-com crash. This included new terms like ‚Äòsocial software‚Äô to be defined, blogs going mainstream, RSS feeds, the introduction of Google‚Äôs AdSense, Friendster, MySpace, the iTunesMusicStore, and the use of Adobe Flash everywhere. Why was 2003 so significant? The article‚Äôs final paragraph provides some perspective:\n\nSo by the end of 2003, the web felt primed for a new era: social software was going mainstream, web standards were stable while Flash brought the pizzazz, and business models were emerging. Could this be the second generation of the internet coming online?\n\nA really fun read. Check it out.\nFrom my family to yours, I wish you a happy holiday season and a great start to to the new year. I look forward to sharing more in 2026.\nCheers üéâ!\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 008},\n  date = {2025-12-25},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúThe Hex Update: Issue 008.‚Äù\nDecember 25, 2025."
  },
  {
    "objectID": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html",
    "href": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html",
    "title": "Implementing a next and back button in Shiny",
    "section": "",
    "text": "As part of the R4DS bookclub, our cohort has been working through Hadley Wickham‚Äôs (2020) Mastering Shiny book. Chapter 4: Case study: ER injuries had an interesting, challenging exercise I couldn‚Äôt figure out. This post aims to walk through my process of trying to solve this exercise, overviewing my thought process while trying to solve it. My hope is future me will look back at this post when I need to implement such a feature in a future Shiny application. I hope others find it useful as well."
  },
  {
    "objectID": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "href": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "title": "Implementing a next and back button in Shiny",
    "section": "The initial runtime variables",
    "text": "The initial runtime variables\nAs part of my testing of the actionButton() UI function, I found out the initial value being sent to the server was zero. I also found out that zero can‚Äôt be used for subsetting (i.e, nothing is gets returned to the UI). To address this issue, a variable with a reactive value of one needed to be in the environment upon runtime of the application. This is so we can use the initial value of one to return the first element of our data to our output$series in the textOutput() function in the UI when the application starts. Let‚Äôs take a look at this in action.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n\n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n}\n\nshinyApp(ui, server)\n\nYou‚Äôll notice a new function here in the server, reactiveVal(). According to the documentation, this function is used to create a ‚Äúreactive value‚Äù object within the app‚Äôs environment. Basically, I understand this function is just creating a reactive expression where the initial value is one upon the runtime of the application, which is then used in the subsetting operation applied in the renderText() function. Great, we have partly solved the indexing issue with the use of reactiveVal(1). You‚Äôll also notice the buttons don‚Äôt work here because there is no dependency on them as an input, but I‚Äôll get to that here shortly by applying some observeEvents() functions.\n\nThe maximum index value\nI also needed a solution to help limit the range of values that could be used for indexing in our subsetting operation. I now had the lower value one available in the environment, however I did not have the maximum value. At this point, I needed a function to calculate the length of the data and to treat it as a reactive expression, as this number might be dynamic in the larger application, and the users‚Äô inputs will determine what data gets displayed within the application (e.g., filtering by product code selection). We can easily calculate the length of our data using the length() function and making this a reactive expression by wrapping it with the reactive() function. Here is what this looks like with code.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n  # Determine the upper part of the subset index range\n  max_no_values &lt;- reactive(length(series))\n\n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n\n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n}\n\nshinyApp(ui, server)\n\nIt‚Äôs challenging to show this value in the environment in writing, but now given the current code, I have the lower value of the range, one, and the maximum value three corresponding to the number of values in our data structure available in the environment. This is great, so now I have those two values available to help with subsetting. At this point, we also need to incorporate the two user inputs, the Back and Next buttons. However, since we know these two buttons increment by one every time they are pressed, I need to rely on some mathematical operations to control the range of values used to subset the data. Given the simplified application, I know 1, 2, or 3 is the values and range of values I need to properly apply within a subsetting operation."
  },
  {
    "objectID": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "href": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "title": "Implementing a next and back button in Shiny",
    "section": "Enter the %%",
    "text": "Enter the %%\nPart of getting this functionality to work required the use of the modulus %% and modular arithmetic. Basically, modulus is an arithmetic operation that performs a division and returns the remainder from the operation. I learned a lot about this in this article here (Busbee and Braunschweig n.d.). The R for Data Science book (Wickham and Grolemund 2017) also introduces the use of %% as well. While researching the modulus, I found many useful applications for it within programming. It‚Äôs definitely worth some more time learning of its other uses. When applied in our case, though, we needed it to keep the subsetting index within the bounds of the size of our data structure.\nI am far from a mathematician, so the following explanation of the logic behind how a modulus is applied here is going to be a little fast and loose. However, I‚Äôm going to take a crack at it. Take for example our application. On runtime, we have a reactive value place() that starts at the value one. We also know that our maximum number of values that can be used as an index for our subsetting operation is three, our max_no_values reactive (i.e., c(\"a\", \"b\", \"c\")). We can now use the modulus with these two values to limit the number we are using in the index of our subsetting based on the number of clicks by the user. Here is a simplified example using code illustrating this point.\n\nmax_no_values &lt;- 3\n\n# User clicks the button to increment the index of the subset\n# Vector corresponds to the value outputted by the `actionButton()`\nuser_clicks &lt;- c(0:12)\n\nuser_clicks %% max_no_values\n\n [1] 0 1 2 0 1 2 0 1 2 0 1 2 0\n\n\nEarlier in the post, we found out that we can‚Äôt use zero to subset, as nothing gets returned. So to solve our issue, we need to shift these values by adding one to the vector. Notice how that with every ‚Äòclick‚Äô the range of these values never goes below one or exceeds three, even when a user‚Äôs click count (keep in mind every click of the actionButton() increments by one) goes above three. This is the power of the %%, as this operation keeps our index range between 1 - 3, regardless of how many times the user clicks an action button.\n\nuser_clicks %% max_no_values + 1\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3 1\n\n\nThe math is a little different for the Back button, though. However, the same principles apply.\n\n((user_clicks - 2) %% 3) + 1\n\n [1] 2 3 1 2 3 1 2 3 1 2 3 1 2\n\n\nLet‚Äôs use some print debugging here to show how the of %% works in action. I‚Äôm going to use the glue package to help make the messages sent to the console more human readable.\n\nlibrary(shiny)\nlibrary(glue)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n  # Determine the total number\n  max_no_values &lt;- reactive(length(series))\n\n  position &lt;- reactiveVal(1)\n\n  # These cause a side-effect by changing the place value\n  observeEvent(input$forward, {\n    position((position() %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n\n  observeEvent(input$back, {\n    position(((position() - 2) %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n\n  output$series &lt;- renderText({\n    series[position()]\n  })\n}\n\nshinyApp(ui, server)\n\nIf you click the Back and Next button and watch your console, you‚Äôll see the position value for every click being printed. While clicking these values, you will observe a couple of things:\n\nYou‚Äôll notice the value zero is never passed as a subsetting index value.\nThe arithmetic operations constrain our subsetting values within a range of 1 - 3, the length of our character vector.\nMultiple clicks remain in order, regardless if the user clicks the Next or Back buttons (e.g., 1, 2, 3 or 3, 2, 1).\n\nAt this point, we can get rid of our print debugging code, test our working example, and bask in our accomplishment of understanding how this solution works. The next step is to now integrate what we know into the larger application. We‚Äôll do that here in the next section of this post."
  },
  {
    "objectID": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "href": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "title": "Implementing a next and back button in Shiny",
    "section": "Product selection",
    "text": "Product selection\nAs part of the original functionality of the app, users were given a selectInput() in the UI to filter for injuries that were the result of different products. The requirements stated the outputted narratives also needed to reflect the users‚Äô filter selection. This functionality needed to be added back in, and it also needed to be reactive. I do this by adding the selected &lt;- reactive(injuries %&gt;% filter(prod_code == input$code)) near the beginning portion of the server section of the code. You‚Äôll also notice we are using the filter() function and %&gt;% operator here, so we need to also bring in the dplyr package (i.e., library(dplyr)).\nThere are now two areas in the server that have a dependency on the selected() reactive expression, the max_no_stories() reactive and our output$narrative object. Since our reprex was using a simplified vector of data (e.g., c(\"a\", \"b\", \"c\")), we need to modify the code to use these reactives. The biggest change is we are now passing a tibble of data rather than a character vector of data. As such, I need to use selected()$narrative to refer to the narrative vectors we want to use in our server function. Nothing else really changes, as the underlying process of determining the range of values and using a mathematical operation to limit the indexing stays the same. We are just now applying this process to a different set of data, although it is technically a reactive expression rather than an object in our environment."
  },
  {
    "objectID": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "href": "blog/posts/2021-09-12-post-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "title": "Implementing a next and back button in Shiny",
    "section": "Cases where users select a new product code",
    "text": "Cases where users select a new product code\nGiven the functionality provided within our application, it‚Äôs reasonable to expect users would change the product code (i.e., the main purpose is to give users tools to explore the data). It‚Äôs also reasonable that the user would then expect the narrative values to change based on their product selection, and indeed we have built this functionality into the app. However, what we didn‚Äôt account for yet was what users expectations are for the order to which the new filter data will be presented. When users make a change to their filtering criteria, they would most likely expect that the updated narrative data would start at the beginning, not where their previous clicks would place them within their previously selected data. Given this expectation, I now need some code to ‚Äòreset‚Äô the subsetting index when a user changes their product code filter.\nWhy might this be important? Take for example if the aim of this functionality was to output the most recent injury reported for a specific product code. Our user would expect that any time they switch their product code filtering input, the displayed narrative would be the most recent reported injury, and that each subsequent click would result in a chronological walk through the narratives, either forwards or backwards. This would especially be important if the app was connected to a streaming data source that isn‚Äôt static. Moreover, you might even modify the output$narrtive object to include the date, so the user is informed on when a specific injury was treated. For the sake of keeping things simple though, we will only add the reset behavior to the app in this post.\nThis reset of the indexing value was provided in the solutions guide referenced above, and it adds another observeEvent() to make this work. Specifically, it directed me to add this code to the server section of the application:\n\nobserveEvent(input$code, {\n  place(1)\n})\n\nHere you can see that the observeEvent() is waiting for any changes to the input$code input. When a change occurs to this input, the place(1) is run, and the subsetting index is set back to one. We now have included functionality to the app where when the user changes the product code filtering, the narrative increment index will display the first value in that subset of injuries as selected by the user."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html",
    "title": "Shiny summary tiles",
    "section": "",
    "text": "Effective reporting tools include user interface (UI) elements to quickly and effectively communicate summary metrics. Shiny, a free software package written in the R statistical computing language, provides several tools to communicate analysis and insights. Combining several of these elements together, a developer can create user interface elements that clearly communicate important summary metrics (e.g., Key Performance Indicators) to an application‚Äôs users.\nThis post details the steps to create the following simple Shiny application. Specifically, this post overviews the use of Shiny‚Äôs built-in functions to create simple summary metric tiles. In addition, this post describes how to add styling to UI elements by applying custom css to a Shiny application."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "title": "Shiny summary tiles",
    "section": "The reactive graph",
    "text": "The reactive graph\nAlthough this app is simple and most of the elements can be easily managed, it‚Äôs always good practice to see the big picture of the app by plotting out a reactive graph first. It‚Äôs also good to have the intended reactive graph available as a quick reference, just in case unexpected results and/or behaviors are displayed while developing the application, and as a method for identifying any situations where computing resources are not being used efficiently.\nBelow is the reactive graph for the application to be developed:\n\n\n\n\n\n\n\nReactive graph for summary metric tiles\n\n\n\n\nAgain, a really simple application‚Äìone input (date), a reactive expression (data()), and five outputs (users; page_view; session_start; purchase; and event_date). The graph also details the dependencies clearly, where the outputs are dependent on the reactive data() object‚Äìwhich in cohort with the outputs‚Äìdepends on the date input."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#the-setup",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#the-setup",
    "title": "Shiny summary tiles",
    "section": "The setup",
    "text": "The setup\nThe first step is to import the R packages used within the application. The following code chunk contains the packages used for the application. A brief description of each is included.\n\nlibrary(shiny) # The Shiny app library\nlibrary(readr) # Import data\nlibrary(dplyr) # Pipe and data manipulation\nlibrary(tidyr) # Tidying data function\nlibrary(purrr) # Used for functional programming\nlibrary(glue) # Used for string interpolation\n\nMany of these packages are part of the tidyverse, and thus the import could be simplified to just running library(tidyverse). Be aware this may bring in unused, unneeded libraries. There is nothing wrong with this approach. However, I opted to be more verbose with this example, so as to be clear about what libraries are utilized within the example application and to have more control on what packages were imported by the application."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#application-layout",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#application-layout",
    "title": "Shiny summary tiles",
    "section": "Application layout",
    "text": "Application layout\nThe next step is to code the layout of the UI. To keep the design simple, a sidebar will contain the application‚Äôs inputs, while the outputs will be placed within the main panel of the application. The general skeleton of the layout looks like this:\n\nui &lt;- fluidPage(\n  # Inputs\n  sidebarLayout(\n    sidebarPanel()\n  ),\n  # Outputs\n  mainPanel(\n    # Summary tiles\n    fluidRow(),\n    br(),\n    # Data information output\n    fluidRow()\n  )\n)\n\nThere‚Äôs nothing too fancy about this code, outside of it establishing the general layout of the application, so not much else will be said about what each element does here. However, Chapter 6 of Mastering Shiny discusses application layout if a more detailed description is needed."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#the-date-input",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#the-date-input",
    "title": "Shiny summary tiles",
    "section": "The date input",
    "text": "The date input\nThe app requirements state users need to have the ability to modify the dates to which the data represents, and the summary metric tiles will change based on this user input. However, the app will not have any user input upon startup, so it needs to default to the most recent date within the data. To meet these requirements, we use the following code:\n\n# Code excluded for brevity\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    )\n\nThe shiny:dateInput() function is used to create the HTML for the input, which resides in the application‚Äôs sidebar. The function‚Äôs id argument is given the value of date, which will establish a connection to elements within the server. More on this later. Then, a string value of Select a date for summary: is passed along to the label argument. This value will be displayed above the date input in the UI.\nSince the app won‚Äôt have an initial user input upon the startup of the application, max(ga4_date$event_date) is passed along to the value argument. This will default the input to the most recent date within the data. In addition, the functions max and min arguments are passed similar calls. However, in the case of the min argument the base R min() function is used on the ga4_data$event_date."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "title": "Shiny summary tiles",
    "section": "First iteration of the summary metric tiles",
    "text": "First iteration of the summary metric tiles\n\nThe server side\n\nThe reactive data() object\nBefore the summary metrics can be displayed in the UI, the application needs data to create the outputs. In addition, since this data will be dependent on users‚Äô input (i.e., the user can select a new date which subsequently changes the summary metric tile), this object needs to be reactive. To do this, the following code is added to the server side of the application.\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    ga4_data %&gt;% filter(.data[[\"event_data\"]] == input$date)\n  })\n}\n\nIn practical terms, this code just filters the data for the date being passed along as the input$date object.\nAgain, this object could be the most recent date within the data, the date set by the max argument in the dateInput() function, or it could be based on a user‚Äôs modification of the date input. Since this code was wrapped inside of the reactive({}) function, Shiny will be listening for any changes made to the to the input$date object. Any changes that occur will result in the data() reactive expression to be modified, followed by new output values being displayed via the UI.\nOne other key concept is being exhibited here, tidy evaluation, specifically data-masking. Since technically dplyr::filter() is being used inside of a function, an explicit reference to the data is required. Thus, .data[[\"event_data\"]] notation is used to make it explicit on what data will be filtered. The specifics on how to use data-masking in the context of a Shiny app is beyond the scope of this post. However, the previously linked materials provide a more detailed description of these concepts.\n\n\nThe outputs\nLooking back at the reactive graph, the application requires five outputs to be in the server. These outputs will just be simple text outputs, so the use of the shiny::renderText() function will be sufficient to meet our requirements. The format() function is also applied to comma format any outputs that contain numbers (e.g., 2,576 vs 2576). Here is what the server looks like currently:\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_date, .data[[\"event_date\"]] == input$date)\n  })\n\n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(\n    data()$session_start,\n    big.mark = ','\n  ))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\n    \"Estimates represent data from {data()$event_date}\"\n  ))\n}\n\nAs part of the functionality requirements, the app needed some UI element informing users what date is being represented in the summary tiles. The output$date object was included to meet this requirement. The output$date object, aside from using the renderText() function, includes the use of the glue::glue() function to make the outputted message more informative.\nThe {glue} package is used to manipulate string literals with the use of the curly braces (e.g., {}). When applied here, the {data()$event_date} is evaluated as an R call, its value becomes appended to the string, and the whole string is then outputted to the application‚Äôs UI.\n\n\n\nBack to the UI\nNow that there are five elements being outputted from the server, UI elements need to be included to display the rendered outputs.\nWhen making early design decisions about the application‚Äôs layout, it was decided these elements were going to reside within the main panel of the application. Another decision made was to keep the summary metric tile elements on the same row, so as to seem as though they are related to one another (i.e., related KPIs). As for the UI element informing the user on the date the summary metric tiles represent, it was decided that this element would be placed on its own row.\nTo achieve the intended design, additional Shiny layout functions were applied to the application‚Äôs code. This includes using the fluidRow() and column() functions to achieve the wanted UI organization. The following code was used to achieve the placement of the summary tiles within the application‚Äôs layout:\n\nmainPanel(\n  fluidRow(\n    column(),\n    column(),\n    column()\n  ),\n  br(),\n  fluidRow()\n)\n\nAs for the design of the summary metric tiles, each tile needed to include some type of title followed by the text representing the metric. To achieve this, the shiny::div() function was used. This function creates an individual HTML tag that outputs the text being passed along into the function. Directly below the title element, the textOutput() function is used to display the outputs coming from the application‚Äôs server. The code for one summary metric tile would look like the following:\n\ncolumn(3, div(\"Unique Users\"), textOutput(\"users\"))\n\nBy combining these elements, the application code in its current state can be seen here:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\",\n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n)\n\n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\n        \"date\",\n        \"Select a date for summary:\",\n        value = max(ga4_data$event_date),\n        max = max(ga4_data$event_date),\n        min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        column(3, div(\"Users\"), textOutput(\"users\")),\n        column(3, div(\"Page Views\"), textOutput(\"page_view\")),\n        column(3, div(\"Session Starts\"), textOutput(\"session_start\")),\n        column(3, div(\"Purchases\"), textOutput(\"purchase\"))\n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")\n      )\n    )\n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n\n  # Text output\n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(\n    data()$session_start,\n    big.mark = ','\n  ))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\n    \"Estimates represent data from {data()$event_date}\"\n  ))\n}\n\nshinyApp(ui, server)\n\nIndeed, this code works and meets the functionality requirements. However, it‚Äôs quite verbose and contains a lot of redundant, repeated code. Different techniques could be applied to make the application more eloquent and efficient in its design. The goal of the next few sections, then, will be to simplify the application through the development of functions and applying functional programming principles.\n\nSimplifying the outputs\nReviewing the server, most of the outputs are created through the use of repeated patterns of the same code. This breaks the DRY principle (Don‚Äôt Repeat Yourself) of software development. Both functions and the application of functional programming principles will be applied to address this issue.\nAn obvious pattern used to create the outputs is output$foo &lt;- renderText(format(bar, big.mark = ',')). This pattern could be converted into a function, and then this function could be used to iterate over the several reactive objects (e.g., data()$users) with the use of a {purrr} function. Since the side-effects are intended to be used rather than outputting a list object from our iteration, purrr::walk() will do the trick.\nUtilizing this strategy simplifies our code to the following:\n\nc('users', 'page_view', 'session_start', 'purchase') %&gt;%\n  walk(\n    ~ {\n      output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))\n    }\n  )\n\nIndeed, I can‚Äôt take full credit for this solution. Thanks goes to @Kent Johnson in the R4DS Slack channel for helping me out.\nThe output$date object was left out of this simplification of the code. Certainly, the function could be made to be more general and flexible to handle this repetition of the renderText() function. However, this would be over engineering a solution to the problem."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "title": "Shiny summary tiles",
    "section": "Back to the UI",
    "text": "Back to the UI\nFunctions and functional programming principles will now be used to address these same issues on the UI side of the application. Much of the repetition occurs with the use of the following pattern:\n\ncolumn(3, div(\"Metric Title\"), textOutput(\"metric_output\"))\n\nIndeed, this pattern is applied four times. Since it was copied and pasted more than twice and breaks the DRY principle, it would be best to convert it into a function and iterate it using functional programming tools."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "title": "Shiny summary tiles",
    "section": "Simplifying the UI with functional programming",
    "text": "Simplifying the UI with functional programming\nA helper function, make_summary_tile(), is added to the setup section of the application. The function looks like this:\n\nmake_summary_tile &lt;- function(title, text_output) {\n  column(2, div(title), textOutput(text_output))\n}\n\nThere‚Äôs nothing too fancy or complicated about this function. It simply generalizes the pattern applied within the UI side of the first iteration of our application. As for placement, this function could be defined at the top of the application file or in a separate .R file embedded in a R/ sub-folder. Both strategies would make the function available for the app. Deciding which to use comes down to the intended organizational structure of the application.\nThe next step is to apply functional programming to iterate the make_summary_tile() function over the text outputs. Since the function requires two inputs, title and text_output, they were placed inside of a tibble to improve organization of the inputs being passed to the function through pmap().\n\n# Defined in the Setup section\ntiles &lt;- tribble(\n  ~header          , ~text_output    ,\n  \"Users\"          , \"users\"         ,\n  \"Page Views\"     , \"page_view\"     ,\n  \"Session Starts\" , \"session_start\" ,\n  \"Purchases\"      , \"purchase\"\n)\n\n# Used within the UI\npmap(tiles, make_summary_tile)\n\nWhat once required sixteen line‚Äôs of code was cut in half to eight (including the explicit definition of the inputs). In addition, coding the tiles using functional programming also makes it more flexible, where summary tiles could be easily added or taken away.\nDoing this would require some slight modification to the make_summary_tile() helper function, though. That is, a width argument would need to be added to the function, so the column width could be set to accommodate the number of outputs for the UI. There are lots of different options that could be explored here. At this point, though, the solution meets the functionality requirements.\nIn its current state, the application code looks like this:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\nmake_summary_tile &lt;- function(header, text_output) {\n  column(2, div(header), textOutput(text_output))\n}\n\ntiles &lt;- tribble(\n  ~header          , ~text_output    ,\n  \"Users\"          , \"users\"         ,\n  \"Page Views\"     , \"page_view\"     ,\n  \"Session Starts\" , \"session_start\" ,\n  \"Purchases\"      , \"purchase\"\n)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\",\n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n)\n\n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\n        \"date\",\n        \"Select a date for summary:\",\n        value = max(ga4_data$event_date),\n        max = max(ga4_data$event_date),\n        min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        pmap(tiles, make_summary_tile)\n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")\n      )\n    )\n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n\n  c('users', 'page_view', 'session_start', 'purchase') %&gt;%\n    walk(\n      ~ {\n        output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))\n      }\n    )\n\n  output$date &lt;- renderText(glue(\n    \"Estimates represent data from {data()$event_date}\"\n  ))\n}\n\nshinyApp(ui, server)\n\nThe application works, meets the functionality requirements, and now is written in a way that reduces repetition and redundant patterns within the code. However, the summary metric tiles just blend into the UI, and nothing about the styling communicates they contain important information.\nSince these elements are meant to highlight key, important summary metrics, they need to be styled in a way that creates contrast between themselves and the application‚Äôs background. The next section focuses on applying custom CSS to give some contrast between these elements and the application‚Äôs background."
  },
  {
    "objectID": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "href": "blog/posts/2021-12-30-post-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "title": "Shiny summary tiles",
    "section": "Creating the www folder and CSS file",
    "text": "Creating the www folder and CSS file\nSince the design opted for a file-based CSS approach, a separate www sub-folder in the application‚Äôs main project directory needs to be created. Once created, the custom CSS file will be placed inside this folder. The placement of this file can be seen in this Github repo.\nThe purpose of this folder is to make the file available to the web browser when the application starts. Placement of this file is critical. If it is not placed in the www sub-folder, then the CSS file will not be available when the application starts, and any custom styling will not be applied.\nOnce the www sub-folder is created, you can create a CSS file for the application in Rstudio by clicking File, hovering over New File, and selecting CSS File. Save the file in the www sub-folder and give it an informative name. In the case of this example, the file is named app-styling.css.\nThe main goal of the styling will be to create some contrast between the summary metric tiles and the application‚Äôs background. Specifically, CSS will be used to create a container that is a different color from the application‚Äôs background and includes some shading to make it seem like the element is hovering above the application‚Äôs main page. To do this, the app-styling.css file includes the following:\n#summary-tile{\n  font-size: 25px;\n  color:White;\n  text-align: center;\n  margin: 10px;\n  padding: 5px;\n  background-color: #0A145A;\n  border-radius: 15px;\n  box-shadow: 0 5px 20px 0 rgba(0,0,0, .25);\n  transition: transform 300ms;\n}\nA detailed description on how to create CSS selectors is outside the scope of this post. However, in general terms, this selector sets several values for multiple CSS properties by defining the id, #summary-tile within the file. More about this process of creating different CSS selectors can be found here.\nNow it‚Äôs just a matter of modifying the code to call this file and pass these style values to the summary tiles within the application. The following code is added to the ui side of our application to include our app-styling.css file:\n\ntags$head(tags$link(\n  rel = \"stylesheet\",\n  type = \"text/css\",\n  href = \"app-styling.css\"\n))\n\nSince the styling is being applied to the summary metric tiles, the make_summary_tile() function is modified to bring in the CSS elements. A css_id argument is added to the function.\n\nmake_summary_tile &lt;- function(header, text_output, css_id) {\n  column(2, div(header), textOutput(text_output), id = css_id)\n}\n\nNow that we made this modification to the make_summary_tile(), its application in the UI is also modified. Specifically, the #summary-tile CSS element is explicitly called in pmap(). To do this, the code is modified like this:\n\npmap(\n  tiles,\n  ~ make_summary_tile(\n    header = ..1,\n    text_output = ..2,\n    css_id = \"summary-tile\"\n  )\n)\n\nThe header, text_output, and css_id arguments are now explicitly defined in the pmap() call. To refer to the first two elements in the tiles data object, the ..1 (i.e., header column) and the ..2 (i.e., text_output column) are used. Check out the pmap() docs on how to apply the ..1, ..2 (?pmap) for more information."
  },
  {
    "objectID": "blog/posts/2024-12-08-til-data-wrangling-numeric-summaries-logical-vectors/index.html",
    "href": "blog/posts/2024-12-08-til-data-wrangling-numeric-summaries-logical-vectors/index.html",
    "title": "TIL: Summarize logical vectors to calculate numeric summaries",
    "section": "",
    "text": "Background\nToday I relearned you can easily calculate counts and proportions with a logical vector (e.g., c(TRUE, FALSE, FALSE, TRUE)) in R.\n\nlibrary(tidyverse)\nlibrary(ids)\n\nI‚Äôve been re-reading the second edition of R for Data Science for a Data Science Learning Community bookclub (check us out). While reading Chapter 12: Logical vectors, I was reminded counts and proportions can be calculated from a logical vector.\nI wanted to share what I learned out loud, so others have another example. I also hope writing this post helps me remember it in the future.\n\n\nSummaries from logical vectors\nThe concept is simple:\n\nsum() gives the number of TRUEs and mean() gives the proportion of TRUEs (because mean() is just sum() divided by length())\n\nThis works because TRUE = 1 and FALSE = 0 in the R programming language.\nLet‚Äôs look at an example of this in action. We start by creating an example dataset, which builds on the example used in the book:\n\ndata_user_engagement &lt;- data.frame(\n  date = sort(rep(\n    seq(as_date(\"2024-12-02\"), as_date(\"2024-12-08\"), by = 1),\n    times = 100\n  )),\n  user_id = rep(random_id(bytes = 4, n = 100), times = 7),\n  time_engaged_sec = sample(c(1:100), 700, replace = TRUE)\n) |&gt;\n  tibble()\n\nThis dataset is loosely based on the domain I work in: digital analytics. It‚Äôs modeled after event-based timeseries data for a week of web site visits. The dataset contains the following columns:\n\ndate - The date the event occurred.\nuser_id - A 4-byte user ID.\ntime_engaged_sec - Time spent engaged during the event (e.g., time spent on a webpage).\n\nSome questions we might ask about this dataset are: How many daily events were considered low-engagement events for users? What proportion of events each day were users engaged? Here‚Äôs the code to answer these questions, leveraging the summarization of logical vectors:\n\ndata_user_engagement |&gt;\n  group_by(date) |&gt;\n  summarise(\n    count_low_engagement = sum(time_engaged_sec &lt;= 10, na.rm = TRUE),\n    proportion_engaged = mean(time_engaged_sec &gt;= 30, na.rm = TRUE)\n  )\n\n# A tibble: 7 √ó 3\n  date       count_low_engagement proportion_engaged\n  &lt;date&gt;                    &lt;int&gt;              &lt;dbl&gt;\n1 2024-12-02                    5               0.77\n2 2024-12-03                   12               0.68\n3 2024-12-04                   11               0.68\n4 2024-12-05                   11               0.72\n5 2024-12-06                   13               0.71\n6 2024-12-07                    9               0.63\n7 2024-12-08                   14               0.74\n\n\nAt first glance, you might ask where are the logical vectors? They‚Äôre created in the sum() and mean() functions when we use the &lt;= and &gt;= operators. That is, the statement time_engaged_sec &lt;= 10 initially creates the logical vector in the background, and then the sum() or mean() is computed on that logical vector.\nPretty neat, huh ?!\n\n\nWrap up\nThere are many other uses for logical vectors, but this was the most useful one I recently relearned. Check out Chapter 12: Logical vectors from the R4DS book to learn more.\nOne more tool to add to the analysis tool box. Thanks for spending time learning with me.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {TIL: {Summarize} Logical Vectors to Calculate Numeric\n    Summaries},\n  date = {2024-12-08},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúTIL: Summarize Logical Vectors to\nCalculate Numeric Summaries.‚Äù December 8, 2024."
  },
  {
    "objectID": "blog/posts/2024-07-10-hex-update-001/index.html",
    "href": "blog/posts/2024-07-10-hex-update-001/index.html",
    "title": "The Hex Update: Issue 001",
    "section": "",
    "text": "üëã Welcome to The Hex Update, a monthly blog post summarizing media indsutry topics I found to be interesting and relevant. I‚Äôm Collin, a media research analyst working in public media (learn more about me here).\nMy aim for these posts is to make sense of what‚Äôs happening in the media industry, one topic at a time. The Media Industry is big. There‚Äôs no way I could ever know everything. But I can attempt to learn and hopefully understand it a little more with every post I write. I learn best through writing. Thus, my intention is to treat each post as a form of ‚Äòlearning out loud,‚Äô where I share what I‚Äôm learning and thinking about on a monthly basis.\nIf you‚Äôre here looking for someone with all the answers, they‚Äôre not here. It would be naive of me to think I can provide answers to all the challenges facing the media industry. There are much smarter people than myself exploring these topics in depth and have more thoughtful answers to what‚Äôs impacting the media industry. Reach out; I can provide a list. One goal of these posts, though, is to be a space to highlight the views and work of these experts.\nYou may also come across my other work on this site, which includes posts more focused on my data science interests. For the time being, I‚Äôll lump all these posts together‚Äìyou‚Äôre welcome to deduct points from my content strategy. Feel free to explore these as well. If you‚Äôre the handful of readers looking for my data science work, I welcome you to read these posts, though you may be disappointed as they‚Äôre specifically focused on topics relevant to the media industry, and they may have little to do with data science."
  },
  {
    "objectID": "blog/posts/2024-07-10-hex-update-001/index.html#google-zero",
    "href": "blog/posts/2024-07-10-hex-update-001/index.html#google-zero",
    "title": "The Hex Update: Issue 001",
    "section": "Google Zero",
    "text": "Google Zero\nIf you have a website, you likely rely on referral traffic. What would happen to your site (or your business for that matter) if this traffic suddenly went away? Nilay Patel from the Verge‚Äôs Decoder Podcast has been exploring this topic recently. Nilay and folks (i.e., some major tech and media company CEOs) have some interesting thoughts and perspectives, since it‚Äôs been posited that Google Zero is already here.\n\nHere are some links to go deeper\n\nGoogle Zero is here - now what? (Decoder podcast)\nHow to play the long game, with New York Times CEO Meredith Kopit Levien (Decoder podcast). Listen around 19:50 to hear more discussion specific to Google Zero.\nFandom runs some of the Biggest communities on the internet - Can CEO Perkins Miller keep them happy? (Decoder podcast episode) Listen around 42:13 to hear more discussion closely to Google Zero.\n\n\n\nWhat‚Äôs the takeaway?\nI came away with several takeaways from these conversations, but I think it comes down to the idea that a business or media property can‚Äôt be built on referral traffic anymore. Or, at the very least, it‚Äôs on shaky ground if the business model solely relies on referral traffic from other platforms. Simply put, according to Nilay, if you can‚Äôt explain what your business is without referral traffic, then do you really have a business?\nBuilding an audience online is challenging. In fact, Nilay suggests that if you were to create a media business today, you‚Äôd likely start on some video platform rather than starting from the point of ‚Äòbuilding a website‚Äô first, followed by other tactics.\nIt‚Äôs about the value you create. If you‚Äôre creating content, products, and experiences people are willing to spend their time and money on, then referral traffic shouldn‚Äôt matter. Perhaps this is an opportunity to recenter the focus back to what‚Äôs being created, rather than creating content to rank high by gaming an algorithm. I know it‚Äôs aspirational but unrealistic when confronted with economic realities."
  },
  {
    "objectID": "blog/posts/2024-07-10-hex-update-001/index.html#googles-ai-overviews",
    "href": "blog/posts/2024-07-10-hex-update-001/index.html#googles-ai-overviews",
    "title": "The Hex Update: Issue 001",
    "section": "Google‚Äôs AI overviews",
    "text": "Google‚Äôs AI overviews\nIf you‚Äôve used Google as of late, your search queries have likely returned responses from Google Search‚Äôs new AI Overviews feature. The utility provided to users seems interesting. It‚Äôs certainly made my search experience a little better. However, I‚Äôve noticed some questions being raised about the feature as it relates to referrals: If users receive the answers they want natively within the AI overviews, will they still navigate to the sites of other publishers? According to Google, users are clicking links in AI overviews more than they would if the links were included as a traditional web listing for that query. However, data supporting this is yet to be released or made available to publishers. Some have even gone so far as to say this new feature could kill what‚Äôs left of the traffic sent to publishers, especially traffic sent to news publishers.\n\nHere are some links to go deeper\n\nPublishers horrified at new Google AI feature that could kill what‚Äôs left of journalism (The_Byte)\nNews publishers sound alarm on Google‚Äôs new AI-infused search, warn of ‚Äòcatastrophic‚Äô impacts (CNN Business)\nGoogle CEO Sundary Pichai on AI-powered search and the future of the web (Decoder Podcast). The episode also includes some discussion about Google Zero and AI overviews. Listen around 07:32.\n\n\n\nWhat‚Äôs the takeaway?\nI believe the effect of this new feature is yet to be seen. It just rolled out in late May. More data will certainly help publishers assess this feature‚Äôs impact on referral traffic. However, it may be important for publishers to explore and better understand how AI Overviews work within Search, so as to identify strategies that result in content to rank high and be linked within these overviews. Indeed, I‚Äôm aware of the contradiction with the points I made above."
  },
  {
    "objectID": "blog/posts/2024-07-10-hex-update-001/index.html#a-snapshot-of-how-news-directors-view-the-use-of-over-the-top-ott-services-and-nextgen-tv",
    "href": "blog/posts/2024-07-10-hex-update-001/index.html#a-snapshot-of-how-news-directors-view-the-use-of-over-the-top-ott-services-and-nextgen-tv",
    "title": "The Hex Update: Issue 001",
    "section": "A snapshot of how news directors view the use of over-the-top (OTT) services and NextGen TV",
    "text": "A snapshot of how news directors view the use of over-the-top (OTT) services and NextGen TV\nA recent survey report by RTDNA and the Syracuse University Newhouse School of Public Communications provides a snapshot on how broadcasters are using over-the-top services (OTT) and NextGen TV (ATSC 3.0).\nNextGen TV promises many enhancements for both consumers (i.e., better sound and video quality) and broadcasters (i.e., interactivity and addressibility), which is due to the combining of internet and television technologies. Although these enhancements make the future of TV exciting, the report makes a pretty blunt assertion:\n\nBut there‚Äôs a big difference between ‚Äúoperating‚Äù and actually doing something meaningful with [NextGen TV].\n\nSo how many broadcasters report doing something with NextGen TV? According to the report, only 20.9% of TV news directors say they are doing ‚Äúsomething‚Äù with NextGen TV, which is slightly down from last year (25%). What about OTT services? According to survey respondents, 59.9% say they are ‚Äúdoing something‚Äù with OTT services. When it comes to all TV, the report states OTT is allowing broadcasters to ‚Äòreach new audiences‚Äô (70.1%), ‚Äògo deeper with content‚Äô (57.1%), and ‚Äòmake extra revenue‚Äô (36.4%). The report includes a further breakdown by market size, which provides some more relevant context (i.e., broadcasters operating in smaller markets may not have the resources to manage their own OTT services).\n\nHere‚Äôs a link to go deeper\n\nLocal TV explores OTT, NextGen, AI\n\n\n\nWhat‚Äôs the takeaway?\nAlthough NextGen TV is purported to provide many enhancements, adoption faces many hurdles. Many of which I myself‚Äìfrom a technology, tv viewer adoption, and regulatory standpoint‚Äìdon‚Äôt fully understand. However, the report makes it clear: the majority of news directors have yet to do anything meaningful with NextGen TV. This certainly isn‚Äôt a criticism of the technology or about what it promises for audiences and broadcasters in the future. Rather it‚Äôs a question of whether the technology can overcome the obstacles it faces, be adopted by TV consumers, and become a technology utilized to deliver services.\nAs for OTT, these services are being utilized by a majority of broadcasters, especially to reach new audiences. Despite ‚Äòmaking extra revenue‚Äô being in the top 3, it was surprising to see the limited role OTT services play within broadcasters‚Äô business models. I assumed this percentage would be much higher than 36.4%. Perhaps new revenue models will be created and explored in the future, resulting in OTT services playing a greater role in revenue generating functions."
  },
  {
    "objectID": "blog/posts/2024-07-10-hex-update-001/index.html#audience-segmentation-dashboard",
    "href": "blog/posts/2024-07-10-hex-update-001/index.html#audience-segmentation-dashboard",
    "title": "The Hex Update: Issue 001",
    "section": "Audience segmentation dashboard",
    "text": "Audience segmentation dashboard\nOne critical function for media organizations is to identify and understand their target audiences. According to researchers from the Northwestern University Spiegel Research Center, this process tends to be more gut-driven rather than data-driven. To aid this process and make it more data-driven, Jaewon Royce Choi created a dashboard that maps audience segments based on population data from various zip codes. It provides a useful interface to explore the various types of audiences reached when targeting different geographic locations. Check it out if you‚Äôre a community focused media organization who wants to make more data-driven audience targeting decisions.\n\nHere‚Äôs a link to go deeper\n\nSpiegel Audience Segment Dashboard\n\n\n\nWhat‚Äôs the takeaway?\nThe identification and exploration of target audiences can certainly be more data driven, if not at least more data informed. This tool is useful in this case; it makes data accessible to media organizations to allow for audience targeting decisions to be more data driven. Check it out and explore the communities your organization serves.\nAside from being a useful tool to explore various audience segments, this was a great reminder that a plethora of public data is available to make more data driven decisions when it comes to audience. Products just need to be developed to make this data more accessible to practitioners."
  },
  {
    "objectID": "blog/posts/2024-07-10-hex-update-001/index.html#are-audiences-worn-out-by-the-news",
    "href": "blog/posts/2024-07-10-hex-update-001/index.html#are-audiences-worn-out-by-the-news",
    "title": "The Hex Update: Issue 001",
    "section": "Are audiences worn out by the news?",
    "text": "Are audiences worn out by the news?\nA recent study released by the Pew Research Center explored how American‚Äôs get their news on various social media platforms. Although the study reports American‚Äôs news consumption on various social media platforms (e.g., Facebook, Instagram, X, and TikTok), I found the results on how worn out news consumers are with news on social media platforms informative. Specifically,\n\nMore than half of news consumers on three of the four sites studied at least sometimes feel worn out by the amount of news they see on these sites.\n\nThe report goes further by breaking down these results by each platform.\n\nHere‚Äôs a link to go deeper\n\nHow Americans Get News on TikTok, X, Facebook and Instagram\n\n\n\nWhat‚Äôs the takeawy?\nMy initial reaction to this finding was to posit additional questions. What are the factors leading to this feeling of being worn out by the news on these platforms? Is it due to the amount of news users are confronted with? Is it a function of the type of news or how it is being produced? Or, does misinformation and disinformation play a role? I certainly don‚Äôt have the answers to these questions. However, publishers may want to consider factors they can control when publishing their content on these platforms. Moreover, publishers may also want to explore these questions with their audiences."
  },
  {
    "objectID": "blog/posts/2025-01-27-til-notes-github-projects/index.html",
    "href": "blog/posts/2025-01-27-til-notes-github-projects/index.html",
    "title": "Notes: The use and management of GitHub projects",
    "section": "",
    "text": "I recently spent some time focusing on my approach to project management, in hopes of developing skills to be a better team lead. One way I try to manage work is using GitHub Projects.\nBelow are some links and notes about what I‚Äôve recently learned about GitHub projects.\nIn the spirit of attempting to do more link- and micro-blogging, some of these notes may seem disjointed, incomplete, or incohorent. However, it‚Äôs what I‚Äôve learned thus far about managing projects in GitHub."
  },
  {
    "objectID": "blog/posts/2025-01-27-til-notes-github-projects/index.html#adding-issues-to-a-project",
    "href": "blog/posts/2025-01-27-til-notes-github-projects/index.html#adding-issues-to-a-project",
    "title": "Notes: The use and management of GitHub projects",
    "section": "Adding issues to a project",
    "text": "Adding issues to a project\nUse the ‚Äò+ Add item‚Äô to add issues or pull requests to a project. You can then locate the issue via the UI prompts, or you can paste the issue or pull request URL (very helpful). The bulk add issues and pull requests and Adding multiple issues or pull requests from a repository features seem really useful."
  },
  {
    "objectID": "blog/posts/2025-01-27-til-notes-github-projects/index.html#adding-fields-to-help-manage-projects",
    "href": "blog/posts/2025-01-27-til-notes-github-projects/index.html#adding-fields-to-help-manage-projects",
    "title": "Notes: The use and management of GitHub projects",
    "section": "Adding fields to help manage projects",
    "text": "Adding fields to help manage projects\nAn iteration field seems useful for managing sprint intervals. More about how to setup an iteration field can be found here."
  },
  {
    "objectID": "blog/posts/2025-01-27-til-notes-github-projects/index.html#useful-commands-for-managing-issues-and-projects",
    "href": "blog/posts/2025-01-27-til-notes-github-projects/index.html#useful-commands-for-managing-issues-and-projects",
    "title": "Notes: The use and management of GitHub projects",
    "section": "Useful commands for managing issues and projects",
    "text": "Useful commands for managing issues and projects\nCreate an issue with an interactive prompt.\ngh issue create\nSometimes you fall into common patterns when creating issues, so command flags are often helpful.\ngh issue create\ngh issue create -a \"@me\" -t \"New project to work on\" -l \"project\"\ngh issue create -a \"@coworker\" -t \"Fix this\" -l \"bug\"\nMore about how to manage issues in a repo via the command line can be found here.\nYou can list all your projects using the following commands:\n# Your projects\ngh project list\n\n# Organizational owned projects\ngh project list --owner owner_of_project\nView the project either in the command line or open it in a web browser.\n# From command-line\ngh project view 5\n\n# Open in web browser\ngh project view 5 --web\nYou can edit the project README by running the following:\ngh project edit 5 --readme \"Here be some info about the project\"\nHowever, this command seems to only allow you to add a new README and not edit it."
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html",
    "title": "2023 data science rig: Set up and configuration",
    "section": "",
    "text": "What the new year brings to you will depend a great deal on what you bring to the new year.\n‚Äì Vern McLellan"
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "title": "2023 data science rig: Set up and configuration",
    "section": "A few extra configs to the operating system",
    "text": "A few extra configs to the operating system\nI also like to customize the appearance, system keymappings, and terminal aliases (more on this in the section on setting up Zsh) of my operating system. For one, I‚Äôm a fan of dark mode, so I set the system settings accordingly. I‚Äôm also a minimalist when it comes to the menu dock. I prefer to only include shortcuts that are necessary to my workflow. I also like to change the settings to automatically hide the dock when it‚Äôs not being used. I do this to maximize my workspace area. Here is a link to some docs if you‚Äôre interested in modifying your macOS system settings.\nThe caps lock key is useless in my workflow. Instead, I remap the ctrl key to the caps lock key. This is mostly done out of convenience, as I‚Äôll use my machine as a true laptop from time to time. This is also essential because my IDE, Neovim, requires extensive use of the ctrl keys (more on the use of Neovim later). Since the MacBook Pro does not include a right-hand side ctrl key, and the left-hand side ctrl key is not in a comfortable position, this remap affords me some additional comfort when I use my machine as a laptop."
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#homebrew",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#homebrew",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Homebrew",
    "text": "Homebrew\nHomebrew coins itself as the missing package manager for macOS (or Linux). It makes downloading open-source software much easier. Downloading and installing Homebrew is straight forward. Run the following command in a terminal to download Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nIf you need more specific instructions on downloading and installing Homebrew, check out the docs I linked above. With the Homebrew package manager installed, it‚Äôs a cinch to download other tools."
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#oh-my-zsh",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#oh-my-zsh",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Oh My Zsh",
    "text": "Oh My Zsh\nNow it‚Äôs time to unleash the terminal by downloading Oh My Zsh. Download Oh My Zsh by running the following in your terminal:\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nOh My Zsh‚Äôs docs contain the best description of what it does:\n\nOh My Zsh will not make you a 10x developer‚Ä¶but you may feel like one.\n‚Äì Zsh docs\n\nFor reals though, Oh My Zsh is a convenient, intuitive means to configure your terminal. For one, it allows plugin installation. Plugins enhance the terminal experience and extend its utility. The following is a list of Zsh plugins I find useful:\n\ngit\nzsh-syntax-highlighting for terminal syntax highlighting.\nzsh-autosuggestions for command suggestions based on previous history.\n\n\nCustomized Zsh prompt\nAnother great feature of Zsh is the ability to customize the command line prompt. Many options are available. For me, I like the prompt to contain four pieces of information:\n\nThe time (24-hours with seconds);\nThe file path of the current working directory;\ngit branch information;\nAn indicator if any uncommitted changes exist in the directory.\n\nHere is what my prompt looks like:\n\n\n\nCustomized Zsh prompt\n\n\nTo achieve this custom setup, I place the following into my .zshrc file:\n# Prompt formatting\nautoload -Uz add-zsh-hook vcs_info\nsetopt prompt_subst\nadd-zsh-hook precmd vcs_info\nPROMPT='%F{blue}%*%f %F{green}%~%f %F{white}${vcs_info_msg_0_}%f$ '\n\nzstyle ':vcs_info:*' check-for-changes true\nzstyle ':vcs_info:*' unstagedstr ' *'\nzstyle ':vcs_info:*' stagedstr ' +'\nzstyle ':vcs_info:git:*' formats '(%b%u%c)'\nzstyle ':vcs_info:git:*' actionformats '(%b|%a%u%c)'\nIndeed, this might not be the custom prompt for everyone. So, the following are links to blog posts that do an excellent job describing how to customize the different prompt elements:\n\nCustomizing my Zsh Prompt by Cassidy Williams\nCustomize your ZSH prompt with vcs_info by Arjan van der Gaag\n\n\n\nTerminal aliases\nThis year, I focused on transitioning to a more terminal based workflow. As part of this transition, I began utilizing terminal aliases. Aliases can be used to automate common tasks, like opening specific programs, web pages, or project files from the terminal.\nWith Zsh, creating aliases is pretty straightforward. To do this, you‚Äôll need to place a file into the ~/oh-my-zsh/custom directory. This file can be named anything, but it needs to end in the .zsh extension. In this file you can include aliases like the following:\n# aliases to improve productivity\nalias email=\"open https://path-to-email.com/mail/inbox\"\nalias calendar=\"open https://path-to-calendar.com/\"\nalias projects=\"open https://path-to-github-projects.com/\"\nNow if you run email in your terminal prompt, a browser window with your email inbox will open. The above is just an example to get you started. I have additional aliases beyond the ones in the example. To get an idea of all the aliases I use, check out the dotfile here. You can customize any of these to your specific needs.\nThe rest of my Zsh configuration is pretty standard. Here is a link to a repo containing additional files to configure Zsh. Check it out if you‚Äôre interested in seeing how I specifically do something."
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#additional-terminal-utilities",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#additional-terminal-utilities",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Additional terminal utilities",
    "text": "Additional terminal utilities\n\nJump\nNavigating the file system from the terminal can be tiring. Jump is a terminal utility that solves this problem. Simply put, this utility learns your navigational habits and allows you to easily jump back and forth between directories with very little typing.\nInstall jump using Homebrew. Run the following code in your terminal to install Jump:\nbrew install jump\n\n\ntmux\ntmux is a terminal multiplexer. It lets you create multiple windows and terminals in a single session. I find it useful in situations where you want multiple files, projects, or terminal windows to be open while you‚Äôre working.\nInstall tmux using Homebrew:\nbrew install tmux\nAlthough tmux is useful out of the box, some configuration steps are needed to make it more useful. My configuration mostly changes tmux‚Äôs keymaps, which makes them easier to remember and use (i.e., some of the defaults require some keyboard gymnastics).\nMuch of my tmux configuration is a derivative of the one discussed in the Getting Started with: tmux YouTube series from Learn Linux TV. If you want some more specific detail, you can check out my .tmux.conf configuration file here.\n\n\ngit\nI use git for version control. Homebrew can be used to install git:\nbrew install git\nSome additional configuration is needed for the local setup of git. Run the following code in the terminal. Make sure to replace what is in quotations with your information.\ngit config --global user.name \"&lt;full-name&gt;\"\ngit config --global user.email \"&lt;email&gt;\"\ngit config --global core.editor \"nvim\"\nThe user.name and user.email variables are required. You can exclude the core.editor configuration if you want to use the default editor. However, I like to use Neovim (more on Neovim in a later section) as my text editor, so I make it my default when working with git.\nAlong with git, I use GitHub for remote repositories. Some additional steps are needed to authenticate with this service. The GitHub CLI tool simplifies these steps.\n\n\nGitHub‚Äôs CLI tool\nBring GitHub to your command line with the GitHub CLI. This tool provides commands to do many of the same things you do on GitHub, but with terminal commands. Need to create an issue in a repo, run the following in your terminal:\ngh create issue\nWant to see all the pull requests in a repo needing review, run the following in your terminal:\ngh pr list\nYou can also use these commands within aliases to streamline your workflows. I particularly like my custom aliases to list and create issues and PRs.\n# Custom alias to list GitHub issues\n.il\n\n# Custom alias to create an issue\n.ic\nHomebrew, again, is used for the installation.\nbrew install gh\n\nAuthenticate using the GitHub CLI\nOnce installed, run the gh auth login command to walk you through the authentication flow. During the flow, you‚Äôll have to make a few decisions. Your first decision will be the protocol you want to use for git operations. I select HTTPS. Second, you‚Äôll need to decide how you want to authenticate the GitHub CLI. I select the web browser setting out of convenience. If you‚Äôre interested in other forms of authentication, I suggest checking out GitHub‚Äôs docs.\nOne minor, additional configuration step is to set Neovim as the default editor for use with the GitHub CLI. If you want to use the default editor, then skip this step. To modify the default editor, run the following command in the terminal:\ngh config set editor nvim"
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#install-rig",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#install-rig",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install rig",
    "text": "Install rig\nHomebrew handles the installation of rig. Run the following in your terminal:\nbrew tap r-lib/rig\nbrew install --cask rig"
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#install-the-most-recent-version-of-r",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#install-the-most-recent-version-of-r",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install the most recent version of R",
    "text": "Install the most recent version of R\nOnce rig is installed, download the most recent version of R by running the following in the terminal:\nrig add\nOnce the most recent version is downloaded, you can verify the installation was successful by printing out a list of all the R versions installed on your machine. If this is a fresh start on a new machine or it‚Äôs your first time downloading R, you should only see one version listed.\nrig list"
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#download-rstudio",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#download-rstudio",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Download RStudio",
    "text": "Download RStudio\nAlthough I have made the switch to using a different IDE (more on this in the next section), I still teach classes and present to groups who mainly use RStudio. So to keep everything up to date and in synch, I download the current version of RStudio using Homebrew:\nbrew install --cask rstudio\nrig also makes it easy to open up a new session of RStudio from the terminal. To do this, run the following in the terminal:\nrig rstudio"
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#install-r-packages",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#install-r-packages",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install R packages",
    "text": "Install R packages\nThis section overviews the installation of R packages I use most often. Indeed, it would be excessive to download and overview all the packages in my workflow. In addition, the following sections contain a brief description of what each package does and how it is used when I work with R.\nThe following code downloads packages I rely on most. If you use R, many of these packages will be familiar.\n\ninstall.packages(c(\n  devtools,\n  usethis,\n  roxygen2,\n  tidyverse,\n  lubridate,\n  testthat,\n  googleAnalyticsR,\n  bigrquery\n))\n\nIf you‚Äôre unfamiliar with loading packages in R, you‚Äôll need to run this code an R console. This can be done either in RStudio or via an iTerm2 system terminal. From the system terminal, type the letter R and hit Enter. Doing this should change your terminal prompt, as you are now running in a R session. You‚Äôll then run the code from above. Information will be printed to the terminal during the installation of the packages.\nOnce all these packages have been installed, run the quit() function to return back to the system‚Äôs original prompt. When quitting this R session, you may be prompted to save the workspace. Enter no, as there is no need to save this session‚Äôs information. The next few sections provide a brief description of how each of the installed packages are used within my workflow.\n\ndevtools\n\nThe aim of devtools is to make your life as a package developer easier by providing R functions that simplify many common tasks.\n‚Äì devtools package docs\n\nSimply put, I rely on devtools for package development. This package provides many convenience functions to manage the mundane tasks involved in package development.\n\n\nusethis\nusethis is a workflow package. It automates many tasks involved when setting up a project. It also contains convenience functions to help with other R project workflow tasks. I‚Äôm still exploring all the package‚Äôs functions, but using the one‚Äôs I‚Äôve learned have made me more productive.\n\n\nroxygen2\nPackages need documentation. The roxygen2 package helps with the documentation setup and development process. If you‚Äôre familair with comments in R, you‚Äôll find writing package documentation with roxygen2 intuitive.\n\n\ntidyverse\ntidyverse is mainly used for common data wrangling and analysis tasks. Although I use base functions from time-to-time, I learned R by using tidyverse packages; they‚Äôre ingrained throughout my workflow.\nIndeed, the tidyverse is not just a single package, but a collection of packages. Some of the tidyverse packages I rely on most often include:\n\nggplot2 for data visualization\ndplyr for manipulating data\ntidyr for common data tidying tasks\npurrr for functional programming\nstringr for working with string data\n\n\n\nlubridate\nlubridate is magic when it comes to working with date-time data. I use this package mostly to handle data with a time dimension, which usually occurs in cases where I‚Äôm working with and analyzing time series data. If you work with date time data, look into using lubridate.\n\n\ntestthat\nThe testhat package is used for writing tests (e.g., unit tests) for code, especially when developing a package. To write more robust code, it‚Äôs best practice to write tests. testthat provides a framework and several convenience functions to make composing tests more enjoyable.\n\n\ngoogleAnalyticsR\nPart of my work involves the analysis of web analytics data. Much of this data is collected with and made available via Google Analytics. googleAnalyticsR is a package that allows you to authenticate and export web analytics data using the Google Analytics Reporting API.\n\n\nbigrquery\nGoogle BigQuery is a data warehouse and analytics solution. To access data via its API, I rely on the bigrquery package. This package provides multiple convenience functions to extract, transform, and load data from and into BigQuery. bigrquery also provides several functions to perform some BigQuery administrative tasks.\nThe packages highlighted above are ones I rely on most often in my day-to-day workflow. Indeed, others are used less frequently, especially when performing specific analysis tasks. However, the use of some packages is project dependent and describing all the packages I use would be outside the scope of this post."
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Google Cloud Command Line Interface (CLI)",
    "text": "Google Cloud Command Line Interface (CLI)\nI mainly use the Google Cloud Platform (GCP) for cloud based project development. Although I‚Äôll use GCP‚Äôs web portal occasionally, the command line interface provides some useful utilities to work from the terminal. The Google Cloud CLI is made available by installing the Google Cloud Software Development Kit (SDK).\nGoogle BigQuery, a data warehouse solution, is a GCP service I use quite often. The Google Cloud CLI has the bq command, which is an interface with BigQuery. I also manage some compute instances in the cloud, so I use the gcloud compute instances command as well.\n\nInstalling the Google Cloud SDK\nInstall the GCP SDK with Homebrew. To download, run the following code in your terminal:\nbrew install --cask google-cloud-sdk\n\n\nAuthorizing the Google Cloud CLI\nYou can review Google Cloud CLI‚Äôs authentication steps here. I provided the link to these docs because depending on your current setup and needs, you may need to use different steps to authenticate. Most likely, though, if you‚Äôre intending to authenticate with a user account, you can run the following command in your terminal to walk through the authentication steps:\ngcloud init\nAgain, it‚Äôs best to review the docs linked above, so you‚Äôre aware of the steps needed to authenticate with your specific setup."
  },
  {
    "objectID": "blog/posts/2023-01-29-post-2023-rig/index.html#neovim",
    "href": "blog/posts/2023-01-29-post-2023-rig/index.html#neovim",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Neovim",
    "text": "Neovim\nTo be honest, there was no real reason why I chose Neovim. I just saw others using and suggesting to give it a try. I did briefly read some of the arguments for why Neovim is a good choice, though. From my shallow reading of the topic, most of the arguments I came across pertained to Neovim‚Äôs use of the lua programming language, a better plugin management experience, and some additional points that made it appealing. In fact, Neovim is considered to be an extension of Vim, rather than its own stand-alone text editor. It aims to be extensible, usable, and retain the good parts of Vim. Now, I haven‚Äôt developed a sufficient understanding of these arguments to fully articulate the benefits of using one Vim like editor from another. I just know I‚Äôm enjoying it thus far. I suggest giving it a try.\n\nInstalling Neovim\nHomebrew is used to download Neovim.\nbrew install neovim\n\n\nConfiguring Neovim\nAs mentioned in the intro to this section, Neovim‚Äôs setup and configuration can be its own series of posts; there are so many options and plugins available. The focus of the following sections is to draw attention to some of the tools I find useful when working in Neovim. Keep in mind, the configuration of Neovim is a bit of a learning curve. It can be frustrating when you first start, but very rewarding at times. You can review my configuration files here.\nMy Neovim setup is based on chris@machine‚Äôs Neovim from Scratch YouTube tutorial series. This series does an excellent job overviewing a complete Neovim setup using the Lua programming language. While my setup is mostly based on the one described in this series, I have added some custom configuration for my specific workflow.\n\n\nNeovim package manager\nI use packer for plugin management. Packer simplifies plugin installation. For example, here is the Lua code to install some plugins I highlight in the following sections:\nreturn packer.startup(function(use)\n    use(\"wbthomason/packer.nvim\") -- Have packer manage itself\n    use(\"jalvesaq/Nvim-R\") -- Tools to work with R in nvim\n\n    -- Colorschemes\n    use(\"lunarvim/colorschemes\") -- A selection of various colorschemes\n    use(\"tomasiser/vim-code-dark\")\n    use(\"EdenEast/nightfox.nvim\")\n    use(\"folke/tokyonight.nvim\")\n\n    -- LSP\n    use(\"neovim/nvim-lspconfig\") -- enable LSP\n    use(\"williamboman/mason.nvim\")\n    use(\"williamboman/mason-lspconfig.nvim\")\n\n    -- Telescope\n    use(\"nvim-telescope/telescope.nvim\")\n\n    -- Treesitter\n    use({\n        \"nvim-treesitter/nvim-treesitter\",\n        run = \":TSUpdate\",\n    })\n\n    -- Git\n    use(\"lewis6991/gitsigns.nvim\")\n    use(\"tpope/vim-fugitive\")\n\n    if PACKER_BOOTSTRAP then\n        require(\"packer\").sync()\n    end\nend)\nThis code might not make much sense, as I only included a snippet of the code needed to install plugins I use most often. It‚Äôs mainly intended to show with a few lines of code, packer can manage all the plugin installation steps. This example code deviates slightly from the original packer docs on how to install plugins. Check out the previously linked docs if you would like an alternative setup while using Packer.\nHere is a link to a file with all the plugins I use in my setup. Admittedly, some plugins are carry overs from chris@machine‚Äôs YouTube series, and I will fully admit I‚Äôm still learning the reason why some of these plugins are present within my configuration. Thus, my setup is not as lean as I would like it to be. But hey, I‚Äôm still learning.\n\n\nNeovim plugins\n\nNvim-R\nSince I mostly work with R, I use Nvim-R to write code and interact with the R console directly in Neovim. Nvim-R provides utilities to have the Vim experience, while also affording interactive analysis right at your fingertips. Here is what a session using Nvim-R looks like:\n\n\n\nNvim-r running in Neovim\n\n\nThe power of Nvim-R comes from its predefined keybindings keybindings, which allow you to quickly and easily do interactive analysis tasks using just a few keystrokes. I‚Äôve found it‚Äôs the best option to work with R in Neovim. A whole blog post could be written about the use of Nvim-R, and I only hit the highlights here. I highly suggest checking it out if you‚Äôre looking to write R code with Neovim.\n\n\nvim-devtools-plugin\nAs mentioned above, I use devtools for package development. To leverage its functionality in Neovim, I use the vim-devtools-plugin. This plugin provides several convenient commands to run different devtools functions. This is especially useful as you can configure keymaps to these commands for added convenience and speed.\n\n\nTelescope\nFind, filter, preview, and pick. Telescope is great at these actions. Specifically, Telescope is a fuzzy file finder. However, it provides additional features that go beyond just working with a project‚Äôs files. I‚Äôm attempting to use it more and more in my workflow, as I mostly use it to find and navigate to specific files. However, I‚Äôve begun to explore more of its functionality and integration with git.\n\n\nvim-fugitive\nDo yourself a favor, use vim-fugitive. Fugitive is a plugin that helps you work with Git while working in Neovim. In the past, my git and GitHub workflow was mainly done from the command line. However, jumping in and out of Neovim back to run this workflow became old quickly. To solve this, Fugitive provides the :Git or :G command to call git commands directly from the editor. Also, since I use Neovim as my editor for commit messages, I‚Äôm able to directly compose them without having to leave my current Neovim session.\n\n\nLSP\nNeovim supports the Language Server Protocol (LSP). LSP provides many different features. This includes go-to-definition (a great feature that speeds up editing), find references, hover, completion, and many other types of functionality. Most IDEs have LSP set up out-of-the-box. This is done so you can get started quickly working with any language without too much configuration.\nNeovim does provide an LSP client, but you‚Äôll have to set up the individual servers for each language you would like to work with. This sounds harder then it is, but it does take a few steps to complete. A good rundown can be found in this video here, which is from the Neovim series I linked above. I recently made the switch over to the Mason plugin, which makes LSP server management so much simpler. I would suggest checking it out if you‚Äôre intending to work with other languages in Neovim.\nI‚Äôm still learning about LSP and how to set it up. I highly suggest reading up on the docs and reviewing other‚Äôs setups rather than relying solely on mine. Mine is still a work in progress.\n\n\nnvim-treesitter\nSetting up syntax highlighting is another important step when setting up Neovim. I use nvim-treesitter to improve the syntax highlighting within Neovim. This is another advanced topic I‚Äôm still learning about, so I just use a basic setup. You can read more about it in the plugin docs linked earlier.\n\n\nCustom Neovim keymaps\nSince Neovim is all about customization of your development environment, one thing to modify is Neovim‚Äôs keymaps. To configure, you just have to define the configuration in your Neovim config files. For example, the following is some code I use to customize my keymap setup.\n-- BigQuery keymappings\nkeymap(\"n\", \"&lt;C-b&gt;\", \":w | :! bq query &lt; % --format pretty &lt;CR&gt;\", opts)\n\n-- R coding keymappings\nkeymap(\"n\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"i\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;ts\", \":RStop&lt;cr&gt;\", opts)\nkeymap(\"n\", \"tt\", \"&lt;Esc&gt;&lt;C-w&gt;&lt;C-w&gt;i\", opts)\n\n-- R devtools keymappings\nkeymap(\"n\", \"&lt;leader&gt;I\", \":RInstallPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;L\", \":RLoadPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;B\", \":RBuildPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;E\", \":RCheckPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;T\", \":RTestPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;D\", \":RDocumentPackage&lt;Esc&gt;\", opts)\nI have additional custom keymappings in my setup, but including the entire file would be too much for this post. Nevertheless, you can access my keymapping configuration files to get a sense of other keymaps I have within my setup."
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Here you‚Äôll find the materials for talks I‚Äôve given.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "now-old.html",
    "href": "now-old.html",
    "title": "Past Now Page Updates",
    "section": "",
    "text": "2024-12-14 update\n\nBurning it all down and building it back up, my Neovim setup\nIt was inevitable; I‚Äôm revamping my Neovim configuration. It started with a kickstarter, but now I‚Äôm looking to further customize. I‚Äôm also attempting to address some bugs that have creeped into my configuration. I‚Äôve recently come to the conclusion that I need to burn everything down and start anew. Alas, I‚Äôm not weary. It‚Äôs another great learning experience.\n\n\nAttended Posit::conf(2023) Chicago\nI was fortunate to attend this year‚Äôs Posit conference in Chicago, a five day conference with workshops and speaker sessions. I‚Äôm still sorting through all that I learned. Here are some of the highlights:\n\nAttending the Introduction to Tidymodels workshop\nAttending the Package Development Masterclass workshop\nAll the keynote speakers, especially J.D. Long‚Äôs It‚Äôs Abstractions All the Way Down talk.\nNetworking and meeting some folks from the R4DS Online Learning Community (in person).\n\n\n\nExperimenting more with Tableau and plotly\n\nIf the only tool you have is a hammer, it is tempting to treat everything as if it were a nail.\n- Abraham Maslow\n\nAlthough I prefer to do the majority of my analysis and visualization work using code based tools like R, I‚Äôm attempting to broaden my experience by experimenting more with different BI tools. This includes Tableau. I‚Äôve also been experimenting with creating interactive data visualizations with plotly. To develop these skills, I‚Äôve been contributing to the #tidytuesday social data project. Check out my recent blog posts to view my contributions. If you‚Äôre interested in what I‚Äôve created using Tableau, check out my public profile.\n\n\n\n2024-03-17 update\n\nWorking on a 30 day tidymodels recipes package challenge\nI‚Äôve been developing my machine learning and modeling skills. Specifically, I‚Äôve been focusing on learning the various steps to preprocess data for modelling and feature engineering. This includes becoming more proficient with the tidymodels recipes package. Check out my post to keep up with the latest.\n\n\nContinuing to experiment with Neovim\nI‚Äôve heavily leaned into using Neovim for most of my workflows. During this transition, I‚Äôve come across some really useful tools, like telescope, harpoon, and vim-fugitive. I‚Äôm trying to learn as much as I can, as I still don‚Äôt know if my configuration is correct ‚Ä¶ lol\n\n\n\n2024-01-15 update\n\nIt‚Äôs a Python summer\nThis summer I‚Äôve been focusing on developing my Python programming skills. I have a pretty good handle of R, so I felt it was time to learn Python. This started with learning Python‚Äôs basic data types, struggling through understanding Numpy arrays, getting a handle on the extensive use cases of the pandas library, and learning how to manage environments using conda. I‚Äôm aiming to write more blog posts focused in this space to document what I‚Äôm learning.\n\n\n\nPre 2024-01-15 updates\n\nProject Conduit\nCurrently developing and maintaining a data pipeline project built using R and Python. Technology utilized includes Google Cloud resources, Docker, Apache Airflow, Google BigQuery, Google Analytics, Google Data Studio, and Shiny. The goal is to centralize and automate data processing, storage, and reporting.\n\n\nR for Data Science Online Learning Community book club facilitator\nRecently started facilitating an R for Data Science Online Learning Community online book club (check it out by joining the Slack workspace). This group is currently reading through Hadley Wickham‚Äôs Advanced R book. The group meets weekly online over Zoom. Meetings are open to anyone who is a part of the Slack group (Join the #book_club-advr channel to keep up with the book club). Check out the playlist of past meeting recordings here. I would love for more to join and be a part of this group.\n\n\nExperimenting with Neovim\nI‚Äôve been experimenting more and more with Neovim for my development work. I‚Äôm becoming more comfortable with the different modes, movements, actions, and various tools for editing text and code. Still struggling through the configuration and plugin ecosystem to set up workflows that are the most productive. Going through this process has been a challenge, but has me really reflecting on how I approach my work, evaluating what is needed, not needed, and focusing on the bad habits I need to break."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Keeping public media RAD: Leveraging R for audience research and business intelligence ‚Ä¶ along with other things\nCollin K. Berke, Ph.D."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#hi-im-collin",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#hi-im-collin",
    "title": "Collin K. Berke, Ph.D.",
    "section": "üëã Hi, I‚Äôm Collin",
    "text": "üëã Hi, I‚Äôm Collin\n\n   \n\nMedia Research Analyst at Nebraska Public Media (member station of PBS & NPR).\nGoal is to use data to answer questions on how to best reach and engage audiences.\nDeveloping open source tools and utilizing cloud computing resources to achieve this goal."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#keeping-public-media-rad",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#keeping-public-media-rad",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Keeping public media RAD üòé",
    "text": "Keeping public media RAD üòé\nMy team‚Äôs aim:\n\nReproducible\nAutomated\nDocumented and effectively communicated"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#how-does-my-group-stay-rad",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#how-does-my-group-stay-rad",
    "title": "Collin K. Berke, Ph.D.",
    "section": "How does my group stay RAD?",
    "text": "How does my group stay RAD?\n\nPublic media already has some rad content and services ‚Ä¶\n\nCheck us out: NebraskaPublicMedia.org\n\nMy team also works on some pretty RAD projects:\n\nInternal R packages for audience and marketing research.\nAutomating our work with the Google Cloud Platform.\nDeveloping Shiny apps for business intelligence.\nUtilizing Quarto for reporting."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#teaching-r",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#teaching-r",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Teaching R",
    "text": "Teaching R\n\nPartnering with the College of Journalism and Mass Communication (COJMC) and Matt Waite\nTeach SPMC 350: Sports Data Visualization and Analysis\nGoal: Convert R beginners to R users\nIntroduce R &gt;&gt; Wrangling &gt;&gt; Analysis &gt;&gt; Visualization &gt;&gt; Publish original work online"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#data-science-learning-community",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#data-science-learning-community",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Data Science Learning Community",
    "text": "Data Science Learning Community\n\nA welcoming place to get help and learn more about data science.\nWe‚Äôre known for our book clubs.\nI‚Äôm an active member and facilitator.\nI‚Äôm also leading a cohort for R4DS (2e).\nCheck it out:\n\n\n\n\n\nWe just got done facilitating a Quarto website workshop"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-1",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-1",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Oh and one more thing ‚Ä¶"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-2",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-2",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Analysis at the speed of thought üöÄ"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#im-a-huge-modal-editing-nerd",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#im-a-huge-modal-editing-nerd",
    "title": "Collin K. Berke, Ph.D.",
    "section": "I‚Äôm a huge modal editing nerd",
    "text": "I‚Äôm a huge modal editing nerd\n\nNeovim (specifically LazyVim) or Emacs.\nIf you use any of these tools, let‚Äôs geek out about them.\n\n\n\n\n\n\nNeovim logo by Jason Long, CC BY 3.0"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#connect-with-me",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#connect-with-me",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Connect with me!",
    "text": "Connect with me!\n\nI want to meet everyone, but I‚Äôm interested in speaking with:\n\nR Users and R Developers leveraging R in business.\nPeople who teach R.\nAnyone trying to be RAD by using R.\n\nIf we don‚Äôt get a chance to meet in person, let‚Äôs connect online."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-3",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-3",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Collin K. Berke, Ph.D.\n\n\nüë®‚Äçüíª Follow me on GitHub: @collinberke\nüëî LinkedIn: collinberke\nüìß Email: collin.berke@gmail.com\nüîó Blog: www.collinberke.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "I‚Äôm a media research analyst, data enthusiast, and news, sports, and podcast aficianado.\nProfessionally, I use data, audience measurement, and marketing research methods to answer questions on how to best reach and engage audiences‚Äìtowards the goal of enriching lives and engaging minds with public media content and services. I am particularly interested in the use and development of open-source statistical software (i.e.¬†R) to achieve this goal, and gaining a broader understanding of the role these tools play in media, digital, and marketing analytics. I also adjunct university courses on the side.\nListening to NPR, watching PBS (especially NOVA), and college football and baseball are my jam.\n\n\nWant to know more about what I‚Äôm currently working on, reading, or mastering? Check out the now page.\n\n\n\n\nPh.D.¬†in Media and Communication, 2017, Texas Tech University\nM.A.¬†in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota\n\n\n\n\n\nDigital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#now",
    "href": "index.html#now",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Want to know more about what I‚Äôm currently working on, reading, or mastering? Check out the now page."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Ph.D.¬†in Media and Communication, 2017, Texas Tech University\nM.A.¬†in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Digital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2025-12-05-hex-update-005/index.html",
    "href": "blog/posts/2025-12-05-hex-update-005/index.html",
    "title": "The Hex Update: Issue 005",
    "section": "",
    "text": "Let‚Äôs catch up\nWelcome folks! Here‚Äôs Issue 005.\nIf you‚Äôre in the US and celebrated, I hope you had a restful holiday. I unfortunately wasn‚Äôt able to get an issue out last week, as I was taking some much needed rest and prepping for the holiday. We all need the time to recharge, as the final push to end of the year is upon us.\nThis week‚Äôs issue contains an interesting collection of articles focused on tools, copyright, and NextGen TV (ATSC 3.0). Are you interested in open-source tools useful for investigative reporting work? Check out the Bellingcat‚Äôs Online Investigation Toolkit. Are you interested in what Roger Rabbit can teach us about US Copyright Law? Check out the second article. Or, are you curious about the current adoption of next generation television (i.e.¬†ATSC 3.0)? Then check out the last article. For a fun ending to the week, check out strudel: a programming language used to create music.\nLet‚Äôs get to the articles from this week.\n\n\nThree things from this week\nHere are three things that caught my attention this week.\nArticle: Lessons from Building an Online Toolkit to Aid Open-Source Investigations\nHere‚Äôs an interesting, useful read. This article highlights a recent project developed as part of a fellowship with Harvard‚Äôs Nieman Foundations for Journalism and the Berkman Klein Center for the Internet and Society by Johanna Wild. The goal of the project was to identify and make it easier for journalists to find and use open-source tools for investigative work. The project resulted in a volunteer created toolkit, which provides an easy to navigate interface and tool descriptions useful for investigative work. The article also discusses some of the lessons learned when building a collaborative toolkit.\nThe Bellingcat‚Äôs Online Investigation Toolkit can be found here.\n\nWhy does this matter?\nThe open-source ecosystem has so many useful tools for journalists and content producers. However, the challenge is identifying what is useful or relevant, given the fluidity and volume of the open-source ecosystem This toolkit does some of the heavy lifting. Not only is having filterable lists based on category useful, the product descriptions make it even easier to quickly decide if further research of a tool could lead to meaningful outcomes. I highly suggest checking this toolkit out if you‚Äôre doing any type of investigative work.\nBlog: Disney has lost Roger Rabbit\nIf you grew up around the time I did, you likely remember the 1988 film Who Framed Roger Rabbit. Well, while you take a trip down memory lane, why not also learn some more about US Copyright Law? This article discusses the Termination of Transfer provision, which was introduced into the 1976 Copyright Act. That is:\n\nIt allows creators to unilatterally cancel the copyright licenses they have signed over to others, by waiting 35 years and then filing some paperwork with the US Copyright Office.\n\nUsing the original creator‚Äôs, Gary K Wolf, recent reacquisition of the copyright for Roger the Rabbit, the article overviews an interesting example of how this provision was used. Also, it will be interesting to observe how Disney proceeds with this matter, as they currently distribute the original movie and have a ride featuring these characters at Disneyland in California.\n\n\nWhy does this matter?\nI‚Äôm certainly no a legal expert. Reading this piece, however, reminded me of how critical copyright law is to publishers and creators. The Termination of Transfer provision is one such portion of the law to be aware of, especially if you‚Äôre a creator. It provides a mechanism for creators to reclaim rights back to their original IP in cases that turned out to be successful for the copyright holder. Like the article mentions:\n\nTermination is copyright‚Äôs lookback, a way to renegotiate the deal once you‚Äôve gotten the leverage that comes from success.\n\nArticle: ATSC 3.0: ‚ÄòI Can‚Äôt Imagine Anyone Defending Our Current Adoption Strategy‚Äô via this newsletter here.\nNextGen TV (AKA ATSC 3.0) continues to receive criticism regarding its rollout. This article contains additional thoughts, and it provides some explanation and solutions. Aside from manufacturers‚Äô unwilliness to integrate the required tuners into consumers‚Äô TVs, broadcasters and regulators continue to create and contend with issues. These include issues around the use of encryption to limit signal access, the hurdles consumers are confronted with to access an ATSC 3.0 signal, and the lack of incentive for broadcasters to develop engaging experiences leveraging NextGen technology. If these issues continue to go unaddressed, then adoption and regulation will continue to falter and likely could fall to the wayside in the sea of other media viewing options (i.e., internet streaming services). Here‚Äôs a quote from the article that summarizes the impact this slow rollout might have on broadcast television:\n\nNextGen can be an excellent advancement with real world benefits in viewer behavior. It can also become incredibly destructive and hurtful. Or we can do nothing at all and see how ridged 8-VSB, MPEG, braindead purely linear broadcast ages out. Let‚Äôs not find out how long before TV manufacturers smother or abandon OTA.\n\n\n\nWhy does this matter?\nNextGen TV has the potential to be a revolution for the broadcast industry. I‚Äôve been witness to what the technology can provide broadcasters in terms of creating a more interactive, informative experience for viewers. The idea that all this can be done with an over the air signal feels like something significant. However, the slow rollout caused by issues from both broadcasters and regulators is only limiting its promised potential. NextGen TV won‚Äôt become a part of consumers‚Äô viewing experiences if they are confronted with:\n\nAccess hurdles. Who‚Äôs going to buy an antenna or other physical hardware that needs to be connected to a TV, especially when streaming internet services are already native within the viewing environment and make it easy for viewers to access content?\nUnenjoyable experiences, which results from the lack of development due to little incentive for broadcasters to create these engaging experiences.\n\nNextGen TV has been around for awhile, so it‚Äôs not a new idea. If you want to get up to speed on the current state of ATSC 3.0, I highly suggest @AttenaMan‚Äôs recent video. Check it out here.\n\n\n\nJust for fun\nVideos: Strudel Showcase\nI‚Äôm a huge nerd for all things tech, computers, coding, and data. I find great joy in discovering creators who leverage these tools to do something creative. I‚Äôm also a huge fan of ambient and trance music, especially while I work. strudel mashes all these interests together. It‚Äôs a programming language that allows you to make music with code. It‚Äôs been fun discovering what others have created with it. Check out the showcase in strudel‚Äôs docs here. You might find a few gems for your listening pleasure. If you want to learn more, it looks like @terryds has the beginnings of an online course and interactive sandbox available in a GitHub repo.\nI hope you continue to have a great start to December. I‚Äôll see you next week. Cheers üéâ!\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 005},\n  date = {2025-12-05},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúThe Hex Update: Issue 005.‚Äù\nDecember 5, 2025."
  },
  {
    "objectID": "blog/posts/2025-02-15-til-links-notes-sql-postgresql-date-time-types/index.html",
    "href": "blog/posts/2025-02-15-til-links-notes-sql-postgresql-date-time-types/index.html",
    "title": "Note: Date and time data types in PostgreSQL",
    "section": "",
    "text": "Today I spent some time learning more about date and time data types in PostgreSQL. Much of this was learned reviewing PostgreSQL‚Äôs documentation and some other reading (check out the list at the end of the post if you want to go deeper). This documentation deep dive will cover what I‚Äôve learned recently about working with these data types, which includes:\n\nDate and time data types available in PostgreSQL\nArithmetic with date and time data types\nUseful date and time functions\nExtracting specific elements from dates or times\nBeing aware of and managing the trickiness of time zones\n\nBelow are notes, links, and some examples overviewing what I‚Äôve learned recently. This post is a bit of a scratchpad, and it will only be lightly edited. Be aware, there will likely be grammatical or syntactical errors. This post also does not seek to be a comprehensive overview, but rather it aims to be a collection of topics I thought would be useful to refer back when working with date and time data types in PostgreSQL. I prioritized resource and documentation links to aid in deeper review of these topics.\n\nDate and time data types\nFour data types are available to represent dates and times within a PostgreSQL database:\n\ntimestamp or timestamp with timezone is a data type representing a specific date and time.\ndate is a data type recording some exact date.\ntime is a data type recording some point in time.\ninterval is a value representing some unit of time.\n\nPostgreSQL‚Äôs documentation further details date and time types.\nPostgreSQL has some special date and time input strings, which include the following:\nSELECT\n    'epoch'::timestamp, -- 1970-01-01 00:00:00+00\n    'now'::timestamp, -- Current transaction start time\n    'today'::timestamp, -- Midnight today\n    'tomorrow'::timestamp, -- Midnight tomorrow\n    'yesterday'::timestamp; -- Midnight yesterday\nIf we need to return current date or time values, the following functions are useful within a simple SELECT statement:\nSELECT\n    CURRENT_DATE,\n    CURRENT_TIME,\n    CURRENT_TIMESTAMP,\n    LOCALTIME,\n    LOCALTIMESTAMP;\nWe can further observe some specific examples of these types by running the following SQL statement:\nSELECT\n    '2024-01-01'::date,\n    '01:32:02.732'::time without time zone,\n    '01:32:02.732 CST'::time with time zone,\n    '2 years 3 months 12 hours'::interval;\nIntervals are also an interesting data type, which represent some unit of time as a number. For instance, you can write a SQL statement like the following:\nSELECT\n    '1.8 weeks'::interval,\n    '15 seconds'::interval,\n    '2 years'::interval,\n    '2 decades'::interval,\n    '4 13:33:33'::interval,\n    '100-09'::interval,\n    'P0020-10-05T23:10:16'::interval;\n\n\nArithmetic with date and time data types\nArithmetic operations can also be performed on date and time data types. Here‚Äôs a few examples:\n-- date + integer &gt;&gt; date\n-- Add one day\nSELECT current_date + 1 as tomorrow;\n\n-- date + interval &gt;&gt; date\nSELECT current_date + '1 year'::interval as a_year_from_today;\n\n-- date - date &gt;&gt; integer\n-- Number of days elapsed\nSELECT current_date - '2025-01-01'::date days_in_year;\n\n-- interval * double precision &gt;&gt; interval\nSELECT interval '1 hour' * 24 as hours_in_day;\nPostgreSQL‚Äôs docs goes into more detail and provides additional examples of the use of these operations.\n\n\nUseful date and time functions\nPostgreSQL provides several useful date and time functions. Some functions get a current date, date time, or time. Some are date and time constructors. Others assist in the completion of some type of operation.\nSELECT\n    now(), -- Current date and time\n    timeofday(), -- Current date and time formatted\n    current_time(0) as hmstz, -- Current time of day\n    localtime(0), -- Current time of day, with less precision\n    localtimestamp(0), -- Current date and time\n    make_date(2025, 02, 09), -- Create a date from integer values\n    make_time(9, 50, 40.5), -- Create a time with integer values\n    to_timestamp(1739116840), -- Unix epoch to timestamp with time zone\n    statement_timestamp(), -- Timestamp at the start of the statement\n    age(\n        timestamp '2025-02-09',\n        timestamp '1985-11-12'\n    ) AS how_old, -- Symbolic representation of age\n    date_bin(\n        '7 minutes',\n        timestamp '2025-02-09 09:50:40', timestamp '2025-02-09 00:00:00'\n    ), -- Bin into specified intervals, given a specific origin\n    date_trunc(\n        'hour',\n        timestamp '2025-02-09 09:50:40'\n    ), -- Truncate to a specific date or time unit\n    date_part(\n        'day',\n        timestamp '2025-02-09 09:50:40'\n    ); -- Extract a specific unit from a timestamp\n\n\nExtracting specific elements from date or time values\nSay we just want one element from our date objects, we can use PostgreSQL‚Äôs date_part() function. For example:\nSELECT\n  date_part('year', '2024-01-01'::date) as year,\n  date_part('month', '2024-01-01'::date) as month,\n  date_part('day', '2024-01-01'::date) as day,\n  date_part('epoch', '2024-01-01'::date) as epoch; -- # of seconds elapsed since 1970-01-01\nAdditional elements can be extracted using date_part, especially if you have a timestamp with a timezone field.\nThe extract function is also useful to extract subfields from date, date time, or time values. Below are several examples I thought would be useful.\nSELECT\n    EXTRACT(\n        DAY FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        DOW FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        MONTH FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        QUARTER FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        EPOCH FROM TIMESTAMP '2025-02-09 09:50:40'\n    )\n;\n\n\nThe trickiness of time zones\nIt‚Äôs critical to be aware of time zones when using date and time values, so it‚Äôs always good to be aware of the current time zone setting used for the database system you‚Äôre working with. There‚Äôs two ways to check this setting while working with a PostgreSQL database:\nSHOW timezone;\n-- or\nSELECT current_setting('timezone');\nThe current_setting() function can be handy for when you need to create a timestamp using the system‚Äôs time zone (example via)\nSELECT make_timestamptz(2025, 2, 02, 10, 39, 22.5, current_setting('timezone'))\nI also like that you can find all the time zones and narrow it down to a specific region by doing the following (example via):\nSELECT * FROM pg_timezone_names\nWHERE name LIKE 'America%'\nORDER BY name;\nThe table returned from the previous example also contains an is_dst field. This column denotes whether the timezone is exhibiting day lights savings time or not. This is useful because day lights savings time is a function of geography and politics. Not all regions of the world exhibit day lights savings time uniformly. Take for example Lord Howe Island. How this part of the world observes day lights savings time and its time zones is some interesting reading.\n\n\nAdditional resources\nHere‚Äôs a collection of additional resources to go deeper:\n\nDate/Time Types from the PostgreSQL documentation\nData Type Formatting Functions from the PostgreSQL documentation\nDate/Time Functions and Operators from the PostgreSQL documentation\nChapter 13: Working with dates and times from Practical SQL, 2nd Edition: A Beginner‚Äôs Guide to Storytelling with Data\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Note: {Date} and Time Data Types in {PostgreSQL}},\n  date = {2025-02-15},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúNote: Date and Time Data Types in\nPostgreSQL.‚Äù February 15, 2025."
  },
  {
    "objectID": "blog/posts/2025-11-13-hex-update-003/index.html",
    "href": "blog/posts/2025-11-13-hex-update-003/index.html",
    "title": "The Hex Update: Issue 003",
    "section": "",
    "text": "Let‚Äôs catch up\nHi. I haven‚Äôt done much with The Hex Update in a while. The first couple of issues were a bit of a false start. I wasn‚Äôt realistic with what I could achieve with the time available.\nSo, here we go again.\nThis is another attempt to restart The Hex Update. The aim is to be simple and easy to manage. Thus, I‚Äôll be making a few changes. The goal is a weekly issue, which will include three sections: Let‚Äôs catch up; Three things from this week; and Just for fun.\nThe sections are as follows:\n\nLet‚Äôs catch up: a space to discuss what‚Äôs on my mind regarding media, entertainment, or other industry news.\nThree things from this week: links to articles, blogs, etc. I bumped into this week, each with a brief summary and a discussion about why what I share matters.\nJust for fun: one thing I found fun during the week. What‚Äôs parked here may be media related or not. It‚Äôs more of a place to end the week on a high note, maybe even a laugh.\n\nI also intend for the article summaries to be shorter. The goal is paragraph length. This will be a challenge for me. My writing can be a bit verbose at times.\nWith a renewed vision, let‚Äôs highlight what I found interesting this week.\n\n\nThree things from this week\nArticle: YouTube Just Ate TV. It‚Äôs Only Getting Started from The Hollywood Reporter\nHere‚Äôs an article focused on the rapid growth of YouTube, no longer just a place of user-generated content, but a dominant force in media. The platform aims to expand its footprint by capturing the other hours people are spend while watching televisions. Indeed, if you read YouTube‚Äôs big bets, you‚Äôll be confronted with the fact that YouTube viewing on TVs has surpassed viewing on mobile devices. This is a fact Neal Mohan, YouTube‚Äôs CEO, is leaning into. In addition, it‚Äôs staggering that YouTube claims 1 billion podcast users monthly. The article contains some further discussion about how various media formats (e.g., cooking, children‚Äôs content, and late night) are now finding success and moving toward a YouTube first strategy‚Äìthough the platform is still challenged by scripted content. However, some examples of creators trying to grow this space were highlighted. The article then wraps up with a conversation about content production, the economics of content creation, and the role creators and larger production studios play.\n\nWhy does this matter?\nYouTube is the new TV. It‚Äôs now moving into other spaces traditionally dominated by traditional broadcasters and media organizations: live events like sports. More striking is audiences are interacting with YouTube‚Äôs content beyond a mobile device, which includes televisions. YouTube‚Äôs intentions are to drive more competition for audience‚Äôs attention. Pay attention to this space.\nPodcast: Why GM will give you Gemini, but not Carplay from Decoder with Nilay Patel\nHere‚Äôs a conversation between Nilay Patel and General Motors CEO Mary Barra and Chief Product Officer Sterling Anderson. The conversation centered around electric vehicles, current market dynamics, product development, and various GM product offerings. Relevant to media was the conversation about the integration of AI into new GM models and the step back from offering Apple‚Äôs Carplay within newer GM models. Here are some points in the conversation I found worth review:\n\nThe rollout of Gemini AI into 2026 GM models and the step back from smart phone projection apps like Apple Car Play (37M03S).\nAn interesting point for why users want access to the large library of apps made available via phone projection (41M41S): smaller apps will likely not be supported.\nNatural language and LLM interfaces are being further integrated into infotainment systems. This affords expanded functionality for more customized experiences for users (46M58S).\n\n\n\nWhy does this matter?\nIt‚Äôs about the human machine interface. The legacy radio experience of knobs, dials, and buttons to a touchscreen infotainment system will only continue to change in the near and distant future. Some car manufacturers don‚Äôt view phone projection systems to be the path forward, and they are developing their own software solutions. Natural language interfaces will also be a part of and further impact media consumption patterns for car owners. If so, media organizations need to consider the impact this interface change may have on content discovery and the consumption experience.\nReport: Relatively few Americans are getting news from AI chatbots like ChatGPT from Pew Research\nHere‚Äôs a short report I bumped into from The Pew Research Center. It‚Äôs no surprise Americans are increasingly turning to artificial chatbots. According to this report, though, many still do not use these as a regular source for news. In fact, 75% of those surveyed say they ‚ÄòNever‚Äô get news from an AI chatbot like ChatGPT or Gemini. However additional reporting from Reuters finds AI to be used more and more for information seeking tasks, though many are still skeptical of its role in news. Many users cite mixed experiences regarding the quality of the information, which includes the identification of inaccurate information.\n\n\nWhy does this matter?\nSomething has to give. AI use is increasing, which includes the growth of AI use for information seeking tasks. News is information. Although the experience and quality of this information via AI chatbots may not meet the needs of many users at this time, their quality will likely continue to improve. Publishers will need to consider the impact this will have on news consumption, including how to get their content in AI summaries while also identifying the impact summaries might have on traffic to their online properties.\n\n\n\nJust for fun\nPlaylist: ùêîùêìùêéùêèùêàùêÄùêç ùêíùêÇùêáùêéùêãùêÄùêíùêìùêàùêÇ üìñ 1990s Nostalgic Atmospheres (Music & Ambience) ‚ú®üõ∞Ô∏èüåçü¶íüåø by Kester Spach\nTaking it back to the 90‚Äôs to wrap up the week. I came across this ambient playlist while looking for new ambient playlists on YouTube. Listening just brought up lots of good memories from growing up in the 90‚Äôs. I became nostalgic for times playing Encarta MindMaze. I hope the playlist‚Äôs vibe results in some good memories for you as well.\nHave a good end to your week.\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 003},\n  date = {2025-11-13},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúThe Hex Update: Issue 003.‚Äù\nNovember 13, 2025."
  },
  {
    "objectID": "blog/posts/2024-02-10-til-r-dput-store-objects/index.html",
    "href": "blog/posts/2024-02-10-til-r-dput-store-objects/index.html",
    "title": "TIL: Use base::dput() to easily create and save objects",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\nlibrary(testthat)\n\nWarning: package 'testthat' was built under R version 4.5.2\n\n\n\nBackground\nLately, I‚Äôve been doing a lot of data validation tests for a package I‚Äôm working on. Using testthat for the testing framework, some of the tests I‚Äôm writing verify dataset column names. For instance, these tests tend to look something like this:\n\ntest_that(\"column names are as expected\", {\n  expect_named(\n    mtcars,\n    c(\n      \"mpg\",\n      \"cyl\",\n      \"disp\",\n      \"hp\",\n      \"drat\",\n      \"wt\",\n      \"qsec\",\n      \"vs\",\n      \"am\",\n      \"gear\",\n      \"carb\"\n    )\n  )\n})\n\nTest passed with 1 success üò∏.\n\n\nSince mtcars only has 11 columns, the character vector used for the column name test is pretty small. Creating this by hand isn‚Äôt too bad. However, what if we need to create a character vector for a dataset much larger than this. Say a dataset with 150+ columns‚Äìsoul crushing. I don‚Äôt know about you, but I would hate to hand key a character vector this long (I‚Äôm sad to report I‚Äôve done this more times than I would like to admit). Of course, there‚Äôs a better way. Use base::dput().\n\n\nTIL: Use dput()\nAccording to the docs, dput:\n\nWrites an ASCII text representation of an R object to a file, the R console, or a connection, or uses one to recreate the object.\n\nNow that we have a tool to make this easier, all we need to do is pass the data to names(), and then wrap dput() around the return value of names(). What results is a character vector that gets printed to the console. All we need to do now is copy and paste this output into our file. This is what this looks like:\n\ndput(names(mtcars))\n\nc(\"mpg\", \"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \n\"gear\", \"carb\")\n\n\nPretty neat.\ndput() also has a file argument, so you can pass along a file string the object will be written. Since I tend to save multiple objects in one file from time to time in a tests fixtures file, I rarely output to a file. Here‚Äôs the code to output the object to a file if your interested, though:\n\ndput(\n  names(mtcars),\n  here::here(\"til/posts/2024-02-10-til-r-dput-store-objects/mtcars-names.R\")\n)\n\n\n\nOne more tip, if you use vim or nvim\nI‚Äôm particular with how I style long character vectors within a file. If the objects can‚Äôt fit on one line, each will be placed on their own line. So you can output your object and use the following substitution command to place each object on it‚Äôs own line.\n:.,+1s/, /,\\r/g\nThis command will make our object look like this:\n\n\n\nUse substitution to finish cleaning up the character vector\n\n\nIndeed, it‚Äôs not perfect, but it‚Äôs close. We only needed to make some minor edits to finish it. But in the end, we‚Äôve saved so much time, and we have a well formatted character vector.\n\n\nWrap up\nI wish I came across dput() much earlier. Not only is it one of those entire workflow changing tips, it‚Äôs one that would have saved me so much time. Hopefully if you‚Äôre reading this post, you avoid hand creating large character vectors and just use base::dput().\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {TIL: {Use} `Base::dput()` to Easily Create and Save Objects},\n  date = {2024-02-10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúTIL: Use `Base::dput()` to Easily Create\nand Save Objects.‚Äù February 10, 2024."
  },
  {
    "objectID": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html",
    "href": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html",
    "title": "TIL: Identifying explicit and implicit missing values",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\nlibrary(skimr)\nlibrary(nycflights13)\nToday I learned more about identifying explicit and missing values in R. During our weekly Data Science Learning Community‚Äôs (DSLC) bookclub meeting for the R for Data Science (R4DS) book, I was re-introduced to several methods to identify explicit and implicit missing values. Much of what is covered here comes from Chapter 18: Missing values of the book. I wanted to share what I‚Äôve learned, in hopes I can better remember this information in the future."
  },
  {
    "objectID": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#base-rs-sapply",
    "href": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#base-rs-sapply",
    "title": "TIL: Identifying explicit and implicit missing values",
    "section": "Base R‚Äôs sapply()\n",
    "text": "Base R‚Äôs sapply()\n\nThe first suggestion was to use base R‚Äôs sapply() with an anonymous function. There‚Äôs two variations: one that identifys the presence of any NAs across the columns. The second provides a count of NAs for each column.\n\nsapply(starwars, function(x) any(is.na(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year        sex     gender \n     FALSE       TRUE       TRUE       TRUE      FALSE      FALSE       TRUE       TRUE       TRUE \n homeworld    species      films   vehicles  starships \n      TRUE       TRUE      FALSE      FALSE      FALSE \n\n\n\nsapply(starwars, function(x) sum(is.na(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year        sex     gender \n         0          6         28          5          0          0         44          4          4 \n homeworld    species      films   vehicles  starships \n        10          4          0          0          0"
  },
  {
    "objectID": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#purrrmap_df-with-any-and-is.na",
    "href": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#purrrmap_df-with-any-and-is.na",
    "title": "TIL: Identifying explicit and implicit missing values",
    "section": "\npurrr::map_df with any() and is.na()\n",
    "text": "purrr::map_df with any() and is.na()\n\nSimilar to the base R approach is the use of purrr::map_df() with an anonymous function. I‚Äôm quite partial to this approach, as it‚Äôs even more succinct, though it requires purrr as a dependency. However, if you‚Äôre already importing the tidyverse into your session, then why not go ahead and use it?\n\nmap_df(starwars, \\(x) any(is.na(x)))\n\n# A tibble: 1 √ó 14\n  name  height mass  hair_color skin_color eye_color birth_year sex   gender homeworld species films\n  &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt; &lt;lgl&gt;      &lt;lgl&gt;      &lt;lgl&gt;     &lt;lgl&gt;      &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt;     &lt;lgl&gt;   &lt;lgl&gt;\n1 FALSE TRUE   TRUE  TRUE       FALSE      FALSE     TRUE       TRUE  TRUE   TRUE      TRUE    FALSE\n# ‚Ñπ 2 more variables: vehicles &lt;lgl&gt;, starships &lt;lgl&gt;\n\n\n\nmap_df(starwars, \\(x) sum(is.na(x)))\n\n# A tibble: 1 √ó 14\n   name height  mass hair_color skin_color eye_color birth_year   sex gender homeworld species films\n  &lt;int&gt;  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;int&gt;     &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;int&gt;     &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n1     0      6    28          5          0         0         44     4      4        10       4     0\n# ‚Ñπ 2 more variables: vehicles &lt;int&gt;, starships &lt;int&gt;"
  },
  {
    "objectID": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#dplyrsummarise",
    "href": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#dplyrsummarise",
    "title": "TIL: Identifying explicit and implicit missing values",
    "section": "dplyr::summarise()",
    "text": "dplyr::summarise()\nAnother approach involved the use of dplyr‚Äôs summarise() along with across(), everything(), and an anonymous function. This approach was meant only to count the amount of missing values within each column.\n\nstarwars |&gt;\n  summarise(across(everything(), \\(x) sum(is.na(x))))\n\n# A tibble: 1 √ó 14\n   name height  mass hair_color skin_color eye_color birth_year   sex gender homeworld species films\n  &lt;int&gt;  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;int&gt;     &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;int&gt;     &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n1     0      6    28          5          0         0         44     4      4        10       4     0\n# ‚Ñπ 2 more variables: vehicles &lt;int&gt;, starships &lt;int&gt;"
  },
  {
    "objectID": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#skimrskim",
    "href": "blog/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#skimrskim",
    "title": "TIL: Identifying explicit and implicit missing values",
    "section": "skimr::skim()",
    "text": "skimr::skim()\nskimr::skim() was also discussed, though the output is more verbose than the other options. The output contains a sum of the number of missing values within each column. This is certainly the most succinct way to obtain this information, and it provides additional summary information about your data. However, it may be more information then you need to answer your question about the presence of missing values in your data.\n\nskim(starwars)\n\n\nData summary\n\n\nName\nstarwars\n\n\nNumber of rows\n87\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nlist\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1.00\n3\n21\n0\n87\n0\n\n\nhair_color\n5\n0.94\n4\n13\n0\n11\n0\n\n\nskin_color\n0\n1.00\n3\n19\n0\n31\n0\n\n\neye_color\n0\n1.00\n3\n13\n0\n15\n0\n\n\nsex\n4\n0.95\n4\n14\n0\n4\n0\n\n\ngender\n4\n0.95\n8\n9\n0\n2\n0\n\n\nhomeworld\n10\n0.89\n4\n14\n0\n48\n0\n\n\nspecies\n4\n0.95\n3\n14\n0\n37\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\nfilms\n0\n1\n24\n1\n7\n\n\nvehicles\n0\n1\n11\n0\n2\n\n\nstarships\n0\n1\n16\n0\n5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nheight\n6\n0.93\n174.60\n34.77\n66\n167.0\n180\n191.0\n264\n‚ñÇ‚ñÅ‚ñá‚ñÖ‚ñÅ\n\n\nmass\n28\n0.68\n97.31\n169.46\n15\n55.6\n79\n84.5\n1358\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nbirth_year\n44\n0.49\n87.57\n154.69\n8\n35.0\n52\n72.0\n896\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ"
  },
  {
    "objectID": "blog/posts/2025-10-11-tir-notes-boykis-the-ghosts-in-the-data/index.html",
    "href": "blog/posts/2025-10-11-tir-notes-boykis-the-ghosts-in-the-data/index.html",
    "title": "TIR: The ghosts in the data by Vicki Boykis",
    "section": "",
    "text": "Note\n\n\n\nThis post is written in the spirit of publishing more frequent blog posts. It‚Äôs a bit of a scratchpad of ideas, concepts, and/or ways of working that I found to be useful and interesting. As such, what‚Äôs here is lightly edited. Be aware: there will likely be spelling, grammatical, or syntactical errors along with some disjointed, incomplete ideas.\n\n\nToday I read The ghosts in the data blog post by Vicki Boykis. Below are notes, quotes, and links I‚Äôm taking away from my review of the post. Alongside my notations, I reflect on some of the ideas presented in hopes of extending the discussion.\n\n\n\n\n\n\nImportant\n\n\n\nDo me a favor. Thank the author by clicking on the above link and reading the blog post. What‚Äôs here is not a substitute for the original work.\n\n\nI really liked the post‚Äôs framing of explicit vs.¬†implicit knowledge as it relates to data work. That is, much of what‚Äôs needed to work with data‚Äìespecially untidy data‚Äìisn‚Äôt explicitly written down in a manual or some type of documentation. Rather, experience leads to the implicit knowledge needed for data work. This is all summarized in a shared concept from David R. Maclver called ‚Äòghost knowledge‚Äô, which is defined as:\n\nknowledge that exists within expert communities but is never written down and basically doesn‚Äôt exist for you unless you have access to those communities.\n\nGetting to clean data may require ghost knowledge to be known. The challenge is the needs and processes to get the clean data you need is often not written down or is unavailable. Rather, this knowledge is developed from the experience of working with the data.\nGoing further, the post collected community feedback and documented other areas in data work considered to be forms of ghost knowledge. This portion of the post included the following discussions:\n\nThe power law\nCollecting data\nData is programming work\nWorking with people is hard\nPeople do not operate based on the data\n\nEach of the documented observations has their own merit and contains points relevant to data work. I only notate and reflect on the ones I found relevant to my work from the post.\nRegarding the power law, this quote hit the mark:\n\nReal life phenomena, as I‚Äôve seen them in industry, mostly do not follow a bell curve.\n\nIn my experience, as someone who‚Äôs worked with event based behavioral data (e.g., marketing and digital abalytics data), many of the distributions I‚Äôve confronted exhibit long tails. That is, people do a lot of a certain activity, while others don‚Äôt do much at all. There‚Äôs implications to this, which this quote from the piece does an excellent job summarizing:\n\nIn particular, it means that paying attention to tail-end phenomena is just as as important as understanding an ‚Äúaverage‚Äù user.\n\nIndeed, working with data exhibiting these types of distributions also requires you to consider and further evaluate the types of analysis available.\nWith this quote in mind, I was reminded of another related blog post worth reading:\n\nThe Most Useful Probability Distributions for Marketing Analytics by Joe Domaleski\n\nRegarding the observations surronding the collection of data, this quote resonated with me:\n\nYou don‚Äôt do the process of verifying the data once, you do it many times because a lot of times some process upstream will change. As long as you don‚Äôt control the upstream process, you don‚Äôt control your data.\n\nTruer words have never been spoken. Data validation is often a constant task, especially in environments where data ownership is limited. Some steps can be automated, others require an awareness of the ‚Äòghost knowledge‚Äô needed to complete the validation process. If you don‚Äôt control your data, you‚Äôll inevitablly have to manage ghost knowledge, and you won‚Äôt be able to fully fix it.\nOwnership and control of data collection is also an important topic. I believe this statement to also be true, if you don‚Äôt have ownership of the data collection process, then you don‚Äôt control your data. You‚Äôre just managing changes and doing the best with what you have.\nThe point that data work as programming work is also reflective of my experience. That is,\n\nThe further we get away from working with small data sets, and more with large, complicated (often) cloud-based, distributed systems, the more we‚Äôll all have to become developers and adapt development best practices.\n\nSurely, best practices exist. Many of which have been developed within software engineering and programming. The post has some really good reccomendations.\nFollowing the technical topics, the post covers subjects pertaining to people, teams, and organizations. I won‚Äôt go into too much detail here in these notes. However, check this section out. The blog‚Äôs linked resources provided are useful and interesting.\nBesides being a great data read, reviewing and writing down my thoughts for this post reminded me that the internet has some really good blog posts. You just need to identify those voices. Vicki Boykis is one of them.\nIf you found these notes and reflections useful, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {TIR: {The} Ghosts in the Data by {Vicki} {Boykis}},\n  date = {2025-10-11},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúTIR: The Ghosts in the Data by Vicki\nBoykis.‚Äù October 11, 2025."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "",
    "text": "Managing an e-commerce websites requires some ability to predict and plan for the future. Some questions a site owner might want to predict include: What content/products will users view the most this coming period (i.e., daily, weekly, etc.)? To what effect will time of year have on the content/products users view (e.g., holiday shopping)? What and how much of each product needs to be in stock to meet users‚Äô demands? Most importantly, what types of content/product views result in more completed orders?\nA crystal ball to consult would be ideal. Unfortunately, one has not been developed, yet. And the future isn‚Äôt looking bright for such an innovation. So then, what other tools are available to assist in the planning and goal setting surrounding the development and management of an e-commerce website?\nEducated guesses are a start (i.e., hypotheses). These hypotheses could be based on some domain knowledge, further supported with historical data. Some common guesses might be: This year will be similar to last year. This quarter‚Äôs sales will be similar to last quarters. Site views will increase this quarter. XYZ will see a decrease in sales. Although domain knowledge and past experiences can be used to inform our hypotheses, other tools can be used to generate evidence to support our hypotheses. One such tool is forecasting. A forecast is a tool to help reduce uncertainty; the process and methods used to predict the future as accurately as possible, given all the information available (Hyndman and Athanasopoulos 2021).\n\n\nThis series of blogposts will focus on creating forecasts using Google Analytics 4 data. Specifically, this series overviews the steps and methods involved when developing forecasts of time series data. This blog series begins with a post overviewing the wrangling, visualization, and exploratory analysis involved when working with time series data. The primary focus of the exploratory analysis will be to identify interesting trends for further analysis and application within forecasting models. Then, subsequent posts will focus on developing different forecasting models. The primary goal of this series is to generate a forecast of online order completions on the Google Merchandise store.\n\n\n\nAnother intention of this series is to document and organize my learning and practice of time series analysis. Although I try my best to perform and report valid and accurate analysis, I will most likely get something wrong at some point in this series. I‚Äôm not an expert in this area. However, it‚Äôs my hope that this series can be a supplement to others who may be learning and practicing time series analysis. In fact, seeing somebody (i.e., myself) do something wrong might be a valuable learning experience for someone else, even if that someone is my future self. If I got something wrong, I would greatly appreciate the feedback and will make the necessary changes.\nThroughout the series, I will document the resources I used to learn the process involved when generating forecasting models. I highly suggest using these as the primary source to learn this subject, especially if you intend to use this type of analysis in your own work. Specifically, the process and methods detailed in this series are mostly inspired by the Forecasting: Principles and Practice online textbook by Rob J Hyndman and George Athanasopoulos, and it utilizes several packages to wrangle, visualize, and forecast time series data (e.g., tsibble; fable; and feasts). I am very thankful to the authors and contributors for making these materials open-source."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "",
    "text": "This series of blogposts will focus on creating forecasts using Google Analytics 4 data. Specifically, this series overviews the steps and methods involved when developing forecasts of time series data. This blog series begins with a post overviewing the wrangling, visualization, and exploratory analysis involved when working with time series data. The primary focus of the exploratory analysis will be to identify interesting trends for further analysis and application within forecasting models. Then, subsequent posts will focus on developing different forecasting models. The primary goal of this series is to generate a forecast of online order completions on the Google Merchandise store."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "",
    "text": "Another intention of this series is to document and organize my learning and practice of time series analysis. Although I try my best to perform and report valid and accurate analysis, I will most likely get something wrong at some point in this series. I‚Äôm not an expert in this area. However, it‚Äôs my hope that this series can be a supplement to others who may be learning and practicing time series analysis. In fact, seeing somebody (i.e., myself) do something wrong might be a valuable learning experience for someone else, even if that someone is my future self. If I got something wrong, I would greatly appreciate the feedback and will make the necessary changes.\nThroughout the series, I will document the resources I used to learn the process involved when generating forecasting models. I highly suggest using these as the primary source to learn this subject, especially if you intend to use this type of analysis in your own work. Specifically, the process and methods detailed in this series are mostly inspired by the Forecasting: Principles and Practice online textbook by Rob J Hyndman and George Athanasopoulos, and it utilizes several packages to wrangle, visualize, and forecast time series data (e.g., tsibble; fable; and feasts). I am very thankful to the authors and contributors for making these materials open-source."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Setup",
    "text": "Setup\nThe following is the setup steps needed to perform this exploratory analysis.\n\n# Libraries needed\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(fable)\nlibrary(feasts)\nlibrary(fuzzyjoin)\nlibrary(bigrquery)\nlibrary(glue)\nlibrary(GGally)\nlibrary(scales)\nbq_auth()\n\n## Replace with your Google Cloud `project ID`\nproject_id &lt;- 'your.google.project.id'\n\n\n## Configure the plot theme\ntheme_set(\n  theme_minimal() +\n    theme(\n      plot.title = element_text(size = 14, face = \"bold\"),\n      plot.subtitle = element_text(size = 12),\n      panel.grid.minor = element_blank()\n    )\n)"
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The data",
    "text": "The data\nAs was used in previous posts, Google Analytics 4 data for the Google Merchandise Store are used for the examples below. Data represents website usage from 2020-11-01 to 2021-12-31. Google‚Äôs Public Datasets initiative makes this data open and available for anyone to use (as long as you have a Google account and have access to Google Cloud resources). Data are stored in Google BigQuery, a data analytics warehouse solution, and are exported using a SQL like syntax. Details on how this data were exported can be found in this GitHub repository. More about the data can be found here."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Export the data",
    "text": "Export the data\nThe first step in the process was to export all page_view events. To do this, the following SQL code was submitted to BigQuery using the bigrquery package. Keep in mind Google charges for data processing performed by BigQuery. Each Google account‚Äìat least since the writing of this post‚Äìhad a free tier of usage. If you‚Äôre following along and you don‚Äôt have any current Google Cloud projects attached to your billing account, this query should be well within the free usage quota. However, terms of service may change at anytime, so this might not always be the case. Nevertheless, it is best to keep informed about the data processing pricing rates before submitting any query to BigQuery.\nselect \n    event_date,\n    user_pseudo_id,\n    event_name,\n    key,\n    value.string_value\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nCROSS JOIN UNNEST(event_params)\nwhere \n   event_name = 'page_view' and \n   _TABLE_SUFFIX between '20201101' and '20211231' and \n   key = 'page_location'\nThe query returns a data set with 1,350,428 rows and the following columns:\n\nevent_date - represents the date the website event took place.\nuser_pseudo_id - represents a unique User ID.\nevent_name - The name of the event specified by Google Analytics. In our case this will just be page_view given the filtering criteria.\nkey - represents the page_location dimension from the data. This column should only contain page_location.\nstring_value - represents the page to which the event took place. In other words, the page path a page_view event was counted.\n\nThis query returns a lot of data. Thus, the analysis‚Äô scope needed to be narrowed to make the exploratory analysis more manageable. To do this, top-level pages were identified and data wrangling procedures were performed to reduce the data down to pages relevant to the exploratory analysis.\n\nNarrowing the analysis‚Äô scope to relevant pages\nThe overall aim of the series is to forecast Order Completed page views. As part of this, relevant pages that could be used to improve forecasting models needed to be a part of the exploratory analysis. However, this is challenging given the sheer amount of pages being represented within the data. Some pages relevant to the analysis, others, not so much. Given the number of possible pages, a decision was made to only examine key, top-level pages. The question is, then, what pages should be considered relevant to the analysis?\n\n\nDetermining top-level pages\nThe navigation bar of the Google Merchandise Store was used to determine the top-level pages. Indeed, it‚Äôs reasonable to expect the navigation bar is designed to drive users to key areas of the site. Developers won‚Äôt waste valuable navbar real-estate for content users would consider useless and/or irrelevant (i.e., these are developers at Google, so they mostly likely have a good idea of how users use a website). With this in mind, the following pages were identified as potential candidates for further analysis.\n\nNew products\nApparel\nLifestyle\nStationery\nCollections\nShop by Brand\nSale (i.e., Clearance)\nCampus Collection\n\nThe checkout flow is another key component of any e-commerce website. Indeed, a main goal of the site is to convert visits into order completions. As such, pages related to the checkout flow might be another area of interest in the analysis. It‚Äôs challenging to piece together the checkout flow by just looking at the data. So, I purchased a few products to observe the checkout flow and track the different pages that came up. The checkout flow‚Äìat least when I made my purchase‚Äìwent in this specific order:\n\nReview basket\nSign in (I wasn‚Äôt signed into my Google account)\nReview my information\nPayment info\nOrder review\nOrder completed\n\nAlthough these pages were identified as potential candidates for further analysis, it‚Äôs important to recognize the Google Merchandise store is not static, and thus the design and layout may have changed from the dates the data represents vs.¬†when I went through the checkout flow. Regardless, these initial observations provided a starting point to help narrow the analysis‚Äô focus."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Filtering out homepage events",
    "text": "Filtering out homepage events\nNow that the analysis‚Äô scope had been narrowed to top-level pages, events associated with homepage views were filtered out to reduce the number of events within the data. To do this, the regex_filter variable was created using the glue() function from the glue package, which was then applied within a filter statement.\n\nregex_page_filter &lt;- glue(\n  \"(\",\n  \"^https://www.googlemerchandisestore.com/$|\",\n  \"^https://googlemerchandisestore.com/$|\",\n  \"^https://shop.googlemerchandisestore.com/$\",\n  \")\"\n)\n\nThe variable contained multiple regex expressions, as several page paths in the data represented home page visits. Defining the variable in this way ensured the filter excluded all data associated with homepage visits.\nOnce the filter statement was set up, the str_to_lower() function from the stringr package was used to convert all the page paths to lower case. The following code chunk demonstrates how these operations were performed.\n\nga4_pagepaths &lt;- ga4_pageviews %&gt;%\n  filter(!str_detect(string_value, regex_page_filter)) %&gt;%\n  mutate(string_value = str_to_lower(string_value))\n\nThe filtering resulted in a reduced data set (i.e., ~1 million rows). Since the intent was to further narrow the analysis‚Äô scope, additional filtering was performed. Specifically, the data were filtered to return a data set containing the top-level pages identified previously.\nAnother variable‚Äìsimilar to regex_filter‚Äìwas created and used to filter the data further. Given the number of pages, though, a filtering join would be more appropriate (e.g., semi-join). The problem is the join operation needed to filter the data needed to be based on several regular expressions.\nA semi-join using a regular expression is not supported with dplyr‚Äôs joins, so the regex_semi_join() function from David Robinson‚Äôs {fuzzyjoin} package was used. This package provides a set of join operations based on inexact matching. A separate data set (tracked_data), containing the regular expressions was then created, imported into the session, and used within the join operation. A dplyr::left_join() was then used to include this data within a tidy dataset. The following chunk provides example code to perform these operations.\n\ntracked_pages &lt;- read_csv(\"tracked_pages.csv\")\n\ntop_pages_data &lt;- ga4_pagepaths %&gt;%\n  mutate(\n    string_value = str_remove(\n      string_value,\n      'https://shop.googlemerchandisestore.com/'\n    )\n  ) %&gt;%\n  regex_semi_join(tracked_pages, by = c(\"string_value\" = \"page_path\")) %&gt;%\n  regex_left_join(tracked_pages, by = c(\"string_value\" = \"page_path\"))\n\nAt this point, the data is more manageable and easier to work with. At the start, the initial export contained around 1.6 million rows. By narrowing the focus of the analysis and performing several data wrangling steps to filter the data, the final tidy data set contained around 320,000 rows.\nGiven the limited amount of storage available and how this post is hosted makes authentication into BigQuery challenging, I opted to not integrate the extraction steps into the rendering steps and to exclude the full data with this post. However, I included the filtered data set in a .rds file to conserve space. I imported this file by running the following code chunk to continue the exploratory analysis. I would skip this step and just directly export the data from BigQuery if this analysis was performed outside the forum of a blog post.\n\ntop_pages_data &lt;- readRDS(\"top_pages_data.rds\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Data exploration",
    "text": "Data exploration\nWith data in a tidy format, the exploratory analysis and further identification of relevant series for forecasts of Order Completed page views can take place. One area of possible exploration is to identify which pages generate a significant amount of traffic. Indeed, it‚Äôs possible that pages with a lot of traffic might also result in more order completions: more traffic indicates more interest; more interest could mean more orders.\nA few questions to answer:\n\nWhich top-level pages have the most unique users?\nWhat pages get the most traffic (i.e., page views)?\n\nA simple bar plot is created to answer these questions. Here‚Äôs the code to create these plots, using the ggplot2 package.\n\npage_summary &lt;- top_pages_data %&gt;%\n  group_by(page) %&gt;%\n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n()\n  ) %&gt;%\n  arrange(desc(unique_users))\n\n\nggplot(page_summary, aes(x = unique_users, y = reorder(page, unique_users))) +\n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(\n    title = \"Top-level pages by users\",\n    subtitle = \"Apparel page viewed by a significant amount of users\",\n    y = \"\",\n    x = \"Users\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nggplot(page_summary, aes(x = views, y = reorder(page, views))) +\n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(\n    title = \"Top-level pages by views\",\n    subtitle = \"Apparel and basket pages generate significant amount of views\",\n    y = \"\",\n    x = \"Views\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\nApparel clearly seems to not only have received a significant amount of users, but a high number of page views as well. It‚Äôs also interesting to note that the basket had nearly half the amount of users compared to apparel, but the amount of page views was similar. It‚Äôs also apparent, at least with the data available, that more users browsed clearance then they did new items during this period. Just looking at the current summary for the period, apparel might be a potential time series to include within forecasting models of order completions.\nAlthough the apparel page is a likely candidate for the forecasting models, supplemental data should be examined to justify its inclusion. For instance, actual purchase/financial data could provide further justification of the business case and value of focusing on this specific area within future analyses. For instance, apparel may drive a lot of traffic, but it may not be an area where much revenue or profit is generated. Thus, the focus on more money generating/profitable products may be better candidates to improve the accuracy of our forecasting models. Despite this, actual purchase and financial data are not available. As a result, this is not explored further."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Create time series data visualizations",
    "text": "Create time series data visualizations\nVisualizing the time series is the next step in the exploratory analysis. This step will be further helpful in identifying potential time series that may be of value in creating a forecast of Order Completed page views.\n\nConvert the tibble into a tsibble\nThe top_pages_data tibble is now converted to an object that contains temporal structure. To do this, the as_tsibble() function from the {tsibble} package is used. This package provides a set of tools to create and wrangle tidy temporal data. Before the temporal structure could be mapped to the data set, a few wrangling steps were performed: 1). the event_date column was converted into a date variable; and 2). data were aggregated to count the number of unique_users and views. The following code chunk contains an example of these steps.\n\npages_of_interest &lt;- c(\n  \"Apparel\",\n  \"Campus Collection\",\n  \"Clearance\",\n  \"Lifestyle\",\n  \"New\",\n  \"Order Completed\",\n  \"Shop by Brand\"\n)\n\ntidy_trend_data &lt;- top_pages_data %&gt;%\n  mutate(event_date = parse_date(event_date, \"%Y%m%d\")) %&gt;%\n  group_by(event_date, page) %&gt;%\n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  as_tsibble(index = event_date, key = c(\"page\")) %&gt;%\n  filter(page %in% pages_of_interest)\n\nAt this point, several trend plots could be created using the ggplot2 package. However, the feasts package provides a convenient wrapper function to quickly make trend visualizations of tsibble objects, autoplot(). The outputted plot was then formatted to improve readability.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of page views\",\n    subtitle = \"Apparel drove a significant amount of views\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  ) +\n  theme(legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n‚Ñπ The deprecated feature was likely used in the fabletools package.\n  Please report the issue at &lt;https://github.com/tidyverts/fabletools/issues&gt;.\n\n\n\n\n\n\n\n\n\nggplot2‚Äôs facet_wrap() function was used to create a plot for each series. Splitting the plots into separate entities allowed for a clearer view of the characteristics within each series.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(. ~ page, scales = \"free_y\") +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plots of page views\",\n    subtitle = \"Various characteristics are present within each series\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Identify notable features using time plots",
    "text": "Identify notable features using time plots\nApparel again emerges as a potential series to include within the forecasting models, as this page generates a significant amount of traffic. Despite the sheer amount of traffic to the apparel page, though, other time series peak interest. Specifically, the Campus Collection, Clearance, Lifestyle, and New pages all have some interesting characteristics that could be used to improve forecasting models. The following plots contain the isolated trends. A description of the characteristics within each trend is provided.\n\nApparel page‚Äôs characteristics\n\ntidy_trend_data %&gt;%\n  filter(page == \"Apparel\") %&gt;%\n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of Apparel page views\",\n    subtitle = \"Series exhibits positive trend; slight cyclic patterns; no seasonal patterns present\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nA clear positive trend.\nThe series contains some cyclic elements and very little indication of a seasonal pattern. However, with a greater amount of points, a seasonal pattern might be revealed (e.g., holiday season shopping).\n\n\n\nCampus Collection page‚Äôs notable characteristics\n\ntidy_trend_data %&gt;%\n  filter(page == \"Campus Collection\") %&gt;%\n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of Campus Collection page views\",\n    subtitle = \"Cyclic behavior present within the series\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nA slight positive trend is present up until the middle of the series. Towards the middle of the series, no real trend is present.\nGiven the variation is not of a fixed frequency, this series exhibits some cyclical behavior.\n\n\n\nClearance page‚Äôs notable characteristics\n\ntidy_trend_data %&gt;%\n  filter(page == \"Clearance\") %&gt;%\n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of Clearance page views\",\n    subtitle = \"Slight trend components are present; weekly seasonality is also present\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nA slight upward trend towards the middle of the series, followed by a steep downward trend, and then a slight upward trend towards the end of the series is present.\nThe series also has a clear seasonal pattern, which seems to be weekly in nature. Perhaps products are moved to clearance on a weekly basis.\n\n\n\nLifestyle page‚Äôs notable characteristics\n\ntidy_trend_data %&gt;%\n  filter(page == \"Lifestyle\") %&gt;%\n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of Lifestyle page views\",\n    subtitle = \"Trend not clear in this series; some strong cyclic behavior\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nTrend is not clear in this series.\nThere is some strong cyclic behavior being exhibited with limited seasonality with the time frame available.\n\n\n\nNew page‚Äôs notable characteristics\n\ntidy_trend_data %&gt;%\n  filter(page == \"New\") %&gt;%\n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of New page views\",\n    subtitle = \"No trend present; some some strong cyclic behavior\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nNo real trend is present.\nStrong cyclic behavior is present within the series. Some seasonality is present. Indeed, this is similar to the Clearance series, as the seasonality seems to be weekly. Perhaps new products are released each week.\n\n\n\nShop by brand characteristics\n\ntidy_trend_data %&gt;%\n  filter(page == \"Shop by Brand\") %&gt;%\n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of Shop by Brand page views\",\n    subtitle = \"Some trend components; slight cyclic behavior\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nThe trend here seems to be positive from the start, then declines sharply, and then exhibits a slight positive trend towards the end of the series.\nThere also seems to be some slight cyclicity with very little seasonality.\n\n\n\nOrder completed characteristics\n\ntidy_trend_data %&gt;%\n  filter(page == \"Order Completed\") %&gt;%\n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y = \"Views\",\n    x = \"\",\n    title = \"Time plot of Order Completed page views\",\n    subtitle = \"Some trend components; slight cyclic behavior\",\n    caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n  )\n\n\n\n\n\n\n\n\n\nThe trend from the start seems to be positive, up until the middle of the series. From there, a steep decline is present. Towards the end of the series, there is a subtle positive trend.\nTowards the beginning of the series, there seems to be some strong cyclicity. Towards the end of the series, there seems to be more of a seasonal pattern within the data. This cyclicity may be due to the time of year which this data represents, the holiday season.\n\nSince this analysis is focused on creating a forecast for order completions, additional work needed to be done to identify potential series that may improve the forecasts. To do this, several scatter plots were created to help identify variables that relate to order completions.\nBefore additional exploratory plots can be created, though, additional data wrangling steps needed to be taken. Specifically, the data was transformed from a long format to a wide format, where the page variable is turned into several numeric columns within the transformed data set. The following code chunk was used to perform this task.\n\ntidy_trend_wide &lt;- tidy_trend_data %&gt;%\n  select(-unique_users) %&gt;%\n  mutate(page = str_replace(str_to_lower(page), \" \", \"_\")) %&gt;%\n  pivot_wider(\n    names_from = page,\n    values_from = views,\n    names_glue = \"{page}_views\"\n  )\n\nWith data in a wide format, the ggpairs() function from the GGally package was used to create a matrix of scatterplots and correlation estimates for the various series within the dataset.\nHere is the code to perform this analysis and output the matrix of plots.\n\ntidy_trend_wide %&gt;%\n  ggpairs(columns = 2:8)\n\n\n\n\n\n\n\n\nThe scatterplots and correlations output revealed some interesting relationships. For one, although previous exploratory analysis revealed apparel generated high volumes of views, the correlation analysis revealed a slight negative relationship with order completions. However, five variables seem to be highly correlated with order completions: Clearance (.877), Campus Collection (.846), Lifestyle (.769), New (.753), and Shop by Brand (.659). Evidence points to these series as being potentially valuable components of a forecasting model of Order Completed page views."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Narrow the focus further",
    "text": "Narrow the focus further\nAt this point, this post transitions into examining just the Order Completed page views, as this is the time series intended to be forecasted within future analyses done in this series of blog posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Lag plots",
    "text": "Lag plots\nIt‚Äôs time to shift focus onto exploring the characteristics of the outcome variable of future forecasting models, order completions, in more depth. The next step, then, is to examine for any lagged relationships present within the Order Completed page views time series.\nThe gg_lag function from the feats package makes it easy to produce the lag plots. Here the tidy_trend_wide data will be used.\n\ntidy_trend_wide %&gt;%\n  gg_lag(order_completed_views, geom = \"point\")\n\nWarning: `gg_lag()` was deprecated in feasts 0.4.2.\n‚Ñπ Please use `ggtime::gg_lag()` instead.\n\n\n\n\n\n\n\n\n\nThe plots provide little evidence that any lagged relationships‚Äìpositive or negative‚Äìare present within this time series. Thus, no further steps were taken to account for lagged relationships at this point in the analysis."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nExploring for autocorrelation is the next step. A correlogram is created to explore for this characteristic within the series. A correlogram ‚Äúmeasures the linear relationship between lagged values of a time series‚Äù (Hyndman and Athanasopoulos 2021). The ACF is first calculated for each value within the series. This value is then plotted according to it‚Äôs corresponding lag values. The ACF() function from the feasts package was used to calculate these values. The resulting data object is then passed along to the autoplot() function, which creates the correlogram for the data. Here is what the code looks like, along with the outputted plot.\n\ntidy_trend_wide %&gt;%\n  ACF(order_completed_views, lag_max = 28) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Inspect the correlogram",
    "text": "Inspect the correlogram\nThe correlogram clearly shows the data is not a white noise series. Moreover, the plot reveals several structural characteristics within the time series.\n\nThe correlogram, given the smaller lags are large, positive, and seem to decrease with each subsequent lag, which suggests the series contains some type of trend.\nThe plot also reveals a slight scalloped shape (i.e., peaks and valleys at specific intervals), which suggests some seasonality occurring within the process. Indeed, it seems peaks occur every seven days (e.g., lags 7 and 14). Thus, a slight weekly seasonality may be present within the time series.\n\nGiven these structural characteristics of the series, future forecasting steps will need to account for these issues. This topic will be further discussed in future posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "href": "blog/posts/2022-03-03-post-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Trend Decomposition",
    "text": "Trend Decomposition\nThe final exploratory analysis step is to split the series into its several components. This includes the seasonal, trend, and remainder components. Here an additive decomposition is performed. Transformations were not applied to this data before decomposition was performed.\nAn argument could be made to transform this time series using some mathematical operation. Indeed, transforming the series may improve forecasts generated from the data (Hyndman and Athanasopoulos 2021). However, this analysis doesn‚Äôt have access to a complete series of data. Having more data could lead to more informed decisions on the appropriate application of transformations. A full year or multiple years worth of data would be preferred. Interpretability is also a concern, as transformations would need to be converted back to the original scale once the forecast was created. Thus, it was decided that transformations were not going to be applied to the data. More about transforming the series can be referenced here.\nThe series was broken down into its multiple components: seasonal, trend-cycle, and remainder (Hyndman and Athanasopoulos 2021). Decomposing the series allows for more to be learned about the underlying structure of the time series. As a result, allowing for structural components of the time series that could improve forecasting models to be identified. Several functions from the {feasts} and {fabletools} packages simplified the decomposition process.\nFirst, the trend components are calculated using the STL() and model() functions. STL() decomposes the trend. The model() function creates a mabel object of estimates. The components() function is then used to view the model object.\n\norder_views_dcmp &lt;- tidy_trend_wide %&gt;%\n  model(stl = STL(order_completed_views))\n\ncomponents(order_views_dcmp)\n\nWarning in as_dable.tbl_ts(object, method = attrs[[\"method\"]], resp = !!attrs[[\"response\"]], :\npartial argument match of 'resp' to 'response'\n\n\n# A dable: 92 x 7 [1D]\n# Key:     .model [1]\n# :        order_completed_views = trend + season_week + remainder\n   .model event_date order_completed_views trend season_week remainder season_adjust\n   &lt;chr&gt;  &lt;date&gt;                     &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 stl    2020-11-01                    14  47.8      -37.6      3.80           51.6\n 2 stl    2020-11-02                    73  47.9       12.9     12.2            60.1\n 3 stl    2020-11-03                    69  47.9       23.1     -1.99           45.9\n 4 stl    2020-11-04                    47  48.2       11.8    -13.0            35.2\n 5 stl    2020-11-05                    28  48.5       -4.93   -15.5            32.9\n 6 stl    2020-11-06                    62  48.7       13.0      0.285          49.0\n 7 stl    2020-11-07                    34  48.9      -17.9      3.00           51.9\n 8 stl    2020-11-08                    32  51.7      -37.9     18.2            69.9\n 9 stl    2020-11-09                    58  54.5       12.4     -8.95           45.6\n10 stl    2020-11-10                    70  56.6       22.7     -9.33           47.3\n# ‚Ñπ 82 more rows\n\n\nPlotting the decomposition is done by piping the output from the components() function to autoplot(). The visualization will contain the original trend, the trend component, the seasonal component, and the remainder of the series after the trend and seasonal components are removed.\n\ncomponents(order_views_dcmp) %&gt;%\n  autoplot() +\n  labs(x = \"Event Date\")\n\nWarning in as_dable.tbl_ts(object, method = attrs[[\"method\"]], resp = !!attrs[[\"response\"]], :\npartial argument match of 'resp' to 'response'\n\n\n\n\n\n\n\n\n\nScanning the components outputted by the decomposition, a few conclusions were drawn. Looking at the trend component, it seems a steady upward trend takes place from the start to the middle of the series. Then, a sharp negative trend followed by a slight increase towards the tail end of the series is present. Indeed, this might be additional seasonality that might become more apparent if additional data were available.\nThe seasonal component is also interesting here, as some type of cyclic weekly pattern seems to be present. This includes less traffic around the beginning of the week and weekends, where the majority of this cyclic pattern occurs during the week. It‚Äôs also interesting to note a consistent drop occured on most Thursdays of the week."
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html",
    "title": "TIL: Find and replace in Vim",
    "section": "",
    "text": "Today I learned how to find and replace in Vim. I‚Äôve found knowing a few variations of the substitute (:s or su for short) command to be a powerful skill to quickly and efficiently edit code and text within a file. By knowing a few simple command variations, you can greatly improve your productivity. You just have to know the different patterns and when to apply them.\nThis TIL post aims to highlight some of the basics of using Vim‚Äôs :s command. My intention is to get you up and running quickly. As such, this post provides several simple examples applying the command to some practical use cases. Although most of the examples use the R programming language, these concepts can be applied to any programming language or text editing task.\nThis post focuses on the basics. Indeed, the substitute command provides a lot of utility and different options to perform various find and replace editing tasks. If you‚Äôre looking to learn more advanced features, I suggest reading the docs (:help substitute). I also provide some additional links to other resources throughout and at the end of the post if you‚Äôre interested in learning more."
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#the-basics",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#the-basics",
    "title": "TIL: Find and replace in Vim",
    "section": "The basics",
    "text": "The basics\n:s can be used to find each occurance of a text string, and replace it with another text string. Say I have a character vector basket, and it contains an assortment of fruit. However, what if I want to replace the first apple in my basket with an orange using :s? First, I need to move my cursor to the line I want to find the first string. Then, I can enter the following into the command prompt to find the first instance of the string orange and replace it with the string apple:\n:s/orange/apple\nHere is what this looks like in action.\n\nHowever, what if I don‚Äôt want any oranges, and instead I just want apples rather than orangesin my basket. I can append the previous command with g to replace all instances of orange with apple. The g flag indicates to Vim that I want to replace globally to the current line. In other words, replace all instances on the current line.\n:s/orange/apple/g\nBelow is what this will look like in your editor.\n\nWant to find and replace text globally to the line and including multiple lines, then add % to the beginning of the command.\n:%s/orange/apple/g\n\n% is really useful if you want to refactor code efficiently within a file. Check out these two examples, one more contrived, the other a more practical, common application.\n:%s/power/horsepower/g\n:%s/data/cars_data/g"
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#confirming-replacement",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#confirming-replacement",
    "title": "TIL: Find and replace in Vim",
    "section": "Confirming replacement",
    "text": "Confirming replacement\nNot sure what all will be replaced and would rather go through each replacement step-by-step? Add c to the end of your command. Adding this flag will make Vim prompt you to confirm each replacement.\n:%s/orange/apple/gc\n\nIn the prompt, you‚Äôll see something like replace with apple (y/n/a/q/l/^E/^Y). You‚Äôll select the option that fulfills the action you want to perform. Here is a list of what each selection does:\n\ny - substitute this one match and move to the next (if any).\nn - skip this match and move to the next (if any).\na - substitue all (and it‚Äôs all matches) remaining matches.\nq - quit out of the prompt.\nl - subsitute this one match and quit. l is synonymous with ‚Äúlast‚Äù.\n^E - or Ctrl-e will scroll up.\n^Y - or Ctrl-y will scroll down.\n\nThe example above only highlights the use of y, so I suggest experimenting with each selection to get a feel for what they do."
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-by-range",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-by-range",
    "title": "TIL: Find and replace in Vim",
    "section": "Replacing by range",
    "text": "Replacing by range\nTake a look at the command pattern again, specifically the first portion, [range]:\n:[range]s[ubstitute]/{pattern}/{string}/[flags] count\nThe s command provides functionality to scope the find and replace operation to a specific part of your file. Indeed, this functionality was highlighted earlier when we passed % in an earlier command. % just indicated to Vim that we wanted to find and replace all lines in the file. However, we can be more specific.\nSay we now have a much larger basket, one that can hold both fruits and veggies. In the R programming language, this can be modeled using a tribble from the tribble package.\nWhat if we wanted to find the first two instances of carrots in our basket and replace it with kale. This can be done by passing a range at the start of the :s command. In this specific instance, I want to find and replace the carrots on lines 5 and 7 with the string kale, but I don‚Äôt want to change the one on line 8. To do this, I can run the following command:\n:5,7s/carrot/kale/g\n\nAnother variation is to start on your current line . and specify to Vim how many additional lines I would like to find and replace in the range. Keep in mind . represents the current line your cursor is located currently within the file. Once you postion your cursor, your command will look something like this:"
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "title": "TIL: Find and replace in Vim",
    "section": "Replacing from current location to n lines",
    "text": "Replacing from current location to n lines\n:.,+2s/carrot/kale/g\n\nWhat if I had a basket with even more fruits and veggies, and I just wanted to start at my current location and replace all instances that follow? We can use the $ in the range input. The use of the dollar sign indicates to Vim that we want to replace starting at line 8 and go to the end of the file."
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-to-end-of-file",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-to-end-of-file",
    "title": "TIL: Find and replace in Vim",
    "section": "Replacing to end of file",
    "text": "Replacing to end of file\n:8,$s/carrot/kale/g\n\nOr, if you want to start from the current line and replace to the end of the file, you can do the following:\n:.,$s/carrot/kale/g"
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-using-visual-mode",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#replacing-using-visual-mode",
    "title": "TIL: Find and replace in Vim",
    "section": "Replacing using visual mode",
    "text": "Replacing using visual mode\nWe can also use visual mode to set the range of the find and replace operation. Just enter visual mode v or visual line mode Shift-v, highlight the range you want your find and replace operation to be applied, enter into command mode with :, and then enter your find and replace statement. Doing this will start the command line off with '&lt;,'&gt;, and you‚Äôll just need to enter the rest of the command, the {pattern} and {string} portions.\n:'&lt;,'&gt;s/carrot/kale`"
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "title": "TIL: Find and replace in Vim",
    "section": "Use objects in your search buffer",
    "text": "Use objects in your search buffer\nYour previous search history can also be used to do find and replace. Let‚Äôs go back to our miles-per-gallon plot example again. First I‚Äôll hover my cursor over the word I want to replace and hit *. Now we can use the search value in our subsititution command. All I need to do is leave the {pattern} blank in the command. The command will look like this:\n:%s//horsepower"
  },
  {
    "objectID": "blog/posts/2023-03-14-til-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "href": "blog/posts/2023-03-14-til-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "title": "TIL: Find and replace in Vim",
    "section": "Replace with what‚Äôs under your cursor",
    "text": "Replace with what‚Äôs under your cursor\nTo keep things simple, let‚Äôs go back to our first basket example. Specifically, let‚Äôs say I want to modify the string strawberry with the string banana, but use my cursor position to do this. First I have to make sure the cursor is hovering over the word I want to use for my replacement. Then, I enter the below command. When you see &lt;c-r&gt;&lt;c-w&gt;, this means you actually hit Ctrl-R and Ctrl-W on your keyboard. You‚Äôll notice the string banana is populated into the command for us.\n%s/strawberry/&lt;c-r&gt;&lt;c-w&gt;"
  },
  {
    "objectID": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html",
    "href": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html",
    "title": "Exploring objects launched into space and gross domestic product",
    "section": "",
    "text": "3‚Ä¶ 2‚Ä¶ 1‚Ä¶ blastoff üöÄ. This week‚Äôs #tidytuesday dataset focuses on annual objects launched into space by various entities.\nThis data is maintained by the United Nations Office for Outer Space Affairs, and it is made available via the Online Index of Objects Launched into Outer Space. Objects include things like satellites, probes, landers, crewed spacecrafts, and space station flight elements launched into Earth orbit or beyond. Although this list aims to be comprehensive, it only includes launches submitted to the UN by participating nations. In addition, joint launches count as one launch for each country (i.e., counts when examined by country may be duplicated). Initially, Our World in Data processed this data and created an annual trend for each country.\nSince this data is focused on country, my interest peaked by asking the following question: what is the relationship between a country‚Äôs Gross Domestic Product (GDP), a broad indicator or a country‚Äôs economic output, and objects launched into space? To answer this question, I create a scatter plot and quantify this relationship using a simple linear regression in this post."
  },
  {
    "objectID": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html#use-wbstats-package-to-obtain-gdp",
    "href": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html#use-wbstats-package-to-obtain-gdp",
    "title": "Exploring objects launched into space and gross domestic product",
    "section": "Use wbstats package to obtain GDP",
    "text": "Use wbstats package to obtain GDP\nThe original dataset didn‚Äôt contain Gross Domestic Product (GDP). As such, I had to supplement it with additional data from the World Bank. The world bank makes data containing an estimate of GDP available via an API. In fact, the wbstats R package provides an intuitive interface to access data via this API. Here‚Äôs the code I used to return data from the API using the wbstats package:\n\n# Interested in looking at:\n#   * Gross Domestic Product (GDP)\nwb_variables &lt;- c(\n  \"gdp\" = \"NY.GDP.MKTP.CD\"\n)\n\ndata_wb &lt;- wb_data(\n  wb_variables,\n  start_date = 1957,\n  end_date = 2023\n) |&gt;\n  select(\n    code = iso3c,\n    year = date,\n    country,\n    gdp,\n    starts_with(\"tax\")\n  )"
  },
  {
    "objectID": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html",
    "href": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html",
    "title": "Notes: Using gganimate to animate plots",
    "section": "",
    "text": "I recently created a TidyTuesday data visualization utilizing the gganimate RStats package. While using the package, my focus was to ‚Äòget something up and running quickly‚Äô. I wanted to go deeper, though. Below are my notes from diving into the package‚Äôs ‚ÄòGetting Started‚Äô vignette."
  },
  {
    "objectID": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#transitions",
    "href": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#transitions",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Transitions",
    "text": "Transitions\nTransitions is the first concept to know when using gganimate. The terms/key functions include:\n\nThe transition_states() function creates transitions based on some discrete variable within your data. That is, it‚Äôs main purpose is to split the data into various frames, which are then later complied into a .gif file. In the example here, we‚Äôre splitting by the species variable.\ntweening: a calculation performed to ensure the transitions between each state of the animation are smooth.\n\nHere‚Äôs how we create the transitions utilizing the transitions_states() function, outputted as a .gif file.\n\nvis_flip_mass_trans &lt;- vis_flip_mass +\n  transition_states(\n    species,\n    transition_length = 2,\n    state_length = 1\n  )\n\nvis_flip_mass_trans\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhere‚Äôs the .gif?\nI noticed gganimate opens Preview (I‚Äôm on a Mac) after rendering the .gif file. As such, all you see is a collection of static plots. This didn‚Äôt allow me to ‚Äòsee‚Äô the animation.\ngganimate writes the .gif file to a temporary directory. The tempdir() function can be used to print the file path of the temporary directory used for the current R session. This file path can then be used to point a web browser to the location of the .gif file. This video does a pretty good job highlighting how to do this using Google Chrome.\nThe anim_save() function can be used to save the .gif file wherever is most convenient."
  },
  {
    "objectID": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#easing",
    "href": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#easing",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Easing",
    "text": "Easing\nEasing is the next key concept to know when using the gganimate package.\n\nEasing: a calculation that takes place to create intermediary data for the tweening to occur. In other words, it‚Äôs a calculation to specify the velocity of change taking place between the transitions. gganimate has various types of easing that can be applied to the transitions. gganimate‚Äôs ease_aes() function is used to specify the different types of easing.\n\nHere are a few examples applied to our penguins scatterplot:\n\nvis_flip_mass_trans +\n  ease_aes(\"quintic-in-out\")\n\n\n\n\n\n\nvis_flip_mass_trans +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\n\n\nvis_flip_mass_trans +\n  ease_aes(\"quadratic-in-out\")\n\n\n\n\n\n\nvis_flip_mass_trans +\n  ease_aes(\"exponential-in-out\")\n\n\n\n\n\nA key point here is you have to specify the type of easing function to use, along with a modifier. You can read more about what‚Äôs available by viewing the ?ease_aes function‚Äôs documentation. Many combinations are available."
  },
  {
    "objectID": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#labelling",
    "href": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#labelling",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Labelling",
    "text": "Labelling\nThe ability to add labels to our animations is the next key concept to know. gganimate makes it easy to show dynamic labels. This is due to the package providing glue like syntax for plot labelling. For instance, let‚Äôs say we wanted to include the type of penguin species within the title to match the current state of the data being shown. This can be done by doing the following:\n\nvis_flip_mass_trans +\n  labs(\n    title = \"Flipper length and body mass for {closest_state} penguins\"\n  )\n\n\n\n\n\ngganimate also makes other transition variables available for labelling. These include frame and nframes. Check out the docs for more information and additional examples."
  },
  {
    "objectID": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#object-permanence",
    "href": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#object-permanence",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Object Permanence",
    "text": "Object Permanence\nObject permanence is another important concept to consider, especially as it relates to the semantics and validity of your plot. I think of object permanence like this: data points may not be connected, so the animation applied should avoid implying a connection. This isn‚Äôt true in all cases (i.e., timeseries data). However, it‚Äôs important to consider that if you have distinct classes in your data, the animations should make this clear. In the context of the penguins example, the different penguin species are not related, but with how the animation morphs to different species implies that they are connected‚Äìthis is not true for this data.\nAs the docs mention, we need to tell gganimate to not morph observations between different categories in our data to make it clear that observations are not connected. The docs provide two suggestions for fixing this: add an aesthetic to distinguish between the groups or set the group directly. So, let‚Äôs apply the doc‚Äôs preferred fix to our example: set the group directly.\n\nggplot(penguins, aes(flipper_len, body_mass)) +\n  geom_point(aes(colour = species, group = 1L)) +\n  transition_states(\n    species,\n    transition_length = 2,\n    state_length = 1\n  ) +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\n\nNow the transitions make it a little more clear that these data are separate and not connected. This is done utilizing color and the different transitions states. Despite our best efforts, though, a different type of transition might make these more clear. This is where entering and exiting can be applied."
  },
  {
    "objectID": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#entering-and-exiting",
    "href": "blog/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#entering-and-exiting",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Entering and Exiting",
    "text": "Entering and Exiting\nAs the docs mention, another alternative to fix our problem above is to provide transitions to have data appear and disapear. This is where the enter_*() and exit_*() family of functions become useful.\nLet‚Äôs apply some of these functions to our penguins plot. Here‚Äôs a few examples:\n\nanim &lt;- ggplot(penguins, aes(flipper_len, body_mass)) +\n  geom_point(aes(colour = species), size = 2) +\n  transition_states(\n    species,\n    transition_length = 2,\n    state_length = 1\n  )\n\nanim +\n  enter_fade() +\n  exit_shrink()\n\n\n\n\n\n\nanim +\n  enter_grow() +\n  exit_recolour(colour = \"#000000\")"
  },
  {
    "objectID": "blog/posts/2022-09-20-post-flattening-google-analytics-4-data/index.html",
    "href": "blog/posts/2022-09-20-post-flattening-google-analytics-4-data/index.html",
    "title": "Flattening Google Analytics 4 data",
    "section": "",
    "text": "Introduction\nWith the introduction of the Google Analytics 4 (GA4) BigQuery integration, understanding how to work with the underlying analytics data has become increasingly important. When first diving into this data, some of the data types may seem hard to work with. Specifically, analysts might be unfamiliar with the array and struct data types. Even more unfamiliar may be the combination of these two data types into complex, nested and repeated data structures. As such, some may become frustrated writing queries against this data. I know I did.\nIf you‚Äôre mainly coming from working with flat data files, these more complex data types may not be intuitive to work with, as the SQL syntax is not as straight forward as a simple SELECT FROM statement. Much of this unfamiliarity may come from the required use of unfamiliar BigQuery functions and operators, many of which are used to transform data from nested, repeated, or nested repeated structures to a flattened, denormalized form.\nAs such, this post aims to do three things: 1. Overview the array, struct, and array of struct data types in BigQuery; 2. Overview some of the approaches to flatten these data types; and 3. Apply this knowledge in the denormalization of Google Analytics 4 data stored in BigQuery.\nThis post mostly serves as notes that I wish I had when I began working with these data structures.\n\n\nArrays, structs, and array of structs\nBefore discussing the use of these data types in GA4 data, let‚Äôs take a step back and simply define what array and struct data types are in BigQuery. A good starting point is BigQuery‚Äôs arrays and structs documentation. According to the docs,\n\nAn array is an ordered list of zero or more elements of non-Array values. Elements in an array must share the same type.\n\n\nA struct is a container of ordered fields each with a type (required) and field name (optional).\n\nBoth definitions contain technical jargon that don‚Äôt really define, in an intuitive, useful way, what these data types are and how to use them, especially in the analysis of GA4 data. So let‚Äôs break each down by bringing in additional perspectives and through the use of several simplified examples.\nWhile learning more about arrays and structs, I found several blog posts that helped me better understand these structures and how to use them. Here is a list of the ones I found to be very helpful:\n\nHow to work with Arrays and Structs in Google BigQuery by Deepti Garg\nExplore Arrays and Structs for Better Query Performance in Google BigQuery by Skye Tran\nTutorial: BigQuery arrays and structs from Sho‚Äôt left to data science\n\nI highly suggest reading all of these. In fact, much of what follows is adapted from these posts, with a few examples I created to help me better understand how these data types are structured, stored, and queried. Towards the end of the post, the techniques learned from these posts and overviewed here will be applied to GA4 data, specifically the publicly available bigquery-public-data.ga4_obfuscated_sample_ecommerce data.\n\n\nArrays\nArrays are a collection of elements of the same datatype. If you‚Äôre familiar with the R programming language, an array is similair to a vector.\nLet‚Äôs create a table containing an array of planets in our solar system as an example, and then use the INFORMATION_SCHEMA view to verify the data was entered correctly. The following code will create this table in BigQuery:\ncreate or replace table examples.array_planets_example as \nwith a as (\n   select [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"] as planets\n)\nselect planets from a;\nThe INFORMATION_SCHEMA.COLUMNS view for the array_planets_example table can be queried to verify the data was entered correctly. This table is available for every table created in BigQuery, and it contains metadata about the table and the fields within. Here is the query needed to return this information:\nselect table_name,\n   column_name,\n   is_nullable,\n   data_type\nfrom examples.INFRORMATION_SCHEMA.COLUMNS\nwhere table_name = \"array_planets_example\";\nThe returned table will contain a data_type field, where the value ARRAY&lt;STRING&gt; will be present. This value represents the field in the array_planets_example contains an array with a list of string values. Although this example array contains a series of string values, arrays can hold various other data types, as long as the values are the same type across the collection. Overviewing all of the different data types that can be stored in an array is beyond the scope of this post, but check out the BigQuery docs for more examples.\n\nQuerying an array\nMultiple approaches are available to query an array. The type of approach will depend on if the returned data needs to maintain its grouped, repeated structure, or if the returned data needs to be flattened. If maintaining the repeated structure is required, then a simple SELECT statement will work. Using the array_planets_example table as an example, the query applying this approach will look something like this:\nselect planets\nfrom examples.array_planets_example\nIf each element of the array is to be outputted onto its own row (i.e., denormalized), multiple approaches are available. The first approach is to use the unnest() function. Here is an example using the planets array we created earlier:\nselect planets\nfrom examples.array_planets_example,\nunnest(planets) as planets\nThe second approach is to apply a correlated join through the use of cross join unnest(). This approach looks like this:\nselect planets\nfrom examples.array_planets_example\ncross join unnest(planets) as planets\nYou‚Äôll notice this is only slightly different than the query above, and in fact the , used in the FROM clause is short-hand for the cross join statement. The last and final approach is to use a comma-join. This is similair to our first query, but now we refer to the table name before the array name we want flattened.\nselect planets\nfrom examples.array_planets_example, array_planets_example.planets as planets;\nWhich one do you choose? It really comes down to a matter of preference. All three approaches will lead to the same result. It depends on how explicit you want the code to be.\nThere is one note to be aware of if you‚Äôre applying these conventions to other arrays outside of analyzing GA4 data. The cross join approach will exclude NULL arrays. So if you want to retain rows containing NULL arrays, you‚Äôll need to apply a left join. More about this is described in the BigQuery docs.\nKeep these approaches top-of-mind. They will be applied to flatten some of the fields in the GA4 dataset. In other words, get comfortable with using them.\n\n\n\nStructs\nThe structs data type holds attributes in key-value pairs. Structs can hold many different data types, even structs. We will see the use of structs within structs in the GA4 data. Keeping with the solar system theme of the post, the following example code will create a table utilizing the struct data type to hold the dimensions and distances of the planets in our solar system. The data used for this table is reported here.\ncreate or replace table examples.struct_solar_system as\nwith a as (\n  select \"Mercury\" as planet,\n  struct(0.39 as au_sun, 57900000 as km_sun, 4879 as km_diameter) as dims_distance union all\n  select \"Venus\" as planet,\n  struct(0.72 as au_sun, 108200000 as km_sun, 12104 as km_diameter) as dims_distance union all\n  select \"Earth\" as planet,\n  struct(1 as au_sun, 149600000 as km_sun, 12756 as km_diameter) as dims_distance union all\n  select \"Mars\" as planet,\n  struct(1.52 as au_sun, 227900000 as km_sun, 6792 as km_diameter) as dims_distance union all\n  select \"Jupiter\" as planet,\n  struct(5.2 as au_sun, 778600000 as km_sun, 142984 as km_diameter) as dims_distance union all\n  select \"Saturn\" as planet,\n  struct(9.54 as au_sun, 1433500000 as km_sun, 120536 as km_diameter) as dims_distance union all\n  select \"Uranus\" as planet,\n  struct(19.2 as au_sun, 2872500000 as km_sun, 51118 as km_diameter) as dims_distance union all\n  select \"Neptune\" as planet,\n  struct(30.06 as au_sun, 4495100000 as km_sun, 49528 as km_diameter) as dims_distance \n)\nselect * from a;\nThis table contains two columns. A column that holds a string value for the name of the planet and a struct column that contains a list of key value pairs of distance and dimensions for each planet.\nThe INFRORMATION_SCHEMA.COLUMNS table can then be queried again to verify the datatypes for each column were inputted correctly. Here is the code to do this:\nselect \n  table_name, \n  column_name,\n  is_nullable,\n  data_type\nfrom examples.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"struct_solar_system\";\nThe returned table will contain a data_type column with two values: STRING and STRUCT&lt;au_sun FLOAT64, km_sun INT64, km_diameter INT64&gt;. Take notice that the STRUCT value contains information about the data types contained within.\n\nQuerying a struct\nQuerying a struct requires the use of the . operator (i.e., dot operator) in the FROM clause to flatten the table. Take for example the case where we want to return a table of only the distance of each planet from the sun in kilometers. The following query will be used:\nselect \n  planet,\n  dims_distance.km_sun\nfrom examples.struct_solar_system;\nSay a denormalized table that contains both the distance from the sun in kilometers and each planet‚Äôs diameter in kilometers is wanted. The following query would be used:\nselect \n  planet,\n  dims_distance.km_sun,\n  dims_distance.km_diameter\nfrom examples.struct_solar_system;\nWhen reviewing these two examples, observe how the dot notation is being used. In the first, our select statement contains dims_distance.km_sun, which unnests the values and gives each its own row for each planet. This is expanded in the second query, where an additional line is added to the select statement, dims_distance.km_diameter. To unnest all the values in the struct, use the following query:\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system;\nIn fact, let‚Äôs expand this query to answer the following question: which planets are the closest and farthest from our sun. Take notice how the ORDER BY portion of the query doesn‚Äôt require the dims_distance prefix for the field we want to arrange our data.\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system\norder by km_sun;\n\n\n\nArray and structs in GA4 data\nNow that we have learned a little bit about our solar system, let‚Äôs return to Earth and the task at hand, flattening GA4 data. We just discussed how these data types are created and queried, it is now time to combine them into more complex data structures, as both of these structures are combined to create nested repeated data structures in the GA4 data. It‚Äôs best to start with an example. Specifically, let‚Äôs look at how these structures are applied in the event_params field.\nWe can start off by querying the INFORMATION_SCHEMA.COLUMNS view for one event to get an idea of its structure. The query to do this can be seen here:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"events_20210131\" and column_name = \"event_params\";\nThe data type is described in the returned table‚Äôs data_type field. This field contains the following value ARRAY&lt;STRUCT&lt;key STRING, value STRUCT&lt;string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64&gt;&gt;&gt;. It should be immediately apparent that both the array and struct values are being used here to create a repeated nested structure. In fact, the event_params value uses a struct within a struct. Given this structure, all the above methods will need to be employed to flatten this data.\nTo simplify this, let‚Äôs look at one instance of one event in the GA4 data. Specifically, let‚Äôs look at one instance of a page_view event. With this simplified example, we‚Äôll go step-by-step, adding additional elements to the query needed to flatten this data.\nselect \n  event_date,\n  event_timestamp,\n  event_name,\n  event_params\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = 'page_view'\nlimit 1;\nAfter running this query, you‚Äôll notice the output to the console is quite verbose, especially if you‚Äôre using the bq command-line tool. The verbosity of the output is due to the event_params field holding much of the data.\nThe first layer of the structure is an array, so the initial step is to use the unnest() function. The following can be done to achieve this:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param;\nYou‚Äôll notice a nested FROM statement is being used here. This is done to limit the result set to one row, representing one page_view event for this simplified example. Later iterations of the query will eliminate this nested query.\nNow say we‚Äôre only interested in viewing the page_location parameter. We can use a where statement to filter out this information. Here is what this will look like:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key = 'page_location';\nInterested in viewing both the page_location and page_title parameters? Use the IN operator in the WHERE clause.\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key in ('page_location', 'page_title');\nWanna turn the key field into columns so you only have one row for this specific event? Use BigQuery‚Äôs pivot() operator. Here is how to achieve this in a query:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    key,\n    value.string_value\n  from (\n    select \n      event_date,\n      event_timestamp,\n      event_name,\n      event_params\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nSince the string values are all we care about here, the value.string_value was the only one retained in the query. The other nested value elements were eliminated from the SELECT statement.\n\n\nCombine other nested fields in the GA4 data\nNow that the event_params field has been flattened, let‚Äôs supplement this information with additional data in the table. Moreover, this will provide another example of how to apply these steps to flatten other elements in the GA4 data. Knowing where users originate is some additional context that may add to our event analysis, so let‚Äôs add that data to our flattened data. But first, let‚Äôs get some more information on what data type is used for the geo field in the GA4 data.\nOnce again, querying the INFORMATION_SCHEMA.COLUMNS view can be used to explore the geo field‚Äôs data type. Here is what the query looks like:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS`\nwhere table_name = \"events_20210131\" and column_name = \"geo\";\nThe value STRUCT&lt;continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING&gt; is returned. Let‚Äôs write a query to return the table without first unnesting the data.\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo \nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = \"page_view\"\nlimit 1;\nYou‚Äôll notice this field contains a struct, where the dot operator will need to be applied to flatten this data. Let‚Äôs start by flattening this data and then combine it with the events_param data. For the sake of keeping the returned table simple, let‚Äôs just return the region and city fields in a denormalized form. The following will return a flattened table with these fields:\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo.region,\n  geo.city\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nlimit 1;\nAs expected, the table will return a flattened table containing five fields: event_date, event_name, user_pseudo_id, geo.region, and geo.city. This table was also limited to return only the first instance of the page_view in the table.\nNow, the next step is to add this geo data to our flattened event_params query. This is as simple as adding the . operator with the needed geo elements into the FROM statement. The query will now look like this:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nThe resulting table will contain one row with several fields representing the specific event. This is great for one event, but the next step will be to expand this denormalization to all page_view events in the table.\n\n\nExpand the unnesting to multiple page view events\nNow that we have the flattened table for one page_view event, let‚Äôs expand it to additional events. This requires a simple modification to the initial nested query, remove the limit 1 line.\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\nWe can now refactor the query to be more concise. Here is what this will look like:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\n\n\nApply these approaches across multiple days\nGenerating results for one day may not be enough, so there‚Äôs a few modifications that can be made to expand the final query to return additional days. This involves modifying the FROM and WHERE statements in the initial query.\nThe first step is to modify the FROM statement to use the * wildcard operator at the end of the table name. Since the GA4 tables are partitioned by day, this will allow for a range of tables to be defined within the WHERE clause. The table name will now be bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*.\nTo define the range of dates for the events (i.e., to query multiple tables), the WHERE clause will be expanded to include the use of _table_suffix. The _table_suffix is a special column used within a separate wildcard table that is used to match the range of values. Explaining the use of the wildcard table is beyond the scope of this post, but more about how this works can be found here. The WHERE clause will now look like this:\nwhere event_name = 'page_view' and\nkey in ('page_location', 'page_title') and\n_table_suffix between \"20210126\" and \"20210131\"\nYou‚Äôll notice this statement uses the between operator, where two string values representing the date range are passed. This statement is inclusive, so it will include partitioned tables from 20210126 and 20210131, and all tables in between. Here is the query in its final form:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and \n  key in ('page_location', 'page_title') and\n  _table_suffix between \"20210126\" and \"20210131\"\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\norder by event_date\n\n\nWrap up\nThis post started out simple by defining what arrays, structs, and array of structs data types are in BigQuery. Through the use of several examples, this post overviewed several approaches to query these different data types, specifically highlighting how to flatten each type. A second aim of this post was to show the application of these methods to the flattening of GA4 data stored in BigQuery. This included the flattening and combination of the complex, nested, repeated and nested repeated data types used in the event_params and geo fields. Finally, this post shared queries that expanded the result set across multiple days worth of data.\nIf you found this post helpful or just have interest in this type of content, I would appreciate the follow on GitHub and/or Twitter. If you have suggestions on how to improve these queries or found something that I missed, please file an issue in the repo found here.\n\n\nAdditional resources\nI spent a lot of time researching how to write, use, and query arrays and structs in BigQuery. In the process of preparing this post, I wrote a lot of example queries and followed along with BigQuery‚Äôs turtorial on working with arrays and structs. As a result, I created multiple files that I organized into the GitHub repo for this post. These might be useful as a review after reading this post, or they might be a helpful quickstart quide for your own analysis of GA4 data stored in BigQuery. These additional notes can be found here.\n\n\nAdditional references\n\nHow to work with Arrays and Structs in Google BigQuery\nExplore Arrays and Structs for Better Query Performance in Google BigQuery\nTutorial: BigQuery arrays and structs\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2022,\n  author = {Berke, Collin K},\n  title = {Flattening {Google} {Analytics} 4 Data},\n  date = {2022-09-20},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2022. ‚ÄúFlattening Google Analytics 4\nData.‚Äù September 20, 2022."
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "",
    "text": "The initial rounds of the NCAA women‚Äôs volleyball tournament have just begun. As such, I felt it was a good opportunity to understand more about the game while learning to specify models using Big Ten women‚Äôs volleyball match data and the tidymodels framework. This post sought to specify a predictive model of wins and loses. It then used this model to explore and predict match outcomes of the #1 team going into the tournament, the Nebraska Cornhuskers.\nThis post overviews the use of the tidymodels framework to fit and train predictive models. Specifically, it aims to be an introductory tutorial on the use of tidymodels to split data into test and training sets, specify a model, and assess model fit using both the training and testing data. To do this, I explored the fit of two binary classification models to NCAA Big Ten women‚Äôs volleyball match data, with the goal to predict wins and loses.\nBeing a high-level overview, this post will not cover topics like feature engineering, resampling techniques, hyperparameter tuning, or ensemble methods. Most assuredly, additional modeling procedures would lead to improved model predictions. As such, I plan to write future posts overviewing these topics.\nLet‚Äôs attach the libraries we‚Äôll need for the session.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(here)\nlibrary(glue)\nlibrary(rpart.plot)\nlibrary(patchwork)\ntidymodels_prefer()"
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#feature-exploration",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#feature-exploration",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Feature exploration",
    "text": "Feature exploration\nGiven the number of features in the data, we can easily obtain summary information using skimr::skim().\n\nskim(data_vball_train)\n\n\nData summary\n\n\nName\ndata_vball_train\n\n\nNumber of rows\n1243\n\n\nNumber of columns\n27\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nfactor\n1\n\n\nnumeric\n23\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nteam_name\n0\n1\n13\n25\n0\n14\n0\n\n\nopponent\n0\n1\n3\n40\n0\n329\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2021-01-22\n2023-11-25\n2022-09-09\n228\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nw_l\n0\n1\nFALSE\n2\nwin: 701, los: 542\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nset_wins\n0\n1\n2.03\n1.22\n0.00\n1.00\n3.00\n3.00\n3.00\n‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñá\n\n\nset_loss\n0\n1\n1.65\n1.31\n0.00\n0.00\n2.00\n3.00\n3.00\n‚ñÜ‚ñÉ‚ñÅ‚ñÇ‚ñá\n\n\ns\n0\n1\n3.68\n0.76\n3.00\n3.00\n4.00\n4.00\n5.00\n‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÉ\n\n\nkills\n0\n1\n47.02\n11.49\n15.00\n39.00\n46.00\n55.00\n84.00\n‚ñÅ‚ñÜ‚ñá‚ñÖ‚ñÅ\n\n\nerrors\n0\n1\n18.86\n6.57\n3.00\n14.00\n19.00\n23.00\n44.00\n‚ñÉ‚ñá‚ñá‚ñÇ‚ñÅ\n\n\ntotal_attacks\n0\n1\n125.31\n30.19\n63.00\n101.00\n121.00\n148.00\n237.00\n‚ñÉ‚ñá‚ñÜ‚ñÇ‚ñÅ\n\n\nhit_pct\n0\n1\n0.23\n0.09\n-0.10\n0.17\n0.23\n0.30\n0.54\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÅ\n\n\nassists\n0\n1\n43.23\n10.84\n15.00\n36.00\n43.00\n50.50\n75.00\n‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñÅ\n\n\naces\n0\n1\n5.18\n2.86\n0.00\n3.00\n5.00\n7.00\n18.00\n‚ñÖ‚ñá‚ñÉ‚ñÅ‚ñÅ\n\n\nserr\n0\n1\n7.97\n3.41\n1.00\n5.00\n8.00\n10.00\n23.00\n‚ñÖ‚ñá‚ñÖ‚ñÅ‚ñÅ\n\n\ndigs\n0\n1\n51.55\n15.14\n16.00\n40.00\n49.00\n61.00\n108.00\n‚ñÇ‚ñá‚ñÖ‚ñÇ‚ñÅ\n\n\nblock_solos\n0\n1\n1.64\n1.62\n0.00\n1.00\n1.00\n2.00\n12.00\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nblock_assists\n0\n1\n14.25\n6.78\n0.00\n10.00\n14.00\n18.00\n38.00\n‚ñÇ‚ñá‚ñÜ‚ñÇ‚ñÅ\n\n\nopp_kills\n0\n1\n43.64\n15.01\n0.00\n34.00\n44.00\n54.00\n84.00\n‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñÅ\n\n\nopp_errors\n0\n1\n19.34\n7.05\n0.00\n15.00\n20.00\n24.00\n46.00\n‚ñÅ‚ñÜ‚ñá‚ñÇ‚ñÅ\n\n\nopp_total_attacks\n0\n1\n121.76\n36.90\n0.00\n100.00\n119.00\n148.00\n237.00\n‚ñÅ‚ñÇ‚ñá‚ñÖ‚ñÅ\n\n\nopp_hit_pct\n0\n1\n0.19\n0.10\n-0.14\n0.13\n0.20\n0.25\n0.50\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÅ\n\n\nopp_assists\n0\n1\n40.20\n13.99\n0.00\n31.00\n41.00\n50.00\n75.00\n‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñÅ\n\n\nopp_aces\n0\n1\n4.56\n2.91\n0.00\n2.00\n4.00\n6.00\n14.00\n‚ñÜ‚ñá‚ñÜ‚ñÇ‚ñÅ\n\n\nopp_serr\n0\n1\n7.58\n3.66\n0.00\n5.00\n7.00\n10.00\n23.00\n‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÅ\n\n\nopp_digs\n0\n1\n48.99\n17.90\n0.00\n38.00\n48.00\n60.00\n108.00\n‚ñÅ‚ñÜ‚ñá‚ñÉ‚ñÅ\n\n\nopp_block_solos\n0\n1\n1.43\n1.44\n0.00\n0.00\n1.00\n2.00\n12.00\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nopp_block_assists\n0\n1\n12.71\n7.30\n0.00\n8.00\n12.00\n18.00\n40.00\n‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÅ\n\n\n\n\n\nA few things to note from the initial exploratory data analysis:\n\nTeam errors, attacks, and digs distribution exhibits a slight right skew.\nAces, service errors, block solos, opponent aces, opponent errors, opponent block solos, and opponent block assists exhibit a greater degree of skewness to the right.\n\nAn argument could be made for further exploratory analysis of these variables, followed by some feature engineering. Although this additional work may improve our final predictive model, this post is a general overview of specifying, fitting, and assessing models using the tidymodels framework. I will thus not address these topics further. However, I intend to write a future post focusing on feature engineering using tidymodels‚Äô recipes package."
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#examine-correlations-among-features",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#examine-correlations-among-features",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Examine correlations among features",
    "text": "Examine correlations among features\nThe next step in the exploratory analysis is to identify the presence of any correlations among features. This can easily be done using functions from the corrr package. Specifically, the correlate() function calculates correlations among the various numeric features within our data. The output from the correlate() function is then passed to the autoplot() method, which outputs a visualization of the correlations values.\n\ndata_vball_train |&gt;\n  correlate() |&gt;\n  corrr::focus(-set_wins, -set_loss, -s, mirror = TRUE) |&gt;\n  autoplot(triangular = \"lower\")\n\nNon-numeric variables removed from input: `date`, `team_name`, `opponent`, and `w_l`\nCorrelation computed with\n‚Ä¢ Method: 'pearson'\n‚Ä¢ Missing treated using: 'pairwise.complete.obs'\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n‚Ñπ The deprecated feature was likely used in the corrr package.\n  Please report the issue at &lt;https://github.com/tidymodels/corrr/issues&gt;.\n\n\n\n\n\n\n\n\n\nThe plot indicates correlations of varying degrees among features. Feature engineering and feature reduction approaches could be used to address these correlations. However, these approaches will not be explored in this post."
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#specify-our-models",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#specify-our-models",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Specify our models",
    "text": "Specify our models\nTo keep things simple, I‚Äôll explore the fit of two models to the training data. However, tidymodels has interfaces to fit a wide-range of models, many of which are implemented via the parsnip package.\nThe models I intend to fit to our data include:\n\nA logistic regression using glm.\nA decision tree using rpart.\n\nWhen specifying a model with tidymodels, we do three things:\n\nUse parsnip functions to specify the mathematical structure of the model we intend to use (e.g., logistic_reg(); decision_tree()).\nSpecify the engine we want to use to fit our model. This is done using the set_engine() function.\nWhen required, we declare the mode of the model (i.e., is it regression or classification). Some models can perform both, so we need to explicitly set the mode with the set_mode() function.\n\nSpecifying the two models in this post looks like this:\n\n# Logistic regression specification\nlog_reg_spec &lt;-\n  logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n# Decision tree specification\ndt_spec &lt;-\n  decision_tree() |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nLet‚Äôs take a moment to breakdown what‚Äôs going on here. The calls to logistic_regression() and decision_tree() establishes the mathematical structure we want to use to fit our model to the data. set_engine(\"glm\") and set_engine(\"rpart\") specifies the model‚Äôs engine, i.e., the software we want to use to fit our model. For our decision tree, since it can perform both regression and classification, we specify it‚Äôs mode using set_mode(\"classification\"). You‚Äôll notice our logistic regression specification excludes this function. This is because logistic regression is only used to perform classification, thus we don‚Äôt need to set its mode.\nIf you‚Äôre curious or want more information on what parsnip is doing in the background, you can pipe the model specification object to the translate() function. Here‚Äôs what the output looks like for our decision tree specification:\n\ndt_spec |&gt; translate()\n\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\nModel fit template:\nrpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\nIf you‚Äôre interested in viewing the types of engines available for your model, you can use parsnip‚Äôs show_engines() function. Here you‚Äôll need to pass a string character of the model function you want to explore as an argument. This is what this looks like for logistic_reg():\n\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 √ó 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification"
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#create-workflows",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#create-workflows",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Create workflows",
    "text": "Create workflows\nFrom here, we‚Äôll create workflow objects using tidymodel‚Äôs workflow package. Workflow objects make it easier to work with different modeling objects by combining objects into one object. Although this isn‚Äôt too important for our current modeling task, the use of workflows will be beneficial later when we attempt to improve upon our models, like I‚Äôll do in future posts.\nIn this case, our model specification and model formula are combined into a workflow object. Here I just choose a few features to include within the model. For this post, I mainly focused on using team oriented features within our model to predict wins and losses. Indeed, others could have been included, as the data also contained opponent oriented statistics. To keep things simple, however, I chose to only include the following features within our model:\n\nHitting percentage\nErrors\nBlock solos\nBlock assists\nDigs\n\nThe workflow() function sets up the beginning of our workflow object. We‚Äôll add the model object with add_model(), followed by the formula object using add_formula().\n\nlog_reg_wflow &lt;-\n  workflow() |&gt;\n  add_model(log_reg_spec) |&gt;\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n\ndt_wflow &lt;-\n  workflow() |&gt;\n  add_model(dt_spec) |&gt;\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n\nThis syntax can be a bit long, so there‚Äôs a shortcut. We can pass both the model formula and the model specification as arguments to the workflow() function instead of using a piped chain of functions.\n\nlog_reg_wflow &lt;-\n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    log_reg_spec\n  )\n\ndt_wflow &lt;-\n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    dt_spec\n  )"
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#fit-our-models",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#fit-our-models",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Fit our models",
    "text": "Fit our models\nNow with our models specified, we can go about fitting our model to the training data using the fit() method. We do the following to fit both models to the training data:\n\nlog_reg_fit &lt;- log_reg_wflow |&gt; fit(data = data_vball_train)\n\nWarning in y_levels$lvl: partial match of 'lvl' to 'lvls'\n\ndt_fit &lt;- dt_wflow |&gt; fit(data = data_vball_train)\n\nWarning in y_levels$lvl: partial match of 'lvl' to 'lvls'\n\n\nLet‚Äôs take a look at the log_reg_fit and dt_fit fit objects.\n\nlog_reg_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Formula\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:      1703 \nResidual Deviance: 874.4    AIC: 886.4\n\n\n\ndt_fit\n\n‚ïê‚ïê Workflow [trained] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Formula\nModel: decision_tree()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct&lt; 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists&lt; 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists&gt;=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct&lt; 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct&gt;=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs&lt; 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs&gt;=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct&gt;=0.2255 642  81 win (0.1261682 0.8738318) *\n\n\nWhen the fit objects are called, tidymodels prints information about our fitted models to the console. First, we get notified this object is a trained workflow. Second, preprocessing information is included. Since we only set a model function during preprocessing, we only see the model formula printed in this section. Lastly, tidymodels outputs model specific information and summary information about the model fit.\n\nExplore the fit\nNow that we have the fit object, we can obtain more information about the fit using the extract_fit_engine() function.\n\nlog_reg_fit |&gt; extract_fit_engine()\n\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:      1703 \nResidual Deviance: 874.4    AIC: 886.4\n\n\n\ndt_fit |&gt; extract_fit_engine()\n\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct&lt; 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists&lt; 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists&gt;=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct&lt; 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct&gt;=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs&lt; 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs&gt;=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct&gt;=0.2255 642  81 win (0.1261682 0.8738318) *\n\n\nThe output when passing the fit object to the extract_fit_engine() is similar to what was printed when we called the fit object alone. However, the extract_* family of workflow functions are great for extracting elements of a workflow. According to the docs (?extract_fit_engine), this family of functions are helpful when accessing elements within the fit object. This is especially helpful when needing to pass along elements of the fit object to generics like print(), summary(), and plot().\n\n# Not evaluated to conserve space, but I encourage\n# you to run it on your own\nlog_reg_fit |&gt; extract_fit_engine() |&gt; plot()\n\nAlthough extract_* functions afford convenience, the docs warn to avoid situations where you invoke a predict() method on the extracted object. Specifically, the docs state:\n\nThere may be preprocessing operations that workflows has executed on the data prior to giving it to the model. Bypassing these can lead to errors or silently generating incorrect predictions.\n\nIn other words,\n\n# BAD, NO NO\nlog_reg_fit |&gt; extract_fit_engine() |&gt; predict(new_data)\n\n# Good\nlog_reg_fit |&gt; predict(new_data)\n\nThe fit object can also be passed to other generics, like broom::tidy(). The general tidy() method, when passed a fit object, is useful to view and use the coefficients table from the logistic regression model.\n\ntidy(log_reg_fit)\n\n# A tibble: 6 √ó 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -8.55     0.769     -11.1   1.10e-28\n2 hit_pct        30.1      2.09       14.4   5.50e-47\n3 errors         -0.0128   0.0212     -0.603 5.46e- 1\n4 block_solos     0.200    0.0515      3.89  1.02e- 4\n5 block_assists   0.0876   0.0137      6.39  1.64e-10\n6 digs            0.0137   0.00703     1.96  5.05e- 2\n\n\nBeyond summarizing the model with the coefficients table, we can also create some plots from the model‚Äôs predictions from the training data. Here we need to use the augment() function. Later, we‚Äôll explore this function in more depth when we calculate assessment metrics. For now, I‚Äôm using it to obtain the prediction estimates for winning.\n\ndata_vball_aug &lt;- augment(log_reg_fit, data_vball_train)\n\nWith this data, we can visualize these prediction estimates with the various features used within the model. Since we‚Äôre creating several visualizations using similar code, I created a plot_log_mdl() function to simplify the plotting. Lastly, I used the patchwork package to combine the plots into one visualization. Below is the code to create these visualizations.\n\nplot_log_mdl &lt;- function(data, x_var, y_var, color) {\n  ggplot() +\n    geom_point(\n      data = data_vball_aug,\n      aes(x = {{ x_var }}, y = {{ y_var }}, color = {{ color }}),\n      alpha = .4\n    ) +\n    geom_smooth(\n      data = data_vball_aug,\n      aes(x = {{ x_var }}, y = {{ y_var }}),\n      method = \"glm\",\n      method.args = list(family = \"binomial\"),\n      se = FALSE\n    ) +\n    labs(color = \"\") +\n    theme_minimal()\n}\n\n\nplot_hit_pct &lt;-\n  plot_log_mdl(data_vball_aug, hit_pct, .pred_win, w_l)\n\nplot_errors &lt;-\n  plot_log_mdl(data_vball_aug, errors, .pred_win, w_l)\n\nplot_block_solos &lt;-\n  plot_log_mdl(data_vball_aug, block_solos, .pred_win, w_l) +\n  scale_x_continuous(labels = label_number(accuracy = 1))\n\nplot_block_assists &lt;-\n  plot_log_mdl(data_vball_aug, block_assists, .pred_win, w_l)\n\nplot_digs &lt;-\n  plot_log_mdl(data_vball_aug, digs, .pred_win, w_l)\n\nwrap_plots(\n  plot_hit_pct,\n  plot_errors,\n  plot_block_solos,\n  plot_block_assists,\n  plot_digs,\n  guides = \"collect\"\n) &\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nTo summarise our decision tree, we need to use the rpart.plot package to create a plot of the tree. The code to do this looks like this:\n\ndt_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\nBefore transitioning to model assessment, let‚Äôs explore the predictions for both models using the augment() function again. According to the docs,\n\nAugment accepts a model object and a dataset and adds information about each observation in the dataset.\n\naugment() produces new columns from the original data set to which makes it easy to examine model predictions. For instance, we can create a data set with the .pred_class, .pred_win, and .pred_loss columns. augment() also makes a guarantee that a tibble with the same number of rows as the passed data set will be returned, and all new column names will be prefixed with a ..\nHere we‚Äôll pipe the tibble returned from augment() to the relocate() function. This will make it easier to view the variables we are interested in further examining by moving these columns to the left of the tibble.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n\n# A tibble: 1,243 √ó 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 loss  loss          0.0616       0.938 2021-01-29 Illinois Figh‚Ä¶ Wiscons‚Ä¶        0        3     3\n 2 loss  loss          0.297        0.703 2021-01-30 Illinois Figh‚Ä¶ Wiscons‚Ä¶        1        3     4\n 3 loss  loss          0.138        0.862 2021-02-06 Illinois Figh‚Ä¶ @ Penn ‚Ä¶        2        3     5\n 4 loss  loss          0.0572       0.943 2021-02-19 Illinois Figh‚Ä¶ Ohio St.        1        3     4\n 5 loss  loss          0.199        0.801 2021-02-20 Illinois Figh‚Ä¶ Ohio St.        2        3     5\n 6 loss  loss          0.0787       0.921 2021-03-05 Illinois Figh‚Ä¶ Nebraska        0        3     3\n 7 loss  loss          0.0281       0.972 2021-03-06 Illinois Figh‚Ä¶ Nebraska        0        3     3\n 8 loss  win           0.636        0.364 2021-03-12 Illinois Figh‚Ä¶ @ Minne‚Ä¶        2        3     5\n 9 loss  loss          0.00129      0.999 2021-03-13 Illinois Figh‚Ä¶ @ Minne‚Ä¶        0        3     3\n10 loss  loss          0.0114       0.989 2021-04-02 Illinois Figh‚Ä¶ @ Purdue        0        3     3\n# ‚Ñπ 1,233 more rows\n# ‚Ñπ 20 more variables: kills &lt;dbl&gt;, errors &lt;dbl&gt;, total_attacks &lt;dbl&gt;, hit_pct &lt;dbl&gt;,\n#   assists &lt;dbl&gt;, aces &lt;dbl&gt;, serr &lt;dbl&gt;, digs &lt;dbl&gt;, block_solos &lt;dbl&gt;, block_assists &lt;dbl&gt;,\n#   opp_kills &lt;dbl&gt;, opp_errors &lt;dbl&gt;, opp_total_attacks &lt;dbl&gt;, opp_hit_pct &lt;dbl&gt;,\n#   opp_assists &lt;dbl&gt;, opp_aces &lt;dbl&gt;, opp_serr &lt;dbl&gt;, opp_digs &lt;dbl&gt;, opp_block_solos &lt;dbl&gt;,\n#   opp_block_assists &lt;dbl&gt;\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n\n# A tibble: 1,243 √ó 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 loss  loss            0.131      0.869 2021-01-29 Illinois Figh‚Ä¶ Wiscons‚Ä¶        0        3     3\n 2 loss  loss            0.131      0.869 2021-01-30 Illinois Figh‚Ä¶ Wiscons‚Ä¶        1        3     4\n 3 loss  loss            0.131      0.869 2021-02-06 Illinois Figh‚Ä¶ @ Penn ‚Ä¶        2        3     5\n 4 loss  loss            0.131      0.869 2021-02-19 Illinois Figh‚Ä¶ Ohio St.        1        3     4\n 5 loss  loss            0.131      0.869 2021-02-20 Illinois Figh‚Ä¶ Ohio St.        2        3     5\n 6 loss  loss            0.131      0.869 2021-03-05 Illinois Figh‚Ä¶ Nebraska        0        3     3\n 7 loss  loss            0.131      0.869 2021-03-06 Illinois Figh‚Ä¶ Nebraska        0        3     3\n 8 loss  win             0.603      0.397 2021-03-12 Illinois Figh‚Ä¶ @ Minne‚Ä¶        2        3     5\n 9 loss  loss            0.131      0.869 2021-03-13 Illinois Figh‚Ä¶ @ Minne‚Ä¶        0        3     3\n10 loss  loss            0.131      0.869 2021-04-02 Illinois Figh‚Ä¶ @ Purdue        0        3     3\n# ‚Ñπ 1,233 more rows\n# ‚Ñπ 20 more variables: kills &lt;dbl&gt;, errors &lt;dbl&gt;, total_attacks &lt;dbl&gt;, hit_pct &lt;dbl&gt;,\n#   assists &lt;dbl&gt;, aces &lt;dbl&gt;, serr &lt;dbl&gt;, digs &lt;dbl&gt;, block_solos &lt;dbl&gt;, block_assists &lt;dbl&gt;,\n#   opp_kills &lt;dbl&gt;, opp_errors &lt;dbl&gt;, opp_total_attacks &lt;dbl&gt;, opp_hit_pct &lt;dbl&gt;,\n#   opp_assists &lt;dbl&gt;, opp_aces &lt;dbl&gt;, opp_serr &lt;dbl&gt;, opp_digs &lt;dbl&gt;, opp_block_solos &lt;dbl&gt;,\n#   opp_block_assists &lt;dbl&gt;"
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#model-assessment",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#model-assessment",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Model assessment",
    "text": "Model assessment\nSince we‚Äôre fitting a binary classification model, we will use several measurements to assess model performance. Many of these measurements can be calculated using functions from the yardstick package. To start, we can calculate several measurements using the hard class predictions: a confusion matrix; accuracy; specificity; ROC curves; etc.\n\nCreate a confusion matrix\nFirst, let‚Äôs start by creating a confusion matrix. A confusion matrix is simply a cross-tabulation of the observed and predicted classes, and it summarizes how many times the model predicted a class correctly vs.¬†how many times it predicted it incorrectly. The calculation of the table is pretty straight forward for a binary-classification model. The yardstick package makes it easy to calculate this table with the conf_mat() function.\nconf_mat()‚Äôs two main arguments are truth and estimate. truth pertains to the column containing the true class predictions (i.e., what was actually recorded). The estimate is the name of the column containing the discrete class prediction (i.e., the prediction made by the model).\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class)\n\n          Truth\nPrediction loss win\n      loss  438  89\n      win   104 612\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class)\n\n          Truth\nPrediction loss win\n      loss  411  64\n      win   131 637\n\n\nThe conf_mat() also has an autoplot() method. This makes it easier to visualize the confusion matrix, either as a mosaic plot or a heatmap.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class) |&gt;\n  autoplot(type = \"mosaic\")\n\n\n\n\n\n\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n\nA few things to note from the confusion matrices created from our two models:\n\nThe logistic regression does well predicting wins and losses, though it slightly over predicts wins in cases of losses and losses in cases of wins. However, prediction accuracy is pretty balanced.\nThe decision tree does better reducing cases where it predicts a loss when a win occurred, but it predicted more wins when a loss took place. Thus, the decision tree model seems fairly optimistic when it comes to predicting wins when a loss occurred.\n\nAfter examining the confusion matrix, we can move forward with calculating some quantitative summary metrics from the results of the confusion matrix, which we can use to better compare the fit between the two models.\n\n\nMeasure model accuracy\nOne way to summarize the confusion matrix is to calculate the proportion of data that is predicted correctly, also known as accuracy. yardstick‚Äôs accuracy() function simplifies this calculation for us. Again, we just pipe our augment() function to the accuracy() function, and we specify which column is the truth and which is the estimate class prediction from the model.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  accuracy(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.845\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  accuracy(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.843\n\n\nWhen it comes to accuracy, both models are fairly similar in their ability to predict cases correctly. The logistic regression‚Äôs accuracy is slightly better, though.\n\n\nMeasure model sensitivity and specificity\nSensitivity and specificity are additional assessment metrics we can calculate. Sensitivity in this case is the percentage of matches that were wins that were correctly identified by the model. Specificity is the percentage of matches that were losses that were correctly identified by the model. The @StatQuest YouTube channel has a good video breaking down how these metrics are calculated.\nyardstick makes it easy to calculate these metrics with the sensitivity() and specificity() functions. As we did with calculating accuracy, we pipe the output of the augment() function to the sensitivity() function. We also specify the column that represents the true values to the truth argument, then pass the class predictions made by the model to the estimate argument. This looks like the following for both our logistic regression and decision tree models:\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  sensitivity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.808\n\n\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  specificity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.873\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  sensitivity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.758\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  specificity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.909\n\n\nA few things to note:\n\nThe logistic regression (sensitivity = 80.8%) was much better at predicting matches that were wins than the decision tree model (sensitivity = 75.8%).\nThe decision tree was much better at identifying losses, though (90.9% vs.¬†87.3%).\n\n\nSimplify metric calculations with metric_set()\nAlthough the above code provided the output we were looking for, we can simplify our code by using yardstick‚Äôs metric_set() function. Inside metric_set() we specify the different metrics we want to calculate for each model.\n\nvball_mdl_metrics &lt;-\n  metric_set(accuracy, sensitivity, specificity)\n\nThen we do as before, pipe the output from augment() to our metric set object vball_mdl_metrics, and specify the column that represents the truth and the column that represents the model‚Äôs class prediction. Here‚Äôs what this looks like for both our models:\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.845\n2 sensitivity binary         0.808\n3 specificity binary         0.873\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n\n# A tibble: 3 √ó 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.843\n2 sensitivity binary         0.758\n3 specificity binary         0.909\n\n\nNow it‚Äôs much easier to make comparisons, and we write less code for the same amount of information. A big win!\n\n\n\nROC curves and AUC estimates\nReceiver operating characteristic (ROC) curves visually summarise classification model specificity and sensitivity using different threshold values. From this curve, an area under the curve (AUC) metric can be calculated. The AUC is a useful summary metric and can be used to compare the fit of two or more models. Again, @StatQuest has a pretty good video explaining the fundamentals of ROC curves and AUC estimates.\nBeing a useful way to summarise model performance, the yardstick package makes several functions available to calculate both the ROC curve and AUC metric. An autoplot() method is also available to easily plot the ROC curve for us.\nLet‚Äôs take a look at how this is done with our logistic regression model. Here‚Äôs the code:\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss)\n\n# A tibble: 1,245 √ó 3\n     .threshold specificity sensitivity\n          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf             0                 1\n 2    0.0000835     0                 1\n 3    0.000181      0.00143           1\n 4    0.000405      0.00285           1\n 5    0.000415      0.00428           1\n 6    0.000496      0.00571           1\n 7    0.000824      0.00713           1\n 8    0.000839      0.00856           1\n 9    0.000922      0.00999           1\n10    0.000979      0.0114            1\n# ‚Ñπ 1,235 more rows\n\n\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_auc(truth = w_l, .pred_loss)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.920\n\n\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nYou‚Äôll likely notice the syntax is pretty intuitive. You‚Äôll also notice the code is similar to our other model performance metric calculations. First we use augment() to create the data we need. Second, we pipe the output of the augment() function to either the roc_curve() or roc_auc() function. The roc_curve() function calculates the ROC curve values and returns a tibble, which we will later pipe to the autoplot() method. The roc_auc() function calculates the area under the curve metric.\nSince we‚Äôre comparing two models, we perform these steps again for the decision tree model.\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss)\n\n# A tibble: 7 √ó 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1   -Inf           0          1     \n2      0.126       0          1     \n3      0.397       0.800      0.851 \n4      0.714       0.909      0.758 \n5      0.869       0.914      0.740 \n6      0.875       0.994      0.0517\n7    Inf           1          0     \n\n\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_auc(truth = w_l, .pred_loss)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.864\n\n\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nA few notes from comparing the ROC curve and AUC metrics:\n\nThe AUC indicates a better model fit across different thresholds for the logistic regression model (AUC = .920) vs.¬†the decision tree (AUC = .864).\nWhen visually examining the ROC curves for both models, it seems the logistic regression model is a better fitting model for the data."
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#extract-the-final-workflow",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#extract-the-final-workflow",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Extract the final workflow",
    "text": "Extract the final workflow\nOnce the final candidate model is identified, we can extract the final workflow using the hardhat package‚Äôs extract_workflow() function. Here we‚Äôll use this workflow object to make predictions, but this workflow object is also useful if you intend to deploy this model.\n\nfinal_fit_wflow &lt;- extract_workflow(final_log_reg_fit)"
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#make-predictions",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#make-predictions",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "Make predictions",
    "text": "Make predictions\nAt this point in the season, let‚Äôs see how the Nebraska women‚Äôs volleyball team stacked up in several of their matches using our model. First, let‚Äôs examine Nebraska‚Äôs win against Wisconsin, a five set thriller.\n\nwisc_mtch_one &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-10-21\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_one)\n\n# A tibble: 1 √ó 1\n  .pred_class\n  &lt;fct&gt;      \n1 loss       \n\n\nAccording to our model, Nebraska should have lost this match. This makes Nebraska‚Äôs win even more impressive. The grittiness to pull out a win, even when evidence suggests they shouldn‚Äôt have, speaks volumes of this team. Indeed, wins and losses for volleyball matches are a function of many different factors. Factors that may not be fully captured by the data or this specific model.\nWhat about Nebraska‚Äôs 0-3, second match loss against Wisconsin?\n\nwisc_mtch_two &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-24\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_two)\n\n# A tibble: 1 √ó 1\n  .pred_class\n  &lt;fct&gt;      \n1 loss       \n\n\nNo surprise, the model predicted Nebraska would lose this match. It‚Äôs a pretty steep hill to climb when you hit a .243 and only have 5 total blocks.\nAnother nail-biter was Nebraska‚Äôs second match against Penn State. Let‚Äôs take a look at what the model would predict.\n\npenn_state &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-03\"))\n\npredict(final_fit_wflow, new_data = penn_state)\n\n# A tibble: 1 √ó 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nEven though the match was close, the model predicted Nebraska would win this match. It may have been a nail-biter to watch, but Nebraska played well enough to win the match, according to our model."
  },
  {
    "objectID": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#the-ncaa-tournament-and-our-model",
    "href": "blog/posts/2023-12-07-post-tidymodels-bigten-volleyball/index.html#the-ncaa-tournament-and-our-model",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women‚Äôs volleyball data",
    "section": "The NCAA tournament and our model",
    "text": "The NCAA tournament and our model\nWe‚Äôre through the initial rounds of the 2023 NCAA women‚Äôs volleyball tournament. Let‚Äôs look at a couple of scenarios for Nebraska using our final model.\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôm extrapolating a bit here, since the data I‚Äôm using only includes Big Ten volleyball team matches. The NCAA tournament will include teams from many other conferences, so the predictions don‚Äôt fully generalize to tournament matches.\nWe could avert the extrapolation here by obtaining match data for all NCAA volleyball matches for the 2021, 2022, and 2023 seasons. For the sake of keeping this post manageable, I did not obtain this data.\n\n\nFirst, let‚Äôs just say Nebraska plays to up to their regular season average for hit percentage, errors, block solos, block assists, and digs in NCAA tournament matches. What does our model predict in regards to Nebraska winning or losing a match?\n\nseason_avg &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |&gt;\n  summarise(across(where(is.numeric), mean)) |&gt;\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = season_avg)\n\n# A tibble: 1 √ó 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nIf Nebraska can hit at least a .290, commit less than 17 errors, have one solo block, have 16 block assists, and dig the ball roughly 48 times, then according to the model, they should win matches. Put another way, if Nebraska performs close to their regular season average for these statistics, then the model suggests they will win matches.\nThis is very encouraging, since the Huskers should be playing their best volleyball here at the end of the season. One would hope this means they perform near or better than their average in tournament matches.\nOne last scenario, let‚Äôs look at the low end of Nebraska‚Äôs performance this season. Specifically, let‚Äôs see what the model predicts if Nebraska will win or lose a match at the 25% quartile for these statistics.\n\nquantile_25 &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |&gt;\n  summarise(across(where(is.numeric), ~ quantile(.x, .25))) |&gt;\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = quantile_25)\n\n# A tibble: 1 √ó 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nAccording to the model, if Nebraska can perform up to their 25% quartile of their regular season statistics, the model suggests they should win matches. Matches like those in the NCAA tournament. So even if Nebraska doesn‚Äôt perform to their potential or just has an off match, they should win if they can at least achieve the 25% quartile of their regular season statistics.\n\n‚ÄúAll models are wrong, but some are useful.‚Äù\n- George Box\n\nAgain, many factors determine if a team wins or loses a match in volleyball (see the model‚Äôs prediction for Nebraska‚Äôs first match against Wisconsin). This is just one, simple model aimed at predicting wins and losses based on hit percentage, errors, block solos, block assists, and digs. A model that certainly could be improved."
  },
  {
    "objectID": "blog/posts/2024-12-30-til-base-r-list2env-glue-data-string-interpolation/index.html",
    "href": "blog/posts/2024-12-30-til-base-r-list2env-glue-data-string-interpolation/index.html",
    "title": "TIL: Use list2env() or glue::glue_data() to use a set of elements from a tibble in a string",
    "section": "",
    "text": "Motivation\nToday I learned base R‚Äôs list2env() function can be used to assign a set of variables to the Global Environment.\nFor a personal project, I was creating a simple CRUD application using Shiny. The purpose of the application was pretty straightforward: to serve as a tool for entering data into a database. The app had the following requirements:\n\nProvide inputs as fields for users to enter data that will be stored in a database.\nDisplay entered data via the user interface for easy visual inspection before writing data to the database.\nInclude a ‚ÄòSubmit‚Äô button for the user to submit the data to the database.\n\nOn the back end of this simple app, data was stored in the Global Environment as a tibble, so it could be easily displayed via the application‚Äôs UI. The tibble only contained one row of data, where the values in the variable were to be written via a SQL INSERT statement upon the user hitting a ‚ÄòSubmit‚Äô button.\nWhile working on this part of the app, my initial approach confronted me with a code smell.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\nlibrary(glue)\n\nThis approach stinks\nThe additional nuances of the Shiny application are not important. Rather, let‚Äôs focus on the actual problem I was confronted with: how do you create Global Environment variables from an existing object, specifically a tibble in my case?\nHere‚Äôs some example data to work with:\n\ndata_employee &lt;- tibble(\n  first_name = \"John\",\n  last_name = \"Smith\",\n  start_date = \"2024-03-04\",\n  department = \"accounting\"\n)\n\nAt this point, there is one object in the Global Environment, data_employee. To prove this, let‚Äôs submit ls() to the console, which will print all the objects in our current Global Environment.\n\nls()\n\n[1] \"data_employee\"\n\n\nWhat if I also wanted the tibble‚Äôs variables to be their own objects? That is, I wanted code resulting in four objects being made available in the Global Environment, each containing a value from a variable in the tibble: first_name, last_name, start_date, and department.\nMy overall aim in doing this was to pass the values of these variables to a SQL INSERT statement using the glue() package:\n\nquery_insert &lt;- glue(\n  \"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\"\n)\n\nMy initial solution was to do this:\n\nfirst_name &lt;- data_employee[[\"first_name\"]]\nlast_name &lt;- data_employee[[\"last_name\"]]\nstart_date &lt;- data_employee[[\"start_date\"]]\ndepartment &lt;- data_employee[[\"department\"]]\n\nIndeed, we can confirm this works by once again submitting ls() to the console.\n\nls()\n\n[1] \"data_employee\" \"department\"    \"first_name\"    \"last_name\"     \"start_date\"   \n\n\nFurther confirmation results from inspecting the SQL INSERT statement string outputted from our use of the glue() function.\n\nquery_insert &lt;- glue(\n  \"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\"\n)\n\nquery_insert\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n)\n\n\nThis approach, though it works and isn‚Äôt too cumbersome for this specific example, it stinks and feels off. This especially became apparent when writing this out for the 10 fields of data I wanted to store within a database. I even physically cringed when I implemented it within the context of my application. There had to be a better way.\nLet‚Äôs start fresh by clearing the Global Environment, but keep our data_employee tibble to try another approach:\n\n\n\n\n\n\nNote\n\n\n\nI wouldn‚Äôt do this in my actual code. But I‚Äôm doing it here to better highlight the example.\n\n\n\nrm(\n  first_name,\n  last_name,\n  start_date,\n  department,\n  query_insert\n)\n\nls()\n\n[1] \"data_employee\"\n\n\nBase R‚Äôs list2env()\n\nlist2env() was the solution I was looking for. Here‚Äôs the description from the function‚Äôs documentation:\n\nFrom a named list x, create an envrionment containing all list components as objects, or ‚Äúmulti-assign‚Äù from x into a pre-existing environment.\n\nA little esoteric, so I found the following resources to be quite helpful:\n\nIteratively create global environment objects from tibble (Stack Overflow post)\nDynamic variable assignment in R\n\nEnvironments are an advanced topic, though a little context is helpful. Environments are just like any other data structure in R, but they serve as fenced object containers that can hold objects (my shallow interpretation). The Global Environment is one such container that can hold objects for an R session, though additional named environments could be created. As such, list2env() provides functionality to write named objects stored from a list to any environment we specify. Review Chapter 7: Environments from the Advanced R book for additional detail.\nUsing these concepts and list2env(), here‚Äôs how I fixed my code smell:\n\nlist2env(data_employee, .GlobalEnv)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\nls()\n\n[1] \"data_employee\" \"department\"    \"first_name\"    \"last_name\"     \"start_date\"   \n\n\n\nquery_insert &lt;- glue(\n  \"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\"\n)\n\nquery_insert\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n)\n\n\nü§Ø.\nWhat was once ~10 lines of messy, smelly code is now a one-liner. I was shook upon learning this.\nWrap up\nThe take away from this TIL is a game changer: use list2env() if you need to convert existing list elements into objects in your Global Environment. I certainly was witness to its utility when trying to solve my own code smell. I hope you can find a use for it in your own work.\nUntil next time, cheers! üéâ\nA follow up: glue::glue_data()\n\nThanks to the power of community, Tan Ho shared an even less anti-pattern-ish / code-smell-ish solution to my problem in the Data Science Learning Community‚Äôs Slack channel. He suggested using glue::glue_data().\nHere‚Äôs how it works:\n\nglue_data(\n  data_employee,\n  \"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n\"\n)\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n\n\nü§Øü§Ø.\nThis works as expected and comes with a couple added benefits. For one, the code‚Äôs simpler. In addition, we‚Äôre no longer creating variables that only get used once. The Global Environment is now much cleaner.\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {TIL: {Use} `List2env()` or `Glue::glue\\_data()` to Use a Set\n    of Elements from a Tibble in a String},\n  date = {2024-12-30},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúTIL: Use `List2env()` or\n`Glue::glue_data()` to Use a Set of Elements from a Tibble in a\nString.‚Äù December 30, 2024."
  },
  {
    "objectID": "blog/posts/2023-10-14-til-edit-old-commit/index.html",
    "href": "blog/posts/2023-10-14-til-edit-old-commit/index.html",
    "title": "TIL: Edit an older unpushed commit",
    "section": "",
    "text": "Today I learned how to edit older unpushed commit messages using git rebase.\nI‚Äôve been attempting to be better about linking git commits to specific GitHub issues. Although I try to be disciplined, I forget to reference the issue in the commit message from time-to-time. Luckily, I researched and came upon a solution. The purpose of this post is to briefly document what I‚Äôve learned.\nA quick note: I am not a Git Fu master. The approach I share here (which I learned from a Stack Overflow post) worked for a small project not intended to be in production. In fact, there may be better approaches to solve this problem given your specific situation. I for sure want to avoid receiving angry messages where someone applied what is discussed, and it took down a critical, in production system. Thus, make sure you are aware of what these commands will do to your commit history before applying them.\n\nThe problem\nLet‚Äôs take a look at a log from a practice repo I created. I‚Äôm using git‚Äôs --pretty=format flag here to simplify the printed output for this post; a simple git log will also return the same information but in a more verbose way.\ngit log --pretty=format:\"%h %s %n%b\"\nThis returns the following log information. Printed to the console is a log containing the various commit‚Äôs abbreviated SHA-1 values, subjects, and message bodies.\n31964b0 fix-found_bug\n- #1\n\nf8256d6 feat-you_get_the_point\n- #1\n\nb1b99e9 feat-another_awesome_new_feat\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nShoot! I forgot to tag the b1b99e9 commit as being related to issue #1. How can I edit before I push?\n\n\nThe solution\ngit rebase can be used here to edit the past commit message. Again, keep in mind these commits have not been pushed to the remote repository.\nFirst, we need to target the commit we want to edit. git rebase, with the --interactive flag, and the abbreviated SHA-1 value of the commit to be edited is used to do this:\ngit rebase --interactive b1b99e9~\nThis command will open our system‚Äôs default text editor. In it should be something like the following:\npick b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n\n# Rebase 5d9b87c..31964b0 onto 5d9b87c (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\nYou‚Äôll notice the instructions and different options (formatted as comments) are plentiful. I have yet to explore what all these operations can do (maybe a future post). But here, we are focused on editing a past commit message.\nThe next step in the process was a little confusing. With a bit of reading of the Stack Overflow post and a little experimentation, I found out we need to manually change any pick to edit for any commit intended to be edited in the currently open file. Our file will look something like this:\nedit b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n...\nWe save the file and close our editor. Once back in the terminal, we‚Äôll be on the commit targeted for edits. To make our edits, submit the following to the terminal:\ngit commit --amend\nOnce ran, the text editor will be opened to the commit message we targeted for edits. We‚Äôll then make our changes, save them, and exit the text editor.\nNow, we need to return to the previous HEAD commit. To do this, we run the following command in our terminal:\ngit rebase --continue\n\n\nRewriting history\nLet‚Äôs look at the log and view our changes. We can do that again by submitting the following to our terminal:\ngit log --pretty=format:\"%h %s %n%b\"\nBelow is what gets printed.\n709c173 fix-found_bug\n- #1\n\ne0ed7ba feat-you_get_the_point\n- #1\n\n179be4a feat-another_awesome_new_feat\n- #1\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nSuccess! All our commits are now associated with issue #1. However, take a moment to compare the SHA-1 values from our previous log with the current log. Notice anything different? The SHA-1 values for both our edited commit message and all its children have been modified. We have just re-written part of our commit history.\nImportant point: You can break repos doing this if you‚Äôre not careful. This re-writing of history should only be applied in cases with unpushed commit messages and when you‚Äôre not collaborating on a branch with other people. If you make edits to your history using this approach, you‚Äôll want to make sure to avoid using commands like git push --force. See the original Stack Overflow post for more detail.\n\n\nWrap-up\nSo there you have it. A little Git Fu magic to help edit past, unpushed commit messages.\nIf you know a better approach or if my Git Fu is way off, let me know. I have far from mastered git.\nHappy rebasing!\n\n\nResources to learn more\n\nHow do I modify a specific commit? Stack Overflow post submitted by Sam Liao and top answer from ZelluX\nGit Rebase Interactive :: A Practical Example YouTube tutorial from EdgeCoders\n7.6 Git Tools - Rewriting History from the git documentation\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {TIL: {Edit} an Older Unpushed Commit},\n  date = {2023-10-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. ‚ÄúTIL: Edit an Older Unpushed\nCommit.‚Äù October 14, 2023."
  },
  {
    "objectID": "blog/posts/2024-02-23-tidytuesday-2024-02-20-r-consortium-grants/index.html",
    "href": "blog/posts/2024-02-23-tidytuesday-2024-02-20-r-consortium-grants/index.html",
    "title": "Exploring R Consortium ISC Grants",
    "section": "",
    "text": "library(tidyverse)\nlibrary(plotly)\nlibrary(skimr)\nlibrary(tidytext)\nlibrary(here)\nlibrary(scales)\n\n\nBackground\nI‚Äôve never really contributed to tidytuesday. Recently, I‚Äôve been trying to spark some inspiration, so I thought contributing to this social data project would be a good start. I used this post as an opportunity to get more comfortble using plotly and Tableau for creating data visualizations.\n\ndata_isc_grants &lt;-\n  read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-20/isc_grants.csv'\n  )\n\n\n\nData description\nThe data represents information about past projects funded by the R Consortium Infrastructure Committee (ISC) Grant Program. The purpose of these grants is to support projects contributing to the R community. Learn more about the most recent round of funding by checking out their blog post announcing this round of grants.\nThe data includes columns like: year, group (i.e., funding cycle), title, funded (i.e., funding amount), and summary. Before creating some data visualizations, let‚Äôs do some quick exploratory analysis.\n\nglimpse(data_isc_grants)\n\nRows: 85\nColumns: 7\n$ year        &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, ‚Ä¶\n$ group       &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ title       &lt;chr&gt; \"The future of DBI (extension 1)\", \"Secure TLS Communications for R\", \"volcalc‚Ä¶\n$ funded      &lt;dbl&gt; 10000, 10000, 12265, 3000, 15750, 8000, 8000, 22000, 6000, 25000, 15000, 20000‚Ä¶\n$ proposed_by &lt;chr&gt; \"Kirill M√ºller\", \"Charlie Gao\", \"Kristina Riemer\", \"Mark Padgham\", \"Jon Harmon‚Ä¶\n$ summary     &lt;chr&gt; \"This proposal mostly focuses on the maintenance and support for {DBI}, the {D‚Ä¶\n$ website     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n\n\n\nskim(data_isc_grants)\n\n\nData summary\n\n\nName\ndata_isc_grants\n\n\nNumber of rows\n85\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntitle\n0\n1.00\n4\n120\n0\n85\n0\n\n\nproposed_by\n0\n1.00\n8\n63\n0\n66\n0\n\n\nsummary\n0\n1.00\n31\n2210\n0\n85\n0\n\n\nwebsite\n33\n0.61\n21\n224\n0\n48\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2019.14\n2.08\n2016\n2017\n2019\n2021\n2023\n‚ñá‚ñÜ‚ñá‚ñÖ‚ñÖ\n\n\ngroup\n0\n1\n1.40\n0.49\n1\n1\n1\n2\n2\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\n\n\nfunded\n0\n1\n13781.14\n11325.80\n0\n6000\n10000\n16000\n62400\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nWhat‚Äôs the trend for grant funding?\nLet‚Äôs take a look at the funding trend by funding cycle (i.e., fall and spring).\n\n\nCode\ndata_by_year_grp &lt;- data_isc_grants |&gt;\n  mutate(\n    group = case_when(\n      group == 1 ~ \"Spring\",\n      group == 2 ~ \"Fall\"\n    )\n  ) |&gt;\n  group_by(year, group) |&gt;\n  summarise(funded = sum(funded), .groups = \"drop\") |&gt;\n  arrange(group, year) |&gt;\n  pivot_wider(names_from = group, values_from = funded)\n\n\n\n\nCode\nplot_ly(\n  data_by_year_grp,\n  x = ~year,\n  y = ~Fall,\n  name = \"Fall\",\n  type = 'scatter',\n  mode = 'lines',\n  line = list(width = 5),\n  text = ~ paste(\n    \"Funding awarded: $\",\n    comma(Fall),\n    \"&lt;br&gt;Year: \",\n    year\n  ),\n  hoverinfo = \"text\"\n) |&gt;\n  add_trace(\n    y = ~Spring,\n    name = \"Spring\",\n    text = ~ paste(\n      \"Funding awarded: $\",\n      comma(Spring),\n      \"&lt;br&gt;Year: \",\n      year\n    ),\n    hoverinfo = \"text\"\n  ) |&gt;\n  layout(\n    title = list(\n      text = \"&lt;b&gt;Funding trend for R Consortium ISC grants by funding round&lt;/b&gt;\",\n      xanchor = \"center\",\n      yanchor = \"top\",\n      font = list(family = \"arial\", size = 24)\n    ),\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"Funding amount ($US)\")\n  )\n\n\n\n\n\n\n\n\nWhat words are used most often within descriptions of funded projects?\nNow, let‚Äôs explore the words used within descriptions most often in awarded grant applications.\n\n\nCode\ndata_word_fund_trend &lt;- data_isc_grants |&gt;\n  mutate(\n    summary = str_remove_all(str_to_lower(summary), \"[[:punct:]]\"),\n    summary = str_remove_all(summary, \"[0-9]\"),\n  ) |&gt;\n  unnest_tokens(word, summary) |&gt;\n  anti_join(get_stopwords()) |&gt;\n  group_by(year) |&gt;\n  count(word) |&gt;\n  arrange(word, year) |&gt;\n  group_by(word) |&gt;\n  mutate(\n    n_cume = cumsum(n)\n  )\n\n\n\n\nCode\ntop_words &lt;- data_word_fund_trend |&gt;\n  ungroup() |&gt;\n  summarise(top = quantile(n_cume, .99)) |&gt;\n  pull(top)\n\ndata_top_words &lt;- data_word_fund_trend |&gt;\n  filter(n_cume &gt;= top_words) |&gt;\n  distinct(word)\n\nplot_ly(\n  data = data_word_fund_trend,\n  x = ~year,\n  y = ~n_cume,\n  mode = \"lines\",\n  line = list(color = \"#d3d3d3\", width = 3),\n  type = \"scatter\",\n  mode = \"lines\",\n  name = \"\",\n  text = ~ paste(\n    \"Word: \",\n    word,\n    \"&lt;br&gt;Cumulative mentions: \",\n    n_cume,\n    \"&lt;br&gt;Year: \",\n    year\n  ),\n  hoverinfo = \"text\"\n) |&gt;\n  add_lines(\n    data = data_word_fund_trend |&gt; semi_join(data_top_words),\n    x = ~year,\n    y = ~n_cume,\n    line = list(color = \"#0C2D48\", width = 3),\n    type = \"scatter\",\n    mode = \"lines\",\n    name = \"\"\n  ) |&gt;\n  layout(\n    title = list(\n      text = \"&lt;b&gt;Aiming for RConsortium grant funding? Consider using these words&lt;/b&gt;\",\n      xanchor = \"center\",\n      yanchor = \"top\",\n      font = list(family = \"arial\", size = 24)\n    ),\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"Cumulative mentions\"),\n    showlegend = FALSE\n  )\n\n\n\n\n\n\n\n\nAn attempt using Tableau\nTo learn more about using Tableau, I took this week‚Äôs data as an opportunity to learn more. Here‚Äôs what I came up with.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring {R} {Consortium} {ISC} {Grants}},\n  date = {2024-02-26},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. ‚ÄúExploring R Consortium ISC Grants.‚Äù\nFebruary 26, 2024."
  },
  {
    "objectID": "blog/posts/2025-12-19-hex-update-007/index.html",
    "href": "blog/posts/2025-12-19-hex-update-007/index.html",
    "title": "The Hex Update: Issue 007",
    "section": "",
    "text": "Let‚Äôs catch up\nHowdy folks! Here‚Äôs Issue 007.\nThree topics caught my attention this week: AI use in content generation and the impacts in can have; some 2026 predictions for journalism; and the impact the non-humaness of internet content might have on our ability to think. In the first article, Amazon‚Äôs use of AI to create a season-one recap for its Fallout series is discussed, along with several highlighted missteps. Next, digital media experts‚Äô predictions for journalism in 2026, collected by NiemanLab, are overviewed. The third article provides some opinions on the impact of non-human web content and traffic‚Äìnow the majority online‚Äìhas on users‚Äô experiences. For a little fun to end the week, I share a short article with facts about the origins of the Times New Roman font.\nEnough summary. Let‚Äôs get into what was interesting.\n\n\nThree things from this week\nHere are three things that caught my attention:\n\nArticle: Amazon‚Äôs Official ‚ÄòFallout‚Äô Season 1 Recap Is AI Garbage Filled With Mistakes\nIn issue 006, I shared an article highlighting the potential impact media restoration for streaming environments might have on a series‚Äô original storytelling. Expanding on what was shared, I further shared some thoughts regarding the potential impact AI may have on creator‚Äôs original storytelling. Gizmodo, in the linked article, provides an example using the AI generated recap Prime Video used for the premier of Fallout‚Äôs second season. The article reports on the inaccuracies and halucinations injected into the recap resulting from the use of generative AI. Indeed, this is not a new use of AI by the streaming platform, as the article links to a blog post from Amazon exploring their use of generative AI for other season recaps. The goal, according to G√©rard Medioni, vice president of technology at Prime Video, is to use innovation to make the viewing experience more accessible and enjoyable for customers. Despite this intention, the use of this innovation has resulted in some of the audience to be critical of its impact on the original storytelling.\n\nWhy does this matter?\nIs this what audiences‚Äô want? Will fans enjoy AI summarized recaps that, yes, improves the productivity from the standpoint of the platform, and potentially increases the accessbility and enjoyment of the viewing experience? At what cost, though? Are these improvements worth the expense of impacting the original storytelling intended by the series? I believe there‚Äôs some give and take here. Blindly generating content without blending it with the human craft of storytelling doesn‚Äôt seem, to me anyways, what audiences want. In the future, generative AI may become good enough to fully replicate the craft. Even then, audiences will likely still want and appreciate the nuances provided by the work of humans, likely aided by AI tools. In addition, for now, the chances for inaccuracies and halucinations to impact the original storytelling are too high of a cost, even if it‚Äôs just a short season recap.\n\n\n\nCollection: Predictions of Journalism 2026\nAnnually, NiemanLab publishes predictions from some of the leading experts in digital media. Journalism is the focus of this collection. These predictions range from the actual work of journalists, AI, and the business behind the newsroom. Each prediction has its own merit, and I wish I had the time to read and reflect on all of them. Nonetheless, here are several that caught my attention, each with a brief description:\n\nNo more loose taxonomies or dirty data: makes the case for why media organizations need to be mindful of the state of their taxonomy data‚Äìso they can better understand their audiences.\nGoogle will look beyond volume journalism: provides an argument why volume journalism will no longer as a content strategy, and it provides publishers with some suggestions on where to focus their efforts in the current digital news ecosystem.\nEvery media business becomes an events business: two quotes summarize this prediction really well\n\n\nThe success of a media product can be gauged by its ability to bring strangers together in person.\n\n\nCommunity is when people keep showing up to a series of events, when they meet eyes and shake hands and exchange numbers.\n\n\nJournalism will become the center of gravity for YouTube‚Äôs next era: discusses the role journalism will play in the future of YouTube.\nShort-form video drives reach. That‚Äôs exactly the problem: provides a case for why newsrooms may not want to solely focus on publishing short-form videos for audiences.\n\n\nWhy does this matter?\nIt‚Äôs that time of year again when predictions for the coming year are made. This collection comes from some of the leading experts and leaders in the digital media space, so they‚Äôre close to the trends happening in journalism. These are trends media organizations should be paying attention to, and may even want to lean into. Hearing these perspectives provides an anchor point to consider where and what to focus on next. Not all these predictions may actualize, but many are worth some awareness going into 2026.\n\n\n\nArticle: Humans are now the minority online\nHere‚Äôs an interesting opinion piece on the current state of the internet. Specifically, it links to some reports sharing data showing how the majority of web content and traffic is now from bots. Going further, the article shares some opinion on the effect this will have on users‚Äô experiences while browsing the web. The impact of this is captured in the following quote from the piece:\n\nWe think of the internet as a tool, but it has long been a habitat for our public reasoning; a place where we gather information, form opinions, build trust, and search for meaning. As that habitat fills with synthetic content and automated behaviour, the quality of human thought degrades because the environment itself becomes less human.\n\n\nWhy does this matter?\nWhere do we go and how should we feel engaging in a place that is no longer human? Are there still online spaces media organizations can currate and operate in that are still human and can be a space where human thought doesn‚Äôt degrade? I still think so, but there must be firm commitment from media organizations to build and cultivate these spaces. I believe there‚Äôs also opportunity for media organizations to build products the help the internet feel more human once again, even if the bots and AI have taken over.\n\n\n\n\nJust for fun\nBlog: A brief history of Times New Roman\nTimes New Roman served as the pre-2007 default font for Microsoft Word until Calibri took over. However, many published documents and reports still till this day use this font. Where did it come from? What‚Äôs its origin story? This post provides answers to these questions. In short, Stanley Morrison designed the font, who initially created the text for the Times of London in 1929. The piece also contains some additional facts about the font. It‚Äôs a fun, short read.\nHey, you know what? It‚Äôs the end of the week, and we‚Äôre on the cusp of starting the weekend. I hope you have a great one.\nCheers üéâ!\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 007},\n  date = {2025-12-19},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúThe Hex Update: Issue 007.‚Äù\nDecember 19, 2025."
  },
  {
    "objectID": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html",
    "href": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html",
    "title": "TIL: Use a file to draft a GitHub issue or PR",
    "section": "",
    "text": "Recently, while using GitHub‚Äôs CLI tool, I ran into an error editing an issue. Thinking I‚Äôd found a bug, I searched the tool‚Äôs issues for any information on what may be going on. I later identified it as a version issue, as I hadn‚Äôt updated the tool for some time. While researching the error, though, I came across this issue submitted by ThePlenkov. In the issue, I saw this code example:\ngh pr edit &lt;pr_number&gt; --body-file /tmp/release-pr.md\nI was taken aback. I‚Äôd never seen this before.\nWhy? GitHub has been core to my workflow for several years. As an extension and from someone who enjoys working from the command line, I‚Äôve come to really enjoy GitHub‚Äôs CLI tool. In addition to the version control features, It‚Äôs also great for project management tasks. However, one feature I disliked was the use of temporary files when drafting a new issue or PR. Seeing the --body-file option used in the code example made me wonder: Is there a better way to draft issues and PRs? There was. So, I had to document and share what I learned in this post.\n\n\n\n\n\n\nNote\n\n\n\nThis post is written in the spirit of publishing more frequent blog posts. It‚Äôs a bit of a scratchpad of ideas, concepts, and/or ways of working that I found to be useful and interesting. As such, what‚Äôs here is lightly edited. Be aware: there will likely be spelling, grammatical, or syntactical errors along with some disjointed, incomplete ideas."
  },
  {
    "objectID": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html#a-place-to-put-files",
    "href": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html#a-place-to-put-files",
    "title": "TIL: Use a file to draft a GitHub issue or PR",
    "section": "A place to put files",
    "text": "A place to put files\nI needed a place to park my drafts. A directory in the repo made sense. I debated different directory names, and I even consulted AI for some additional ideas. Some of these names included:\n\nissues/\nissues-prs/\nissue-prs-drafts/\ndrafts/\ntmp/\ndraft-den/ (my favorite, but it didn‚Äôt make the final cut)\nworkspace/\n.staging/ (I liked the idea of a hidden directory)\n\nI decided to go with .workspace/. I felt this could be a directory for not only issue drafts, but for other things that I didn‚Äôt want version controlled but was expirmenting with. In reality, choose what makes most sense for you and your team.\nNow that I had a place to park my drafts, I needed to exclude these files from version control."
  },
  {
    "objectID": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html#ignoring-draft-files-from-version-control",
    "href": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html#ignoring-draft-files-from-version-control",
    "title": "TIL: Use a file to draft a GitHub issue or PR",
    "section": "Ignoring draft files from version control",
    "text": "Ignoring draft files from version control\nIndeed, there‚Äôs two options to do this. Files can be ignored at the global or repo level. A .gitignore file is used in both instances.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôre not familiar with what a .gitignore file is or how to edit it, I higly suggest reading this tutorial here.\n\n\nSince this directory and drafts are specific to me, I opted for a global approach. Indeed, the potential for an untidy mess may occur if repo level .gitignore files were written to account for everyone‚Äôs nuanced setups. Also, global .gitignore files are great for helping ignore other pesky files. I‚Äôm looking at you .DS_Store files ‚Ä¶\nThis article here provides a good description of how to set this up. As such, here‚Äôs a simplified summary:\n\nCreate a global .gitignore file, most likely in your system‚Äôs root directory (e.g., ~/.gitignore).\nAdd any files you want to ignore at a global level to the file. That is:\n\n.DS_Store\n.workspace/\n\nFor a Mac, configure git to use this new global ignore file by running the following from your command line (Windows is a little different so check out the post I linked above).\n\ngit config --global core.excludesfile ~/.gitignore\nSetup is now complete. These draft files should now be ignored. There‚Äôs also now less of a chance these files accidentally get commited to our repos."
  },
  {
    "objectID": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html#using-the---body-file-argument-flag",
    "href": "blog/posts/2025-11-01-til-notes-github-issues-file/index.html#using-the---body-file-argument-flag",
    "title": "TIL: Use a file to draft a GitHub issue or PR",
    "section": "Using the --body-file argument flag",
    "text": "Using the --body-file argument flag\nSince we have a place to park and draft our issues and PRs, we just need to create an .md file and start drafting. Once we complete our draft, run the following command in your console to submit using the .md file.\n# Create an issue\ngh issue create --body-file /.workspace/110-draft-issue.md\n\n# Create a PR\ngh pr create --body-file /.workspace/111-draft-pr.md\nFor short hand purposes, you can use the short option -F to achieve the same results. I like to think of this option as ‚ÄòFile‚Äô when I use it. Choose the flavor you like the most.\n# Create an issue with short option\ngh issue create -F /.workspace/110-draft-issue.md\n\n# Create a PR with short option\ngh pr create -F /.workspace/111-draft-pr.md\nYour draft files can also be used to edit issues and PRs. You just need to modify your command using edit instead of create.\n# Edit an issue\ngh issue edit 110 --body-file /.workspace/110-draft-issue.md\n\n# Edit a PR\ngh pr edit 110 --F /.workspace/111-draft-pr.md\nHowever, know that the file in your .workspace will not update if you make any edits to an issue via the web UI. If you do submit again, know the body file contents will relpace the current version of the issue with what‚Äôs in your file. Indeed, the edits you just created will be version controlled in the issue and you can refer back to them. They just will no longer be current.\nWant to know more about these commands? Access the CLI tool‚Äôs docs by running the following:\ngh issue create --help\ngh issue edit --help"
  },
  {
    "objectID": "blog/posts/2025-02-23-til-notes-links-r-nvim-key-bindings/index.html",
    "href": "blog/posts/2025-02-23-til-notes-links-r-nvim-key-bindings/index.html",
    "title": "Notes: Customizing Neovim‚Äôs key bindings",
    "section": "",
    "text": "Background\nThe following post provides some notes on setting custom key bindings for R.nvim, a plugin that adds R support to Neovim. Specifically, this post focuses on how to set custom key bindings while using LazyVim. This involved some initial trial and error, along with some review of the docs and several resources. I‚Äôve linked all the resources or point to relevant docs that helped me figure out how to do this.\n\n\nMotivation\nWhile transitioning over to LazyVim, I inherited some of my old config files into this new setup. This resulted in some conflicts with the new setup. Some of my old config‚Äôs key bindings overlapped with some of LazyVim‚Äôs global key bindings. Specifically, I wanted to map &lt;localleader&gt;L to run devtools::load_all(), but it was already mapped to bring up the change log for LazyVim. So, I wanted to modify this to better fit my workflow.\n\n\nDisabling global key bindings\nLazyVim‚Äôs website is pretty clear that global key bindings are disabled with the vim.keymap.del function. Another good starting point was the docs, which are reviewable by running help vim.keymap.del via nvim‚Äôs command line prompt (opened by pressing :).\nThe vim.keymap.del function has three parameters:\n\n{modes} - a string or an array of strings you want the key binding to be made available (e.g., n = normal mode; i = insert mode; v = visual mode).\n{lhs} - a string of the key map you‚Äôre looking to disable.\n{opts} - a lua table with any additional options you want to pass along.\n\nIn my case, disabling the current &lt;leader&gt;L key binding was pretty straightforward, so I added the following line to my ~/.config/lazyvim/lua/config/keymaps.lua file.\nvim.keymap.del(\"n\", \"&lt;leader&gt;L\")\nThis disabled the key binding from my current configuration. Now it was time to modify R.nvim in my config.\n\n\nCustomizing R.nvim‚Äôs key bindings\nBeyond disabling the global key binding, I wasn‚Äôt too sure where to start when it came to setting up a custom key bindings for LazyVim.\nAndrew Courter‚Äôs (@ascourter) video broadly overviewed the steps for setting up a key bindings for a plugin, and I found it to be a good starting point. The specific steps are detailed at ~4m53s of the video.\nNext, I consulted R.nvim‚Äôs docs to better understand how the plugin expected key bindings to be defined. I use LazyVim Extras along with a plugin extension file (located at ~/.config/lazyvim/lua/plugins/extend-r-nvim.lua) to manage setup and configuration of this plugin. This extension file follows the conventions detailed in the R.nvim-key-bindings section of the docs for the custom key bindings definitions.\nThe custom key binding configuration is defined deeply within several nested levels of a lua table. Specifically, this custom key binding configuration is defined in the table like this, which is associated with the config key of the table.\nconfig = function()\n    local opts = {\n        hook = {\n            on_filetype = function()\n                -- Map local leader L to run `devtools::load_all()`\n                vim.api.nvim_buf_set_keymap(\n                    0,\n                    \"n\",\n                    \"&lt;LocalLeader&gt;L\",\n                    \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::load_all()')&lt;CR&gt;\",\n                    { desc = \"R devtools::load_all()\" }\n                )\n            end,\n        },\n    }\nend\nThis configuration code uses the vim.api.nvim_buf_set_keymap() function to set a local key binding for the current buffer, which will likely be open to a file type used within the R programming environment (e.g., .R, .Rmd, or .qmd file). You can run help nvim_buf_set_keymap from neovim‚Äôs command line to view more info about the function. Within the function, we pass values to several parameters:\n\n{buffer} - specifies the buffer the key binding will be made available.\n{mode} - specifies the modal mode the key binding will be made available (e.g., normal, visual, insert, etc.).\n{lhs} - a parameter expecting a string representing the key binding definition. In our case &lt;LocalLeader&gt;L.\n{rhs} - a parameter expecting a string representing the lua command to be run. In our case the &lt;Cmd&gt;lua require('r.send').cmd('devtools::load_all()')&lt;CR&gt; is run upon pressing the key binding, which will send the devtools::load_all() function to the R interpreter. It‚Äôs important to call out the lua function r.send.cmd() is being used here to run our R code. Shortly, we‚Äôll see another lua function that can run functions that take objects in the environment as an input.\n{opts} - a parameter that expects a lua table with additional options. Because the which.key plugin is a default plugin for the LazyVim distribution, my config passes along a lua table containing a desc key value with a string describing what the keymap does. which.key will then include this description in the help popup window when hitting specific key bindings.\n\n\n\nOther useful key bindings for R.nvim\nYou‚Äôll likely want to add more key bindings then the one I‚Äôve shared above. For instance, you‚Äôll likely want to add additional package development convenience functions provided by the devtools package to your configuration:\nvim.api.nvim_buf_set_keymap(\n    0,\n    \"n\",\n    \"&lt;LocalLeader&gt;D\",\n    \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::document()')&lt;CR&gt;\",\n    { desc = \"R devtools::test()\" }\n)\n\nvim.api.nvim_buf_set_keymap(\n    0,\n    \"n\",\n    \"&lt;LocalLeader&gt;T\",\n    \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::test()')&lt;CR&gt;\",\n    { desc = \"R devtools::test()\" }\n)\n\nvim.api.nvim_buf_set_keymap(\n    0,\n    \"n\",\n    \"&lt;LocalLeader&gt;U\",\n    \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::install()')&lt;CR&gt;\",\n    { desc = \"R devtools::install()\" }\n)\nIf you do any Shiny development, it‚Äôs convenient to have a key binding that quickly kicks off an app:\nvim.api.nvim_buf_set_keymap(\n    0,\n    \"n\",\n    \"&lt;LocalLeader&gt;sa\",\n    \"&lt;Cmd&gt;lua require('r.send').cmd('shiny::runApp()')&lt;CR&gt;\",\n    { desc = \"R shiny::runApp()\" }\n)\nYou‚Äôll also likely want a key binding that interrupts a busy R terminal:\nvim.keymap.set(\"n\", \"&lt;LocalLeader&gt;ts\", \"&lt;Cmd&gt;RStop&lt;CR&gt;\", { desc = \"R stop terminal\" })\nAbove you‚Äôll notice we‚Äôre using an alternative function to specify our key binding, vim.keymap.set(). This is an alternative function for specifying key bindings.\nFinally, one of my favorites is to configure key bindings for common operations I perform all the time, like running dplyr::glimpse() on objects. The configuration code looks like this:\nvim.api.nvim_buf_set_keymap(\n    0,\n    \"n\",\n    \"&lt;LocalLeader&gt;g\",\n    \"&lt;Cmd&gt;lua require('r.run').action('dplyr::glimpse')&lt;CR&gt;\",\n    { desc = \"R dplyr::glimpse()\" }\n)\nTake notice of what‚Äôs happening here in the {rhs} parameter of the vim.api.nvim_buf_set_keymap() function. require() is calling r.run.action(), a lua function that will run an R function on the object currently under the cursor. This is very convenient, as you can put your cursor over any object and pass that object to the R function.\nCertainly, endless possibilities exist for the types of key binds one can configure. Having the ability to set key bindings to specific R functions and operations opens up the possibilities even more to enhance specific workflows.\n\n\nWrap up\nTo wrap up, this post overviewed the process of defining custom key bindings for Neovim while using the LazyVim distribution. Specifically, this involved describing how to disable global key bindings that may overlap with the intended setup, how to set key bindings for the R.nvim plugin, and it provided some additional examples of key bindings that might be helpful. Customization is a core reasong for using Neovim, so being able to customize key bindings useful for your workflows is a powerful tool to learn and use.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes: {Customizing} {Neovim‚Äôs} Key Bindings},\n  date = {2025-02-23},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúNotes: Customizing Neovim‚Äôs Key\nBindings.‚Äù February 23, 2025."
  },
  {
    "objectID": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html",
    "href": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "",
    "text": "According to estimates cited by Forbes, the 2024 global e-commerce market is expected to be worth over $6.3 trillion (Snyder and Aditham 2024). Behind this figure is an unfathomable amount of transaction data. If you work in marketing, you more than likely have encountered this type of data, where customer purchases are tracked and each item is logged. How, then, can this data be used to create actionable insights about customer‚Äôs purchasing behavior? Market basket analysis is one such tool.\nMarket basket analysis is used to identify actionable rules about customers‚Äô purchase behavior. As an unsupervised machine learning method, market basket analysis utilizes an algorithm to create association rules from a set of unlabeled data (Lantz 2023). If you‚Äôre working with transaction data, this type of analysis is essential to know. It‚Äôs an interpretable, useful machine learning technique.\nThis post explores the use of market basket analysis to identify clear and interesting insights about customer purchase behavior, in the context of an online merchandise store. Specifically, this post uses obfuscated Google Analytics data from the Google Analytics Merchandise Store.\n\n\n\n\n\n\nWarning\n\n\n\nThe data used in this post is example data, and it is used for tutorial purposes. Conclusions drawn here are not representative, at least to my knowledge, of real customer purchasing behavior.\n\n\nSpecifically, this post overviews the steps involved when performing market basket analysis using Google Analytics data. The data used in this analysis represents transaction data collected during the 2020 US holiday season (2020-11-01 to 2020-12-31). First, I highlight the use of Google BigQuery and the bigrquery R package to extract Google Analytics data. Second, I overview the wrangling and exploratory steps involved when performing a market basket analysis. Then, I utilize the arules package to generate association rules from the data. Finally, the last section discusses rule interpretation, where I aim to identify clear and interesting insights from the generated rules.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.5.2\n\n\nWarning: package 'readr' was built under R version 4.5.2\n\nlibrary(bigrquery)\nlibrary(here)\nlibrary(glue)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(reactable)\n\nWarning: package 'reactable' was built under R version 4.5.2\n\nlibrary(arules)\nlibrary(arulesViz)"
  },
  {
    "objectID": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#address-missing-transaction-ids",
    "href": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#address-missing-transaction-ids",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Address missing transaction ids",
    "text": "Address missing transaction ids\nAfter reviewing the dataset‚Äôs structure, my first question is how many examples are missing a transaction_id? transaction_ids are critical here, as they are used to group items into transactions. To answer this question, I used the following code:\n\ndata_ga_transactions |&gt;\n  mutate(\n    has_id = case_when(\n      is.na(transaction_id) | transaction_id == \"(not set)\" ~ FALSE,\n      TRUE ~ TRUE\n    )\n  ) |&gt;\n  count(has_id) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 2 √ó 3\n  has_id     n  prop\n  &lt;lgl&gt;  &lt;int&gt; &lt;dbl&gt;\n1 FALSE   1498 0.114\n2 TRUE   11615 0.886\n\n\nOut of the 13,113 events, 1,498 (or 11.4%) are missing a transaction_id. Missing transaction_id‚Äôs can take two forms. First, a missing value can be an NA value. Second, missing values occur when examples contain the (not set) character string.\nI address missing values by dropping them. Indeed, other approaches are available to handle missing values. Given the data and context you‚Äôre working within, you may decide dropping over 11% of examples is not appropriate. As such, the use of nearest neighbor or imputation methods might be explored.\nAlthough the temporal aspects of the data are not relevant for this analysis, for consistency, I‚Äôm also going to parse the event_date column into type date. lubridate‚Äôs ymd() function can be used for this task.\nHere‚Äôs the code to perform the wrangling steps described above:\n\ndata_ga_transactions &lt;-\n  data_ga_transactions |&gt;\n  drop_na(transaction_id) |&gt;\n  filter(transaction_id != \"(not set)\") |&gt;\n  mutate(event_date = ymd(event_date)) |&gt;\n  arrange(event_date, transaction_id)\n\nWe can verify these steps have been applied by once again using dplyr‚Äôs glimpse() function. Base R‚Äôs summary() function is also useful to verify the data is as expected.\n\nglimpse(data_ga_transactions)\n\nRows: 11,615\nColumns: 3\n$ event_date     &lt;date&gt; 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 20‚Ä¶\n$ transaction_id &lt;chr&gt; \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"1342‚Ä¶\n$ item_name      &lt;chr&gt; \"Google Recycled Writing Set\", \"Google Emoji Sticker Pack\", \"Android Iconic‚Ä¶\n\n\n\nsummary(data_ga_transactions)\n\n   event_date         transaction_id      item_name        \n Min.   :2020-11-11   Length:11615       Length:11615      \n 1st Qu.:2020-11-24   Class :character   Class :character  \n Median :2020-12-04   Mode  :character   Mode  :character  \n Mean   :2020-12-04                                        \n 3rd Qu.:2020-12-13                                        \n Max.   :2020-12-31                                        \n\n\nLooks good from a structural standpoint. However, I did notice that our wrangling procedure resulted in events occurring before 2020-11-11 to be removed. This might indicate some data issues prior to this date, which might be worth further exploration. Given that I don‚Äôt have the ability to speak with the developers of the Google Merchandise Store to explore a potential measurement issue, I‚Äôm just going to move forward with the analysis. Indeed, our initial goal was to identify association rules during the holiday shopping season, so our data is still within the date range intended for our analysis."
  },
  {
    "objectID": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#brief-overview-of-association-rules",
    "href": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#brief-overview-of-association-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Brief overview of association rules",
    "text": "Brief overview of association rules\nAlthough others have gone more in-depth on this topic (Lantz 2023), it‚Äôs worth taking a moment to discuss what an association rule is before we use a model to create them. Let‚Äôs say we have the following five transactions:\n{t-shirt, sweater, hat}\n{t-shirt, hat}\n{socks, beanie}\n{t-shirt, sweater, hat, socks, beanie, pen}\n{socks, pen}\nEach line represents an individual transaction. What we aim to do is use an algorithm to identify rules that give us a sense if someone buys one item, what other items will they also likely buy. For example, does buying a t-shirt lead someone to also buy a hat? If so, given our data, can we quantify how confident we are in this rule? Not everyone buying a t-shirt will buy a hat.\nWhen we create rules, they‚Äôll be made up of a left-hand (i.e., an antecedent) and right-hand side (i.e., a consequent). They‚Äôll look something like this:\n{t-shirt} =&gt; {hat}\n\n# or\n\n{t-shirt, hat} =&gt; {sweater}\nRule interpretation is pretty straightforward. In simple terms, the first rule states that when a customer purchases a t-shirt, they‚Äôll also likely purchase a hat. For the second, if a customer purchases a t-shirt and a hat, then they‚Äôll also likely purchase a sweater. It‚Äôs important to recognize that the left-hand side can be one or many items. Now that we understand the general makeup of a rule, let‚Äôs explore some metrics to quantify each."
  },
  {
    "objectID": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#market-basket-analysis-metrics",
    "href": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#market-basket-analysis-metrics",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Market basket analysis metrics",
    "text": "Market basket analysis metrics\nBefore overviewing the model specification steps, we need to understand the key metrics calculated with a market basket analysis. Specifically, we need a little background on the following:\n\nSupport\nConfidence\nLift\n\nIn this section, I‚Äôll spend a little time defining and describing each. However, others do a more through treatment of each metric (see Lantz 2023, chap. 8; Kadlaskar 2021; Li 2017), and I suggest checking out each for more detailed information.\n\nSupport\nSupport is calculated using the following formula:\n\\[\nsupport(x) = \\frac{count(x)}{N}\n\\]\ncount(x) is the number of transactions containing a specific set of items. N is the number of transactions within our data.\nSimply put, support is interpreted as how frequently an item occurs within the data.\n\n\nConfidence\nAlongside support, confidence is another metric provided by the analysis. It‚Äôs built upon support and calculated using the following formula:\n\\[\nconfidence(X\\rightarrow Y) = \\frac{support(X, Y)}{support(X)}\n\\]\nConfidence is a proportion of transactions containing item X (or itemset) results in the presence of item Y (or itemset).\nBoth confidence and support are important; both are parameters we‚Äôll set when specifying our model. These two parameters are the dials we adjust to narrow or expand our rule set generated from the model.\n\n\nLift\nLift‚Äôs importance will become more evident once we specify our model and look at some rule sets. But let‚Äôs discuss its definition. It‚Äôs calculated using the following formula:\n\\[\nlift(X \\rightarrow Y) = \\frac{confidence(X \\rightarrow Y)}{support(Y)}\n\\]\nPut into simple terms, lift gives us a number of how likely one item or itemset is to be purchased to its typical rate of purchase. In even simpler terms, the higher the lift, the stronger evidence that there is a true connection between items or item sets."
  },
  {
    "objectID": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#use-the-apriori-function",
    "href": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#use-the-apriori-function",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Use the apriori() function",
    "text": "Use the apriori() function\nNow let‚Äôs do some modeling. Here we‚Äôll use arules‚Äô apriori() function to specify our model. This step requires a little trial and error, as there‚Äôs no exact method for picking support and confidence values. As such, let‚Äôs just start with apriori‚Äôs defaults for the support, confidence, and minlen parameters. The code looks like the following:\n\n# Start with the default support, confidence, minlen\n#   support: 0.1\n#   confidence: 0.8\n#   minlen: 2\nmdl_ga_rules &lt;- apriori(\n  ga_transactions,\n  parameter = list(support = 0.1, confidence = 0.8, minlen = 2)\n)\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext\n        0.8    0.1    1 none FALSE            TRUE       5     0.1      2     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 356 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[371 item(s), 3562 transaction(s)] done [0.00s].\nsorting and recoding items ... [0 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 done [0.00s].\nwriting ... [0 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nmdl_ga_rules\n\nset of 0 rules \n\n\n0 rules were created. This was due to the default support and confidence parameters being too restrictive. We will need to modify these values so the algorithm is able to identify rules from the data. However, we also need to be aware that loosening our rules could increase the size of our rule set, and this might result in a rule set unreasonably large to explore. This is where the trial and error comes into play.\nSo, then, are there any means for determining reasonable starting points? Yes, we just need to consider the business case. Let‚Äôs start with support, a parameter measuring how frequently an item or item set occurs within the transactions. A good starting point is to think about how many times a typical item might appear within a transaction throughout the measured period (Lantz 2023).\nSince this is an online merchandise store, my expectation for item and item set purchase is quite low. Thus, I‚Äôd expect a typical item to be purchased at least once a week, that is, a total of 8 times during the period. A reasonable starting point for support, then, would be:\n\n# Determining a reasonable support parameter\n8 / 3562\n\n[1] 0.002245929\n\n\nWhen it comes to confidence, it is more about picking a starting point and adjusting from there. For our specific case, I‚Äôll start at .1.\nminlen represents the minimum length the rule (including both the right- and left-hand sides) needs to be before it‚Äôs considered for inclusion by the algorithm. It is the last parameter to be set. Our exploratory analysis identified the majority of items in a transaction were quite low, so I believe setting minlen = 2 is sufficient given our data.\nHere‚Äôs the updated code for our model with the adjusted parameters:\n\n# Use parameters we think are reasonable\n#   support: 0.002\n#   confidence: 0.1\n#   minlen: 2\nmdl_ga_rules &lt;- apriori(\n  ga_transactions,\n  parameter = list(support = 0.002, confidence = 0.1, minlen = 2)\n)\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext\n        0.1    0.1    1 none FALSE            TRUE       5   0.002      2     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 7 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[371 item(s), 3562 transaction(s)] done [0.00s].\nsorting and recoding items ... [298 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [277 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nmdl_ga_rules\n\nset of 277 rules \n\n\nWith more reasonable parameters in place, the model identified 277 rules. Is 277 rules too much, too little? You‚Äôll have to decide. For this specific case, 277 rules seems reasonable."
  },
  {
    "objectID": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#interpret-rules",
    "href": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#interpret-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Interpret rules",
    "text": "Interpret rules\nLet‚Äôs use our first rule as an example, where we‚Äôll seek to interpret it.\n\nfirst_rule &lt;- mdl_ga_rules |&gt;\n  head(1, by = \"lift\") |&gt;\n  as(\"data.frame\") |&gt;\n  tibble()\n\nfirst_rule$rules\n\n[1] \"{Google NYC Campus Mug,Google Seattle Campus Mug} =&gt; {Google Kirkland Campus Mug}\"\n\n\nWritten out, the rule means: If someone buys the Google NYC Campus Mug and the Google Seattle Campus Mug, then they‚Äôll also likely purchase the Google Kirkland Campus Mug. Support is 0.00225, which indicates this rule is included in roughly .2% of transactions. And when these two mugs are purchased together, this rule, where the third mug is purchased, covers around 62% of these transactions. Moreover, the lift metric indicates the presence of a strong rule, where people who buy the first two mugs are more than 169 times more likely to purchase the third mug.\nWhy might this be? Perhaps it‚Äôs customers who, by purchasing the first two mugs, are primed to just go ahead and buy the third to finish the set. This might be a great cross-selling opportunity. Maybe we present this rule to our developers and suggest a store feature that encourages customers‚Äìwhen they buy a certain set of items‚Äìto complete sets of items within their purchases. Maybe the marketing team could think up some type of pricing scheme along with this feature to further encourage the additional purchase to complete the set.\nDespite this rule, we also need to consider it alongside the count metric. If you recall, count represents the number of transactions this rule‚Äôs item set is included. 8 transactions might be too low, and the lift metric may be biased here as a result. We may want to include additional transaction data to further explore this rule or simply be aware this limitation exists when making decisions."
  },
  {
    "objectID": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#subset-rules",
    "href": "blog/posts/2024-06-11-post-machine-learning-market-basket-analysis-google-analytics/index.html#subset-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Subset rules",
    "text": "Subset rules\nIf you recall, earlier in our analysis the Google Camp Mug was identified as being a frequently purchased item. Say our marketing team wants to develop a campaign around this one product and would like to review all the rules associated with it. arules‚Äô subset() generic function in conjunction with some infix operators (e.g., %in%) and inspect() is useful to complete this task. Here is what the code looks like to return these rules:\n\ninspect(subset(mdl_ga_rules, items %in% \"Google Camp Mug Ivory\"))\n\n    lhs                              rhs                          support     confidence coverage  \n[1] {Google Flat Front Bag Grey}  =&gt; {Google Camp Mug Ivory}      0.005053341 0.2571429  0.01965188\n[2] {Google Camp Mug Ivory}       =&gt; {Google Flat Front Bag Grey} 0.005053341 0.1016949  0.04969118\n[3] {Google Unisex Eco Tee Black} =&gt; {Google Camp Mug Ivory}      0.002245929 0.1311475  0.01712521\n[4] {Google Large Tote White}     =&gt; {Google Camp Mug Ivory}      0.002807412 0.1851852  0.01516002\n[5] {Google Magnet}               =&gt; {Google Camp Mug Ivory}      0.002807412 0.1562500  0.01796743\n[6] {Google Camp Mug Gray}        =&gt; {Google Camp Mug Ivory}      0.005053341 0.1836735  0.02751263\n[7] {Google Camp Mug Ivory}       =&gt; {Google Camp Mug Gray}       0.005053341 0.1016949  0.04969118\n    lift     count\n[1] 5.174818 18   \n[2] 5.174818 18   \n[3] 2.639252  8   \n[4] 3.726721 10   \n[5] 3.144421 10   \n[6] 3.696299 18   \n[7] 3.696299 18   \n\n\nSay for example the marketing team finds these rules are not enough to build a campaign around, so they request all rules associated with any mug. The %pin% infix operator can be used for partial matching.\n\ninspect(subset(mdl_ga_rules, items %pin% \"Mug\"))\n\nWarning in seq.default(length = NCOL(quality)): partial argument match of 'length' to 'length.out'\n\n\n     lhs                                   rhs                                    support confidence    coverage       lift count\n[1]  {Google Austin Campus Tote}        =&gt; {Google Austin Campus Mug}         0.002245929  0.6153846 0.003649635  36.533333     8\n[2]  {Google Austin Campus Mug}         =&gt; {Google Austin Campus Tote}        0.002245929  0.1333333 0.016844469  36.533333     8\n[3]  {Google Kirkland Campus Mug}       =&gt; {Google Seattle Campus Mug}        0.003368894  0.9230769 0.003649635  73.066667    12\n[4]  {Google Seattle Campus Mug}        =&gt; {Google Kirkland Campus Mug}       0.003368894  0.2666667 0.012633352  73.066667    12\n[5]  {Google Kirkland Campus Mug}       =&gt; {Google NYC Campus Mug}            0.002245929  0.6153846 0.003649635  27.061728     8\n[6]  {Google Boulder Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002245929  0.3478261 0.006457047  38.717391     8\n[7]  {Google Cambridge Campus Mug}      =&gt; {Google Boulder Campus Mug}        0.002245929  0.2500000 0.008983717  38.717391     8\n[8]  {Google Boulder Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002526670  0.3913043 0.006457047  17.207729     9\n[9]  {Google NYC Campus Mug}            =&gt; {Google Boulder Campus Mug}        0.002526670  0.1111111 0.022740034  17.207729     9\n[10] {Google NYC Campus Bottle}         =&gt; {Google NYC Campus Mug}            0.002245929  0.2962963 0.007580011  13.029721     8\n[11] {YouTube Play Mug}                 =&gt; {YouTube Leather Strap Hat Black}  0.002245929  0.1568627 0.014317799  10.347131     8\n[12] {YouTube Leather Strap Hat Black}  =&gt; {YouTube Play Mug}                 0.002245929  0.1481481 0.015160022  10.347131     8\n[13] {YouTube Play Mug}                 =&gt; {YouTube Twill Sandwich Cap Black} 0.003368894  0.2352941 0.014317799  12.325260    12\n[14] {YouTube Twill Sandwich Cap Black} =&gt; {YouTube Play Mug}                 0.003368894  0.1764706 0.019090399  12.325260    12\n[15] {Google Flat Front Bag Grey}       =&gt; {Google Camp Mug Ivory}            0.005053341  0.2571429 0.019651881   5.174818    18\n[16] {Google Camp Mug Ivory}            =&gt; {Google Flat Front Bag Grey}       0.005053341  0.1016949 0.049691185   5.174818    18\n[17] {Google Unisex Eco Tee Black}      =&gt; {Google Camp Mug Ivory}            0.002245929  0.1311475 0.017125211   2.639252     8\n[18] {Google Chicago Campus Mug}        =&gt; {Google LA Campus Mug}             0.002245929  0.1702128 0.013194834  14.099951     8\n[19] {Google LA Campus Mug}             =&gt; {Google Chicago Campus Mug}        0.002245929  0.1860465 0.012071870  14.099951     8\n[20] {Google Chicago Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002526670  0.1914894 0.013194834  11.368085     9\n[21] {Google Austin Campus Mug}         =&gt; {Google Chicago Campus Mug}        0.002526670  0.1500000 0.016844469  11.368085     9\n[22] {Google Chicago Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002526670  0.1914894 0.013194834   8.420804     9\n[23] {Google NYC Campus Mug}            =&gt; {Google Chicago Campus Mug}        0.002526670  0.1111111 0.022740034   8.420804     9\n[24] {Google LA Campus Sticker}         =&gt; {Google LA Campus Mug}             0.003649635  0.4193548 0.008702976  34.738185    13\n[25] {Google LA Campus Mug}             =&gt; {Google LA Campus Sticker}         0.003649635  0.3023256 0.012071870  34.738185    13\n[26] {Google NYC Campus Zip Hoodie}     =&gt; {Google NYC Campus Mug}            0.003088153  0.1929825 0.016002246   8.486463    11\n[27] {Google NYC Campus Mug}            =&gt; {Google NYC Campus Zip Hoodie}     0.003088153  0.1358025 0.022740034   8.486463    11\n[28] {Google LA Campus Mug}             =&gt; {Google Cambridge Campus Mug}      0.002245929  0.1860465 0.012071870  20.709302     8\n[29] {Google Cambridge Campus Mug}      =&gt; {Google LA Campus Mug}             0.002245929  0.2500000 0.008983717  20.709302     8\n[30] {Google LA Campus Mug}             =&gt; {Google Austin Campus Mug}         0.002245929  0.1860465 0.012071870  11.044961     8\n[31] {Google Austin Campus Mug}         =&gt; {Google LA Campus Mug}             0.002245929  0.1333333 0.016844469  11.044961     8\n[32] {Google LA Campus Mug}             =&gt; {Google NYC Campus Mug}            0.003930376  0.3255814 0.012071870  14.317542    14\n[33] {Google NYC Campus Mug}            =&gt; {Google LA Campus Mug}             0.003930376  0.1728395 0.022740034  14.317542    14\n[34] {Google Cambridge Campus Mug}      =&gt; {Google PNW Campus Mug}            0.002245929  0.2500000 0.008983717  20.238636     8\n[35] {Google PNW Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.1818182 0.012352611  20.238636     8\n[36] {Google Cambridge Campus Mug}      =&gt; {Google Seattle Campus Mug}        0.003088153  0.3437500 0.008983717  27.209722    11\n[37] {Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.003088153  0.2444444 0.012633352  27.209722    11\n[38] {Google Cambridge Campus Mug}      =&gt; {Google Austin Campus Mug}         0.003088153  0.3437500 0.008983717  20.407292    11\n[39] {Google Austin Campus Mug}         =&gt; {Google Cambridge Campus Mug}      0.003088153  0.1833333 0.016844469  20.407292    11\n[40] {Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.005614823  0.6250000 0.008983717  27.484568    20\n[41] {Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.005614823  0.2469136 0.022740034  27.484568    20\n[42] {Google PNW Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.004491859  0.3636364 0.012352611  28.783838    16\n[43] {Google Seattle Campus Mug}        =&gt; {Google PNW Campus Mug}            0.004491859  0.3555556 0.012633352  28.783838    16\n[44] {Google PNW Campus Mug}            =&gt; {Google NYC Campus Mug}            0.003088153  0.2500000 0.012352611  10.993827    11\n[45] {Google NYC Campus Mug}            =&gt; {Google PNW Campus Mug}            0.003088153  0.1358025 0.022740034  10.993827    11\n[46] {Google Sunnyvale Campus Mug}      =&gt; {Google Austin Campus Mug}         0.002245929  0.1600000 0.014037058   9.498667     8\n[47] {Google Austin Campus Mug}         =&gt; {Google Sunnyvale Campus Mug}      0.002245929  0.1333333 0.016844469   9.498667     8\n[48] {Google Sunnyvale Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002526670  0.1800000 0.014037058   7.915556     9\n[49] {Google NYC Campus Mug}            =&gt; {Google Sunnyvale Campus Mug}      0.002526670  0.1111111 0.022740034   7.915556     9\n[50] {Google Seattle Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002526670  0.2000000 0.012633352  11.873333     9\n[51] {Google Austin Campus Mug}         =&gt; {Google Seattle Campus Mug}        0.002526670  0.1500000 0.016844469  11.873333     9\n[52] {Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.003649635  0.2888889 0.012633352  12.703978    13\n[53] {Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.003649635  0.1604938 0.022740034  12.703978    13\n[54] {Google Large Tote White}          =&gt; {Google Camp Mug Ivory}            0.002807412  0.1851852 0.015160022   3.726721    10\n[55] {Google NYC Campus Sticker}        =&gt; {Google NYC Campus Mug}            0.003368894  0.3157895 0.010668164  13.886940    12\n[56] {Google NYC Campus Mug}            =&gt; {Google NYC Campus Sticker}        0.003368894  0.1481481 0.022740034  13.886940    12\n[57] {Google Austin Campus Mug}         =&gt; {Google NYC Campus Mug}            0.004211117  0.2500000 0.016844469  10.993827    15\n[58] {Google NYC Campus Mug}            =&gt; {Google Austin Campus Mug}         0.004211117  0.1851852 0.022740034  10.993827    15\n[59] {Google Magnet}                    =&gt; {Google Camp Mug Ivory}            0.002807412  0.1562500 0.017967434   3.144421    10\n[60] {Google Camp Mug Gray}             =&gt; {Google Camp Mug Ivory}            0.005053341  0.1836735 0.027512633   3.696299    18\n[61] {Google Camp Mug Ivory}            =&gt; {Google Camp Mug Gray}             0.005053341  0.1016949 0.049691185   3.696299    18\n[62] {Google Kirkland Campus Mug,                                                                                                \n      Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002245929  0.6666667 0.003368894  29.316872     8\n[63] {Google Kirkland Campus Mug,                                                                                                \n      Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.002245929  1.0000000 0.002245929  79.155556     8\n[64] {Google NYC Campus Mug,                                                                                                     \n      Google Seattle Campus Mug}        =&gt; {Google Kirkland Campus Mug}       0.002245929  0.6153846 0.003649635 168.615385     8\n[65] {Google Boulder Campus Mug,                                                                                                 \n      Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[66] {Google Boulder Campus Mug,                                                                                                 \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.8888889 0.002526670  98.944444     8\n[67] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Boulder Campus Mug}        0.002245929  0.4000000 0.005614823  61.947826     8\n[68] {Google Cambridge Campus Mug,                                                                                               \n      Google LA Campus Mug}             =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[69] {Google LA Campus Mug,                                                                                                      \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.5714286 0.003930376  63.607143     8\n[70] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google LA Campus Mug}             0.002245929  0.4000000 0.005614823  33.134884     8\n[71] {Google Cambridge Campus Mug,                                                                                               \n      Google PNW Campus Mug}            =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[72] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google PNW Campus Mug}            0.002245929  0.4000000 0.005614823  32.381818     8\n[73] {Google NYC Campus Mug,                                                                                                     \n      Google PNW Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.7272727 0.003088153  80.954545     8\n[74] {Google Cambridge Campus Mug,                                                                                               \n      Google Seattle Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002245929  0.7272727 0.003088153  43.175758     8\n[75] {Google Austin Campus Mug,                                                                                                  \n      Google Cambridge Campus Mug}      =&gt; {Google Seattle Campus Mug}        0.002245929  0.7272727 0.003088153  57.567677     8\n[76] {Google Austin Campus Mug,                                                                                                  \n      Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002245929  0.8888889 0.002526670  98.944444     8\n[77] {Google Cambridge Campus Mug,                                                                                               \n      Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002807412  0.9090909 0.003088153  39.977553    10\n[78] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.002807412  0.5000000 0.005614823  39.577778    10\n[79] {Google NYC Campus Mug,                                                                                                     \n      Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002807412  0.7692308 0.003649635  85.625000    10\n[80] {Google Austin Campus Mug,                                                                                                  \n      Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002526670  0.8181818 0.003088153  35.979798     9\n[81] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Austin Campus Mug}         0.002526670  0.4500000 0.005614823  26.715000     9\n[82] {Google Austin Campus Mug,                                                                                                  \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002526670  0.6000000 0.004211117  66.787500     9\n\n\nThis can also be achieved in a more tidyverse style by doing the following:\n\n# Google Camp Mug Ivory tidy way\nmdl_ga_rules |&gt;\n  as(\"data.frame\") |&gt;\n  tibble() |&gt;\n  filter(str_detect(rules, \"Google Camp Mug Ivory\"))\n\n# A tibble: 7 √ó 6\n  rules                                                    support confidence coverage  lift count\n  &lt;chr&gt;                                                      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 {Google Flat Front Bag Grey} =&gt; {Google Camp Mug Ivory}  0.00505      0.257   0.0197  5.17    18\n2 {Google Camp Mug Ivory} =&gt; {Google Flat Front Bag Grey}  0.00505      0.102   0.0497  5.17    18\n3 {Google Unisex Eco Tee Black} =&gt; {Google Camp Mug Ivory} 0.00225      0.131   0.0171  2.64     8\n4 {Google Large Tote White} =&gt; {Google Camp Mug Ivory}     0.00281      0.185   0.0152  3.73    10\n5 {Google Magnet} =&gt; {Google Camp Mug Ivory}               0.00281      0.156   0.0180  3.14    10\n6 {Google Camp Mug Gray} =&gt; {Google Camp Mug Ivory}        0.00505      0.184   0.0275  3.70    18\n7 {Google Camp Mug Ivory} =&gt; {Google Camp Mug Gray}        0.00505      0.102   0.0497  3.70    18\n\n\n\n# All mugs tidy way\nmdl_ga_rules |&gt;\n  as(\"data.frame\") |&gt;\n  tibble() |&gt;\n  filter(str_detect(rules, \"Mug\"))\n\n# A tibble: 82 √ó 6\n   rules                                                     support confidence coverage  lift count\n   &lt;chr&gt;                                                       &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 {Google Austin Campus Tote} =&gt; {Google Austin Campus Mug} 0.00225      0.615  0.00365  36.5     8\n 2 {Google Austin Campus Mug} =&gt; {Google Austin Campus Tote} 0.00225      0.133  0.0168   36.5     8\n 3 {Google Kirkland Campus Mug} =&gt; {Google Seattle Campus M‚Ä¶ 0.00337      0.923  0.00365  73.1    12\n 4 {Google Seattle Campus Mug} =&gt; {Google Kirkland Campus M‚Ä¶ 0.00337      0.267  0.0126   73.1    12\n 5 {Google Kirkland Campus Mug} =&gt; {Google NYC Campus Mug}   0.00225      0.615  0.00365  27.1     8\n 6 {Google Boulder Campus Mug} =&gt; {Google Cambridge Campus ‚Ä¶ 0.00225      0.348  0.00646  38.7     8\n 7 {Google Cambridge Campus Mug} =&gt; {Google Boulder Campus ‚Ä¶ 0.00225      0.25   0.00898  38.7     8\n 8 {Google Boulder Campus Mug} =&gt; {Google NYC Campus Mug}    0.00253      0.391  0.00646  17.2     9\n 9 {Google NYC Campus Mug} =&gt; {Google Boulder Campus Mug}    0.00253      0.111  0.0227   17.2     9\n10 {Google NYC Campus Bottle} =&gt; {Google NYC Campus Mug}     0.00225      0.296  0.00758  13.0     8\n# ‚Ñπ 72 more rows"
  },
  {
    "objectID": "blog/posts/2025-01-11-post-data-science-analysis-things-you-should-read-watch-listen/index.html",
    "href": "blog/posts/2025-01-11-post-data-science-analysis-things-you-should-read-watch-listen/index.html",
    "title": "What‚Äôs Worth Your Time: Read, Watch, and Listen",
    "section": "",
    "text": "Motivation\nWe‚Äôre a week into 2025, and I thought this would be a perfect time for some reflection. Inspired by Tan Ho‚Äôs post, I felt compiling a non-exhaustive list of content I‚Äôve enjoyed would be a valuable, useful exercise to kick off the new year. Rather than have these resources sit untouched in my bookmarks, I thought I‚Äôd share.\nBelow you will find sections containing curated lists of posts, videos, and other content I‚Äôve found relevant to shaping how I view and approach my work. Much of the content is data science, analysis, technology, career development, and media industry related. Some links are just stuff I‚Äôve found interesting.\nEnjoy!\n\n\nData science, analysis, coding\n\nJenny Bryan: Project-oriented workflow, blog post (~15min read)\n\nThis is one of those pieces that was a paradigm shift in my thinking: self-contained projects. A simple idea with a major impact. In the past, my workflow would be a hodgepodge mess of errant scripts and files. The idea of using some simple conventions to allow my code to work on ‚Äòanother computer‚Äô or in an ‚Äòanother environment‚Äô was absent. Employing some of the post‚Äôs conventions increased my impact, as I was better positioned to share my work with others.\n\nJenny Bryan: Object of type ‚Äòclosure‚Äô is not subsettable, YouTube video (53 mins)\n\nIf you work with the R programming language, this video is worth every minute. Learning how to debug code helps you more quickly identify where something is going wrong, determine what is going wrong, and come to a useful solution.\n\nEmily Riederer: RMarkdown Driven Development (RmdDD), blog post (~20 min read)\n\nRmdDD (now QmdDD for some) just works for analysis project development. A straightforward idea: get your entire process into one file, then refactor from there. This is how I generally approach most analysis projects now.\n\nSharla Gelfand: Don‚Äôt repeat yourself, talk to yourself! Reporting with R | RStudio(2020), YouTube video (21 mins)\n\nThe key takeaway: think of ways to incorporate structure into your projects that will be helpful the next time you have to complete something. This talk motivated me to be more intentional about project structure and to think about how this structure relates to the impact of my work. There‚Äôs power in being nice to your future self and others.\n\nEmily Riederer: Building a team of internal R packages, blog post (~35 mins)\n\nThis post outlines a framework for the development of internal R packages. It frames internal package development as value adds when they act like work colleagues, as they can be helpful to pass along institutional knowledge to others. I aspire to align my internal package development with the suggestions in this post.\n\nDavid Robinson: TidyTuesday live screencasts, YouTube videos (~45mins to ~1hr 15mins)\n\nAlthough David Robinson hasn‚Äôt posted a screencast in some time, I still find great value in these, now older, videos. Just watching someone and hearing their thought process as they work to visualize TidyTuesday datasets has been super valuable. I‚Äôve learned a lot from this great resource.\n\nColin Gillespie: Getting the Most Out of Git - posit::conf(2024), YouTube video (21 mins)\n\nDespite this talk‚Äôs focus being about Git, I think there‚Äôs an even more general lesson shared here. That is, use only what‚Äôs needed for the type of project you‚Äôre working on and don‚Äôt fall into the trap of over burdening yourself and/or team with unneeded complexity. Git and platforms like GitHub have lots of functionality. Critical software is built using these tools, where their use is necessary. But, does my personal project need a DevOps strategy with fully automated testing and app deployment to be successful? Probably not. Can my team get some value from running a few automated tests when they submit a pull request? Maybe, as long as it doesn‚Äôt overburden them to get their work done. I struggle with finding the right balance here from time to time. This talk is a freeing reminder to use only what‚Äôs needed for the project.\n\n\n\n\nProductivity and project development\n\nJenny Bryan: How to Name Files Like a Normie NormConf YouTube video (5 mins) and GitHub repo (10 mins)\n\nTake five minutes to change your life forever. Regardless if you code or not, everyone who uses a computer needs to develop the skills for naming files. Incorporating some simple naming conventions can be a big productivity boost.\n\nHank Green: The Secret to My Productivity, YouTube video (4 mins). Thanks for sharing, Tan.\n\nI came away with a simple takeaway from this video: get your projects to 80%, share it with the world, and move on to the next. Too often we fall into the trap of attempting to get things perfect. Although striving for perfection makes you feel productive, in reality it slows your ability to learn.\n\n\n\n\nManagement, leadership, and organizational culture\n\nEmilie Schario & Taylor A. Murphy, PhD: Run Your Data Team Like a Product Team, blog post (~20 mins)\n\nI like the framing this post uses to describe how to build and run a data organization. It posits two things data teams need to do to meet their full potential: focus on building a Data Product and view others within your organization as your customers. Indeed, I certainly don‚Äôt contend this will work for every data team, but I found it a useful vision for the work my team does.\n\nJD Long: It‚Äôs Abstractions All the Way Down‚Ä¶ - posit::conf(2023) YouTube recording (1hr 2mins)\n\nThis talk provided a clear framework to better understand where my work fits within a larger organization / network. Framing these ideas in terms of abstractions and providing thoughts on how to understand and work these abstractions has impacted how I approach and view the work within my organization.\n\nHarvard Business Review: HBR Guide to Making Every Meeting Matter, book (~$22).\n\nMeetings are inevitable. In fact, you may be tasked with leading a meeting some time in your career. Some meetings move the needle, others could have been an email. Prepare yourself with some useful techniques to make meetings more impactful, useful.\n\n\n\n\nMedia industry specific (the industry I work in)\n\nDecoder podcast, The Verge (1hr episodes)\n\nIf you‚Äôre interested in media or tech, this podcast is for you. Nilay Patel, the host, does a phenomenal job interviewing various CEOs and thought leaders in these industries. Listening to each episode has kept me up to date with the big trends and news happening in these spaces.\n\n\n\n\nPotpourri\nThis section is a miscellaneous collection of stuff I‚Äôve enjoyed but really doesn‚Äôt fit into the other categories.\n\nMarketplace weekday radio show and podcast (~30 min episodes)\n\nI like to keep up with business and economic news. Marketplace is an excellent, weekday collection of stories focused on these topics.\n\nPlanet Money, podcast (~30 min episodes) National Public Radio (NPR)\n\nUnderstanding the complexities of the economy can be challenging. Planet Money helps put these topics into perspective.\n\n\n\n\nFinal thoughts\nThe above list is certainly not comprehensive of all the work I‚Äôve found useful and impactful. Some content that has shaped my work and views is not included here. It will expand over time.\nI hope you found at least one useful takeaway. If so, let‚Äôs connect and chat about it:\n\nLinkedIn\nGitHub\nBluesky\n\nHere‚Äôs to a wonderful start to 2025. Cheers üéâ!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {What‚Äôs {Worth} {Your} {Time:} {Read,} {Watch,} and {Listen}},\n  date = {2025-01-11},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúWhat‚Äôs Worth Your Time: Read, Watch, and\nListen.‚Äù January 11, 2025."
  },
  {
    "objectID": "blog/posts/2025-08-03-til-notes-simple-made-easy/index.html",
    "href": "blog/posts/2025-08-03-til-notes-simple-made-easy/index.html",
    "title": "Notes: Rich Hickey‚Äôs talk Simple Made Easy",
    "section": "",
    "text": "I was introduced to the talk Simple Made Easy presented by Rich Hickey during a 2025 Nebraska.Code() session. It‚Äôs a fairly well-known talk in software development. Though outside of my domain, the topics covered were somewhat related to my work. I thus decided to stretch myself and review the talk.\nSeveral of the talk‚Äôs points stood out to me. Some I‚Äôm still trying to fully grok. Below are my notes and takeaways.\n\n\n\n\n\n\nNote\n\n\n\nThis post is written in the spirit of publishing more frequent blog posts. It‚Äôs a bit of a scratchpad of ideas, concepts, and ways of working that I found to be useful or interesting. As such, what‚Äôs here is lightly edited. Be aware: there will likely be spelling, grammatical, or syntactical errors along with some disjointed, incomplete ideas.\n\n\n\nTakeaways\n\nWe need to build simple systems if we want to build good systems.\n\nThe talk began by differentiating between the definitions of simple and easy in the context of software and system design (01m25s). Simple is about being one fold, one braid, or one twist. Simple is objective. Easy is defined as being near at hand, near to our understanding or current skill set. Easy is relative. Designing towards easy can be a limitation, as it forces familiarity.\n\nIf you want everything to be familiar you will never learn anything new because it can‚Äôt be significantly different from what you already know.\n\nThe talk then uses these two definitions to further distinguish between constructs vs.¬†artifacts (09m05s). Developers work with constructs to create artifacts. At times, too much focus is placed on the constructs of the developer‚Äôs experience. Rather, a focus on software quality, correctness, maintenance, and the ability for change will yield more benefits.\n\nWe have to start assessing our constructs based around the artifacts, not around the look and feel of the experience of typing it in or the cultural aspects of that.\n\nUsers don‚Äôt care about our experience writing the program. They only care about whether the program does what it says it‚Äôs going to do and if it works well or what the level of the complexity the program yields via its output.\nThe talk then addressed the limitations of complexity (12m14s). Our ability to hold onto lots of information is limited. Intertwining of constructs requires us to hold lots of information in our mind. As such:\n\nComplexity undermines understanding.\n\nRich Hickey then provided some criticism of guard rail programming (e.g., using tests)(15m32s). A few questions were posed along with this point:\n\nDo guard rails help you get where you‚Äôre going?\nWhat‚Äôs true about every bug? They were written, it passed a type checker, and it passed all the tests.\n\nSo, we‚Äôll always need to reason about our program because these guard rails are just safety nets. Being able to reason about or programs is important to debugging.\nThis type of development has been criticized for it‚Äôs impact on speed and agility. Hickey addressed this concern:\n\nIgnoring complexity will slow you down over the long haul.\n\nIf you focus to much on ease, you‚Äôll be able to move as fast as possible at the start. However, no matter what, complexity will always catch up to you. The net effect will be that you‚Äôre not moving forward in any significant way. To make this point more concrete, the talk introduced the knitted castle problem (19m36s). What will be easier to change, a castle knitted of yarn or one crafted using Legos? The results can still be complex. But the system may be hard to change in the future. Thus, the benefits of focusing on simplicity include:\n\nEase of understanding\nEase of change\nEasier debugging\nFlexibility\n\nThe talk then introduced and applied the term complecting: to interleave, entwine, braid (31m36s). Complecting is bad. You should avoid complecting things whenever possible.\n\nYou can write as sophisticated a system with dramatically simpler tools, which means you‚Äôre going to be focusing on the system, what it‚Äôs supposed to do instead of all the guck that falls out of the constructs you‚Äôre using. If you want a simpler life, just choose simpler stuff.\n\n\nProgramming is not about typing, it‚Äôs about thinking.\n\nHowever, at times we do need to make our own constructs. This can require abstraction: drawing things away. A good framework to keep abstraction aligned with simplicity is to use the who, what, when, where, why, and how framework. Another is to take the stance of I don‚Äôt know, I don‚Äôt want to know.\nAlongside the above points, I found the following collection of quotes from the talk to be interesting:\n\nSimplicity is a choice. It‚Äôs your fault if you don‚Äôt have a simple system.\n\n\nEasy is not simple.\n\n\nSimplicity often means making more things, not fewer.\n\n\nSimplicity is the ultimate sophistication.\n‚Äì Leonardo da Vinci\n\n\n\nFinal thoughts\nThe distinction between simple and easy stuck with me. Being able to have more precise language to describe the constructs we use in system and software design improves the quality of our output‚Äìfor both developers and users.\nThe discussion of our limitations as developers is a point that resonated with me. Indeed, you can develop towards complexity. However, we‚Äôre all limited in our ability to hold large amounts of information. More information, thus, can result in limited understanding. Complecting only compounds problems due to this limitation.\nComplecting is a term I‚Äôm walking away with from this talk. The idea of multiple braids or folds as being bad seems like great advice. How to avoid this in practice seems like the challenging task. The talk provides some tools to address this, though.\nOverall, a good talk with some thought-provoking points. I‚Äôm still attempting to understand some of what is shared, but I found many of the points were accessible and applicable to the work I do. Despite not being a computer scientist, systems designer, or full-on software developer, I still got great value from reviewing this talk.\nIf I misunderstood these points or you‚Äôre someone who is interested in discussing this topic further, let‚Äôs connect!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes: {Rich} {Hickey‚Äôs} Talk {Simple} {Made} {Easy}},\n  date = {2025-08-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúNotes: Rich Hickey‚Äôs Talk Simple Made\nEasy.‚Äù August 3, 2025."
  },
  {
    "objectID": "blog/posts/2025-11-21-hex-update-004/index.html",
    "href": "blog/posts/2025-11-21-hex-update-004/index.html",
    "title": "The Hex Update: Issue 004",
    "section": "",
    "text": "Let‚Äôs catch up\nWelcome folks! Here‚Äôs Issue 004.\nThis week‚Äôs articles were focused on two topics: AI and users‚Äô relationships with their smartphones. The first article I bumped into reports on Disney‚Äôs exploration of partnerships with AI companies to craft more immersive and customizeable experiences‚Äìa growing expectation for many audiences. Second, I share an article about the changing relationship between people and their smartphones and how the digital detox industry is growing. Lastly, I share an article with a cautionary tale about the impact AI has on online survey data collection. Soon to be published research shows agentic AI, without additional response validation measures, will make future results from this methodology suspect. All interesting topics relevant to the media industry.\nSo, let‚Äôs get into it.\n\n\nThree things from this week\nHere are three things that caught my attention this week.\nArticle: DIY Disney? The company is exploring AI so fans can make content from Disney stories\nHere‚Äôs a read from NPR‚Äôs culture desk that caught my attention. According to the reporting, Disney is exploring how to make Disney+ subscription-based streaming services more interactive and customizeable for its users. The focus is this: develop ways for users to craft their own content using Disney‚Äôs existing intellectual property. In support of these interactive and immersive experiences, the article highlights some thoughts from Netflix‚Äôs CTO, Elizabeth Stone:\n\nThe future of entertainment is likely to be even more personalized, even more interactive, even more immersive.\n\nSuch a focus has lead Bob Iger, Disney‚Äôs current CEO, to hold discussions about partnership with various AI companies. However, not much else was mentioned about these current partnerships. This may be a sign that some of the larger media companies are becoming more open to licensing aggreements of IP to AI companies. The article also contains some discussion about younger audiences (i.e., Gen Z in particular) preferences in this space. That is, younger audiences seek out media and places where they can participate, remix, and respond. It‚Äôs no longer a passive experience. The article also shares research stating younger audiences have lower expectations regarding the content they consume.\n\nWhy does this matter?\nAudience‚Äôs media consumption will likely not remain passive. Instead, audiences will likely come to expect more interactive and customizeable experiences while consuming media. As a result, larger media companies seem more comfortable negotiating deals with tech companies to develop these experiences, while striking up licensing deals for AI companies to use their IP. Besides this shift toward more interactive experiences, it‚Äôs also important to recognize younger audiences are less concerned about the quality of the content they consumed. Like the article states, content doesn‚Äôt need to be fully polished to be popular.\nAudio: The growing business of disconnecting\nHere‚Äôs an interesting listen from Marketplace about the growth of companies aimed at helping people disconnect from their phones. The goal: provide useful tools to limit users‚Äô screentime. This includes companies who‚Äôve developed apps, and others who‚Äôve created physical devices to help limit this screentime. My favorite shared within article is the six pound phone case. Although this may seem like exterme measures, these are signs that the digital detox industry is growing. For instance, from research cited in the article, this industry will grow to $19.44 billion by 2032. There‚Äôs certainly some irony here: you‚Äôre paying for your phone but purchase apps to not use your phone that provide information on how much you‚Äôre not using your phone. Despite this, a good point was shared in the article. It‚Äôs not a personal fault of users‚Äìthey‚Äôre just using these devices for what they were designed to do.\n\n\nWhy does this matter?\nUsers‚Äô relationship with their phones may be shifting. Screentime is top-of-mind for users now more than ever. Many are aware of the stickyness and addictiveness of applications, which is further afforded by the functionality these devices. As such, many users are exploring ways to limit their screentime. Certainly, mobile devices are important delivery vehicles of content and experiences for media organizations. Maybe it‚Äôs not about crafting experiences that are and feel ‚Äòaddictive‚Äô, ‚Äòsticky‚Äô, or ‚Äòfunctional‚Äô. Rather, it‚Äôs about creating content, experiences, and spaces that are useful and impactful to users‚Äô lived experience. Creating value for the time users spend consuming your media or engaging within your space.\nArticle: A Researcher Made and AI That Completely Breaks the Online Surveys Scientists Rely On\nA collegue recently shared this article with me, and it peaked my interest. The article highlights a soon to be released academic research paper by Sean Westwood from the Polarization Research Lab. The paper, soon to be published in the Proceedings of the National Academy of Sciences (PNAS), demonstrates AI puts into question the current methods of synthetic participant response identification. AI bots are now sophisticated enough to mimic human behavior, and these actions taken by the AI can bypass verification methods like attention check questions (ACQs), behavioral flags, and response patterns (e.g., reading time, character-by-character entry, etc.). Besides being able to subvert advanced validation checks, these bots are even good enough to mimic responses comparable to specific demographic groups. Consequently, this raises some serious questions about the validity of survey results, especially if they were collected using some type of online sampling method and some sort of monetary compensation is provided.\nI did some more digging. Here‚Äôs some additional reporting on the study I found to be interesting:\n\nThis AI mimics humans so perfectly it can corrupt public opinion polls and surveys\nAI poses ‚Äòexistential threat‚Äô to polling, mimicking human responses almost perfectly, study finds\n\nI‚Äôll link to the original paper once it becomes available.\n\n\nWhy does this matter?\nSurvey methods are essential to audience measurement. The impact of AI on the realness of survey responses will require those working in the media, marketing, and audience measurement space to consider the validity of specific survey methods and sampling services. To me, it seems greater value will be placed on firms with validation checks that can prove the liveness of participants in their survey panels. Despite this, there may be little that can be done to address the use of agentic AI tools potential survey participants might use, like that of Actions (01M00S) in OpenAI‚Äôs Atlas browser. For publishers, platform, and property owners, this may be a further argument for shoring up first party data collection. The random survey respondent you receive from a user on your platform may no longer reach a level of validty decisions can be made. This is especially true if incentives are tied to these survey responses.\n\n\n\nJust for fun\nSmokey the Bear is iconic. So much so that he even has his own zip code: 20252. While researching more about Zip Codes, I came across this blog post. I discovered Zip Codes don‚Äôt necessarily represent geographic areas. Rather, they are considered to be address groups or delivery routes. The more you know.\nWant to learn more about Smokey the Bear? Here‚Äôs some more I bumped into while falling down the rabbit hole:\n\nSmokey‚Äôs Story\nLetters to Smokey Bear Reveal Promise of Hope for the Future\n\nHave a good end to your week. Cheers üéâ!\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 004},\n  date = {2025-11-21},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúThe Hex Update: Issue 004.‚Äù\nNovember 21, 2025."
  },
  {
    "objectID": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "href": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "title": "TIL: Combine plots using patchwork",
    "section": "",
    "text": "Today I learned the patchwork package makes it easy to combine multiple plots into a single plot. In this post, I overview what I‚Äôve recently learned from using patchwork‚Äôs functions to create plot compositions."
  },
  {
    "objectID": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "href": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "title": "TIL: Combine plots using patchwork",
    "section": "Show me the money",
    "text": "Show me the money\nLet‚Äôs start with a pretty straight forward plot, total revenue over time. Here‚Äôs the code I used to wrangle the data and create the plot:\n\ndata_revenue_trend &lt;- data_google_merch |&gt;\n  group_by(event_date, transaction_id) |&gt;\n  summarise(revenue = max(purchase_revenue_in_usd)) |&gt;\n  summarise(revenue = sum(revenue))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_rev_trend &lt;- ggplot(\n  data_revenue_trend,\n  aes(x = event_date, y = revenue)\n) +\n  geom_line(linewidth = 2) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Total Revenue ($USD)\")"
  },
  {
    "objectID": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "href": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "title": "TIL: Combine plots using patchwork",
    "section": "Items generating the most revenue",
    "text": "Items generating the most revenue\nThe data is from an online store, so we should explore items generating the most revenue. To keep things simple, I use dplyr‚Äôs slice_max(10) to create a plot of the Top 10 revenue generating items. Because we‚Äôre slicing the data, other items are excluded from the plot. The following code wrangles the data and creates the plot for us.\n\ndata_items_rev &lt;-\n  data_google_merch |&gt;\n  group_by(item_name) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_rev_items &lt;- ggplot(\n  data_items_rev,\n  aes(\n    x = fct_reorder(item_name, revenue),\n    y = revenue\n  )\n) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "href": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "title": "TIL: Combine plots using patchwork",
    "section": "Product categories generating most revenue",
    "text": "Product categories generating most revenue\nThe data also categorizes items into more general groupings. As such, we can create a plot ranking product categories by amount of revenue generated. No question, it‚Äôs apparel. The code to create the plot is similar to what we did above with items.\n\ndata_cat_rev &lt;-\n  data_google_merch |&gt;\n  group_by(item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_cat_items &lt;- ggplot(\n  data_cat_rev,\n  aes(\n    x = fct_reorder(item_category, revenue),\n    y = revenue\n  )\n) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "href": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "title": "TIL: Combine plots using patchwork",
    "section": "Product categories trend",
    "text": "Product categories trend\nNow that we‚Äôve created a plot ranking product categories, let‚Äôs create a plot that breaks out several categories of interest over time. To do this, we‚Äôll use the following code.\n\nimportant_cats &lt;-\n  c(\"Accessories\", \"Bags\", \"Apparel\", \"Drinkware\", \"Lifestyle\")\n\ndata_cat_rev_trend &lt;-\n  data_google_merch |&gt;\n  filter(item_category %in% important_cats) |&gt;\n  group_by(event_date, item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_high_cat_trend &lt;- ggplot(\n  data_cat_rev_trend,\n  aes(x = event_date, y = revenue, color = item_category)\n) +\n  geom_line(linewidth = 2, alpha = .7) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Revenue ($USD)\", color = \"\")"
  },
  {
    "objectID": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "href": "blog/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "title": "TIL: Combine plots using patchwork",
    "section": "Other operators to know",
    "text": "Other operators to know\npatchwork has other operators to modify the layout of the plot composition. Each operator is listed below with a brief description of its use.\n\n+ and - - combines plots together on the same level.\n| - combines plots beside each other (i.e., packing).\n/ - places plots on top of each other (i.e., stacking).\n* - adds objects like themes and facets to all plots on the current nesting level.\n& - will add objects recursively into nested patches.\n\nThe following examples highlight the use of several of these operators."
  },
  {
    "objectID": "blog/posts/2025-08-10-tidytuesday-2025-08-05-income-inequality-before-and-after-taxes/index.html",
    "href": "blog/posts/2025-08-10-tidytuesday-2025-08-05-income-inequality-before-and-after-taxes/index.html",
    "title": "TidyTueday contribution: 2028-08-05",
    "section": "",
    "text": "Background\nThis week‚Äôs TidyTuesday included data exploring Income Inequality Before and After Taxes. The article ‚ÄúIncome inequality before and after taxes: how much do countries redistribute income?‚Äù by Joe Hasell inspired this week‚Äôs data. The data were processed by Our World in Data, and they orignated from various sources (see the documentation for more info).\nThe data provides an estimate called the Gini coefficient for both before and after taxes. Here‚Äôs the definition:\n\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Inequality is measured here in terms of income before and after taxes and benefits.\n\nMy contribution\nHere is my contribution for the week of 2025-08-05:\n\n\n\n\nThe code I used to produce this animated plot follows.\nYou can also access the code via my TidyTuesday GitHub repository here.\nHere is the code for this specific contribution.\nIf you like what you see or want to discuss this week‚Äôs contribution, follow and connect with me in these places:\n\nBlueSky: collinberke.bsky.social\n\nGitHub: collinberke\n\nLinkedIn: @collinberke\n\nThe code\n\n# Setup -----\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(berkeBrand)\nlibrary(here)\nlibrary(glue)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(ggalt)\nlibrary(ggtext)\nlibrary(gganimate)\n\n# Get data -----\n\n# `gini_mi_eq` = pre-tax inequality\n# `gini_dhi_eq` = post-tax inequality\n\n# data-import-income-inequality\ntuesdaydata &lt;- tt_load('2025-08-05')\n\nincome_inequality_processed &lt;-\n  tuesdaydata$income_inequality_processed |&gt;\n  clean_names()\n\n# Explore data -----\n\n# data-explore\nnames(income_inequality_processed)\nglimpse(income_inequality_processed)\nskim(income_inequality_processed)\n\n# Questions to explore:\n#   * How many countries are represented in the data?\n#   * How many data points (i.e., years) are available for each country?\n#   * Range of data points per country?\n#   * Over what years does this data represent?\n\n# data-questions\n\n# Countries?\nincome_inequality_processed |&gt;\n  distinct(entity) |&gt;\n  count()\n\n# Data points?\ndata_points &lt;- income_inequality_processed |&gt;\n  group_by(entity) |&gt;\n  count(sort = TRUE) |&gt;\n  print(n = 100)\n\n# Per country?\nrange(data_points$n)\n\n# What country(s) only has one?\n# The Dominican Republic\ndata_points |&gt; filter(n == 1)\n\n# Years represented?\n# 1963 to 2023 for some countries, not all\nrange(income_inequality_processed$year)\n\n# Transform data -----\n\n# trnsfm-inequality-data\ndata_income_inequality &lt;- income_inequality_processed |&gt;\n  arrange(entity, year) |&gt;\n  complete(entity, year = 1963:2023) |&gt;\n  group_by(entity) |&gt;\n  fill(gini_mi_eq, gini_dhi_eq) |&gt;\n  mutate(diff = gini_mi_eq - gini_dhi_eq)\n\n## An animated dumbell chart -----\n\n# plot-text\n\ntitle &lt;- \"&lt;strong&gt;Income inequality before and after taxes&lt;/strong&gt;\"\nsubtitle &lt;- \"{closest_state} Gini coefficient: a measure of income inequality\"\ncaption &lt;- glue(\n  \"&lt;br&gt;\",\n  add_tt(\n    source = \"Processed from multiple sources by Our World in Data\",\n    tt_date = \"2025-08-05\"\n  ),\n  \"&lt;br&gt;\",\n  \"&lt;strong&gt;Note:&lt;/strong&gt; Countries ordered by maxmimum Gini coefficient after taxes, values forward filled\",\n  \"&lt;br&gt;\",\n  add_socials()\n)\nx &lt;- glue(\n  \"Gini coefficient\",\n  \"(&lt;span style = 'color: orange;'&gt;&lt;strong&gt;before&lt;/strong&gt;&lt;/span&gt;\",\n  \"and &lt;span style = 'color: blue;'&gt;&lt;strong&gt;after taxes&lt;/strong&gt;&lt;/span&gt;)\",\n  .sep = \" \"\n)\ny &lt;- \"\"\n\n## Static chart -----\n\n# I found this useful because the rendering of the animation can take some time.\n# Having this static plot allowed for faster iteration.\n\n# vis-dumbell-chart-ggplot2\nvis_income_inequality &lt;- ggplot(\n  data = data_income_inequality\n) +\n  geom_segment(\n    aes(\n      x = gini_mi_eq,\n      xend = gini_dhi_eq,\n      y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE),\n      yend = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)\n    ),\n    color = \"darkgrey\",\n    linewidth = .5\n  ) +\n  geom_point(\n    aes(x = gini_mi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 2,\n    fill = \"black\",\n    shape = 24\n  ) +\n  geom_point(\n    aes(x = gini_mi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 1,\n    fill = \"orange\",\n    shape = 24\n  ) +\n  geom_point(\n    aes(x = gini_dhi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 2,\n    fill = \"black\",\n    shape = 24\n  ) +\n  geom_point(\n    aes(x = gini_dhi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 1,\n    fill = \"blue\",\n    shape = 24\n  ) +\n  theme_minimal() +\n  labs(\n    title = title,\n    subtitle = subtitle,\n    caption = caption,\n    y = \"\",\n    x = x\n  ) +\n  theme(\n    plot.title = element_textbox_simple(size = 20),\n    plot.subtitle = element_text(size = 16, hjust = 0),\n    plot.caption = element_textbox_simple(halign = 1, vjust = -5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 7, color = \"black\"),\n    axis.text.x = element_text(color = \"black\"),\n    axis.title.x = element_textbox_simple(halign = .5)\n  )\n\nggsave(\n  here(\n    \"blog/posts/\",\n    \"2025-08-10-post-tidy-tuesday-2025-08-05-income-inequality-before-and-after-taxes\",\n    \"vis-income-inequality-dumbell.png\"\n  )\n)\n\n# func-get-dumbell-chart\nget_dumbell_chart &lt;- function() {\n  vis_income_inequality\n}\n\n# vis-animation-transitions\ntimelapse_income_inequality_dumbell &lt;- get_dumbell_chart() +\n  transition_states(year, transition_length = 3, state_length = 5) +\n  ease_aes(\"quadratic-in-out\", interval = 1)\n\n# vis-animate-visualization\nanimated_income_inequality_plot &lt;- animate(\n  timelapse_income_inequality_dumbell,\n  nframes = 125,\n  duration = 45,\n  start_pause = 5,\n  end_pause = 5,\n  height = 7,\n  width = 7,\n  res = 150,\n  units = \"in\",\n  fps = 10,\n  renderer = gifski_renderer(loop = TRUE)\n)\n\nanim_save(\n  here(\n    \"blog/posts/\",\n    \"2025-08-10-post-tidy-tuesday-2025-08-05-income-inequality-before-and-after-taxes\",\n    \"vis-income-inequality-dumbell.gif\"\n  ),\n  animated_income_inequality_plot\n)\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {TidyTueday Contribution: 2028-08-05},\n  date = {2025-08-10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. ‚ÄúTidyTueday Contribution:\n2028-08-05.‚Äù August 10, 2025."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Collin K. Berke, Ph.D.¬†is a media research analyst in public media. Professionally, he uses data and media/marketing research methods to answer questions on how to best reach and engage audiences‚Äìtowards the goal of enriching lives and engaging minds with public media content and services. He is especially interested in the use and development of open-source statistical software (i.e.¬†Rstats) to achieve this goal, and developing a broader understanding the role these tools can play in media, digital, and marketing analytics.\nHe has experience using different software, third-party services, and programming languages from developing several analytics projects, both in industry and academia. In regards to programming languages, he has developed projects using R, SQL, $bash, and a little bit of Python. He also has extensive experience using different analytics solutions. For data warehousing, he mostly uses database tools like Postgres and those in the Google Cloud Platform ecosystem (e.g., Google BigQuery). When it comes to automating workflows and data pipelines, he has experience implementing and working with Apache Airflow. He also has extensive experience using third-party tools and software to analyze, wrangle data, and communicate his analyses (e.g., Google Analytics, Google Sheets, Excel, Google Data Studio, and R Shiny). Most of his current work is industry related.\nCollin also serves as an adjunct instructor for the University of Nebraska-Lincoln and Southeast Community College, where he teaches courses in sports data visualization and analysis and communication specific courses. He holds a M.A.¬†in Communication Studies from the The University of South Dakota and a Ph.D.¬†in Media and Communication from Texas Tech University. He has also published and contributed to the publication of several academic journal articles.\nCollin is a self-proclaimed news, sports, and podcast junkie. He really enjoys listening to NPR, watching PBS (especially NOVA), and indulging in college football and baseball. At times, he will write blog posts on topics he finds interesting in these areas.\nCheck out the Now page to see what Collin is currently reading and working on."
  },
  {
    "objectID": "about.html#lightning-talk",
    "href": "about.html#lightning-talk",
    "title": "About",
    "section": "Lightning talk",
    "text": "Lightning talk\nGet to know me in five minutes or less:\n\nView slides fullscreen"
  },
  {
    "objectID": "about.html#note-about-this-site",
    "href": "about.html#note-about-this-site",
    "title": "About",
    "section": "Note about this site",
    "text": "Note about this site\nThe views expressed on this site are my own, and they do not reflect the views of my employer, professional and/or community groups I hold membership. Any analyses hosted on this site were done for professional development or were for fun. I make every attempt to perform valid and accurate data analysis and reporting. Unless otherwise noted, none of the content on this site has been peer-reviewed, and thus any conclusions drawn or uses stemming from this work need to take these limitations into account.\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A now page, why?"
  },
  {
    "objectID": "now.html#projects-im-working-on",
    "href": "now.html#projects-im-working-on",
    "title": "Now",
    "section": "Projects I‚Äôm working on",
    "text": "Projects I‚Äôm working on\n\nTransitioning to LazyVim\nAs of my last update, I decided to burn down my Neovim setup and start over. I was in search of something more enjoyable and manageable to work with. I came across LazyVim and started to really enjoy it. Out of the box it had sane defaults and configurations, so the setup was minimal and I‚Äôve found it more enjoyable to work with. LazyVim Extras also make it easy to install and configure different plugins.\n\n\nBlogging more\nI‚Äôm attempting to increase the publishing of my blog posts. I was following a cadance of roughly posting every month. I want to bump this up to two a month, with a stretch goal being every week. The hardest parts are being realistic with myself, identifying shorter post topics to write about, and being kinder to myself. Writing posts with a tighter turn-around means embracing progress over perfection.\n\n\nThinking about the use of data and organizational culture\nMore and more often I‚Äôm being tasked with stratigizing how to improve the role and impact data has within an organization. These are not easy questions to answer. I‚Äôve been doing a lot of exploring and experimentation. If anyone has any suggestions, let‚Äôs chat.\nIf you‚Äôre interested in what I‚Äôve focused on in the past, check out my past updates"
  },
  {
    "objectID": "now.html#books-im-reading",
    "href": "now.html#books-im-reading",
    "title": "Now",
    "section": "Books I‚Äôm reading",
    "text": "Books I‚Äôm reading"
  },
  {
    "objectID": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "href": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "title": "Now",
    "section": "A list of books I‚Äôve read (ever since I‚Äôve started keeping track)",
    "text": "A list of books I‚Äôve read (ever since I‚Äôve started keeping track)\n\nPractical SQL, 2nd Edition: A Beginner‚Äôs Guide to Storytelling with Data by Anthony DeBarros\nI‚Äôm currently working with another collegue to sharpen and practice our SQL skills. Although I‚Äôm up-to-date on most of the basics and am usually pretty good at completing straight-forward queries, it was a good time to focus and become even more proficient with the language. So, we picked up this book to learn more.\n\n\nR for Data Science (2e) by Hadley Wickham, Mine √áetinkaya-Rundel, and Garrett Grolemund\nCurrently, I‚Äôm leading another Data Science Learning Community bookclub meeting for this book. I haven‚Äôt read the second edition yet, so I‚Äôm excited to see and learn what‚Äôs new.\n\n\nWith the Old Breed: At Peleliu and Okinawa by E.B. Sledge\nThis is a personal read I pick up from time to time. I finished watching the HBO miniseries Band of Brothers and The Pacific, which were based on first hand accounts in books like this. With the Old Breed is a first-hand account of one serviceman‚Äôs experience on the frontline during operations in the Pacific Theater, specifically in Peleliu and Okinawa, during World War II. It‚Äôs unfathomable to comprehend the experiences and hardships many endured. Reading it has made me more deeply appreciate the sacrafices men and women of the armed services make for the United States of America.\n\n\nProfessional development reads\n\nMachine Learning with R - Fourth Edition by Brett Lantz\n\n\nPractical Tableau by Ryan Sleeper\n\n\nHBR Guide to Making Every Meeting Matter from the Harvard Business Review\n\n\nThe Checklist Manifesto: How to Get Things Right by Atul Gawande\n\n\nTidy Modeling with R by Max Kuhn and Julia Silge\n\n\nPython for Data Analysis by Wes McKinney\n\n\nEngineering Production-Grade Shiny Apps by Colin Fay, S√©bastien Rochette, Vincent Guyader, and Cervan Girard\n\n\nAdvanced R by Hadley Wickham. Check out past book club meeting recordings here.\n\n\nR Packages by Hadley Wickham and Jenny Bryan. Check out past book club meeting recordings here.\n\n\nVim help files maintained by Carlo Teubner\n\n\nMastering Ubuntu by Jay LaCroix\n\n\nGoogle BigQuery: The Definitive Guide by Valliappa Lakshmanan and Jordan Tigani\n\n\nMastering Shiny by Hadley Wickham. Check out the past book club meeting recordings here.\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. Check out the past book club meeting recordings here.\n\n\nDocker Deep Dive by Nigel Poulton\n\n\n\nPersonal reads\n\nThe Currents of Space (Galactic Empire 2) by Issac Asimov\n\n\nThe Stars, Like Dust (Galactic Empire 1) by Issac Asimov\n\n\nRobots and Empire (The Robot Series 4) by Issac Asimov\n\n\nThe Robots of Dawn (The Robot Series 3) by Isaac Asimov\n\n\nThe Naked Sun (The Robot Series Book 2) by Isaac Asimov\n\n\nThe Caves of Steel (The Robot Series Book 1) by Isaac Asimov\n\n\nI, Robot by Isaac Asimov\n\n\nLeviathan Falls (The Expanse book 9) by James S.A. Corey\n\n\nTiamat‚Äôs Wrath (The Expanse book 8) by James S.A. Corey\n\n\nPersepolis Rising (The Expanse book 7) by James S.A. Corey\n\n\nBabylon‚Äôs Ashes (The Expanse book 6) by James S.A. Corey\n\n\nNemesis Games (The Expanse book 5) by James S.A. Corey\n\n\nCibola Burn (The Expanse book 4) by James S.A. Corey\n\n\nAbaddon‚Äôs Gate (The Expanse book 3) by James S.A. Corey\n\n\nCaliban‚Äôs War (The Expanse book 2) by James S.A. Corey\n\n\nLeviathon Wakes (The Expanse book 1) by James S.A. Corey\n\n\nThe Galaxy, and the Ground Within: A Novel (Wayfarers 4) by Becky Chambers\n\n\nRecord of a Spaceborn Few (Wayfarers 3) by Becky Chambers\n\n\nA Closed and Common Orbit (Wayfarers 2) by Becky Chambers\n\n\nThe Long Way to a Small, Angry Planet (Wayfarers 1) by Becky Chambers\n\n\nLast of the Breed by Louis L‚ÄôAmour\n\n\nProject Hail Mary by Andy Weir\n\n\nFirebreak by Nicole Kornher-Stace\n\n\nDune Messiah by Frank Herbert\n\n\nDune by Frank Herbert\n\n\nThe Martian: A Novel by Andy Weir"
  },
  {
    "objectID": "blog/posts/2026-01-09-hex-update-010/index.html",
    "href": "blog/posts/2026-01-09-hex-update-010/index.html",
    "title": "The Hex Update: Issue 010",
    "section": "",
    "text": "Let‚Äôs catch up\nWelcome to Issue 010, folks.\nThree topics caught my attention this week:\n\nIs appointment viewing back for streaming series? What‚Äôs new is old, and streaming platforms are turning to chunk releases to seize cultural momemnts.\nA discussion with Sir Tim Berners-Lee about the future of the internet. I was particularly interested to hear the conversation about how agentic AI could change the interface and revenue models of the web.\nPlatforms are turning to hybrid revenue models for streaming-first audiences. It‚Äôs no longer about having the largest number of platform users, rather it‚Äôs about engaged and addressable audiences that make the business model work.\n\nTo end the week with a little fun, I share an interesting data visualization of global shipping. It‚Äôs amazing to see the scale of this industry within the global economy.\n\n\nThree things\nHere‚Äôs what caught my attention this week:\n\nArticle: Your favourite TV shows are changing how episodes are released. Is appointment viewing back?\nThis article provides observations and expert comments about the effects streaming service release schedules have on audiences. Many streaming services have now identified a happy middle ground between appointment viewing and binge watching with staggered releases. Weekly or chunk releases appeal to audiences for multiple reasons: it gives audiences something to look forward to, it affords audiences the ability to still participate in the cultural moments around releases, and it still allows audiences to stack and binge watch episodes. Indeed, according to some of the earnings calls cited in the article, many streaming services find these types of release schedules are what audiences want. This is counter to what platforms were striving for in the past, where availability and volume of content was the strategy. In practical terms, the strategy focused on creating the largest library of content that could be viewed anytime, anywhere, and appealled to the largest audience possible. Despite the positive effects of staggered releases, the article provides additional expert opinion that this is not the only factor for a shows success. Shows still need to generate excitement, and they must have a compelling story for audiences to want to watch it. If a show meets this criteria, then staggered release schedules will lead to boosted benefits for platforms.\n\nWhy does this matter?\nWhat‚Äôs new is old. What matters here, as long as a show has created some excitement and is compelling in terms of the storytelling, is the opportunity to manage the cultural moment. Content accessibility via a platform‚Äôs library is no longer enough. Rather, it‚Äôs about using the release of content to facilitate and amplify the cultural moment. Large drops of whole seasons of popular series by large platforms may be behind us. Audiences‚Äô expectations are driven by large streaming platforms, smaller streaming services should take note. There‚Äôs opportunities to be seized with the modification of release schedules.\n\n\n\nPodcast: Sir Tim Berners-Lee doesn‚Äôt think AI will destroy the web\nI‚Äôve been attempting to get caught up on all my bookmarked podcasts episodes. One episode in the queue, from Decoder, interviews the creator of the internet, Sir Tim Berners-Lee. The episode explores his views on the current state of the web. Some points were interesting, especially these ones linked below:\n\nThe shift towards closed platforms on the web 01M54S\nCentralized players and their impact on open web ecosystems 03M35S\nViews on the commercialization of the web 11M20S\nWith AI agents, will the web be the same? How does the interface change? Does the revenue model of the web survive? 12M45S\nStructural changes to the web to manage AI extraction of data 33M52S\nViews on the role of centralized services and whether this functionality could be built into the web 37M33S\n\n\nWhy does this matter?\nThe web is changing‚Äìnow more than ever. The initial view of it being an open ecosystem of information is eroding, and many publishers and platforms are moving toward closed environments (e.g., walled gardens, apps, etc.). Moreover, agentic AI has the potential to fundamentally impact the web‚Äôs interface, how users interact with it, and traditional revenue models. In this new version of the web, the economics and business models underpinning the web may no longer feasible. What role does advertising serve when AI agents, on the behalf of a user, capture, summarizes, delivers information about or performs some action on sites users no longer access? Ad impressions, a key currency for publishers, diminishes. The role of centralized services on the web are also an important consideration. Although useful and solves problems, centralized service‚Äôs impact on the open web need to be considered. Standards and protocols could be created that more greatly elevates the impact of the web beyond that of a single, centralized service provider. All players involved would need to agree upon adoption, though. This is an unlikely prospect.\n\n\n\nArticle: Industry Insights: How monetization models are being rebuilt for streaming-first audiences\nI recently bumped into this collection of broadcast, streaming, and advertising professional‚Äôs insights into the future of monetizing streaming-first audiences. The conversation included perspectives about the shift of single revenue model strategies to more blended models, which now include SVOD, AVOD, FAST, and transactional forms of monetization. Some have coined this to be a new type of monetization strategy called HVOD (Hybrid Video on Demand). Going further, the expert opinions focused on the need for broadcasters to further develop there collection and use of first-party data. Some experts opined these changes are intended to improve revenue numbers, rather than it being a startegy solely focused on generating the highest user count. There was also discussion that emphasized the importance of engaged and addressable audiences, as these are the audiences most valuable to advertisers. For instance, from the article:\n\nThe next critical step is monetizing this enhanced first-party data into revenue opportunities, specifically through addressable advertising, content sales, and audience development use cases.\n\nMoreover, some of the views expressed first-party data‚Äôs role in generating behavioral and contextual intelligence. That is:\n\nBroadcasters are adopting similar discipline ‚Äî focusing on recency, intent, household composition, and content affinity rather than broad demographic assumptions.\n\nLastly, there was some commentary about the revenue opportunities afforded to broadcasters monetizing extended digital offerings: content bundles, e-commerce, and adjacent areas like gaming and audio.\n\nWhy does this matter?\nMany revenue generating opportunities exist for streaming-first audiences. These opportunities are not just about generating the largest user number, but rather are business models that make the economics work. It‚Äôs all about giving audiences options, which HVOD does, while also making the revenue strategy feasible for a platform. It‚Äôs important to recognize that if other platforms are providing these options, then audiences will come to expect these from the others they use. The conversation about first-party data is also a point of emphasis. It‚Äôs not simply about a user count anymore. Platforms need to have evidence that audiences are engaged and addressable. That‚Äôs where the value lies.\n\n\n\n\nJust for fun\n\nshipmap\nHave you ever considered the role shipping has within the global economy? I hadn‚Äôt either, until I saw this iteractive data visualization. It‚Äôs a really neat, interesting way to show how massive this industry is to the world. Check it out.\nAnother week‚Äìdone. We‚Äôre on the cusp of the weekend. Yay! I hope it‚Äôs a good one.\nCheers üéâ!\n\n\n\nLet‚Äôs connect\nIf you found this content useful, please share. If you find these topics interesting and want to discuss further, let‚Äôs connect:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke\nSay Hi!\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2026,\n  author = {Berke, Collin K},\n  title = {The {Hex} {Update:} {Issue} 010},\n  date = {2026-01-09},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2026. ‚ÄúThe Hex Update: Issue 010.‚Äù January\n9, 2026."
  },
  {
    "objectID": "blog/posts/2026-01-01-notes-github-actions-rstats/index.html",
    "href": "blog/posts/2026-01-01-notes-github-actions-rstats/index.html",
    "title": "Notes: My first GitHub action workflow using RStats and Quarto",
    "section": "",
    "text": "The goal here was simple. I have a resource list‚Äìsaved as a .csv file‚Äìstored in another GitHub repo. In short, it‚Äôs a list of links to several resources in the media and marketing research domain. This list is hosted on my personal website. It is updated from time to time, as I come across new resources. The aim was to automate this process using GitHub Actions, whereby my site would build on a scheduled time and write any new resources that have been added to the site. An additional goal was to become more proficient working with actions from the command-line. As such, you‚Äôll see several of these examples highlight work from the terminal. However, many of these same steps can take place via the UI."
  },
  {
    "objectID": "blog/posts/2026-01-01-notes-github-actions-rstats/index.html#creation-of-an-github-action-yaml-file",
    "href": "blog/posts/2026-01-01-notes-github-actions-rstats/index.html#creation-of-an-github-action-yaml-file",
    "title": "Notes: My first GitHub action workflow using RStats and Quarto",
    "section": "Creation of an GitHub Action YAML file",
    "text": "Creation of an GitHub Action YAML file\nTo start, a file for the workflow needs to be created. For these notes, I created a file named update-data-media-marketing-resources.yml. This file is stored in the file path mentioned above .github/workflow/.\n\n\n\n\n\n\nNote\n\n\n\nWorkflow files can either end in a .yml or .yaml extension.\n\n\nAlthough there exists great flexibility in how these files are constructed, I‚Äôve found the basic sections include:\n\nA metadata section (i.e., usually just a title of the workflow)\nA specification of the on: attribute (i.e., how workflows are triggered)\nJobs (i.e., collection of steps)\nBuild configuration (i.e., the environment in which the workflow runs)\nSteps (i.e., the actual actions taken)\n\nThese sections utilize attributes for their definitions, which, simply defined, are just key:value pairs. Let‚Äôs start with the front matter of our workflow file, the metadata section.\n\nMetadata section\nThe metadata section contains information about your workflow. There‚Äôs several attributes that can be defined in this section of the workflow (see here), but I mostly use this to document information about the workflow. Some of these attributes are required, others are optional. The ones included in this workflow are:\n\nname: The name of the workflow (required)\ndescription: A description of what the wofkflow does (optional)\n\nThe YAML file for this section of our workflow looks like this:\nname: \"Update data-media-marketing-resoures\"\ndescription: \"Updates resource list at collinberke.com/data-media-marketing-resources.html\"\nFirst, take notice of the use of \"s here. YAML syntax requires strings with spaces to be enclosed within quotations. Second, make your name attribute informative and human-readable. You‚Äôll see how this attirbute will be found in several places, including in the UI and when interfacing with actions via the command-line.\n\n\nThe on: attribute\n\n\nJobs\n\n\nBuilds\n\n\nSteps"
  }
]