[
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A now page, why?"
  },
  {
    "objectID": "now.html#projects-im-working-on",
    "href": "now.html#projects-im-working-on",
    "title": "Now",
    "section": "Projects I’m working on",
    "text": "Projects I’m working on\n\nTransitioning to LazyVim\nAs of my last update, I decided to burn down my Neovim setup and start over. I was in search of something more enjoyable and manageable to work with. I came across LazyVim and started to really enjoy it. Out of the box it had sane defaults and configurations, so the setup was minimal and I’ve found it more enjoyable to work with. LazyVim Extras also make it easy to install and configure different plugins.\n\n\nBlogging more\nI’m attempting to increase the publishing of my blog posts. I was following a cadance of roughly posting every month. I want to bump this up to two a month, with a stretch goal being every week. The hardest parts are being realistic with myself, identifying shorter post topics to write about, and being kinder to myself. Writing posts with a tighter turn-around means embracing progress over perfection.\n\n\nThinking about the use of data and organizational culture\nMore and more often I’m being tasked with stratigizing how to improve the role and impact data has within an organization. These are not easy questions to answer. I’ve been doing a lot of exploring and experimentation. If anyone has any suggestions, let’s chat.\nIf you’re interested in what I’ve focused on in the past, check out my past updates"
  },
  {
    "objectID": "now.html#books-im-reading",
    "href": "now.html#books-im-reading",
    "title": "Now",
    "section": "Books I’m reading",
    "text": "Books I’m reading"
  },
  {
    "objectID": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "href": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "title": "Now",
    "section": "A list of books I’ve read (ever since I’ve started keeping track)",
    "text": "A list of books I’ve read (ever since I’ve started keeping track)\n\nPractical SQL, 2nd Edition: A Beginner’s Guide to Storytelling with Data by Anthony DeBarros\nI’m currently working with another collegue to sharpen and practice our SQL skills. Although I’m up-to-date on most of the basics and am usually pretty good at completing straight-forward queries, it was a good time to focus and become even more proficient with the language. So, we picked up this book to learn more.\n\n\nR for Data Science (2e) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\nCurrently, I’m leading another Data Science Learning Community bookclub meeting for this book. I haven’t read the second edition yet, so I’m excited to see and learn what’s new.\n\n\nWith the Old Breed: At Peleliu and Okinawa by E.B. Sledge\nThis is a personal read I pick up from time to time. I finished watching the HBO miniseries Band of Brothers and The Pacific, which were based on first hand accounts in books like this. With the Old Breed is a first-hand account of one serviceman’s experience on the frontline during operations in the Pacific Theater, specifically in Peleliu and Okinawa, during World War II. It’s unfathomable to comprehend the experiences and hardships many endured. Reading it has made me more deeply appreciate the sacrafices men and women of the armed services make for the United States of America.\n\n\nProfessional development reads\n\nMachine Learning with R - Fourth Edition by Brett Lantz\n\n\nPractical Tableau by Ryan Sleeper\n\n\nHBR Guide to Making Every Meeting Matter from the Harvard Business Review\n\n\nThe Checklist Manifesto: How to Get Things Right by Atul Gawande\n\n\nTidy Modeling with R by Max Kuhn and Julia Silge\n\n\nPython for Data Analysis by Wes McKinney\n\n\nEngineering Production-Grade Shiny Apps by Colin Fay, Sébastien Rochette, Vincent Guyader, and Cervan Girard\n\n\nAdvanced R by Hadley Wickham. Check out past book club meeting recordings here.\n\n\nR Packages by Hadley Wickham and Jenny Bryan. Check out past book club meeting recordings here.\n\n\nVim help files maintained by Carlo Teubner\n\n\nMastering Ubuntu by Jay LaCroix\n\n\nGoogle BigQuery: The Definitive Guide by Valliappa Lakshmanan and Jordan Tigani\n\n\nMastering Shiny by Hadley Wickham. Check out the past book club meeting recordings here.\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. Check out the past book club meeting recordings here.\n\n\nDocker Deep Dive by Nigel Poulton\n\n\n\nPersonal reads\n\nThe Currents of Space (Galactic Empire 2) by Issac Asimov\n\n\nThe Stars, Like Dust (Galactic Empire 1) by Issac Asimov\n\n\nRobots and Empire (The Robot Series 4) by Issac Asimov\n\n\nThe Robots of Dawn (The Robot Series 3) by Isaac Asimov\n\n\nThe Naked Sun (The Robot Series Book 2) by Isaac Asimov\n\n\nThe Caves of Steel (The Robot Series Book 1) by Isaac Asimov\n\n\nI, Robot by Isaac Asimov\n\n\nLeviathan Falls (The Expanse book 9) by James S.A. Corey\n\n\nTiamat’s Wrath (The Expanse book 8) by James S.A. Corey\n\n\nPersepolis Rising (The Expanse book 7) by James S.A. Corey\n\n\nBabylon’s Ashes (The Expanse book 6) by James S.A. Corey\n\n\nNemesis Games (The Expanse book 5) by James S.A. Corey\n\n\nCibola Burn (The Expanse book 4) by James S.A. Corey\n\n\nAbaddon’s Gate (The Expanse book 3) by James S.A. Corey\n\n\nCaliban’s War (The Expanse book 2) by James S.A. Corey\n\n\nLeviathon Wakes (The Expanse book 1) by James S.A. Corey\n\n\nThe Galaxy, and the Ground Within: A Novel (Wayfarers 4) by Becky Chambers\n\n\nRecord of a Spaceborn Few (Wayfarers 3) by Becky Chambers\n\n\nA Closed and Common Orbit (Wayfarers 2) by Becky Chambers\n\n\nThe Long Way to a Small, Angry Planet (Wayfarers 1) by Becky Chambers\n\n\nLast of the Breed by Louis L’Amour\n\n\nProject Hail Mary by Andy Weir\n\n\nFirebreak by Nicole Kornher-Stace\n\n\nDune Messiah by Frank Herbert\n\n\nDune by Frank Herbert\n\n\nThe Martian: A Novel by Andy Weir"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "title": "Combine plots using patchwork",
    "section": "",
    "text": "Image generated using the prompt ‘robot stitching a quilt of data visualizations in pop art style’ with the Bing Image Creator\nToday I learned the patchwork package makes it easy to combine multiple plots into a single plot. In this post, I overview what I’ve recently learned from using patchwork’s functions to create plot compositions."
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "title": "Combine plots using patchwork",
    "section": "Show me the money",
    "text": "Show me the money\nLet’s start with a pretty straight forward plot, total revenue over time. Here’s the code I used to wrangle the data and create the plot:\n\ndata_revenue_trend &lt;- data_google_merch |&gt;\n  group_by(event_date, transaction_id) |&gt;\n  summarise(revenue = max(purchase_revenue_in_usd)) |&gt;\n  summarise(revenue = sum(revenue))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_rev_trend &lt;- ggplot(\n  data_revenue_trend, \n  aes(x = event_date, y = revenue)\n) +\n  geom_line(linewidth = 2) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Total Revenue ($USD)\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "title": "Combine plots using patchwork",
    "section": "Items generating the most revenue",
    "text": "Items generating the most revenue\nThe data is from an online store, so we should explore items generating the most revenue. To keep things simple, I use dplyr’s slice_max(10) to create a plot of the Top 10 revenue generating items. Because we’re slicing the data, other items are excluded from the plot. The following code wrangles the data and creates the plot for us.\n\ndata_items_rev &lt;- \n  data_google_merch |&gt;\n  group_by(item_name) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_rev_items &lt;- ggplot(\n  data_items_rev, \n  aes(\n    x = fct_reorder(item_name, revenue),\n    y = revenue)) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "title": "Combine plots using patchwork",
    "section": "Product categories generating most revenue",
    "text": "Product categories generating most revenue\nThe data also categorizes items into more general groupings. As such, we can create a plot ranking product categories by amount of revenue generated. No question, it’s apparel. The code to create the plot is similar to what we did above with items.\n\ndata_cat_rev &lt;- \n  data_google_merch |&gt;\n  group_by(item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_cat_items &lt;- ggplot(\n  data_cat_rev, \n  aes(\n    x = fct_reorder(item_category, revenue),\n    y = revenue)) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "title": "Combine plots using patchwork",
    "section": "Product categories trend",
    "text": "Product categories trend\nNow that we’ve created a plot ranking product categories, let’s create a plot that breaks out several categories of interest over time. To do this, we’ll use the following code.\n\nimportant_cats &lt;- \n  c(\"Accessories\", \"Bags\", \"Apparel\", \n    \"Drinkware\", \"Lifestyle\")\n\ndata_cat_rev_trend &lt;- \n  data_google_merch |&gt;\n  filter(item_category %in% important_cats) |&gt;\n  group_by(event_date, item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_high_cat_trend &lt;- ggplot(\n  data_cat_rev_trend, \n  aes(x = event_date, y = revenue, color = item_category)) +\n  geom_line(linewidth = 2, alpha = .7) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Revenue ($USD)\", color = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "title": "Combine plots using patchwork",
    "section": "Other operators to know",
    "text": "Other operators to know\npatchwork has other operators to modify the layout of the plot composition. Each operator is listed below with a brief description of its use.\n\n+ and - - combines plots together on the same level.\n| - combines plots beside each other (i.e., packing).\n/ - places plots on top of each other (i.e., stacking).\n* - adds objects like themes and facets to all plots on the current nesting level.\n& - will add objects recursively into nested patches.\n\nThe following examples highlight the use of several of these operators."
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "title": "Calculating correlations with corrr",
    "section": "",
    "text": "Photo by Omar Flores\nToday I learned calculating, visualising, and exploring correlations is easy with the corrr package.\nIn the past, I would rely on Base R’s stats::cor() for exploring correlations. This function is a powerful tool if you’re looking to do additional analysis beyond investigating correlation coefficients. stats::cor() has its pain points, though. Sometimes, I just want a package to explore correlations quickly and easily.\nI recently stumbled across the corrr package. It met all the needs I listed above. The purpose of this post is to highlight what I’ve learned while using this package, and to demonstrate functionality I’ve found useful. To get started, let’s attach some libraries and import some example data.\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(here)"
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "title": "Calculating correlations with corrr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I was finishing this post, the Nebraska Women’s Volleyball team beat #1 Wisconsin in a five set thriller.↩︎\nThe Nebraska Women’s Volleyball team broke the World Record for a women’s sporting event on 2023-08-30. Official attendance was 92,003.↩︎"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html",
    "href": "til/posts/2023-03-14-vim-substitution/index.html",
    "title": "Find and replace in Vim",
    "section": "",
    "text": "Today I learned how to find and replace in Vim. I’ve found knowing a few variations of the substitute (:s or su for short) command to be a powerful skill to quickly and efficiently edit code and text within a file. By knowing a few simple command variations, you can greatly improve your productivity. You just have to know the different patterns and when to apply them.\nThis TIL post aims to highlight some of the basics of using Vim’s :s command. My intention is to get you up and running quickly. As such, this post provides several simple examples applying the command to some practical use cases. Although most of the examples use the R programming language, these concepts can be applied to any programming language or text editing task.\nThis post focuses on the basics. Indeed, the substitute command provides a lot of utility and different options to perform various find and replace editing tasks. If you’re looking to learn more advanced features, I suggest reading the docs (:help substitute). I also provide some additional links to other resources throughout and at the end of the post if you’re interested in learning more."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "title": "Find and replace in Vim",
    "section": "The basics",
    "text": "The basics\n:s can be used to find each occurance of a text string, and replace it with another text string. Say I have a character vector basket, and it contains an assortment of fruit. However, what if I want to replace the first apple in my basket with an orange using :s? First, I need to move my cursor to the line I want to find the first string. Then, I can enter the following into the command prompt to find the first instance of the string orange and replace it with the string apple:\n:s/orange/apple\nHere is what this looks like in action.\n\nHowever, what if I don’t want any oranges, and instead I just want apples rather than orangesin my basket. I can append the previous command with g to replace all instances of orange with apple. The g flag indicates to Vim that I want to replace globally to the current line. In other words, replace all instances on the current line.\n:s/orange/apple/g\nBelow is what this will look like in your editor.\n\nWant to find and replace text globally to the line and including multiple lines, then add % to the beginning of the command.\n:%s/orange/apple/g\n\n% is really useful if you want to refactor code efficiently within a file. Check out these two examples, one more contrived, the other a more practical, common application.\n:%s/power/horsepower/g\n:%s/data/cars_data/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "title": "Find and replace in Vim",
    "section": "Confirming replacement",
    "text": "Confirming replacement\nNot sure what all will be replaced and would rather go through each replacement step-by-step? Add c to the end of your command. Adding this flag will make Vim prompt you to confirm each replacement.\n:%s/orange/apple/gc\n\nIn the prompt, you’ll see something like replace with apple (y/n/a/q/l/^E/^Y). You’ll select the option that fulfills the action you want to perform. Here is a list of what each selection does:\n\ny - substitute this one match and move to the next (if any).\nn - skip this match and move to the next (if any).\na - substitue all (and it’s all matches) remaining matches.\nq - quit out of the prompt.\nl - subsitute this one match and quit. l is synonymous with “last”.\n^E - or Ctrl-e will scroll up.\n^Y - or Ctrl-y will scroll down.\n\nThe example above only highlights the use of y, so I suggest experimenting with each selection to get a feel for what they do."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "title": "Find and replace in Vim",
    "section": "Replacing by range",
    "text": "Replacing by range\nTake a look at the command pattern again, specifically the first portion, [range]:\n:[range]s[ubstitute]/{pattern}/{string}/[flags] count\nThe s command provides functionality to scope the find and replace operation to a specific part of your file. Indeed, this functionality was highlighted earlier when we passed % in an earlier command. % just indicated to Vim that we wanted to find and replace all lines in the file. However, we can be more specific.\nSay we now have a much larger basket, one that can hold both fruits and veggies. In the R programming language, this can be modeled using a tribble from the tribble package.\nWhat if we wanted to find the first two instances of carrots in our basket and replace it with kale. This can be done by passing a range at the start of the :s command. In this specific instance, I want to find and replace the carrots on lines 5 and 7 with the string kale, but I don’t want to change the one on line 8. To do this, I can run the following command:\n:5,7s/carrot/kale/g\n\nAnother variation is to start on your current line . and specify to Vim how many additional lines I would like to find and replace in the range. Keep in mind . represents the current line your cursor is located currently within the file. Once you postion your cursor, your command will look something like this:"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "title": "Find and replace in Vim",
    "section": "Replacing from current location to n lines",
    "text": "Replacing from current location to n lines\n:.,+2s/carrot/kale/g\n\nWhat if I had a basket with even more fruits and veggies, and I just wanted to start at my current location and replace all instances that follow? We can use the $ in the range input. The use of the dollar sign indicates to Vim that we want to replace starting at line 8 and go to the end of the file."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "title": "Find and replace in Vim",
    "section": "Replacing to end of file",
    "text": "Replacing to end of file\n:8,$s/carrot/kale/g\n\nOr, if you want to start from the current line and replace to the end of the file, you can do the following:\n:.,$s/carrot/kale/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "title": "Find and replace in Vim",
    "section": "Replacing using visual mode",
    "text": "Replacing using visual mode\nWe can also use visual mode to set the range of the find and replace operation. Just enter visual mode v or visual line mode Shift-v, highlight the range you want your find and replace operation to be applied, enter into command mode with :, and then enter your find and replace statement. Doing this will start the command line off with '&lt;,'&gt;, and you’ll just need to enter the rest of the command, the {pattern} and {string} portions.\n:'&lt;,'&gt;s/carrot/kale`"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "title": "Find and replace in Vim",
    "section": "Use objects in your search buffer",
    "text": "Use objects in your search buffer\nYour previous search history can also be used to do find and replace. Let’s go back to our miles-per-gallon plot example again. First I’ll hover my cursor over the word I want to replace and hit *. Now we can use the search value in our subsititution command. All I need to do is leave the {pattern} blank in the command. The command will look like this:\n:%s//horsepower"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "title": "Find and replace in Vim",
    "section": "Replace with what’s under your cursor",
    "text": "Replace with what’s under your cursor\nTo keep things simple, let’s go back to our first basket example. Specifically, let’s say I want to modify the string strawberry with the string banana, but use my cursor position to do this. First I have to make sure the cursor is hovering over the word I want to use for my replacement. Then, I enter the below command. When you see &lt;c-r&gt;&lt;c-w&gt;, this means you actually hit Ctrl-R and Ctrl-W on your keyboard. You’ll notice the string banana is populated into the command for us.\n%s/strawberry/&lt;c-r&gt;&lt;c-w&gt;"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Collin K. Berke, Ph.D. is a media research analyst in public media. Professionally, he uses data and media/marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. He is especially interested in the use and development of open-source statistical software (i.e. Rstats) to achieve this goal, and developing a broader understanding the role these tools can play in media, digital, and marketing analytics.\nHe has experience using different software, third-party services, and programming languages from developing several analytics projects, both in industry and academia. In regards to programming languages, he has developed projects using R, SQL, $bash, and a little bit of Python. He also has extensive experience using different analytics solutions. For data warehousing, he mostly uses database tools like Postgres and those in the Google Cloud Platform ecosystem (e.g., Google BigQuery). When it comes to automating workflows and data pipelines, he has experience implementing and working with Apache Airflow. He also has extensive experience using third-party tools and software to analyze, wrangle data, and communicate his analyses (e.g., Google Analytics, Google Sheets, Excel, Google Data Studio, and R Shiny). Most of his current work is industry related.\nCollin also serves as an adjunct instructor for the University of Nebraska-Lincoln and Southeast Community College, where he teaches courses in sports data visualization and analysis and communication specific courses. He holds a M.A. in Communication Studies from the The University of South Dakota and a Ph.D. in Media and Communication from Texas Tech University. He has also published and contributed to the publication of several academic journal articles.\nCollin is a self-proclaimed news, sports, and podcast junkie. He really enjoys listening to NPR, watching PBS (especially NOVA), and indulging in college football and baseball. At times, he will write blog posts on topics he finds interesting in these areas.\nCheck out the Now page to see what Collin is currently reading and working on."
  },
  {
    "objectID": "about.html#lightning-talk",
    "href": "about.html#lightning-talk",
    "title": "About",
    "section": "Lightning talk",
    "text": "Lightning talk\nGet to know me in five minutes or less:\n\nView slides fullscreen"
  },
  {
    "objectID": "about.html#note-about-this-site",
    "href": "about.html#note-about-this-site",
    "title": "About",
    "section": "Note about this site",
    "text": "Note about this site\nThe views expressed on this site are my own, and they do not reflect the views of my employer, professional and/or community groups I hold membership. Any analyses hosted on this site were done for professional development or were for fun. I make every attempt to perform valid and accurate data analysis and reporting. Unless otherwise noted, none of the content on this site has been peer-reviewed, and thus any conclusions drawn or uses stemming from this work need to take these limitations into account.\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "title": "Shiny summary tiles",
    "section": "",
    "text": "Photo by Stephen Dawson\nEffective reporting tools include user interface (UI) elements to quickly and effectively communicate summary metrics. Shiny, a free software package written in the R statistical computing language, provides several tools to communicate analysis and insights. Combining several of these elements together, a developer can create user interface elements that clearly communicate important summary metrics (e.g., Key Performance Indicators) to an application’s users.\nThis post details the steps to create the following simple Shiny application. Specifically, this post overviews the use of Shiny’s built-in functions to create simple summary metric tiles. In addition, this post describes how to add styling to UI elements by applying custom css to a Shiny application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "title": "Shiny summary tiles",
    "section": "The reactive graph",
    "text": "The reactive graph\nAlthough this app is simple and most of the elements can be easily managed, it’s always good practice to see the big picture of the app by plotting out a reactive graph first. It’s also good to have the intended reactive graph available as a quick reference, just in case unexpected results and/or behaviors are displayed while developing the application, and as a method for identifying any situations where computing resources are not being used efficiently.\nBelow is the reactive graph for the application to be developed:\n\n\n\n\n\n\n\nReactive graph for summary metric tiles\n\n\n\n\nAgain, a really simple application–one input (date), a reactive expression (data()), and five outputs (users; page_view; session_start; purchase; and event_date). The graph also details the dependencies clearly, where the outputs are dependent on the reactive data() object–which in cohort with the outputs–depends on the date input."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "title": "Shiny summary tiles",
    "section": "The setup",
    "text": "The setup\nThe first step is to import the R packages used within the application. The following code chunk contains the packages used for the application. A brief description of each is included.\n\nlibrary(shiny) # The Shiny app library\nlibrary(readr) # Import data\nlibrary(dplyr) # Pipe and data manipulation\nlibrary(tidyr) # Tidying data function\nlibrary(purrr) # Used for functional programming\nlibrary(glue)  # Used for string interpolation\n\nMany of these packages are part of the tidyverse, and thus the import could be simplified to just running library(tidyverse). Be aware this may bring in unused, unneeded libraries. There is nothing wrong with this approach. However, I opted to be more verbose with this example, so as to be clear about what libraries are utilized within the example application and to have more control on what packages were imported by the application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "title": "Shiny summary tiles",
    "section": "Application layout",
    "text": "Application layout\nThe next step is to code the layout of the UI. To keep the design simple, a sidebar will contain the application’s inputs, while the outputs will be placed within the main panel of the application. The general skeleton of the layout looks like this:\n\nui &lt;- fluidPage(\n   # Inputs\n   sidebarLayout(\n      sidebarPanel()\n   ),\n   # Outputs\n   mainPanel(\n      # Summary tiles\n      fluidRow(),\n      br(),\n      # Data information output\n      fluidRow()\n   )\n)\n\nThere’s nothing too fancy about this code, outside of it establishing the general layout of the application, so not much else will be said about what each element does here. However, Chapter 6 of Mastering Shiny discusses application layout if a more detailed description is needed."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "title": "Shiny summary tiles",
    "section": "The date input",
    "text": "The date input\nThe app requirements state users need to have the ability to modify the dates to which the data represents, and the summary metric tiles will change based on this user input. However, the app will not have any user input upon startup, so it needs to default to the most recent date within the data. To meet these requirements, we use the following code:\n\n# Code excluded for brevity\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    )\n\nThe shiny:dateInput() function is used to create the HTML for the input, which resides in the application’s sidebar. The function’s id argument is given the value of date, which will establish a connection to elements within the server. More on this later. Then, a string value of Select a date for summary: is passed along to the label argument. This value will be displayed above the date input in the UI.\nSince the app won’t have an initial user input upon the startup of the application, max(ga4_date$event_date) is passed along to the value argument. This will default the input to the most recent date within the data. In addition, the functions max and min arguments are passed similar calls. However, in the case of the min argument the base R min() function is used on the ga4_data$event_date."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "title": "Shiny summary tiles",
    "section": "First iteration of the summary metric tiles",
    "text": "First iteration of the summary metric tiles\n\nThe server side\n\nThe reactive data() object\nBefore the summary metrics can be displayed in the UI, the application needs data to create the outputs. In addition, since this data will be dependent on users’ input (i.e., the user can select a new date which subsequently changes the summary metric tile), this object needs to be reactive. To do this, the following code is added to the server side of the application.\n\nserver &lt;- function(input, output, session) {\n   data &lt;- reactive({\n      ga4_data %&gt;% filter(.data[[\"event_data\"]] == input$date)\n   })\n}\n\nIn practical terms, this code just filters the data for the date being passed along as the input$date object.\nAgain, this object could be the most recent date within the data, the date set by the max argument in the dateInput() function, or it could be based on a user’s modification of the date input. Since this code was wrapped inside of the reactive({}) function, Shiny will be listening for any changes made to the to the input$date object. Any changes that occur will result in the data() reactive expression to be modified, followed by new output values being displayed via the UI.\nOne other key concept is being exhibited here, tidy evaluation, specifically data-masking. Since technically dplyr::filter() is being used inside of a function, an explicit reference to the data is required. Thus, .data[[\"event_data\"]] notation is used to make it explicit on what data will be filtered. The specifics on how to use data-masking in the context of a Shiny app is beyond the scope of this post. However, the previously linked materials provide a more detailed description of these concepts.\n\n\nThe outputs\nLooking back at the reactive graph, the application requires five outputs to be in the server. These outputs will just be simple text outputs, so the use of the shiny::renderText() function will be sufficient to meet our requirements. The format() function is also applied to comma format any outputs that contain numbers (e.g., 2,576 vs 2576). Here is what the server looks like currently:\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_date, .data[[\"event_date\"]] == input$date)\n  })\n  \n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nAs part of the functionality requirements, the app needed some UI element informing users what date is being represented in the summary tiles. The output$date object was included to meet this requirement. The output$date object, aside from using the renderText() function, includes the use of the glue::glue() function to make the outputted message more informative.\nThe {glue} package is used to manipulate string literals with the use of the curly braces (e.g., {}). When applied here, the {data()$event_date} is evaluated as an R call, its value becomes appended to the string, and the whole string is then outputted to the application’s UI.\n\n\n\nBack to the UI\nNow that there are five elements being outputted from the server, UI elements need to be included to display the rendered outputs.\nWhen making early design decisions about the application’s layout, it was decided these elements were going to reside within the main panel of the application. Another decision made was to keep the summary metric tile elements on the same row, so as to seem as though they are related to one another (i.e., related KPIs). As for the UI element informing the user on the date the summary metric tiles represent, it was decided that this element would be placed on its own row.\nTo achieve the intended design, additional Shiny layout functions were applied to the application’s code. This includes using the fluidRow() and column() functions to achieve the wanted UI organization. The following code was used to achieve the placement of the summary tiles within the application’s layout:\n\nmainPanel(\n   fluidRow(\n      column(),\n      column(),\n      column()\n   ),\n   br(),\n   fluidRow()\n)\n\nAs for the design of the summary metric tiles, each tile needed to include some type of title followed by the text representing the metric. To achieve this, the shiny::div() function was used. This function creates an individual HTML tag that outputs the text being passed along into the function. Directly below the title element, the textOutput() function is used to display the outputs coming from the application’s server. The code for one summary metric tile would look like the following:\n\ncolumn(3,\n       div(\"Unique Users\"),\n       textOutput(\"users\")\n       )\n\nBy combining these elements, the application code in its current state can be seen here:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n  )\n  \n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        column(3,\n               div(\"Users\"),\n               textOutput(\"users\")\n        ),\n        column(3,\n               div(\"Page Views\"),\n               textOutput(\"page_view\")\n        ),\n        column(3,\n               div(\"Session Starts\"),\n               textOutput(\"session_start\")\n        ),\n        column(3,\n               div(\"Purchases\"),\n               textOutput(\"purchase\")\n        )    \n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  # Text output\n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nIndeed, this code works and meets the functionality requirements. However, it’s quite verbose and contains a lot of redundant, repeated code. Different techniques could be applied to make the application more eloquent and efficient in its design. The goal of the next few sections, then, will be to simplify the application through the development of functions and applying functional programming principles.\n\nSimplifying the outputs\nReviewing the server, most of the outputs are created through the use of repeated patterns of the same code. This breaks the DRY principle (Don’t Repeat Yourself) of software development. Both functions and the application of functional programming principles will be applied to address this issue.\nAn obvious pattern used to create the outputs is output$foo &lt;- renderText(format(bar, big.mark = ',')). This pattern could be converted into a function, and then this function could be used to iterate over the several reactive objects (e.g., data()$users) with the use of a {purrr} function. Since the side-effects are intended to be used rather than outputting a list object from our iteration, purrr::walk() will do the trick.\nUtilizing this strategy simplifies our code to the following:\n\nc('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n\nIndeed, I can’t take full credit for this solution. Thanks goes to @Kent Johnson in the R4DS Slack channel for helping me out.\nThe output$date object was left out of this simplification of the code. Certainly, the function could be made to be more general and flexible to handle this repetition of the renderText() function. However, this would be over engineering a solution to the problem."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "title": "Shiny summary tiles",
    "section": "Back to the UI",
    "text": "Back to the UI\nFunctions and functional programming principles will now be used to address these same issues on the UI side of the application. Much of the repetition occurs with the use of the following pattern:\n\ncolumn(3,\n       div(\"Metric Title\"),\n       textOutput(\"metric_output\")\n       )\n\nIndeed, this pattern is applied four times. Since it was copied and pasted more than twice and breaks the DRY principle, it would be best to convert it into a function and iterate it using functional programming tools."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "title": "Shiny summary tiles",
    "section": "Simplifying the UI with functional programming",
    "text": "Simplifying the UI with functional programming\nA helper function, make_summary_tile(), is added to the setup section of the application. The function looks like this:\n\nmake_summary_tile &lt;- function(title, text_output){\n  column(2,\n         div(title),\n         textOutput(text_output)\n  )\n}\n\nThere’s nothing too fancy or complicated about this function. It simply generalizes the pattern applied within the UI side of the first iteration of our application. As for placement, this function could be defined at the top of the application file or in a separate .R file embedded in a R/ sub-folder. Both strategies would make the function available for the app. Deciding which to use comes down to the intended organizational structure of the application.\nThe next step is to apply functional programming to iterate the make_summary_tile() function over the text outputs. Since the function requires two inputs, title and text_output, they were placed inside of a tibble to improve organization of the inputs being passed to the function through pmap().\n\n# Defined in the Setup section\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Used within the UI\npmap(tiles, make_summary_tile)\n\nWhat once required sixteen line’s of code was cut in half to eight (including the explicit definition of the inputs). In addition, coding the tiles using functional programming also makes it more flexible, where summary tiles could be easily added or taken away.\nDoing this would require some slight modification to the make_summary_tile() helper function, though. That is, a width argument would need to be added to the function, so the column width could be set to accommodate the number of outputs for the UI. There are lots of different options that could be explored here. At this point, though, the solution meets the functionality requirements.\nIn its current state, the application code looks like this:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\nmake_summary_tile &lt;- function(header, text_output){\n  column(2,\n         div(header),\n         textOutput(text_output)\n  )\n}\n\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n)\n\n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        pmap(tiles, make_summary_tile)\n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  c('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n  \n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nThe application works, meets the functionality requirements, and now is written in a way that reduces repetition and redundant patterns within the code. However, the summary metric tiles just blend into the UI, and nothing about the styling communicates they contain important information.\nSince these elements are meant to highlight key, important summary metrics, they need to be styled in a way that creates contrast between themselves and the application’s background. The next section focuses on applying custom CSS to give some contrast between these elements and the application’s background."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "title": "Shiny summary tiles",
    "section": "Creating the www folder and CSS file",
    "text": "Creating the www folder and CSS file\nSince the design opted for a file-based CSS approach, a separate www sub-folder in the application’s main project directory needs to be created. Once created, the custom CSS file will be placed inside this folder. The placement of this file can be seen in this Github repo.\nThe purpose of this folder is to make the file available to the web browser when the application starts. Placement of this file is critical. If it is not placed in the www sub-folder, then the CSS file will not be available when the application starts, and any custom styling will not be applied.\nOnce the www sub-folder is created, you can create a CSS file for the application in Rstudio by clicking File, hovering over New File, and selecting CSS File. Save the file in the www sub-folder and give it an informative name. In the case of this example, the file is named app-styling.css.\nThe main goal of the styling will be to create some contrast between the summary metric tiles and the application’s background. Specifically, CSS will be used to create a container that is a different color from the application’s background and includes some shading to make it seem like the element is hovering above the application’s main page. To do this, the app-styling.css file includes the following:\n#summary-tile{\n  font-size: 25px;\n  color:White;\n  text-align: center;\n  margin: 10px;\n  padding: 5px;\n  background-color: #0A145A;\n  border-radius: 15px;\n  box-shadow: 0 5px 20px 0 rgba(0,0,0, .25);\n  transition: transform 300ms;\n}\nA detailed description on how to create CSS selectors is outside the scope of this post. However, in general terms, this selector sets several values for multiple CSS properties by defining the id, #summary-tile within the file. More about this process of creating different CSS selectors can be found here.\nNow it’s just a matter of modifying the code to call this file and pass these style values to the summary tiles within the application. The following code is added to the ui side of our application to include our app-styling.css file:\n\ntags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"app-styling.css\"))\n\nSince the styling is being applied to the summary metric tiles, the make_summary_tile() function is modified to bring in the CSS elements. A css_id argument is added to the function.\n\nmake_summary_tile &lt;- function(header, text_output, css_id){\n  column(2,\n         div(header),\n         textOutput(text_output),\n         id = css_id\n  )\n}\n\nNow that we made this modification to the make_summary_tile(), its application in the UI is also modified. Specifically, the #summary-tile CSS element is explicitly called in pmap(). To do this, the code is modified like this:\n\npmap(tiles, ~make_summary_tile(\n          header = ..1, text_output = ..2, css_id = \"summary-tile\"))\n\nThe header, text_output, and css_id arguments are now explicitly defined in the pmap() call. To refer to the first two elements in the tiles data object, the ..1 (i.e., header column) and the ..2 (i.e., text_output column) are used. Check out the pmap() docs on how to apply the ..1, ..2 (?pmap) for more information."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "",
    "text": "Image generated using the prompt ‘robot browsing an e-commerce store on laptop in pixel art, warm colors’ with the Bing Image Creator"
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#address-missing-transaction-ids",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#address-missing-transaction-ids",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Address missing transaction ids",
    "text": "Address missing transaction ids\nAfter reviewing the dataset’s structure, my first question is how many examples are missing a transaction_id? transaction_ids are critical here, as they are used to group items into transactions. To answer this question, I used the following code:\n\ndata_ga_transactions |&gt;\n  mutate(\n    has_id = case_when(\n      is.na(transaction_id) | transaction_id == \"(not set)\" ~ FALSE,\n      TRUE ~ TRUE \n    )\n  ) |&gt;\n  count(has_id) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  has_id     n  prop\n  &lt;lgl&gt;  &lt;int&gt; &lt;dbl&gt;\n1 FALSE   1498 0.114\n2 TRUE   11615 0.886\n\n\nOut of the 13,113 events, 1,498 (or 11.4%) are missing a transaction_id. Missing transaction_id’s can take two forms. First, a missing value can be an NA value. Second, missing values occur when examples contain the (not set) character string.\nI address missing values by dropping them. Indeed, other approaches are available to handle missing values. Given the data and context you’re working within, you may decide dropping over 11% of examples is not appropriate. As such, the use of nearest neighbor or imputation methods might be explored.\nAlthough the temporal aspects of the data are not relevant for this analysis, for consistency, I’m also going to parse the event_date column into type date. lubridate’s ymd() function can be used for this task.\nHere’s the code to perform the wrangling steps described above:\n\ndata_ga_transactions &lt;- \n  data_ga_transactions |&gt;\n  drop_na(transaction_id) |&gt;\n  filter(transaction_id != \"(not set)\") |&gt;\n  mutate(event_date = ymd(event_date)) |&gt;\n  arrange(event_date, transaction_id)\n\nWe can verify these steps have been applied by once again using dplyr’s glimpse() function. Base R’s summary() function is also useful to verify the data is as expected.\n\nglimpse(data_ga_transactions)\n\nRows: 11,615\nColumns: 3\n$ event_date     &lt;date&gt; 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 2020-11-11, 20…\n$ transaction_id &lt;chr&gt; \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"133395\", \"1342…\n$ item_name      &lt;chr&gt; \"Google Recycled Writing Set\", \"Google Emoji Sticker Pack\", \"Android Iconic…\n\n\n\nsummary(data_ga_transactions)\n\n   event_date         transaction_id      item_name        \n Min.   :2020-11-11   Length:11615       Length:11615      \n 1st Qu.:2020-11-24   Class :character   Class :character  \n Median :2020-12-04   Mode  :character   Mode  :character  \n Mean   :2020-12-04                                        \n 3rd Qu.:2020-12-13                                        \n Max.   :2020-12-31                                        \n\n\nLooks good from a structural standpoint. However, I did notice that our wrangling procedure resulted in events occurring before 2020-11-11 to be removed. This might indicate some data issues prior to this date, which might be worth further exploration. Given that I don’t have the ability to speak with the developers of the Google Merchandise Store to explore a potential measurement issue, I’m just going to move forward with the analysis. Indeed, our initial goal was to identify association rules during the holiday shopping season, so our data is still within the date range intended for our analysis."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#brief-overview-of-association-rules",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#brief-overview-of-association-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Brief overview of association rules",
    "text": "Brief overview of association rules\nAlthough others have gone more in-depth on this topic (Lantz 2023), it’s worth taking a moment to discuss what an association rule is before we use a model to create them. Let’s say we have the following five transactions:\n{t-shirt, sweater, hat}\n{t-shirt, hat}\n{socks, beanie}\n{t-shirt, sweater, hat, socks, beanie, pen}\n{socks, pen}\nEach line represents an individual transaction. What we aim to do is use an algorithm to identify rules that give us a sense if someone buys one item, what other items will they also likely buy. For example, does buying a t-shirt lead someone to also buy a hat? If so, given our data, can we quantify how confident we are in this rule? Not everyone buying a t-shirt will buy a hat.\nWhen we create rules, they’ll be made up of a left-hand (i.e., an antecedent) and right-hand side (i.e., a consequent). They’ll look something like this:\n{t-shirt} =&gt; {hat}\n\n# or\n\n{t-shirt, hat} =&gt; {sweater}\nRule interpretation is pretty straightforward. In simple terms, the first rule states that when a customer purchases a t-shirt, they’ll also likely purchase a hat. For the second, if a customer purchases a t-shirt and a hat, then they’ll also likely purchase a sweater. It’s important to recognize that the left-hand side can be one or many items. Now that we understand the general makeup of a rule, let’s explore some metrics to quantify each."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#market-basket-analysis-metrics",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#market-basket-analysis-metrics",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Market basket analysis metrics",
    "text": "Market basket analysis metrics\nBefore overviewing the model specification steps, we need to understand the key metrics calculated with a market basket analysis. Specifically, we need a little background on the following:\n\nSupport\nConfidence\nLift\n\nIn this section, I’ll spend a little time defining and describing each. However, others do a more through treatment of each metric (see Lantz 2023, chap. 8; Kadlaskar 2021; Li 2017), and I suggest checking out each for more detailed information.\n\nSupport\nSupport is calculated using the following formula:\n\\[\nsupport(x) = \\frac{count(x)}{N}\n\\]\ncount(x) is the number of transactions containing a specific set of items. N is the number of transactions within our data.\nSimply put, support is interpreted as how frequently an item occurs within the data.\n\n\nConfidence\nAlongside support, confidence is another metric provided by the analysis. It’s built upon support and calculated using the following formula:\n\\[\nconfidence(X\\rightarrow Y) = \\frac{support(X, Y)}{support(X)}\n\\]\nConfidence is a proportion of transactions containing item X (or itemset) results in the presence of item Y (or itemset).\nBoth confidence and support are important; both are parameters we’ll set when specifying our model. These two parameters are the dials we adjust to narrow or expand our rule set generated from the model.\n\n\nLift\nLift’s importance will become more evident once we specify our model and look at some rule sets. But let’s discuss its definition. It’s calculated using the following formula:\n\\[\nlift(X \\rightarrow Y) = \\frac{confidence(X \\rightarrow Y)}{support(Y)}\n\\]\nPut into simple terms, lift gives us a number of how likely one item or itemset is to be purchased to its typical rate of purchase. In even simpler terms, the higher the lift, the stronger evidence that there is a true connection between items or item sets."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#use-the-apriori-function",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#use-the-apriori-function",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Use the apriori() function",
    "text": "Use the apriori() function\nNow let’s do some modeling. Here we’ll use arules’ apriori() function to specify our model. This step requires a little trial and error, as there’s no exact method for picking support and confidence values. As such, let’s just start with apriori’s defaults for the support, confidence, and minlen parameters. The code looks like the following:\n\n# Start with the default support, confidence, minlen\n#   support: 0.1\n#   confidence: 0.8\n#   minlen: 2\nmdl_ga_rules &lt;- apriori(\n  ga_transactions, \n  parameter = list(support = 0.1, confidence = 0.8, minlen = 2)\n)\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext\n        0.8    0.1    1 none FALSE            TRUE       5     0.1      2     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 356 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[371 item(s), 3562 transaction(s)] done [0.00s].\nsorting and recoding items ... [0 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 done [0.00s].\nwriting ... [0 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nmdl_ga_rules\n\nset of 0 rules \n\n\n0 rules were created. This was due to the default support and confidence parameters being too restrictive. We will need to modify these values so the algorithm is able to identify rules from the data. However, we also need to be aware that loosening our rules could increase the size of our rule set, and this might result in a rule set unreasonably large to explore. This is where the trial and error comes into play.\nSo, then, are there any means for determining reasonable starting points? Yes, we just need to consider the business case. Let’s start with support, a parameter measuring how frequently an item or item set occurs within the transactions. A good starting point is to think about how many times a typical item might appear within a transaction throughout the measured period (Lantz 2023).\nSince this is an online merchandise store, my expectation for item and item set purchase is quite low. Thus, I’d expect a typical item to be purchased at least once a week, that is, a total of 8 times during the period. A reasonable starting point for support, then, would be:\n\n# Determining a reasonable support parameter\n8 / 3562\n\n[1] 0.002245929\n\n\nWhen it comes to confidence, it is more about picking a starting point and adjusting from there. For our specific case, I’ll start at .1.\nminlen represents the minimum length the rule (including both the right- and left-hand sides) needs to be before it’s considered for inclusion by the algorithm. It is the last parameter to be set. Our exploratory analysis identified the majority of items in a transaction were quite low, so I believe setting minlen = 2 is sufficient given our data.\nHere’s the updated code for our model with the adjusted parameters:\n\n# Use parameters we think are reasonable \n#   support: 0.002\n#   confidence: 0.1\n#   minlen: 2\nmdl_ga_rules &lt;- apriori(\n  ga_transactions, \n  parameter = list(support = 0.002, confidence = 0.1, minlen = 2)\n)\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext\n        0.1    0.1    1 none FALSE            TRUE       5   0.002      2     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 7 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[371 item(s), 3562 transaction(s)] done [0.00s].\nsorting and recoding items ... [298 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [277 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nmdl_ga_rules\n\nset of 277 rules \n\n\nWith more reasonable parameters in place, the model identified 277 rules. Is 277 rules too much, too little? You’ll have to decide. For this specific case, 277 rules seems reasonable."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#interpret-rules",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#interpret-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Interpret rules",
    "text": "Interpret rules\nLet’s use our first rule as an example, where we’ll seek to interpret it.\n\nfirst_rule &lt;- mdl_ga_rules |&gt;\n  head(1, by = \"lift\") |&gt;\n  as(\"data.frame\") |&gt;\n  tibble()\n\nfirst_rule$rules\n\n[1] \"{Google NYC Campus Mug,Google Seattle Campus Mug} =&gt; {Google Kirkland Campus Mug}\"\n\n\nWritten out, the rule means: If someone buys the Google NYC Campus Mug and the Google Seattle Campus Mug, then they’ll also likely purchase the Google Kirkland Campus Mug. Support is 0.00225, which indicates this rule is included in roughly .2% of transactions. And when these two mugs are purchased together, this rule, where the third mug is purchased, covers around 62% of these transactions. Moreover, the lift metric indicates the presence of a strong rule, where people who buy the first two mugs are more than 169 times more likely to purchase the third mug.\nWhy might this be? Perhaps it’s customers who, by purchasing the first two mugs, are primed to just go ahead and buy the third to finish the set. This might be a great cross-selling opportunity. Maybe we present this rule to our developers and suggest a store feature that encourages customers–when they buy a certain set of items–to complete sets of items within their purchases. Maybe the marketing team could think up some type of pricing scheme along with this feature to further encourage the additional purchase to complete the set.\nDespite this rule, we also need to consider it alongside the count metric. If you recall, count represents the number of transactions this rule’s item set is included. 8 transactions might be too low, and the lift metric may be biased here as a result. We may want to include additional transaction data to further explore this rule or simply be aware this limitation exists when making decisions."
  },
  {
    "objectID": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#subset-rules",
    "href": "blog/posts/2024-06-11-machine-learning-market-basket-analysis-google-analytics/index.html#subset-rules",
    "title": "Messing with models: Market basket analysis of online merchandise store data",
    "section": "Subset rules",
    "text": "Subset rules\nIf you recall, earlier in our analysis the Google Camp Mug was identified as being a frequently purchased item. Say our marketing team wants to develop a campaign around this one product and would like to review all the rules associated with it. arules’ subset() generic function in conjunction with some infix operators (e.g., %in%) and inspect() is useful to complete this task. Here is what the code looks like to return these rules:\n\ninspect(subset(mdl_ga_rules, items %in% \"Google Camp Mug Ivory\"))\n\n    lhs                              rhs                          support     confidence coverage  \n[1] {Google Flat Front Bag Grey}  =&gt; {Google Camp Mug Ivory}      0.005053341 0.2571429  0.01965188\n[2] {Google Camp Mug Ivory}       =&gt; {Google Flat Front Bag Grey} 0.005053341 0.1016949  0.04969118\n[3] {Google Unisex Eco Tee Black} =&gt; {Google Camp Mug Ivory}      0.002245929 0.1311475  0.01712521\n[4] {Google Large Tote White}     =&gt; {Google Camp Mug Ivory}      0.002807412 0.1851852  0.01516002\n[5] {Google Magnet}               =&gt; {Google Camp Mug Ivory}      0.002807412 0.1562500  0.01796743\n[6] {Google Camp Mug Gray}        =&gt; {Google Camp Mug Ivory}      0.005053341 0.1836735  0.02751263\n[7] {Google Camp Mug Ivory}       =&gt; {Google Camp Mug Gray}       0.005053341 0.1016949  0.04969118\n    lift     count\n[1] 5.174818 18   \n[2] 5.174818 18   \n[3] 2.639252  8   \n[4] 3.726721 10   \n[5] 3.144421 10   \n[6] 3.696299 18   \n[7] 3.696299 18   \n\n\nSay for example the marketing team finds these rules are not enough to build a campaign around, so they request all rules associated with any mug. The %pin% infix operator can be used for partial matching.\n\ninspect(subset(mdl_ga_rules, items %pin% \"Mug\"))\n\nWarning in seq.default(length = NCOL(quality)): partial argument match of 'length' to 'length.out'\n\n\n     lhs                                   rhs                                    support confidence    coverage       lift count\n[1]  {Google Austin Campus Tote}        =&gt; {Google Austin Campus Mug}         0.002245929  0.6153846 0.003649635  36.533333     8\n[2]  {Google Austin Campus Mug}         =&gt; {Google Austin Campus Tote}        0.002245929  0.1333333 0.016844469  36.533333     8\n[3]  {Google Kirkland Campus Mug}       =&gt; {Google Seattle Campus Mug}        0.003368894  0.9230769 0.003649635  73.066667    12\n[4]  {Google Seattle Campus Mug}        =&gt; {Google Kirkland Campus Mug}       0.003368894  0.2666667 0.012633352  73.066667    12\n[5]  {Google Kirkland Campus Mug}       =&gt; {Google NYC Campus Mug}            0.002245929  0.6153846 0.003649635  27.061728     8\n[6]  {Google Boulder Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002245929  0.3478261 0.006457047  38.717391     8\n[7]  {Google Cambridge Campus Mug}      =&gt; {Google Boulder Campus Mug}        0.002245929  0.2500000 0.008983717  38.717391     8\n[8]  {Google Boulder Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002526670  0.3913043 0.006457047  17.207729     9\n[9]  {Google NYC Campus Mug}            =&gt; {Google Boulder Campus Mug}        0.002526670  0.1111111 0.022740034  17.207729     9\n[10] {Google NYC Campus Bottle}         =&gt; {Google NYC Campus Mug}            0.002245929  0.2962963 0.007580011  13.029721     8\n[11] {YouTube Play Mug}                 =&gt; {YouTube Leather Strap Hat Black}  0.002245929  0.1568627 0.014317799  10.347131     8\n[12] {YouTube Leather Strap Hat Black}  =&gt; {YouTube Play Mug}                 0.002245929  0.1481481 0.015160022  10.347131     8\n[13] {YouTube Play Mug}                 =&gt; {YouTube Twill Sandwich Cap Black} 0.003368894  0.2352941 0.014317799  12.325260    12\n[14] {YouTube Twill Sandwich Cap Black} =&gt; {YouTube Play Mug}                 0.003368894  0.1764706 0.019090399  12.325260    12\n[15] {Google Flat Front Bag Grey}       =&gt; {Google Camp Mug Ivory}            0.005053341  0.2571429 0.019651881   5.174818    18\n[16] {Google Camp Mug Ivory}            =&gt; {Google Flat Front Bag Grey}       0.005053341  0.1016949 0.049691185   5.174818    18\n[17] {Google Unisex Eco Tee Black}      =&gt; {Google Camp Mug Ivory}            0.002245929  0.1311475 0.017125211   2.639252     8\n[18] {Google Chicago Campus Mug}        =&gt; {Google LA Campus Mug}             0.002245929  0.1702128 0.013194834  14.099951     8\n[19] {Google LA Campus Mug}             =&gt; {Google Chicago Campus Mug}        0.002245929  0.1860465 0.012071870  14.099951     8\n[20] {Google Chicago Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002526670  0.1914894 0.013194834  11.368085     9\n[21] {Google Austin Campus Mug}         =&gt; {Google Chicago Campus Mug}        0.002526670  0.1500000 0.016844469  11.368085     9\n[22] {Google Chicago Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002526670  0.1914894 0.013194834   8.420804     9\n[23] {Google NYC Campus Mug}            =&gt; {Google Chicago Campus Mug}        0.002526670  0.1111111 0.022740034   8.420804     9\n[24] {Google LA Campus Sticker}         =&gt; {Google LA Campus Mug}             0.003649635  0.4193548 0.008702976  34.738185    13\n[25] {Google LA Campus Mug}             =&gt; {Google LA Campus Sticker}         0.003649635  0.3023256 0.012071870  34.738185    13\n[26] {Google NYC Campus Zip Hoodie}     =&gt; {Google NYC Campus Mug}            0.003088153  0.1929825 0.016002246   8.486463    11\n[27] {Google NYC Campus Mug}            =&gt; {Google NYC Campus Zip Hoodie}     0.003088153  0.1358025 0.022740034   8.486463    11\n[28] {Google LA Campus Mug}             =&gt; {Google Cambridge Campus Mug}      0.002245929  0.1860465 0.012071870  20.709302     8\n[29] {Google Cambridge Campus Mug}      =&gt; {Google LA Campus Mug}             0.002245929  0.2500000 0.008983717  20.709302     8\n[30] {Google LA Campus Mug}             =&gt; {Google Austin Campus Mug}         0.002245929  0.1860465 0.012071870  11.044961     8\n[31] {Google Austin Campus Mug}         =&gt; {Google LA Campus Mug}             0.002245929  0.1333333 0.016844469  11.044961     8\n[32] {Google LA Campus Mug}             =&gt; {Google NYC Campus Mug}            0.003930376  0.3255814 0.012071870  14.317542    14\n[33] {Google NYC Campus Mug}            =&gt; {Google LA Campus Mug}             0.003930376  0.1728395 0.022740034  14.317542    14\n[34] {Google Cambridge Campus Mug}      =&gt; {Google PNW Campus Mug}            0.002245929  0.2500000 0.008983717  20.238636     8\n[35] {Google PNW Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.1818182 0.012352611  20.238636     8\n[36] {Google Cambridge Campus Mug}      =&gt; {Google Seattle Campus Mug}        0.003088153  0.3437500 0.008983717  27.209722    11\n[37] {Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.003088153  0.2444444 0.012633352  27.209722    11\n[38] {Google Cambridge Campus Mug}      =&gt; {Google Austin Campus Mug}         0.003088153  0.3437500 0.008983717  20.407292    11\n[39] {Google Austin Campus Mug}         =&gt; {Google Cambridge Campus Mug}      0.003088153  0.1833333 0.016844469  20.407292    11\n[40] {Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.005614823  0.6250000 0.008983717  27.484568    20\n[41] {Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.005614823  0.2469136 0.022740034  27.484568    20\n[42] {Google PNW Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.004491859  0.3636364 0.012352611  28.783838    16\n[43] {Google Seattle Campus Mug}        =&gt; {Google PNW Campus Mug}            0.004491859  0.3555556 0.012633352  28.783838    16\n[44] {Google PNW Campus Mug}            =&gt; {Google NYC Campus Mug}            0.003088153  0.2500000 0.012352611  10.993827    11\n[45] {Google NYC Campus Mug}            =&gt; {Google PNW Campus Mug}            0.003088153  0.1358025 0.022740034  10.993827    11\n[46] {Google Sunnyvale Campus Mug}      =&gt; {Google Austin Campus Mug}         0.002245929  0.1600000 0.014037058   9.498667     8\n[47] {Google Austin Campus Mug}         =&gt; {Google Sunnyvale Campus Mug}      0.002245929  0.1333333 0.016844469   9.498667     8\n[48] {Google Sunnyvale Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002526670  0.1800000 0.014037058   7.915556     9\n[49] {Google NYC Campus Mug}            =&gt; {Google Sunnyvale Campus Mug}      0.002526670  0.1111111 0.022740034   7.915556     9\n[50] {Google Seattle Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002526670  0.2000000 0.012633352  11.873333     9\n[51] {Google Austin Campus Mug}         =&gt; {Google Seattle Campus Mug}        0.002526670  0.1500000 0.016844469  11.873333     9\n[52] {Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.003649635  0.2888889 0.012633352  12.703978    13\n[53] {Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.003649635  0.1604938 0.022740034  12.703978    13\n[54] {Google Large Tote White}          =&gt; {Google Camp Mug Ivory}            0.002807412  0.1851852 0.015160022   3.726721    10\n[55] {Google NYC Campus Sticker}        =&gt; {Google NYC Campus Mug}            0.003368894  0.3157895 0.010668164  13.886940    12\n[56] {Google NYC Campus Mug}            =&gt; {Google NYC Campus Sticker}        0.003368894  0.1481481 0.022740034  13.886940    12\n[57] {Google Austin Campus Mug}         =&gt; {Google NYC Campus Mug}            0.004211117  0.2500000 0.016844469  10.993827    15\n[58] {Google NYC Campus Mug}            =&gt; {Google Austin Campus Mug}         0.004211117  0.1851852 0.022740034  10.993827    15\n[59] {Google Magnet}                    =&gt; {Google Camp Mug Ivory}            0.002807412  0.1562500 0.017967434   3.144421    10\n[60] {Google Camp Mug Gray}             =&gt; {Google Camp Mug Ivory}            0.005053341  0.1836735 0.027512633   3.696299    18\n[61] {Google Camp Mug Ivory}            =&gt; {Google Camp Mug Gray}             0.005053341  0.1016949 0.049691185   3.696299    18\n[62] {Google Kirkland Campus Mug,                                                                                                \n      Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002245929  0.6666667 0.003368894  29.316872     8\n[63] {Google Kirkland Campus Mug,                                                                                                \n      Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.002245929  1.0000000 0.002245929  79.155556     8\n[64] {Google NYC Campus Mug,                                                                                                     \n      Google Seattle Campus Mug}        =&gt; {Google Kirkland Campus Mug}       0.002245929  0.6153846 0.003649635 168.615385     8\n[65] {Google Boulder Campus Mug,                                                                                                 \n      Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[66] {Google Boulder Campus Mug,                                                                                                 \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.8888889 0.002526670  98.944444     8\n[67] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Boulder Campus Mug}        0.002245929  0.4000000 0.005614823  61.947826     8\n[68] {Google Cambridge Campus Mug,                                                                                               \n      Google LA Campus Mug}             =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[69] {Google LA Campus Mug,                                                                                                      \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.5714286 0.003930376  63.607143     8\n[70] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google LA Campus Mug}             0.002245929  0.4000000 0.005614823  33.134884     8\n[71] {Google Cambridge Campus Mug,                                                                                               \n      Google PNW Campus Mug}            =&gt; {Google NYC Campus Mug}            0.002245929  1.0000000 0.002245929  43.975309     8\n[72] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google PNW Campus Mug}            0.002245929  0.4000000 0.005614823  32.381818     8\n[73] {Google NYC Campus Mug,                                                                                                     \n      Google PNW Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002245929  0.7272727 0.003088153  80.954545     8\n[74] {Google Cambridge Campus Mug,                                                                                               \n      Google Seattle Campus Mug}        =&gt; {Google Austin Campus Mug}         0.002245929  0.7272727 0.003088153  43.175758     8\n[75] {Google Austin Campus Mug,                                                                                                  \n      Google Cambridge Campus Mug}      =&gt; {Google Seattle Campus Mug}        0.002245929  0.7272727 0.003088153  57.567677     8\n[76] {Google Austin Campus Mug,                                                                                                  \n      Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002245929  0.8888889 0.002526670  98.944444     8\n[77] {Google Cambridge Campus Mug,                                                                                               \n      Google Seattle Campus Mug}        =&gt; {Google NYC Campus Mug}            0.002807412  0.9090909 0.003088153  39.977553    10\n[78] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Seattle Campus Mug}        0.002807412  0.5000000 0.005614823  39.577778    10\n[79] {Google NYC Campus Mug,                                                                                                     \n      Google Seattle Campus Mug}        =&gt; {Google Cambridge Campus Mug}      0.002807412  0.7692308 0.003649635  85.625000    10\n[80] {Google Austin Campus Mug,                                                                                                  \n      Google Cambridge Campus Mug}      =&gt; {Google NYC Campus Mug}            0.002526670  0.8181818 0.003088153  35.979798     9\n[81] {Google Cambridge Campus Mug,                                                                                               \n      Google NYC Campus Mug}            =&gt; {Google Austin Campus Mug}         0.002526670  0.4500000 0.005614823  26.715000     9\n[82] {Google Austin Campus Mug,                                                                                                  \n      Google NYC Campus Mug}            =&gt; {Google Cambridge Campus Mug}      0.002526670  0.6000000 0.004211117  66.787500     9\n\n\nThis can also be achieved in a more tidyverse style by doing the following:\n\n# Google Camp Mug Ivory tidy way\nmdl_ga_rules |&gt;\n  as(\"data.frame\") |&gt;\n  tibble() |&gt;\n  filter(str_detect(rules, \"Google Camp Mug Ivory\"))\n\n# A tibble: 7 × 6\n  rules                                                    support confidence coverage  lift count\n  &lt;chr&gt;                                                      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 {Google Flat Front Bag Grey} =&gt; {Google Camp Mug Ivory}  0.00505      0.257   0.0197  5.17    18\n2 {Google Camp Mug Ivory} =&gt; {Google Flat Front Bag Grey}  0.00505      0.102   0.0497  5.17    18\n3 {Google Unisex Eco Tee Black} =&gt; {Google Camp Mug Ivory} 0.00225      0.131   0.0171  2.64     8\n4 {Google Large Tote White} =&gt; {Google Camp Mug Ivory}     0.00281      0.185   0.0152  3.73    10\n5 {Google Magnet} =&gt; {Google Camp Mug Ivory}               0.00281      0.156   0.0180  3.14    10\n6 {Google Camp Mug Gray} =&gt; {Google Camp Mug Ivory}        0.00505      0.184   0.0275  3.70    18\n7 {Google Camp Mug Ivory} =&gt; {Google Camp Mug Gray}        0.00505      0.102   0.0497  3.70    18\n\n\n\n# All mugs tidy way\nmdl_ga_rules |&gt;\n  as(\"data.frame\") |&gt;\n  tibble() |&gt;\n  filter(str_detect(rules, \"Mug\"))\n\n# A tibble: 82 × 6\n   rules                                                     support confidence coverage  lift count\n   &lt;chr&gt;                                                       &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1 {Google Austin Campus Tote} =&gt; {Google Austin Campus Mug} 0.00225      0.615  0.00365  36.5     8\n 2 {Google Austin Campus Mug} =&gt; {Google Austin Campus Tote} 0.00225      0.133  0.0168   36.5     8\n 3 {Google Kirkland Campus Mug} =&gt; {Google Seattle Campus M… 0.00337      0.923  0.00365  73.1    12\n 4 {Google Seattle Campus Mug} =&gt; {Google Kirkland Campus M… 0.00337      0.267  0.0126   73.1    12\n 5 {Google Kirkland Campus Mug} =&gt; {Google NYC Campus Mug}   0.00225      0.615  0.00365  27.1     8\n 6 {Google Boulder Campus Mug} =&gt; {Google Cambridge Campus … 0.00225      0.348  0.00646  38.7     8\n 7 {Google Cambridge Campus Mug} =&gt; {Google Boulder Campus … 0.00225      0.25   0.00898  38.7     8\n 8 {Google Boulder Campus Mug} =&gt; {Google NYC Campus Mug}    0.00253      0.391  0.00646  17.2     9\n 9 {Google NYC Campus Mug} =&gt; {Google Boulder Campus Mug}    0.00253      0.111  0.0227   17.2     9\n10 {Google NYC Campus Bottle} =&gt; {Google NYC Campus Mug}     0.00225      0.296  0.00758  13.0     8\n# ℹ 72 more rows"
  },
  {
    "objectID": "blog/posts/2024-02-27-tidytuesday-2024-02-27-leap-day/index.html",
    "href": "blog/posts/2024-02-27-tidytuesday-2024-02-27-leap-day/index.html",
    "title": "Exploring the lifespans of historical figures born on a Leap Day",
    "section": "",
    "text": "Photo by Nick Hillier\n\n\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(here)\n\n\nBackground\nHappy belated Leap Day! This week’s #tidytuesday is focused on significant historical events and people who were born or died on a Leap Day. The aim of this post is to contribute a couple data visualizations to this social data project. Specifically, I used plotly and Tableau to create my contributions.\n\ndata_births &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-02-27-tidytuesday-2024-02-27-leap-day\",\n    \"births.csv\"\n  )\n)\n\nLet’s do a quick glimpse() and skim() of our data, just so we get an idea of what we’re working with here.\n\nglimpse(data_births)\n\nRows: 121\nColumns: 4\n$ year_birth  &lt;dbl&gt; 1468, 1528, 1528, 1572, 1576, 1640, 1692, 1724, 1736, 1792, 1812, 1828, 1836, …\n$ person      &lt;chr&gt; \"Pope Paul III\", \"Albert V\", \"Domingo Báñez\", \"Edward Cecil\", \"Antonio Neri\", …\n$ description &lt;chr&gt; NA, \"Duke of Bavaria\", \"Spanish theologian\", \"1st Viscount Wimbledon\", \"Floren…\n$ year_death  &lt;dbl&gt; 1549, 1579, 1604, 1638, 1614, 1704, 1763, 1822, 1784, 1868, 1880, 1921, 1908, …\n\n\n\nskim(data_births)\n\n\nData summary\n\n\nName\ndata_births\n\n\nNumber of rows\n121\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nperson\n0\n1.00\n6\n29\n0\n121\n0\n\n\ndescription\n1\n0.99\n12\n95\n0\n107\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear_birth\n0\n1.00\n1919.90\n101.01\n1468\n1920\n1944.0\n1976\n2004\n▁▁▁▁▇\n\n\nyear_death\n65\n0.46\n1933.61\n126.53\n1549\n1920\n1989.5\n2013\n2023\n▁▁▁▁▇\n\n\n\n\n\n\n\nData description\nThis week’s data comes from the February 29th Wikipedia page. Three data sets are made available, one focused on significant events, as well as births and deaths of historical figures that occurred on a Leap Day. Given what’s available, I was interested in exploring the age and lifespan of the historical figures born on a Leap Day. Here’s the wrangling code I created to explore the data.\n\ndata_age &lt;- data_births |&gt;\n  mutate(\n    is_alive = ifelse(is.na(year_death), 1, 0),\n    year_death = ifelse(is.na(year_death), 2024, year_death),\n    age = year_death - year_birth\n  ) |&gt;\n  arrange(desc(age)) |&gt; \n  relocate(person, description, year_birth, year_death, age)\n\ndata_age$person &lt;- factor(data_age$person, levels = data_age$person[order(data_age$year_birth)])\n\n\n\nWhat are the lifespans of historical figures born on a leap day?\nTo explore this question, I decided to create a dumbbell chart. In the chart, the blue dots represent the person’s birth year. The black dot represents the year the person died. Absence of the black dot indicates a person is still alive, while the grey line represents the person’s lifespan. If you hover over the dots, a tool tip with information about each person is shown.\n\nnot_alive &lt;- data_age |&gt; filter(is_alive == 0)\n\nplot_ly(\n  data_age, \n  color = I(\"gray80\"),\n  text = ~paste(\n    person, \"&lt;br&gt;\",\n    \"Age: \", age, \"&lt;br&gt;\",\n    description \n  ),\n  hoverinfo = \"text\"\n) |&gt;\n  add_segments(x = ~year_birth, xend = ~year_death, y = ~person, yend = ~person, showlegend = FALSE) |&gt;\n  add_markers(x = ~year_birth, y = ~person, color = I(\"#0000FF\"), name = \"Birth year\") |&gt;\n  add_markers(data = not_alive, x = ~year_death, y = ~person, color = I(\"black\"), name = \"Year passed\") |&gt;\n  layout(\n    title = list(\n      text = \"&lt;b&gt;Lifespans of historical figures born on a Leap Day&lt;/b&gt;\",\n      xanchor = \"center\",\n      yanchor = \"top\",\n      font = list(family = \"arial\", size = 24)\n    ),\n    xaxis = list(\n      title = \"Year born | Year died\"\n    ),\n    yaxis = list(\n      title = \"\"\n    )\n  )\n\n\n\n\n\n\n\nAn attempt using Tableau\nI also created a version of this visualization using Tableau. You can view my attempt here. I was required to make a few concessions with this attempt, as I was unable to have as much fine control of the plot elements as I would have liked. However, I’m happy with what turned out.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring the Lifespans of Historical Figures Born on a\n    {Leap} {Day}},\n  date = {2024-03-05},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Exploring the Lifespans of Historical\nFigures Born on a Leap Day.” March 5, 2024."
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "",
    "text": "Photo by Ricardo Gomez Angel"
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#composing-the-query",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#composing-the-query",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Composing the query",
    "text": "Composing the query\nThe first step is to compose a query. The query is a character string, which is assigned a variable name.\n\nquery_neb_county_pop &lt;- \"\n  with census_total_pop as (\n    select \n      geo_id,\n      total_pop\n    from `bigquery-public-data.census_bureau_acs.county_2020_5yr`\n    where regexp_contains(geo_id, r'^31')\n  ), \n  fips_codes as (\n    select \n      county_fips_code,\n      area_name\n    from `bigquery-public-data.census_utility.fips_codes_all`\n  )\n  \n  select \n    geo_id,\n    area_name,\n    total_pop\n  from census_total_pop left outer join fips_codes on census_total_pop.geo_id = fips_codes.county_fips_code\n\"\n\nUsing a left join, this query transposes the county names column from another dataset. The result of the join is the total population of each county according to the 2020 ACS survey. query_neb_county_pop is now available for the next few steps in the query process.\n\n\n\n\n\n\nNote\n\n\n\nThe query utilizes BigQuery’s convenient with clause to create temporary tables, which are then joined using a left join. Indeed, I can hear the SQL experts pointing out more clever ways to compose this query: it’s just a simple join. However, readability was the goal here."
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#submitting-the-query",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#submitting-the-query",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Submitting the query",
    "text": "Submitting the query\nWhen I first started using the bigrquery package, I struggled to understand what query function to use. I was also slightly confused why the package had a separate query and download table function (more on this in a bit). First, the type of data being queried (e.g., public vs. project data) dictates what query function to use. Second, the argument structure is slightly different between the two query functions. The nuance of these differences is subtle, so I suggest reading the package’s docs (?bq_query) to know what function to use and when.\nIf project data is queried, the bq_project_query() function is used. In cases where you’re not querying project data (e.g., public data), you’ll use bq_dataset_query(). The bq_dataset_query() is used in this post because public data is being queried. This function has parameters to associate the query with a Google Cloud billing account. In regard to the function’s other arguments, you’ll only need to pass a bq_dataset object (in our case a bq_dataset_acs) and a query string.\nGetting data into the R session involves two steps. First, you’ll submit the query to BigQuery using one of the functions highlighted above. BigQuery creates a temporary table in this initial step. Second, this temporary table is downloaded to the R session using the bq_table_download() function.\nThis intermediate, temporary table provides a couple conveniences:\n\nYou can use the bq_table_fields() function to check the temporary table’s fields before downloading it into your R session.\nThe table is essentially cached. As such, submitting the same exact query will return the data faster, and data processing costs will be reduced.\n\n\ntbl_temp_acs &lt;- bq_dataset_query(\n  bq_dataset_acs,\n  query = query_neb_county_pop,\n  billing = '&lt;project-name&gt;'\n)\n\n\nbq_table_fields(tbl_temp_acs)\n# &lt;bq_fields&gt;\n#   geo_id &lt;STRING&gt;\n#   area_name &lt;STRING&gt;\n#   total_pop &lt;FLOAT&gt;\n\n\ndata_nebpm_county &lt;- bq_table_download(tbl_temp_acs)\n\ndata_nebpm_county\n# # A tibble: 93 × 3\n#    geo_id area_name        total_pop\n#    &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt;\n#  1 31005  Arthur County          439\n#  2 31085  Hayes County           889\n#  3 31087  Hitchcock County      2788\n#  4 31103  Keya Paha County       875\n#  5 31113  Logan County           896\n#  6 31115  Loup County            690\n#  7 31117  McPherson County       420\n#  8 31125  Nance County          3525\n#  9 31133  Pawnee County         2640\n# 10 31143  Polk County           5208\n# # ℹ 83 more rows\n# # ℹ Use `print(n = ...)` to see more rows\n\nNow the data is available in the R session. You can work with it like any other type imported via these methods."
  },
  {
    "objectID": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#create-and-write-disposition",
    "href": "blog/posts/2024-10-05-workflow-walkthrough-google-bigquery-bigrquery-rstats/index.html#create-and-write-disposition",
    "title": "Workflow walkthrough: Interacting with Google BigQuery in R",
    "section": "Create and write disposition",
    "text": "Create and write disposition\nbq_table_upload() has some additional arguments, which I’ve found are not clearly documented in the package’s documentation. These include the ability to pass a create_disposition and a write_disposition argument.\n\n\n\n\n\n\nWarning\n\n\n\nBe mindful of how you use these arguments, as the values you pass can overwrite data. Read more about the options by reviewing the linked docs below.\n\n\nMore about what these options do in BigQuery can be reviewed here. Here’s what the code would look like using the arguments listed above:\n\nbq_table_upload(\n  bq_neb_county_table,\n  values = fields_neb_county,\n  create_disposition = \"CREATE_NEVER\",\n  write_disposition = \"WRITE_TRUNCATE\"\n)\n\nThe create_disposition argument specifies how the table will be created, based on whether the table exists or not. A value of CREATE_NEVER requires the table to already exist, otherwise an error is pushed. CREATE_IF_NEEDED creates the table if it does not already exist. However, it’s best to use the bq_table_create() function rather than relying on the bq_table_upload() function to create the table for us. Nevertheless, it’s an option that’s available.\nThe write_disposition specifies what happens to values when they’re written to tables. There are three options: WRITE_TRUNCATE, WRITE_APPEND, and WRITE_EMPTY. Here’s what each of these options do:\n\nWRITE_TRUNCATE: If the table exists, overwrite the data using the schema of the newly inputted data (i.e., a destructive action).\nWRITE_APPEND: If the table exists, append the data to the table (i.e., add it to the bottom of the table).\nWRITE_EMPTY: If the table exists and it already contains data, push an error.\n\nWhen it comes to uploading data, you’ll most likely want to consider the write_disposition you use.\nOne last note about uploading data to your tables: BigQuery optimizes for speed. This optimization some times results in the data to be imported not in the order it is initially imported. Rather, the resulting data import may be shuffled in a way to speed up the process. Thus, you’ll likely need to arrange your data if you need to extract it again."
  },
  {
    "objectID": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "href": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "title": "Flattening Google Analytics 4 data",
    "section": "",
    "text": "Photo by Alvaro Calvo\n\n\n\nIntroduction\nWith the introduction of the Google Analytics 4 (GA4) BigQuery integration, understanding how to work with the underlying analytics data has become increasingly important. When first diving into this data, some of the data types may seem hard to work with. Specifically, analysts might be unfamiliar with the array and struct data types. Even more unfamiliar may be the combination of these two data types into complex, nested and repeated data structures. As such, some may become frustrated writing queries against this data. I know I did.\nIf you’re mainly coming from working with flat data files, these more complex data types may not be intuitive to work with, as the SQL syntax is not as straight forward as a simple SELECT FROM statement. Much of this unfamiliarity may come from the required use of unfamiliar BigQuery functions and operators, many of which are used to transform data from nested, repeated, or nested repeated structures to a flattened, denormalized form.\nAs such, this post aims to do three things: 1. Overview the array, struct, and array of struct data types in BigQuery; 2. Overview some of the approaches to flatten these data types; and 3. Apply this knowledge in the denormalization of Google Analytics 4 data stored in BigQuery.\nThis post mostly serves as notes that I wish I had when I began working with these data structures.\n\n\nArrays, structs, and array of structs\nBefore discussing the use of these data types in GA4 data, let’s take a step back and simply define what array and struct data types are in BigQuery. A good starting point is BigQuery’s arrays and structs documentation. According to the docs,\n\nAn array is an ordered list of zero or more elements of non-Array values. Elements in an array must share the same type.\n\n\nA struct is a container of ordered fields each with a type (required) and field name (optional).\n\nBoth definitions contain technical jargon that don’t really define, in an intuitive, useful way, what these data types are and how to use them, especially in the analysis of GA4 data. So let’s break each down by bringing in additional perspectives and through the use of several simplified examples.\nWhile learning more about arrays and structs, I found several blog posts that helped me better understand these structures and how to use them. Here is a list of the ones I found to be very helpful:\n\nHow to work with Arrays and Structs in Google BigQuery by Deepti Garg\nExplore Arrays and Structs for Better Query Performance in Google BigQuery by Skye Tran\nTutorial: BigQuery arrays and structs from Sho’t left to data science\n\nI highly suggest reading all of these. In fact, much of what follows is adapted from these posts, with a few examples I created to help me better understand how these data types are structured, stored, and queried. Towards the end of the post, the techniques learned from these posts and overviewed here will be applied to GA4 data, specifically the publicly available bigquery-public-data.ga4_obfuscated_sample_ecommerce data.\n\n\nArrays\nArrays are a collection of elements of the same datatype. If you’re familiar with the R programming language, an array is similair to a vector.\nLet’s create a table containing an array of planets in our solar system as an example, and then use the INFORMATION_SCHEMA view to verify the data was entered correctly. The following code will create this table in BigQuery:\ncreate or replace table examples.array_planets_example as \nwith a as (\n   select [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"] as planets\n)\nselect planets from a;\nThe INFORMATION_SCHEMA.COLUMNS view for the array_planets_example table can be queried to verify the data was entered correctly. This table is available for every table created in BigQuery, and it contains metadata about the table and the fields within. Here is the query needed to return this information:\nselect table_name,\n   column_name,\n   is_nullable,\n   data_type\nfrom examples.INFRORMATION_SCHEMA.COLUMNS\nwhere table_name = \"array_planets_example\";\nThe returned table will contain a data_type field, where the value ARRAY&lt;STRING&gt; will be present. This value represents the field in the array_planets_example contains an array with a list of string values. Although this example array contains a series of string values, arrays can hold various other data types, as long as the values are the same type across the collection. Overviewing all of the different data types that can be stored in an array is beyond the scope of this post, but check out the BigQuery docs for more examples.\n\nQuerying an array\nMultiple approaches are available to query an array. The type of approach will depend on if the returned data needs to maintain its grouped, repeated structure, or if the returned data needs to be flattened. If maintaining the repeated structure is required, then a simple SELECT statement will work. Using the array_planets_example table as an example, the query applying this approach will look something like this:\nselect planets\nfrom examples.array_planets_example\nIf each element of the array is to be outputted onto its own row (i.e., denormalized), multiple approaches are available. The first approach is to use the unnest() function. Here is an example using the planets array we created earlier:\nselect planets\nfrom examples.array_planets_example,\nunnest(planets) as planets\nThe second approach is to apply a correlated join through the use of cross join unnest(). This approach looks like this:\nselect planets\nfrom examples.array_planets_example\ncross join unnest(planets) as planets\nYou’ll notice this is only slightly different than the query above, and in fact the , used in the FROM clause is short-hand for the cross join statement. The last and final approach is to use a comma-join. This is similair to our first query, but now we refer to the table name before the array name we want flattened.\nselect planets\nfrom examples.array_planets_example, array_planets_example.planets as planets;\nWhich one do you choose? It really comes down to a matter of preference. All three approaches will lead to the same result. It depends on how explicit you want the code to be.\nThere is one note to be aware of if you’re applying these conventions to other arrays outside of analyzing GA4 data. The cross join approach will exclude NULL arrays. So if you want to retain rows containing NULL arrays, you’ll need to apply a left join. More about this is described in the BigQuery docs.\nKeep these approaches top-of-mind. They will be applied to flatten some of the fields in the GA4 dataset. In other words, get comfortable with using them.\n\n\n\nStructs\nThe structs data type holds attributes in key-value pairs. Structs can hold many different data types, even structs. We will see the use of structs within structs in the GA4 data. Keeping with the solar system theme of the post, the following example code will create a table utilizing the struct data type to hold the dimensions and distances of the planets in our solar system. The data used for this table is reported here.\ncreate or replace table examples.struct_solar_system as\nwith a as (\n  select \"Mercury\" as planet,\n  struct(0.39 as au_sun, 57900000 as km_sun, 4879 as km_diameter) as dims_distance union all\n  select \"Venus\" as planet,\n  struct(0.72 as au_sun, 108200000 as km_sun, 12104 as km_diameter) as dims_distance union all\n  select \"Earth\" as planet,\n  struct(1 as au_sun, 149600000 as km_sun, 12756 as km_diameter) as dims_distance union all\n  select \"Mars\" as planet,\n  struct(1.52 as au_sun, 227900000 as km_sun, 6792 as km_diameter) as dims_distance union all\n  select \"Jupiter\" as planet,\n  struct(5.2 as au_sun, 778600000 as km_sun, 142984 as km_diameter) as dims_distance union all\n  select \"Saturn\" as planet,\n  struct(9.54 as au_sun, 1433500000 as km_sun, 120536 as km_diameter) as dims_distance union all\n  select \"Uranus\" as planet,\n  struct(19.2 as au_sun, 2872500000 as km_sun, 51118 as km_diameter) as dims_distance union all\n  select \"Neptune\" as planet,\n  struct(30.06 as au_sun, 4495100000 as km_sun, 49528 as km_diameter) as dims_distance \n)\nselect * from a;\nThis table contains two columns. A column that holds a string value for the name of the planet and a struct column that contains a list of key value pairs of distance and dimensions for each planet.\nThe INFRORMATION_SCHEMA.COLUMNS table can then be queried again to verify the datatypes for each column were inputted correctly. Here is the code to do this:\nselect \n  table_name, \n  column_name,\n  is_nullable,\n  data_type\nfrom examples.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"struct_solar_system\";\nThe returned table will contain a data_type column with two values: STRING and STRUCT&lt;au_sun FLOAT64, km_sun INT64, km_diameter INT64&gt;. Take notice that the STRUCT value contains information about the data types contained within.\n\nQuerying a struct\nQuerying a struct requires the use of the . operator (i.e., dot operator) in the FROM clause to flatten the table. Take for example the case where we want to return a table of only the distance of each planet from the sun in kilometers. The following query will be used:\nselect \n  planet,\n  dims_distance.km_sun\nfrom examples.struct_solar_system;\nSay a denormalized table that contains both the distance from the sun in kilometers and each planet’s diameter in kilometers is wanted. The following query would be used:\nselect \n  planet,\n  dims_distance.km_sun,\n  dims_distance.km_diameter\nfrom examples.struct_solar_system;\nWhen reviewing these two examples, observe how the dot notation is being used. In the first, our select statement contains dims_distance.km_sun, which unnests the values and gives each its own row for each planet. This is expanded in the second query, where an additional line is added to the select statement, dims_distance.km_diameter. To unnest all the values in the struct, use the following query:\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system;\nIn fact, let’s expand this query to answer the following question: which planets are the closest and farthest from our sun. Take notice how the ORDER BY portion of the query doesn’t require the dims_distance prefix for the field we want to arrange our data.\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system\norder by km_sun;\n\n\n\nArray and structs in GA4 data\nNow that we have learned a little bit about our solar system, let’s return to Earth and the task at hand, flattening GA4 data. We just discussed how these data types are created and queried, it is now time to combine them into more complex data structures, as both of these structures are combined to create nested repeated data structures in the GA4 data. It’s best to start with an example. Specifically, let’s look at how these structures are applied in the event_params field.\nWe can start off by querying the INFORMATION_SCHEMA.COLUMNS view for one event to get an idea of its structure. The query to do this can be seen here:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"events_20210131\" and column_name = \"event_params\";\nThe data type is described in the returned table’s data_type field. This field contains the following value ARRAY&lt;STRUCT&lt;key STRING, value STRUCT&lt;string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64&gt;&gt;&gt;. It should be immediately apparent that both the array and struct values are being used here to create a repeated nested structure. In fact, the event_params value uses a struct within a struct. Given this structure, all the above methods will need to be employed to flatten this data.\nTo simplify this, let’s look at one instance of one event in the GA4 data. Specifically, let’s look at one instance of a page_view event. With this simplified example, we’ll go step-by-step, adding additional elements to the query needed to flatten this data.\nselect \n  event_date,\n  event_timestamp,\n  event_name,\n  event_params\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = 'page_view'\nlimit 1;\nAfter running this query, you’ll notice the output to the console is quite verbose, especially if you’re using the bq command-line tool. The verbosity of the output is due to the event_params field holding much of the data.\nThe first layer of the structure is an array, so the initial step is to use the unnest() function. The following can be done to achieve this:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param;\nYou’ll notice a nested FROM statement is being used here. This is done to limit the result set to one row, representing one page_view event for this simplified example. Later iterations of the query will eliminate this nested query.\nNow say we’re only interested in viewing the page_location parameter. We can use a where statement to filter out this information. Here is what this will look like:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key = 'page_location';\nInterested in viewing both the page_location and page_title parameters? Use the IN operator in the WHERE clause.\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key in ('page_location', 'page_title');\nWanna turn the key field into columns so you only have one row for this specific event? Use BigQuery’s pivot() operator. Here is how to achieve this in a query:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    key,\n    value.string_value\n  from (\n    select \n      event_date,\n      event_timestamp,\n      event_name,\n      event_params\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nSince the string values are all we care about here, the value.string_value was the only one retained in the query. The other nested value elements were eliminated from the SELECT statement.\n\n\nCombine other nested fields in the GA4 data\nNow that the event_params field has been flattened, let’s supplement this information with additional data in the table. Moreover, this will provide another example of how to apply these steps to flatten other elements in the GA4 data. Knowing where users originate is some additional context that may add to our event analysis, so let’s add that data to our flattened data. But first, let’s get some more information on what data type is used for the geo field in the GA4 data.\nOnce again, querying the INFORMATION_SCHEMA.COLUMNS view can be used to explore the geo field’s data type. Here is what the query looks like:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS`\nwhere table_name = \"events_20210131\" and column_name = \"geo\";\nThe value STRUCT&lt;continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING&gt; is returned. Let’s write a query to return the table without first unnesting the data.\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo \nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = \"page_view\"\nlimit 1;\nYou’ll notice this field contains a struct, where the dot operator will need to be applied to flatten this data. Let’s start by flattening this data and then combine it with the events_param data. For the sake of keeping the returned table simple, let’s just return the region and city fields in a denormalized form. The following will return a flattened table with these fields:\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo.region,\n  geo.city\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nlimit 1;\nAs expected, the table will return a flattened table containing five fields: event_date, event_name, user_pseudo_id, geo.region, and geo.city. This table was also limited to return only the first instance of the page_view in the table.\nNow, the next step is to add this geo data to our flattened event_params query. This is as simple as adding the . operator with the needed geo elements into the FROM statement. The query will now look like this:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nThe resulting table will contain one row with several fields representing the specific event. This is great for one event, but the next step will be to expand this denormalization to all page_view events in the table.\n\n\nExpand the unnesting to multiple page view events\nNow that we have the flattened table for one page_view event, let’s expand it to additional events. This requires a simple modification to the initial nested query, remove the limit 1 line.\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\nWe can now refactor the query to be more concise. Here is what this will look like:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\n\n\nApply these approaches across multiple days\nGenerating results for one day may not be enough, so there’s a few modifications that can be made to expand the final query to return additional days. This involves modifying the FROM and WHERE statements in the initial query.\nThe first step is to modify the FROM statement to use the * wildcard operator at the end of the table name. Since the GA4 tables are partitioned by day, this will allow for a range of tables to be defined within the WHERE clause. The table name will now be bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*.\nTo define the range of dates for the events (i.e., to query multiple tables), the WHERE clause will be expanded to include the use of _table_suffix. The _table_suffix is a special column used within a separate wildcard table that is used to match the range of values. Explaining the use of the wildcard table is beyond the scope of this post, but more about how this works can be found here. The WHERE clause will now look like this:\nwhere event_name = 'page_view' and\nkey in ('page_location', 'page_title') and\n_table_suffix between \"20210126\" and \"20210131\"\nYou’ll notice this statement uses the between operator, where two string values representing the date range are passed. This statement is inclusive, so it will include partitioned tables from 20210126 and 20210131, and all tables in between. Here is the query in its final form:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and \n  key in ('page_location', 'page_title') and\n  _table_suffix between \"20210126\" and \"20210131\"\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\norder by event_date\n\n\nWrap up\nThis post started out simple by defining what arrays, structs, and array of structs data types are in BigQuery. Through the use of several examples, this post overviewed several approaches to query these different data types, specifically highlighting how to flatten each type. A second aim of this post was to show the application of these methods to the flattening of GA4 data stored in BigQuery. This included the flattening and combination of the complex, nested, repeated and nested repeated data types used in the event_params and geo fields. Finally, this post shared queries that expanded the result set across multiple days worth of data.\nIf you found this post helpful or just have interest in this type of content, I would appreciate the follow on GitHub and/or Twitter. If you have suggestions on how to improve these queries or found something that I missed, please file an issue in the repo found here.\n\n\nAdditional resources\nI spent a lot of time researching how to write, use, and query arrays and structs in BigQuery. In the process of preparing this post, I wrote a lot of example queries and followed along with BigQuery’s turtorial on working with arrays and structs. As a result, I created multiple files that I organized into the GitHub repo for this post. These might be useful as a review after reading this post, or they might be a helpful quickstart quide for your own analysis of GA4 data stored in BigQuery. These additional notes can be found here.\n\n\nAdditional references\n\nHow to work with Arrays and Structs in Google BigQuery\nExplore Arrays and Structs for Better Query Performance in Google BigQuery\nTutorial: BigQuery arrays and structs\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2022,\n  author = {Berke, Collin K},\n  title = {Flattening {Google} {Analytics} 4 Data},\n  date = {2022-09-20},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2022. “Flattening Google Analytics 4\nData.” September 20, 2022."
  },
  {
    "objectID": "blog/posts/2021-04-02-intro-post/index.html",
    "href": "blog/posts/2021-04-02-intro-post/index.html",
    "title": "Intro Post",
    "section": "",
    "text": "Hello World!\nI have decided to start writing a blog. I have never attempted anything like this before, but I felt it was time to start organizing some of my projects and analyses into one central location. For some time, I really wanted to develop a space where I could discuss topics I find interesting, both professionally and personally. So, Hello World!, my name is Collin, and this is my blog.\nI’m also on other platforms, however, I really don’t engage on them too often. Nevertheless, you can find more information about me and my projects in the following locations:\n\nGitHub is a great place to see the projects I am working on. Most are professional at this time.\nTwitter, I haven’t posted anything in a while, though I retweet and like stuff often.\nEmail, the best channel to get a hold of me: collin.berke@gmail.com.\n\n\n\nThe purpose of this blog\nThe purpose of this blog is to serve as a location for me to express my thoughts on topics I find interesting. To be honest, I don’t expect this to be a really niche blog with one focused, clear purpose. Most likely it will be data analysis and/or visualization focused, which I will apply to develop posts in areas I find interesting: open-source software, media/marketing analytics, data analysis, sports, media, etc. My purpose may become more refined once I find my voice.\n\n\nThe inspiration and motivation to do this blog\nI have spent countless hours reading, re-reading, bookmarking, and Googling multiple topics regarding the use of the statistical computing programming language called R. Much of this time has been spent accessing useful, open-source, and free content that has aided me professionally, and it has contributed to my deeper understanding of the topics I find interesting. I couldn’t even begin to describe how grateful I am for those who have spent time organizing and drafting content others find useful. In fact, it’s the #Rstats community that has motivated me to put this blog together, as I have seen how helpful and open it is to aiding in the development of others.\nI now feel I am in a place of not only being a consumer but a producer of this information. I will never be an expert in this area, as there is too much to learn for just one person. As one of my favorite podcasts (i.e., Make Me Smart With Kai and Molly) states in every episode, “None of us is as smart as all of us.” Thus, I feel it is time to start organizing and drafting content others will hopefully find useful, at the very least amusing. Even if this blog helps one person, that will be enough motivation to keep me working on it.\n\n\nWhat’s up next?\nNot sure. I’m just excited I got this blog up and running. Most likely I’ll do something sports related. Who knows–stay tuned.\n\n\nReferences & Acknowledgements\n\nI make every attempt to properly cite information from other sources. If I have failed to properly attribute credit to a source, please kindly let me know.\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2021,\n  author = {Berke, Collin K},\n  title = {Intro {Post}},\n  date = {2021-04-02},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2021. “Intro Post.” April 2, 2021."
  },
  {
    "objectID": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html",
    "href": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html",
    "title": "Exploring objects launched into space and gross domestic product",
    "section": "",
    "text": "Image generated using the prompt ‘boxy robot throwing a satellite into outer space from earth in a pop art style’ with the Bing Image Creator"
  },
  {
    "objectID": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html#use-wbstats-package-to-obtain-gdp",
    "href": "blog/posts/2024-04-25-tidytuesday-2024-05-03-space-launches/index.html#use-wbstats-package-to-obtain-gdp",
    "title": "Exploring objects launched into space and gross domestic product",
    "section": "Use wbstats package to obtain GDP",
    "text": "Use wbstats package to obtain GDP\nThe original dataset didn’t contain Gross Domestic Product (GDP). As such, I had to supplement it with additional data from the World Bank. The world bank makes data containing an estimate of GDP available via an API. In fact, the wbstats R package provides an intuitive interface to access data via this API. Here’s the code I used to return data from the API using the wbstats package:\n\n# Interested in looking at:\n#   * Gross Domestic Product (GDP)\nwb_variables &lt;- c(\n  \"gdp\" = \"NY.GDP.MKTP.CD\"\n)\n\ndata_wb &lt;- wb_data(\n  wb_variables,\n  start_date = 1957, \n  end_date = 2023\n) |&gt;\nselect(\n  code = iso3c, \n  year = date, \n  country, \n  gdp,\n  starts_with(\"tax\")\n)"
  },
  {
    "objectID": "blog/posts/2024-07-10-2024-07-hex-update/index.html#google-zero",
    "href": "blog/posts/2024-07-10-2024-07-hex-update/index.html#google-zero",
    "title": "The Hex Update: June, 2024",
    "section": "Google Zero",
    "text": "Google Zero\nIf you have a website, you likely rely on referral traffic. What would happen to your site (or your business for that matter) if this traffic suddenly went away? Nilay Patel from the Verge’s Decoder Podcast has been exploring this topic recently. Nilay and folks (i.e., some major tech and media company CEOs) have some interesting thoughts and perspectives, since it’s been posited that Google Zero is already here.\n\nHere are some links to go deeper\n\nGoogle Zero is here - now what? (Decoder podcast)\nHow to play the long game, with New York Times CEO Meredith Kopit Levien (Decoder podcast). Listen around 19:50 to hear more discussion specific to Google Zero.\nFandom runs some of the Biggest communities on the internet - Can CEO Perkins Miller keep them happy? (Decoder podcast episode) Listen around 42:13 to hear more discussion closely to Google Zero.\n\n\n\nWhat’s the takeaway?\nI came away with several takeaways from these conversations, but I think it comes down to the idea that a business or media property can’t be built on referral traffic anymore. Or, at the very least, it’s on shaky ground if the business model solely relies on referral traffic from other platforms. Simply put, according to Nilay, if you can’t explain what your business is without referral traffic, then do you really have a business?\nBuilding an audience online is challenging. In fact, Nilay suggests that if you were to create a media business today, you’d likely start on some video platform rather than starting from the point of ‘building a website’ first, followed by other tactics.\nIt’s about the value you create. If you’re creating content, products, and experiences people are willing to spend their time and money on, then referral traffic shouldn’t matter. Perhaps this is an opportunity to recenter the focus back to what’s being created, rather than creating content to rank high by gaming an algorithm. I know it’s aspirational but unrealistic when confronted with economic realities."
  },
  {
    "objectID": "blog/posts/2024-07-10-2024-07-hex-update/index.html#googles-ai-overviews",
    "href": "blog/posts/2024-07-10-2024-07-hex-update/index.html#googles-ai-overviews",
    "title": "The Hex Update: June, 2024",
    "section": "Google’s AI overviews",
    "text": "Google’s AI overviews\nIf you’ve used Google as of late, your search queries have likely returned responses from Google Search’s new AI Overviews feature. The utility provided to users seems interesting. It’s certainly made my search experience a little better. However, I’ve noticed some questions being raised about the feature as it relates to referrals: If users receive the answers they want natively within the AI overviews, will they still navigate to the sites of other publishers? According to Google, users are clicking links in AI overviews more than they would if the links were included as a traditional web listing for that query. However, data supporting this is yet to be released or made available to publishers. Some have even gone so far as to say this new feature could kill what’s left of the traffic sent to publishers, especially traffic sent to news publishers.\n\nHere are some links to go deeper\n\nPublishers horrified at new Google AI feature that could kill what’s left of journalism (The_Byte)\nNews publishers sound alarm on Google’s new AI-infused search, warn of ‘catastrophic’ impacts (CNN Business)\nGoogle CEO Sundary Pichai on AI-powered search and the future of the web (Decoder Podcast). The episode also includes some discussion about Google Zero and AI overviews. Listen around 07:32.\n\n\n\nWhat’s the takeaway?\nI believe the effect of this new feature is yet to be seen. It just rolled out in late May. More data will certainly help publishers assess this feature’s impact on referral traffic. However, it may be important for publishers to explore and better understand how AI Overviews work within Search, so as to identify strategies that result in content to rank high and be linked within these overviews. Indeed, I’m aware of the contradiction with the points I made above."
  },
  {
    "objectID": "blog/posts/2024-07-10-2024-07-hex-update/index.html#a-snapshot-of-how-news-directors-view-the-use-of-over-the-top-ott-services-and-nextgen-tv",
    "href": "blog/posts/2024-07-10-2024-07-hex-update/index.html#a-snapshot-of-how-news-directors-view-the-use-of-over-the-top-ott-services-and-nextgen-tv",
    "title": "The Hex Update: June, 2024",
    "section": "A snapshot of how news directors view the use of over-the-top (OTT) services and NextGen TV",
    "text": "A snapshot of how news directors view the use of over-the-top (OTT) services and NextGen TV\nA recent survey report by RTDNA and the Syracuse University Newhouse School of Public Communications provides a snapshot on how broadcasters are using over-the-top services (OTT) and NextGen TV (ATSC 3.0).\nNextGen TV promises many enhancements for both consumers (i.e., better sound and video quality) and broadcasters (i.e., interactivity and addressibility), which is due to the combining of internet and television technologies. Although these enhancements make the future of TV exciting, the report makes a pretty blunt assertion:\n\nBut there’s a big difference between “operating” and actually doing something meaningful with [NextGen TV].\n\nSo how many broadcasters report doing something with NextGen TV? According to the report, only 20.9% of TV news directors say they are doing “something” with NextGen TV, which is slightly down from last year (25%). What about OTT services? According to survey respondents, 59.9% say they are “doing something” with OTT services. When it comes to all TV, the report states OTT is allowing broadcasters to ‘reach new audiences’ (70.1%), ‘go deeper with content’ (57.1%), and ‘make extra revenue’ (36.4%). The report includes a further breakdown by market size, which provides some more relevant context (i.e., broadcasters operating in smaller markets may not have the resources to manage their own OTT services).\n\nHere’s a link to go deeper\n\nLocal TV explores OTT, NextGen, AI\n\n\n\nWhat’s the takeaway?\nAlthough NextGen TV is purported to provide many enhancements, adoption faces many hurdles. Many of which I myself–from a technology, tv viewer adoption, and regulatory standpoint–don’t fully understand. However, the report makes it clear: the majority of news directors have yet to do anything meaningful with NextGen TV. This certainly isn’t a criticism of the technology or about what it promises for audiences and broadcasters in the future. Rather it’s a question of whether the technology can overcome the obstacles it faces, be adopted by TV consumers, and become a technology utilized to deliver services.\nAs for OTT, these services are being utilized by a majority of broadcasters, especially to reach new audiences. Despite ‘making extra revenue’ being in the top 3, it was surprising to see the limited role OTT services play within broadcasters’ business models. I assumed this percentage would be much higher than 36.4%. Perhaps new revenue models will be created and explored in the future, resulting in OTT services playing a greater role in revenue generating functions."
  },
  {
    "objectID": "blog/posts/2024-07-10-2024-07-hex-update/index.html#audience-segmentation-dashboard",
    "href": "blog/posts/2024-07-10-2024-07-hex-update/index.html#audience-segmentation-dashboard",
    "title": "The Hex Update: June, 2024",
    "section": "Audience segmentation dashboard",
    "text": "Audience segmentation dashboard\nOne critical function for media organizations is to identify and understand their target audiences. According to researchers from the Northwestern University Spiegel Research Center, this process tends to be more gut-driven rather than data-driven. To aid this process and make it more data-driven, Jaewon Royce Choi created a dashboard that maps audience segments based on population data from various zip codes. It provides a useful interface to explore the various types of audiences reached when targeting different geographic locations. Check it out if you’re a community focused media organization who wants to make more data-driven audience targeting decisions.\n\nHere’s a link to go deeper\n\nSpiegel Audience Segment Dashboard\n\n\n\nWhat’s the takeaway?\nThe identification and exploration of target audiences can certainly be more data driven, if not at least more data informed. This tool is useful in this case; it makes data accessible to media organizations to allow for audience targeting decisions to be more data driven. Check it out and explore the communities your organization serves.\nAside from being a useful tool to explore various audience segments, this was a great reminder that a plethora of public data is available to make more data driven decisions when it comes to audience. Products just need to be developed to make this data more accessible to practitioners."
  },
  {
    "objectID": "blog/posts/2024-07-10-2024-07-hex-update/index.html#are-audiences-worn-out-by-the-news",
    "href": "blog/posts/2024-07-10-2024-07-hex-update/index.html#are-audiences-worn-out-by-the-news",
    "title": "The Hex Update: June, 2024",
    "section": "Are audiences worn out by the news?",
    "text": "Are audiences worn out by the news?\nA recent study released by the Pew Research Center explored how American’s get their news on various social media platforms. Although the study reports American’s news consumption on various social media platforms (e.g., Facebook, Instagram, X, and TikTok), I found the results on how worn out news consumers are with news on social media platforms informative. Specifically,\n\nMore than half of news consumers on three of the four sites studied at least sometimes feel worn out by the amount of news they see on these sites.\n\nThe report goes further by breaking down these results by each platform.\n\nHere’s a link to go deeper\n\nHow Americans Get News on TikTok, X, Facebook and Instagram\n\n\n\nWhat’s the takeawy?\nMy initial reaction to this finding was to posit additional questions. What are the factors leading to this feeling of being worn out by the news on these platforms? Is it due to the amount of news users are confronted with? Is it a function of the type of news or how it is being produced? Or, does misinformation and disinformation play a role? I certainly don’t have the answers to these questions. However, publishers may want to consider factors they can control when publishing their content on these platforms. Moreover, publishers may also want to explore these questions with their audiences."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "",
    "text": "Image generated using the prompt “volleyball analytics in a pop art style” with the Bing Image Creator\nThe initial rounds of the NCAA women’s volleyball tournament have just begun. As such, I felt it was a good opportunity to understand more about the game while learning to specify models using Big Ten women’s volleyball match data and the tidymodels framework. This post sought to specify a predictive model of wins and loses. It then used this model to explore and predict match outcomes of the #1 team going into the tournament, the Nebraska Cornhuskers.\nThis post overviews the use of the tidymodels framework to fit and train predictive models. Specifically, it aims to be an introductory tutorial on the use of tidymodels to split data into test and training sets, specify a model, and assess model fit using both the training and testing data. To do this, I explored the fit of two binary classification models to NCAA Big Ten women’s volleyball match data, with the goal to predict wins and loses.\nBeing a high-level overview, this post will not cover topics like feature engineering, resampling techniques, hyperparameter tuning, or ensemble methods. Most assuredly, additional modeling procedures would lead to improved model predictions. As such, I plan to write future posts overviewing these topics.\nLet’s attach the libraries we’ll need for the session.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(here)\nlibrary(glue)\nlibrary(rpart.plot)\nlibrary(patchwork)\ntidymodels_prefer()"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#feature-exploration",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#feature-exploration",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Feature exploration",
    "text": "Feature exploration\nGiven the number of features in the data, we can easily obtain summary information using skimr::skim().\n\nskim(data_vball_train)\n\n\nData summary\n\n\nName\ndata_vball_train\n\n\nNumber of rows\n1243\n\n\nNumber of columns\n27\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nfactor\n1\n\n\nnumeric\n23\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nteam_name\n0\n1\n13\n25\n0\n14\n0\n\n\nopponent\n0\n1\n3\n40\n0\n329\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2021-01-22\n2023-11-25\n2022-09-09\n228\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nw_l\n0\n1\nFALSE\n2\nwin: 701, los: 542\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nset_wins\n0\n1\n2.03\n1.22\n0.00\n1.00\n3.00\n3.00\n3.00\n▃▂▁▂▇\n\n\nset_loss\n0\n1\n1.65\n1.31\n0.00\n0.00\n2.00\n3.00\n3.00\n▆▃▁▂▇\n\n\ns\n0\n1\n3.68\n0.76\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▅▁▃\n\n\nkills\n0\n1\n47.02\n11.49\n15.00\n39.00\n46.00\n55.00\n84.00\n▁▆▇▅▁\n\n\nerrors\n0\n1\n18.86\n6.57\n3.00\n14.00\n19.00\n23.00\n44.00\n▃▇▇▂▁\n\n\ntotal_attacks\n0\n1\n125.31\n30.19\n63.00\n101.00\n121.00\n148.00\n237.00\n▃▇▆▂▁\n\n\nhit_pct\n0\n1\n0.23\n0.09\n-0.10\n0.17\n0.23\n0.30\n0.54\n▁▃▇▅▁\n\n\nassists\n0\n1\n43.23\n10.84\n15.00\n36.00\n43.00\n50.50\n75.00\n▂▆▇▃▁\n\n\naces\n0\n1\n5.18\n2.86\n0.00\n3.00\n5.00\n7.00\n18.00\n▅▇▃▁▁\n\n\nserr\n0\n1\n7.97\n3.41\n1.00\n5.00\n8.00\n10.00\n23.00\n▅▇▅▁▁\n\n\ndigs\n0\n1\n51.55\n15.14\n16.00\n40.00\n49.00\n61.00\n108.00\n▂▇▅▂▁\n\n\nblock_solos\n0\n1\n1.64\n1.62\n0.00\n1.00\n1.00\n2.00\n12.00\n▇▂▁▁▁\n\n\nblock_assists\n0\n1\n14.25\n6.78\n0.00\n10.00\n14.00\n18.00\n38.00\n▂▇▆▂▁\n\n\nopp_kills\n0\n1\n43.64\n15.01\n0.00\n34.00\n44.00\n54.00\n84.00\n▁▃▇▆▁\n\n\nopp_errors\n0\n1\n19.34\n7.05\n0.00\n15.00\n20.00\n24.00\n46.00\n▁▆▇▂▁\n\n\nopp_total_attacks\n0\n1\n121.76\n36.90\n0.00\n100.00\n119.00\n148.00\n237.00\n▁▂▇▅▁\n\n\nopp_hit_pct\n0\n1\n0.19\n0.10\n-0.14\n0.13\n0.20\n0.25\n0.50\n▁▃▇▅▁\n\n\nopp_assists\n0\n1\n40.20\n13.99\n0.00\n31.00\n41.00\n50.00\n75.00\n▁▃▇▆▁\n\n\nopp_aces\n0\n1\n4.56\n2.91\n0.00\n2.00\n4.00\n6.00\n14.00\n▆▇▆▂▁\n\n\nopp_serr\n0\n1\n7.58\n3.66\n0.00\n5.00\n7.00\n10.00\n23.00\n▃▇▃▁▁\n\n\nopp_digs\n0\n1\n48.99\n17.90\n0.00\n38.00\n48.00\n60.00\n108.00\n▁▆▇▃▁\n\n\nopp_block_solos\n0\n1\n1.43\n1.44\n0.00\n0.00\n1.00\n2.00\n12.00\n▇▂▁▁▁\n\n\nopp_block_assists\n0\n1\n12.71\n7.30\n0.00\n8.00\n12.00\n18.00\n40.00\n▆▇▃▁▁\n\n\n\n\n\nA few things to note from the initial exploratory data analysis:\n\nTeam errors, attacks, and digs distribution exhibits a slight right skew.\nAces, service errors, block solos, opponent aces, opponent errors, opponent block solos, and opponent block assists exhibit a greater degree of skewness to the right.\n\nAn argument could be made for further exploratory analysis of these variables, followed by some feature engineering. Although this additional work may improve our final predictive model, this post is a general overview of specifying, fitting, and assessing models using the tidymodels framework. I will thus not address these topics further. However, I intend to write a future post focusing on feature engineering using tidymodels’ recipes package."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#examine-correlations-among-features",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#examine-correlations-among-features",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Examine correlations among features",
    "text": "Examine correlations among features\nThe next step in the exploratory analysis is to identify the presence of any correlations among features. This can easily be done using functions from the corrr package. Specifically, the correlate() function calculates correlations among the various numeric features within our data. The output from the correlate() function is then passed to the autoplot() method, which outputs a visualization of the correlations values.\n\ndata_vball_train |&gt; \n  correlate() |&gt;\n  corrr::focus(-set_wins, -set_loss, -s, mirror = TRUE) |&gt;\n  autoplot(triangular = \"lower\")\n\nNon-numeric variables removed from input: `date`, `team_name`, `opponent`, and `w_l`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\nThe plot indicates correlations of varying degrees among features. Feature engineering and feature reduction approaches could be used to address these correlations. However, these approaches will not be explored in this post."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#specify-our-models",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#specify-our-models",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Specify our models",
    "text": "Specify our models\nTo keep things simple, I’ll explore the fit of two models to the training data. However, tidymodels has interfaces to fit a wide-range of models, many of which are implemented via the parsnip package.\nThe models I intend to fit to our data include:\n\nA logistic regression using glm.\nA decision tree using rpart.\n\nWhen specifying a model with tidymodels, we do three things:\n\nUse parsnip functions to specify the mathematical structure of the model we intend to use (e.g., logistic_reg(); decision_tree()).\nSpecify the engine we want to use to fit our model. This is done using the set_engine() function.\nWhen required, we declare the mode of the model (i.e., is it regression or classification). Some models can perform both, so we need to explicitly set the mode with the set_mode() function.\n\nSpecifying the two models in this post looks like this:\n\n# Logistic regression specification\nlog_reg_spec &lt;- \n  logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n# Decision tree specification\ndt_spec &lt;- \n  decision_tree() |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nLet’s take a moment to breakdown what’s going on here. The calls to logistic_regression() and decision_tree() establishes the mathematical structure we want to use to fit our model to the data. set_engine(\"glm\") and set_engine(\"rpart\") specifies the model’s engine, i.e., the software we want to use to fit our model. For our decision tree, since it can perform both regression and classification, we specify it’s mode using set_mode(\"classification\"). You’ll notice our logistic regression specification excludes this function. This is because logistic regression is only used to perform classification, thus we don’t need to set its mode.\nIf you’re curious or want more information on what parsnip is doing in the background, you can pipe the model specification object to the translate() function. Here’s what the output looks like for our decision tree specification:\n\ndt_spec |&gt; translate()\n\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\nModel fit template:\nrpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\nIf you’re interested in viewing the types of engines available for your model, you can use parsnip’s show_engines() function. Here you’ll need to pass a string character of the model function you want to explore as an argument. This is what this looks like for logistic_reg():\n\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#create-workflows",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#create-workflows",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Create workflows",
    "text": "Create workflows\nFrom here, we’ll create workflow objects using tidymodel’s workflow package. Workflow objects make it easier to work with different modeling objects by combining objects into one object. Although this isn’t too important for our current modeling task, the use of workflows will be beneficial later when we attempt to improve upon our models, like I’ll do in future posts.\nIn this case, our model specification and model formula are combined into a workflow object. Here I just choose a few features to include within the model. For this post, I mainly focused on using team oriented features within our model to predict wins and losses. Indeed, others could have been included, as the data also contained opponent oriented statistics. To keep things simple, however, I chose to only include the following features within our model:\n\nHitting percentage\nErrors\nBlock solos\nBlock assists\nDigs\n\nThe workflow() function sets up the beginning of our workflow object. We’ll add the model object with add_model(), followed by the formula object using add_formula().\n\nlog_reg_wflow &lt;-\n  workflow() |&gt;\n  add_model(log_reg_spec) |&gt;\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n\ndt_wflow &lt;- \n  workflow() |&gt;\n  add_model(dt_spec) |&gt;\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n\nThis syntax can be a bit long, so there’s a shortcut. We can pass both the model formula and the model specification as arguments to the workflow() function instead of using a piped chain of functions.\n\nlog_reg_wflow &lt;- \n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    log_reg_spec\n  )\n\ndt_wflow &lt;-\n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    dt_spec\n  )"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#fit-our-models",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#fit-our-models",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Fit our models",
    "text": "Fit our models\nNow with our models specified, we can go about fitting our model to the training data using the fit() method. We do the following to fit both models to the training data:\n\nlog_reg_fit &lt;- log_reg_wflow |&gt; fit(data = data_vball_train)\n\ndt_fit &lt;- dt_wflow |&gt; fit(data = data_vball_train)\n\nLet’s take a look at the log_reg_fit and dt_fit fit objects.\n\nlog_reg_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────────────────────────\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n── Model ───────────────────────────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:      1703 \nResidual Deviance: 874.4    AIC: 886.4\n\n\n\ndt_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────────────────────────\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n── Model ───────────────────────────────────────────────────────────────────────────────────────────\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct&lt; 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists&lt; 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists&gt;=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct&lt; 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct&gt;=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs&lt; 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs&gt;=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct&gt;=0.2255 642  81 win (0.1261682 0.8738318) *\n\n\nWhen the fit objects are called, tidymodels prints information about our fitted models to the console. First, we get notified this object is a trained workflow. Second, preprocessing information is included. Since we only set a model function during preprocessing, we only see the model formula printed in this section. Lastly, tidymodels outputs model specific information and summary information about the model fit.\n\nExplore the fit\nNow that we have the fit object, we can obtain more information about the fit using the extract_fit_engine() function.\n\nlog_reg_fit |&gt; extract_fit_engine()\n\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:      1703 \nResidual Deviance: 874.4    AIC: 886.4\n\n\n\ndt_fit |&gt; extract_fit_engine() \n\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct&lt; 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists&lt; 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists&gt;=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct&lt; 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct&gt;=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs&lt; 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs&gt;=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct&gt;=0.2255 642  81 win (0.1261682 0.8738318) *\n\n\nThe output when passing the fit object to the extract_fit_engine() is similar to what was printed when we called the fit object alone. However, the extract_* family of workflow functions are great for extracting elements of a workflow. According to the docs (?extract_fit_engine), this family of functions are helpful when accessing elements within the fit object. This is especially helpful when needing to pass along elements of the fit object to generics like print(), summary(), and plot().\n\n# Not evaluated to conserve space, but I encourage\n# you to run it on your own\nlog_reg_fit |&gt; extract_fit_engine() |&gt; plot()\n\nAlthough extract_* functions afford convenience, the docs warn to avoid situations where you invoke a predict() method on the extracted object. Specifically, the docs state:\n\nThere may be preprocessing operations that workflows has executed on the data prior to giving it to the model. Bypassing these can lead to errors or silently generating incorrect predictions.\n\nIn other words,\n\n# BAD, NO NO\nlog_reg_fit |&gt; extract_fit_engine() |&gt; predict(new_data)\n\n# Good\nlog_reg_fit |&gt; predict(new_data)\n\nThe fit object can also be passed to other generics, like broom::tidy(). The general tidy() method, when passed a fit object, is useful to view and use the coefficients table from the logistic regression model.\n\ntidy(log_reg_fit)\n\n# A tibble: 6 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -8.55     0.769     -11.1   1.10e-28\n2 hit_pct        30.1      2.09       14.4   5.50e-47\n3 errors         -0.0128   0.0212     -0.603 5.46e- 1\n4 block_solos     0.200    0.0515      3.89  1.02e- 4\n5 block_assists   0.0876   0.0137      6.39  1.64e-10\n6 digs            0.0137   0.00703     1.96  5.05e- 2\n\n\nBeyond summarizing the model with the coefficients table, we can also create some plots from the model’s predictions from the training data. Here we need to use the augment() function. Later, we’ll explore this function in more depth when we calculate assessment metrics. For now, I’m using it to obtain the prediction estimates for winning.\n\ndata_vball_aug &lt;- augment(log_reg_fit, data_vball_train)\n\nWith this data, we can visualize these prediction estimates with the various features used within the model. Since we’re creating several visualizations using similar code, I created a plot_log_mdl() function to simplify the plotting. Lastly, I used the patchwork package to combine the plots into one visualization. Below is the code to create these visualizations.\n\nplot_log_mdl &lt;- function(data, x_var, y_var, color) {\n  ggplot() +\n    geom_point(\n      data = data_vball_aug, \n      aes(x = {{ x_var }}, y = {{ y_var }}, color = {{ color }}),\n      alpha = .4\n    ) +\n    geom_smooth(\n      data = data_vball_aug,\n      aes(x = {{ x_var }}, y = {{ y_var }}),\n      method = \"glm\", \n      method.args = list(family = \"binomial\"),\n      se = FALSE\n    ) +\n    labs(color = \"\") +\n    theme_minimal()\n}\n\n\nplot_hit_pct &lt;- \n  plot_log_mdl(data_vball_aug, hit_pct, .pred_win, w_l)\n\nplot_errors &lt;- \n  plot_log_mdl(data_vball_aug, errors, .pred_win, w_l)\n\nplot_block_solos &lt;- \n  plot_log_mdl(data_vball_aug, block_solos, .pred_win, w_l) +\n  scale_x_continuous(labels = label_number(accuracy = 1))   \n\nplot_block_assists &lt;- \n  plot_log_mdl(data_vball_aug, block_assists, .pred_win, w_l)\n\nplot_digs &lt;-\n  plot_log_mdl(data_vball_aug, digs, .pred_win, w_l)\n\nwrap_plots(\n  plot_hit_pct, \n  plot_errors,\n  plot_block_solos, \n  plot_block_assists,\n  plot_digs, \n  guides = \"collect\"\n) & theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nTo summarise our decision tree, we need to use the rpart.plot package to create a plot of the tree. The code to do this looks like this:\n\ndt_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(roundint = FALSE)\n\n\n\n\n\n\n\n\nBefore transitioning to model assessment, let’s explore the predictions for both models using the augment() function again. According to the docs,\n\nAugment accepts a model object and a dataset and adds information about each observation in the dataset.\n\naugment() produces new columns from the original data set to which makes it easy to examine model predictions. For instance, we can create a data set with the .pred_class, .pred_win, and .pred_loss columns. augment() also makes a guarantee that a tibble with the same number of rows as the passed data set will be returned, and all new column names will be prefixed with a ..\nHere we’ll pipe the tibble returned from augment() to the relocate() function. This will make it easier to view the variables we are interested in further examining by moving these columns to the left of the tibble.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n\n# A tibble: 1,243 × 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 loss  loss          0.0616       0.938 2021-01-29 Illinois Figh… Wiscons…        0        3     3\n 2 loss  loss          0.297        0.703 2021-01-30 Illinois Figh… Wiscons…        1        3     4\n 3 loss  loss          0.138        0.862 2021-02-06 Illinois Figh… @ Penn …        2        3     5\n 4 loss  loss          0.0572       0.943 2021-02-19 Illinois Figh… Ohio St.        1        3     4\n 5 loss  loss          0.199        0.801 2021-02-20 Illinois Figh… Ohio St.        2        3     5\n 6 loss  loss          0.0787       0.921 2021-03-05 Illinois Figh… Nebraska        0        3     3\n 7 loss  loss          0.0281       0.972 2021-03-06 Illinois Figh… Nebraska        0        3     3\n 8 loss  win           0.636        0.364 2021-03-12 Illinois Figh… @ Minne…        2        3     5\n 9 loss  loss          0.00129      0.999 2021-03-13 Illinois Figh… @ Minne…        0        3     3\n10 loss  loss          0.0114       0.989 2021-04-02 Illinois Figh… @ Purdue        0        3     3\n# ℹ 1,233 more rows\n# ℹ 20 more variables: kills &lt;dbl&gt;, errors &lt;dbl&gt;, total_attacks &lt;dbl&gt;, hit_pct &lt;dbl&gt;,\n#   assists &lt;dbl&gt;, aces &lt;dbl&gt;, serr &lt;dbl&gt;, digs &lt;dbl&gt;, block_solos &lt;dbl&gt;, block_assists &lt;dbl&gt;,\n#   opp_kills &lt;dbl&gt;, opp_errors &lt;dbl&gt;, opp_total_attacks &lt;dbl&gt;, opp_hit_pct &lt;dbl&gt;,\n#   opp_assists &lt;dbl&gt;, opp_aces &lt;dbl&gt;, opp_serr &lt;dbl&gt;, opp_digs &lt;dbl&gt;, opp_block_solos &lt;dbl&gt;,\n#   opp_block_assists &lt;dbl&gt;\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n\n# A tibble: 1,243 × 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 loss  loss            0.131      0.869 2021-01-29 Illinois Figh… Wiscons…        0        3     3\n 2 loss  loss            0.131      0.869 2021-01-30 Illinois Figh… Wiscons…        1        3     4\n 3 loss  loss            0.131      0.869 2021-02-06 Illinois Figh… @ Penn …        2        3     5\n 4 loss  loss            0.131      0.869 2021-02-19 Illinois Figh… Ohio St.        1        3     4\n 5 loss  loss            0.131      0.869 2021-02-20 Illinois Figh… Ohio St.        2        3     5\n 6 loss  loss            0.131      0.869 2021-03-05 Illinois Figh… Nebraska        0        3     3\n 7 loss  loss            0.131      0.869 2021-03-06 Illinois Figh… Nebraska        0        3     3\n 8 loss  win             0.603      0.397 2021-03-12 Illinois Figh… @ Minne…        2        3     5\n 9 loss  loss            0.131      0.869 2021-03-13 Illinois Figh… @ Minne…        0        3     3\n10 loss  loss            0.131      0.869 2021-04-02 Illinois Figh… @ Purdue        0        3     3\n# ℹ 1,233 more rows\n# ℹ 20 more variables: kills &lt;dbl&gt;, errors &lt;dbl&gt;, total_attacks &lt;dbl&gt;, hit_pct &lt;dbl&gt;,\n#   assists &lt;dbl&gt;, aces &lt;dbl&gt;, serr &lt;dbl&gt;, digs &lt;dbl&gt;, block_solos &lt;dbl&gt;, block_assists &lt;dbl&gt;,\n#   opp_kills &lt;dbl&gt;, opp_errors &lt;dbl&gt;, opp_total_attacks &lt;dbl&gt;, opp_hit_pct &lt;dbl&gt;,\n#   opp_assists &lt;dbl&gt;, opp_aces &lt;dbl&gt;, opp_serr &lt;dbl&gt;, opp_digs &lt;dbl&gt;, opp_block_solos &lt;dbl&gt;,\n#   opp_block_assists &lt;dbl&gt;"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#model-assessment",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#model-assessment",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Model assessment",
    "text": "Model assessment\nSince we’re fitting a binary classification model, we will use several measurements to assess model performance. Many of these measurements can be calculated using functions from the yardstick package. To start, we can calculate several measurements using the hard class predictions: a confusion matrix; accuracy; specificity; ROC curves; etc.\n\nCreate a confusion matrix\nFirst, let’s start by creating a confusion matrix. A confusion matrix is simply a cross-tabulation of the observed and predicted classes, and it summarizes how many times the model predicted a class correctly vs. how many times it predicted it incorrectly. The calculation of the table is pretty straight forward for a binary-classification model. The yardstick package makes it easy to calculate this table with the conf_mat() function.\nconf_mat()’s two main arguments are truth and estimate. truth pertains to the column containing the true class predictions (i.e., what was actually recorded). The estimate is the name of the column containing the discrete class prediction (i.e., the prediction made by the model).\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class)\n\n          Truth\nPrediction loss win\n      loss  438  89\n      win   104 612\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class)\n\n          Truth\nPrediction loss win\n      loss  411  64\n      win   131 637\n\n\nThe conf_mat() also has an autoplot() method. This makes it easier to visualize the confusion matrix, either as a mosaic plot or a heatmap.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class) |&gt;\n  autoplot(type = \"mosaic\") \n\n\n\n\n\n\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\n\n\n\n\nA few things to note from the confusion matrices created from our two models:\n\nThe logistic regression does well predicting wins and losses, though it slightly over predicts wins in cases of losses and losses in cases of wins. However, prediction accuracy is pretty balanced.\nThe decision tree does better reducing cases where it predicts a loss when a win occurred, but it predicted more wins when a loss took place. Thus, the decision tree model seems fairly optimistic when it comes to predicting wins when a loss occurred.\n\nAfter examining the confusion matrix, we can move forward with calculating some quantitative summary metrics from the results of the confusion matrix, which we can use to better compare the fit between the two models.\n\n\nMeasure model accuracy\nOne way to summarize the confusion matrix is to calculate the proportion of data that is predicted correctly, also known as accuracy. yardstick’s accuracy() function simplifies this calculation for us. Again, we just pipe our augment() function to the accuracy() function, and we specify which column is the truth and which is the estimate class prediction from the model.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  accuracy(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.845\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  accuracy(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.843\n\n\nWhen it comes to accuracy, both models are fairly similar in their ability to predict cases correctly. The logistic regression’s accuracy is slightly better, though.\n\n\nMeasure model sensitivity and specificity\nSensitivity and specificity are additional assessment metrics we can calculate. Sensitivity in this case is the percentage of matches that were wins that were correctly identified by the model. Specificity is the percentage of matches that were losses that were correctly identified by the model. The @StatQuest YouTube channel has a good video breaking down how these metrics are calculated.\nyardstick makes it easy to calculate these metrics with the sensitivity() and specificity() functions. As we did with calculating accuracy, we pipe the output of the augment() function to the sensitivity() function. We also specify the column that represents the true values to the truth argument, then pass the class predictions made by the model to the estimate argument. This looks like the following for both our logistic regression and decision tree models:\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  sensitivity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.808\n\n\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  specificity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.873\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  sensitivity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.758\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  specificity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.909\n\n\nA few things to note:\n\nThe logistic regression (sensitivity = 80.8%) was much better at predicting matches that were wins than the decision tree model (sensitivity = 75.8%).\nThe decision tree was much better at identifying losses, though (90.9% vs. 87.3%).\n\n\nSimplify metric calculations with metric_set()\nAlthough the above code provided the output we were looking for, we can simplify our code by using yardstick’s metric_set() function. Inside metric_set() we specify the different metrics we want to calculate for each model.\n\nvball_mdl_metrics &lt;- \n  metric_set(accuracy, sensitivity, specificity)\n\nThen we do as before, pipe the output from augment() to our metric set object vball_mdl_metrics, and specify the column that represents the truth and the column that represents the model’s class prediction. Here’s what this looks like for both our models:\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.845\n2 sensitivity binary         0.808\n3 specificity binary         0.873\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.843\n2 sensitivity binary         0.758\n3 specificity binary         0.909\n\n\nNow it’s much easier to make comparisons, and we write less code for the same amount of information. A big win!\n\n\n\nROC curves and AUC estimates\nReceiver operating characteristic (ROC) curves visually summarise classification model specificity and sensitivity using different threshold values. From this curve, an area under the curve (AUC) metric can be calculated. The AUC is a useful summary metric and can be used to compare the fit of two or more models. Again, @StatQuest has a pretty good video explaining the fundamentals of ROC curves and AUC estimates.\nBeing a useful way to summarise model performance, the yardstick package makes several functions available to calculate both the ROC curve and AUC metric. An autoplot() method is also available to easily plot the ROC curve for us.\nLet’s take a look at how this is done with our logistic regression model. Here’s the code:\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss)\n\n# A tibble: 1,245 × 3\n     .threshold specificity sensitivity\n          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf             0                 1\n 2    0.0000835     0                 1\n 3    0.000181      0.00143           1\n 4    0.000405      0.00285           1\n 5    0.000415      0.00428           1\n 6    0.000496      0.00571           1\n 7    0.000824      0.00713           1\n 8    0.000839      0.00856           1\n 9    0.000922      0.00999           1\n10    0.000979      0.0114            1\n# ℹ 1,235 more rows\n\n\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_auc(truth = w_l, .pred_loss)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.920\n\n\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nYou’ll likely notice the syntax is pretty intuitive. You’ll also notice the code is similar to our other model performance metric calculations. First we use augment() to create the data we need. Second, we pipe the output of the augment() function to either the roc_curve() or roc_auc() function. The roc_curve() function calculates the ROC curve values and returns a tibble, which we will later pipe to the autoplot() method. The roc_auc() function calculates the area under the curve metric.\nSince we’re comparing two models, we perform these steps again for the decision tree model.\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss)\n\n# A tibble: 7 × 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1   -Inf           0          1     \n2      0.126       0          1     \n3      0.397       0.800      0.851 \n4      0.714       0.909      0.758 \n5      0.869       0.914      0.740 \n6      0.875       0.994      0.0517\n7    Inf           1          0     \n\n\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_auc(truth = w_l, .pred_loss)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.864\n\n\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\nA few notes from comparing the ROC curve and AUC metrics:\n\nThe AUC indicates a better model fit across different thresholds for the logistic regression model (AUC = .920) vs. the decision tree (AUC = .864).\nWhen visually examining the ROC curves for both models, it seems the logistic regression model is a better fitting model for the data."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#extract-the-final-workflow",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#extract-the-final-workflow",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Extract the final workflow",
    "text": "Extract the final workflow\nOnce the final candidate model is identified, we can extract the final workflow using the hardhat package’s extract_workflow() function. Here we’ll use this workflow object to make predictions, but this workflow object is also useful if you intend to deploy this model.\n\nfinal_fit_wflow &lt;- extract_workflow(final_log_reg_fit)"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#make-predictions",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#make-predictions",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Make predictions",
    "text": "Make predictions\nAt this point in the season, let’s see how the Nebraska women’s volleyball team stacked up in several of their matches using our model. First, let’s examine Nebraska’s win against Wisconsin, a five set thriller.\n\nwisc_mtch_one &lt;- data_vball |&gt; \n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-10-21\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_one)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 loss       \n\n\nAccording to our model, Nebraska should have lost this match. This makes Nebraska’s win even more impressive. The grittiness to pull out a win, even when evidence suggests they shouldn’t have, speaks volumes of this team. Indeed, wins and losses for volleyball matches are a function of many different factors. Factors that may not be fully captured by the data or this specific model.\nWhat about Nebraska’s 0-3, second match loss against Wisconsin?\n\nwisc_mtch_two &lt;- data_vball |&gt; \n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-24\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_two)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 loss       \n\n\nNo surprise, the model predicted Nebraska would lose this match. It’s a pretty steep hill to climb when you hit a .243 and only have 5 total blocks.\nAnother nail-biter was Nebraska’s second match against Penn State. Let’s take a look at what the model would predict.\n\npenn_state &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-03\"))\n\npredict(final_fit_wflow, new_data = penn_state)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nEven though the match was close, the model predicted Nebraska would win this match. It may have been a nail-biter to watch, but Nebraska played well enough to win the match, according to our model."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#the-ncaa-tournament-and-our-model",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#the-ncaa-tournament-and-our-model",
    "title": "Messing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "The NCAA tournament and our model",
    "text": "The NCAA tournament and our model\nWe’re through the initial rounds of the 2023 NCAA women’s volleyball tournament. Let’s look at a couple of scenarios for Nebraska using our final model.\n\n\n\n\n\n\nNote\n\n\n\nI’m extrapolating a bit here, since the data I’m using only includes Big Ten volleyball team matches. The NCAA tournament will include teams from many other conferences, so the predictions don’t fully generalize to tournament matches.\nWe could avert the extrapolation here by obtaining match data for all NCAA volleyball matches for the 2021, 2022, and 2023 seasons. For the sake of keeping this post manageable, I did not obtain this data.\n\n\nFirst, let’s just say Nebraska plays to up to their regular season average for hit percentage, errors, block solos, block assists, and digs in NCAA tournament matches. What does our model predict in regards to Nebraska winning or losing a match?\n\nseason_avg &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |&gt;\n  summarise(across(where(is.numeric), mean)) |&gt;\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = season_avg)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nIf Nebraska can hit at least a .290, commit less than 17 errors, have one solo block, have 16 block assists, and dig the ball roughly 48 times, then according to the model, they should win matches. Put another way, if Nebraska performs close to their regular season average for these statistics, then the model suggests they will win matches.\nThis is very encouraging, since the Huskers should be playing their best volleyball here at the end of the season. One would hope this means they perform near or better than their average in tournament matches.\nOne last scenario, let’s look at the low end of Nebraska’s performance this season. Specifically, let’s see what the model predicts if Nebraska will win or lose a match at the 25% quartile for these statistics.\n\nquantile_25 &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |&gt;\n  summarise(across(where(is.numeric), ~quantile(.x, .25))) |&gt;\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = quantile_25)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nAccording to the model, if Nebraska can perform up to their 25% quartile of their regular season statistics, the model suggests they should win matches. Matches like those in the NCAA tournament. So even if Nebraska doesn’t perform to their potential or just has an off match, they should win if they can at least achieve the 25% quartile of their regular season statistics.\n\n“All models are wrong, but some are useful.”\n- George Box\n\nAgain, many factors determine if a team wins or loses a match in volleyball (see the model’s prediction for Nebraska’s first match against Wisconsin). This is just one, simple model aimed at predicting wins and losses based on hit percentage, errors, block solos, block assists, and digs. A model that certainly could be improved."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "I’m a media research analyst, data enthusiast, and news, sports, and podcast aficianado.\nProfessionally, I use data, audience measurement, and marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. I am particularly interested in the use and development of open-source statistical software (i.e. R) to achieve this goal, and gaining a broader understanding of the role these tools play in media, digital, and marketing analytics. I also adjunct university courses on the side.\nListening to NPR, watching PBS (especially NOVA), and college football and baseball are my jam.\n\n\nWant to know more about what I’m currently working on, reading, or mastering? Check out the now page.\n\n\n\n\nPh.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota\n\n\n\n\n\nDigital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#now",
    "href": "index.html#now",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Want to know more about what I’m currently working on, reading, or mastering? Check out the now page."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Ph.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Digital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "now-old.html",
    "href": "now-old.html",
    "title": "Past Now Page Updates",
    "section": "",
    "text": "2024-12-14 update\n\nBurning it all down and building it back up, my Neovim setup\nIt was inevitable; I’m revamping my Neovim configuration. It started with a kickstarter, but now I’m looking to further customize. I’m also attempting to address some bugs that have creeped into my configuration. I’ve recently come to the conclusion that I need to burn everything down and start anew. Alas, I’m not weary. It’s another great learning experience.\n\n\nAttended Posit::conf(2023) Chicago\nI was fortunate to attend this year’s Posit conference in Chicago, a five day conference with workshops and speaker sessions. I’m still sorting through all that I learned. Here are some of the highlights:\n\nAttending the Introduction to Tidymodels workshop\nAttending the Package Development Masterclass workshop\nAll the keynote speakers, especially J.D. Long’s It’s Abstractions All the Way Down talk.\nNetworking and meeting some folks from the R4DS Online Learning Community (in person).\n\n\n\nExperimenting more with Tableau and plotly\n\nIf the only tool you have is a hammer, it is tempting to treat everything as if it were a nail.\n- Abraham Maslow\n\nAlthough I prefer to do the majority of my analysis and visualization work using code based tools like R, I’m attempting to broaden my experience by experimenting more with different BI tools. This includes Tableau. I’ve also been experimenting with creating interactive data visualizations with plotly. To develop these skills, I’ve been contributing to the #tidytuesday social data project. Check out my recent blog posts to view my contributions. If you’re interested in what I’ve created using Tableau, check out my public profile.\n\n\n\n2024-03-17 update\n\nWorking on a 30 day tidymodels recipes package challenge\nI’ve been developing my machine learning and modeling skills. Specifically, I’ve been focusing on learning the various steps to preprocess data for modelling and feature engineering. This includes becoming more proficient with the tidymodels recipes package. Check out my post to keep up with the latest.\n\n\nContinuing to experiment with Neovim\nI’ve heavily leaned into using Neovim for most of my workflows. During this transition, I’ve come across some really useful tools, like telescope, harpoon, and vim-fugitive. I’m trying to learn as much as I can, as I still don’t know if my configuration is correct … lol\n\n\n\n2024-01-15 update\n\nIt’s a Python summer\nThis summer I’ve been focusing on developing my Python programming skills. I have a pretty good handle of R, so I felt it was time to learn Python. This started with learning Python’s basic data types, struggling through understanding Numpy arrays, getting a handle on the extensive use cases of the pandas library, and learning how to manage environments using conda. I’m aiming to write more blog posts focused in this space to document what I’m learning.\n\n\n\nPre 2024-01-15 updates\n\nProject Conduit\nCurrently developing and maintaining a data pipeline project built using R and Python. Technology utilized includes Google Cloud resources, Docker, Apache Airflow, Google BigQuery, Google Analytics, Google Data Studio, and Shiny. The goal is to centralize and automate data processing, storage, and reporting.\n\n\nR for Data Science Online Learning Community book club facilitator\nRecently started facilitating an R for Data Science Online Learning Community online book club (check it out by joining the Slack workspace). This group is currently reading through Hadley Wickham’s Advanced R book. The group meets weekly online over Zoom. Meetings are open to anyone who is a part of the Slack group (Join the #book_club-advr channel to keep up with the book club). Check out the playlist of past meeting recordings here. I would love for more to join and be a part of this group.\n\n\nExperimenting with Neovim\nI’ve been experimenting more and more with Neovim for my development work. I’m becoming more comfortable with the different modes, movements, actions, and various tools for editing text and code. Still struggling through the configuration and plugin ecosystem to set up workflows that are the most productive. Going through this process has been a challenge, but has me really reflecting on how I approach my work, evaluating what is needed, not needed, and focusing on the bad habits I need to break."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Keeping public media RAD: Leveraging R for audience research and business intelligence … along with other things\nCollin K. Berke, Ph.D."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#hi-im-collin",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#hi-im-collin",
    "title": "Collin K. Berke, Ph.D.",
    "section": "👋 Hi, I’m Collin",
    "text": "👋 Hi, I’m Collin\n\n   \n\nMedia Research Analyst at Nebraska Public Media (member station of PBS & NPR).\nGoal is to use data to answer questions on how to best reach and engage audiences.\nDeveloping open source tools and utilizing cloud computing resources to achieve this goal."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#keeping-public-media-rad",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#keeping-public-media-rad",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Keeping public media RAD 😎",
    "text": "Keeping public media RAD 😎\nMy team’s aim:\n\nReproducible\nAutomated\nDocumented and effectively communicated"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#how-does-my-group-stay-rad",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#how-does-my-group-stay-rad",
    "title": "Collin K. Berke, Ph.D.",
    "section": "How does my group stay RAD?",
    "text": "How does my group stay RAD?\n\nPublic media already has some rad content and services …\n\nCheck us out: NebraskaPublicMedia.org\n\nMy team also works on some pretty RAD projects:\n\nInternal R packages for audience and marketing research.\nAutomating our work with the Google Cloud Platform.\nDeveloping Shiny apps for business intelligence.\nUtilizing Quarto for reporting."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#teaching-r",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#teaching-r",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Teaching R",
    "text": "Teaching R\n\nPartnering with the College of Journalism and Mass Communication (COJMC) and Matt Waite\nTeach SPMC 350: Sports Data Visualization and Analysis\nGoal: Convert R beginners to R users\nIntroduce R &gt;&gt; Wrangling &gt;&gt; Analysis &gt;&gt; Visualization &gt;&gt; Publish original work online"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#data-science-learning-community",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#data-science-learning-community",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Data Science Learning Community",
    "text": "Data Science Learning Community\n\nA welcoming place to get help and learn more about data science.\nWe’re known for our book clubs.\nI’m an active member and facilitator.\nI’m also leading a cohort for R4DS (2e).\nCheck it out:\n\n\n\n\n\nWe just got done facilitating a Quarto website workshop"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-1",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-1",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Oh and one more thing …"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-2",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-2",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Analysis at the speed of thought 🚀"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#im-a-huge-modal-editing-nerd",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#im-a-huge-modal-editing-nerd",
    "title": "Collin K. Berke, Ph.D.",
    "section": "I’m a huge modal editing nerd",
    "text": "I’m a huge modal editing nerd\n\nNeovim (specifically LazyVim) or Emacs.\nIf you use any of these tools, let’s geek out about them.\n\n\n\n\n\n\nNeovim logo by Jason Long, CC BY 3.0"
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#connect-with-me",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#connect-with-me",
    "title": "Collin K. Berke, Ph.D.",
    "section": "Connect with me!",
    "text": "Connect with me!\n\nI want to meet everyone, but I’m interested in speaking with:\n\nR Users and R Developers leveraging R in business.\nPeople who teach R.\nAnyone trying to be RAD by using R.\n\nIf we don’t get a chance to meet in person, let’s connect online."
  },
  {
    "objectID": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-3",
    "href": "talks/2024_10_29_nebraska_rug_lightning_talk.html#section-3",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Collin K. Berke, Ph.D.\n\n\n👨‍💻 Follow me on GitHub: @collinberke\n👔 LinkedIn: collinberke\n📧 Email: collin.berke@gmail.com\n🔗 Blog: www.collinberke.com"
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html",
    "title": "Implementing a next and back button in Shiny",
    "section": "",
    "text": "Photo by John Barkiple"
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "title": "Implementing a next and back button in Shiny",
    "section": "The initial runtime variables",
    "text": "The initial runtime variables\nAs part of my testing of the actionButton() UI function, I found out the initial value being sent to the server was zero. I also found out that zero can’t be used for subsetting (i.e, nothing is gets returned to the UI). To address this issue, a variable with a reactive value of one needed to be in the environment upon runtime of the application. This is so we can use the initial value of one to return the first element of our data to our output$series in the textOutput() function in the UI when the application starts. Let’s take a look at this in action.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nYou’ll notice a new function here in the server, reactiveVal(). According to the documentation, this function is used to create a “reactive value” object within the app’s environment. Basically, I understand this function is just creating a reactive expression where the initial value is one upon the runtime of the application, which is then used in the subsetting operation applied in the renderText() function. Great, we have partly solved the indexing issue with the use of reactiveVal(1). You’ll also notice the buttons don’t work here because there is no dependency on them as an input, but I’ll get to that here shortly by applying some observeEvents() functions.\n\nThe maximum index value\nI also needed a solution to help limit the range of values that could be used for indexing in our subsetting operation. I now had the lower value one available in the environment, however I did not have the maximum value. At this point, I needed a function to calculate the length of the data and to treat it as a reactive expression, as this number might be dynamic in the larger application, and the users’ inputs will determine what data gets displayed within the application (e.g., filtering by product code selection). We can easily calculate the length of our data using the length() function and making this a reactive expression by wrapping it with the reactive() function. Here is what this looks like with code.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Determine the upper part of the subset index range\n  max_no_values &lt;- reactive(length(series))\n  \n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIt’s challenging to show this value in the environment in writing, but now given the current code, I have the lower value of the range, one, and the maximum value three corresponding to the number of values in our data structure available in the environment. This is great, so now I have those two values available to help with subsetting. At this point, we also need to incorporate the two user inputs, the Back and Next buttons. However, since we know these two buttons increment by one every time they are pressed, I need to rely on some mathematical operations to control the range of values used to subset the data. Given the simplified application, I know 1, 2, or 3 is the values and range of values I need to properly apply within a subsetting operation."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "title": "Implementing a next and back button in Shiny",
    "section": "Enter the %%",
    "text": "Enter the %%\nPart of getting this functionality to work required the use of the modulus %% and modular arithmetic. Basically, modulus is an arithmetic operation that performs a division and returns the remainder from the operation. I learned a lot about this in this article here (Busbee and Braunschweig n.d.). The R for Data Science book (Wickham and Grolemund 2017) also introduces the use of %% as well. While researching the modulus, I found many useful applications for it within programming. It’s definitely worth some more time learning of its other uses. When applied in our case, though, we needed it to keep the subsetting index within the bounds of the size of our data structure.\nI am far from a mathematician, so the following explanation of the logic behind how a modulus is applied here is going to be a little fast and loose. However, I’m going to take a crack at it. Take for example our application. On runtime, we have a reactive value place() that starts at the value one. We also know that our maximum number of values that can be used as an index for our subsetting operation is three, our max_no_values reactive (i.e., c(\"a\", \"b\", \"c\")). We can now use the modulus with these two values to limit the number we are using in the index of our subsetting based on the number of clicks by the user. Here is a simplified example using code illustrating this point.\n\nmax_no_values &lt;- 3\n\n# User clicks the button to increment the index of the subset\n# Vector corresponds to the value outputted by the `actionButton()`\nuser_clicks &lt;- c(0:12)\n\nuser_clicks %% max_no_values\n\n [1] 0 1 2 0 1 2 0 1 2 0 1 2 0\n\n\nEarlier in the post, we found out that we can’t use zero to subset, as nothing gets returned. So to solve our issue, we need to shift these values by adding one to the vector. Notice how that with every ‘click’ the range of these values never goes below one or exceeds three, even when a user’s click count (keep in mind every click of the actionButton() increments by one) goes above three. This is the power of the %%, as this operation keeps our index range between 1 - 3, regardless of how many times the user clicks an action button.\n\nuser_clicks %% max_no_values + 1\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3 1\n\n\nThe math is a little different for the Back button, though. However, the same principles apply.\n\n((user_clicks - 2) %% 3) + 1\n\n [1] 2 3 1 2 3 1 2 3 1 2 3 1 2\n\n\nLet’s use some print debugging here to show how the of %% works in action. I’m going to use the glue package to help make the messages sent to the console more human readable.\n\nlibrary(shiny)\nlibrary(glue)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Determine the total number \n  max_no_values &lt;- reactive(length(series))\n  \n  position &lt;- reactiveVal(1)\n  \n  # These cause a side-effect by changing the place value\n  observeEvent(input$forward, {\n    position((position() %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  observeEvent(input$back, {\n    position(((position() - 2) %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  output$series &lt;- renderText({\n    series[position()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIf you click the Back and Next button and watch your console, you’ll see the position value for every click being printed. While clicking these values, you will observe a couple of things:\n\nYou’ll notice the value zero is never passed as a subsetting index value.\nThe arithmetic operations constrain our subsetting values within a range of 1 - 3, the length of our character vector.\nMultiple clicks remain in order, regardless if the user clicks the Next or Back buttons (e.g., 1, 2, 3 or 3, 2, 1).\n\nAt this point, we can get rid of our print debugging code, test our working example, and bask in our accomplishment of understanding how this solution works. The next step is to now integrate what we know into the larger application. We’ll do that here in the next section of this post."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "title": "Implementing a next and back button in Shiny",
    "section": "Product selection",
    "text": "Product selection\nAs part of the original functionality of the app, users were given a selectInput() in the UI to filter for injuries that were the result of different products. The requirements stated the outputted narratives also needed to reflect the users’ filter selection. This functionality needed to be added back in, and it also needed to be reactive. I do this by adding the selected &lt;- reactive(injuries %&gt;% filter(prod_code == input$code)) near the beginning portion of the server section of the code. You’ll also notice we are using the filter() function and %&gt;% operator here, so we need to also bring in the dplyr package (i.e., library(dplyr)).\nThere are now two areas in the server that have a dependency on the selected() reactive expression, the max_no_stories() reactive and our output$narrative object. Since our reprex was using a simplified vector of data (e.g., c(\"a\", \"b\", \"c\")), we need to modify the code to use these reactives. The biggest change is we are now passing a tibble of data rather than a character vector of data. As such, I need to use selected()$narrative to refer to the narrative vectors we want to use in our server function. Nothing else really changes, as the underlying process of determining the range of values and using a mathematical operation to limit the indexing stays the same. We are just now applying this process to a different set of data, although it is technically a reactive expression rather than an object in our environment."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "title": "Implementing a next and back button in Shiny",
    "section": "Cases where users select a new product code",
    "text": "Cases where users select a new product code\nGiven the functionality provided within our application, it’s reasonable to expect users would change the product code (i.e., the main purpose is to give users tools to explore the data). It’s also reasonable that the user would then expect the narrative values to change based on their product selection, and indeed we have built this functionality into the app. However, what we didn’t account for yet was what users expectations are for the order to which the new filter data will be presented. When users make a change to their filtering criteria, they would most likely expect that the updated narrative data would start at the beginning, not where their previous clicks would place them within their previously selected data. Given this expectation, I now need some code to ‘reset’ the subsetting index when a user changes their product code filter.\nWhy might this be important? Take for example if the aim of this functionality was to output the most recent injury reported for a specific product code. Our user would expect that any time they switch their product code filtering input, the displayed narrative would be the most recent reported injury, and that each subsequent click would result in a chronological walk through the narratives, either forwards or backwards. This would especially be important if the app was connected to a streaming data source that isn’t static. Moreover, you might even modify the output$narrtive object to include the date, so the user is informed on when a specific injury was treated. For the sake of keeping things simple though, we will only add the reset behavior to the app in this post.\nThis reset of the indexing value was provided in the solutions guide referenced above, and it adds another observeEvent() to make this work. Specifically, it directed me to add this code to the server section of the application:\n\nobserveEvent(input$code, {\n    place(1)\n  })\n\nHere you can see that the observeEvent() is waiting for any changes to the input$code input. When a change occurs to this input, the place(1) is run, and the subsetting index is set back to one. We now have included functionality to the app where when the user changes the product code filtering, the narrative increment index will display the first value in that subset of injuries as selected by the user."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html",
    "href": "blog/posts/2023-01-29-2023-rig/index.html",
    "title": "2023 data science rig: Set up and configuration",
    "section": "",
    "text": "Photo by Barn Images"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "title": "2023 data science rig: Set up and configuration",
    "section": "A few extra configs to the operating system",
    "text": "A few extra configs to the operating system\nI also like to customize the appearance, system keymappings, and terminal aliases (more on this in the section on setting up Zsh) of my operating system. For one, I’m a fan of dark mode, so I set the system settings accordingly. I’m also a minimalist when it comes to the menu dock. I prefer to only include shortcuts that are necessary to my workflow. I also like to change the settings to automatically hide the dock when it’s not being used. I do this to maximize my workspace area. Here is a link to some docs if you’re interested in modifying your macOS system settings.\nThe caps lock key is useless in my workflow. Instead, I remap the ctrl key to the caps lock key. This is mostly done out of convenience, as I’ll use my machine as a true laptop from time to time. This is also essential because my IDE, Neovim, requires extensive use of the ctrl keys (more on the use of Neovim later). Since the MacBook Pro does not include a right-hand side ctrl key, and the left-hand side ctrl key is not in a comfortable position, this remap affords me some additional comfort when I use my machine as a laptop."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#homebrew",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#homebrew",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Homebrew",
    "text": "Homebrew\nHomebrew coins itself as the missing package manager for macOS (or Linux). It makes downloading open-source software much easier. Downloading and installing Homebrew is straight forward. Run the following command in a terminal to download Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nIf you need more specific instructions on downloading and installing Homebrew, check out the docs I linked above. With the Homebrew package manager installed, it’s a cinch to download other tools."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#oh-my-zsh",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#oh-my-zsh",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Oh My Zsh",
    "text": "Oh My Zsh\nNow it’s time to unleash the terminal by downloading Oh My Zsh. Download Oh My Zsh by running the following in your terminal:\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nOh My Zsh’s docs contain the best description of what it does:\n\nOh My Zsh will not make you a 10x developer…but you may feel like one.\n– Zsh docs\n\nFor reals though, Oh My Zsh is a convenient, intuitive means to configure your terminal. For one, it allows plugin installation. Plugins enhance the terminal experience and extend its utility. The following is a list of Zsh plugins I find useful:\n\ngit\nzsh-syntax-highlighting for terminal syntax highlighting.\nzsh-autosuggestions for command suggestions based on previous history.\n\n\nCustomized Zsh prompt\nAnother great feature of Zsh is the ability to customize the command line prompt. Many options are available. For me, I like the prompt to contain four pieces of information:\n\nThe time (24-hours with seconds);\nThe file path of the current working directory;\ngit branch information;\nAn indicator if any uncommitted changes exist in the directory.\n\nHere is what my prompt looks like:\n\n\n\nCustomized Zsh prompt\n\n\nTo achieve this custom setup, I place the following into my .zshrc file:\n# Prompt formatting\nautoload -Uz add-zsh-hook vcs_info\nsetopt prompt_subst\nadd-zsh-hook precmd vcs_info\nPROMPT='%F{blue}%*%f %F{green}%~%f %F{white}${vcs_info_msg_0_}%f$ '\n\nzstyle ':vcs_info:*' check-for-changes true\nzstyle ':vcs_info:*' unstagedstr ' *'\nzstyle ':vcs_info:*' stagedstr ' +'\nzstyle ':vcs_info:git:*' formats       '(%b%u%c)'\nzstyle ':vcs_info:git:*' actionformats '(%b|%a%u%c)'\nIndeed, this might not be the custom prompt for everyone. So, the following are links to blog posts that do an excellent job describing how to customize the different prompt elements:\n\nCustomizing my Zsh Prompt by Cassidy Williams\nCustomize your ZSH prompt with vcs_info by Arjan van der Gaag\n\n\n\nTerminal aliases\nThis year, I focused on transitioning to a more terminal based workflow. As part of this transition, I began utilizing terminal aliases. Aliases can be used to automate common tasks, like opening specific programs, web pages, or project files from the terminal.\nWith Zsh, creating aliases is pretty straightforward. To do this, you’ll need to place a file into the ~/oh-my-zsh/custom directory. This file can be named anything, but it needs to end in the .zsh extension. In this file you can include aliases like the following:\n# aliases to improve productivity\nalias email=\"open https://path-to-email.com/mail/inbox\"\nalias calendar=\"open https://path-to-calendar.com/\"\nalias projects=\"open https://path-to-github-projects.com/\"\nNow if you run email in your terminal prompt, a browser window with your email inbox will open. The above is just an example to get you started. I have additional aliases beyond the ones in the example. To get an idea of all the aliases I use, check out the dotfile here. You can customize any of these to your specific needs.\nThe rest of my Zsh configuration is pretty standard. Here is a link to a repo containing additional files to configure Zsh. Check it out if you’re interested in seeing how I specifically do something."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#additional-terminal-utilities",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#additional-terminal-utilities",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Additional terminal utilities",
    "text": "Additional terminal utilities\n\nJump\nNavigating the file system from the terminal can be tiring. Jump is a terminal utility that solves this problem. Simply put, this utility learns your navigational habits and allows you to easily jump back and forth between directories with very little typing.\nInstall jump using Homebrew. Run the following code in your terminal to install Jump:\nbrew install jump\n\n\ntmux\ntmux is a terminal multiplexer. It lets you create multiple windows and terminals in a single session. I find it useful in situations where you want multiple files, projects, or terminal windows to be open while you’re working.\nInstall tmux using Homebrew:\nbrew install tmux\nAlthough tmux is useful out of the box, some configuration steps are needed to make it more useful. My configuration mostly changes tmux’s keymaps, which makes them easier to remember and use (i.e., some of the defaults require some keyboard gymnastics).\nMuch of my tmux configuration is a derivative of the one discussed in the Getting Started with: tmux YouTube series from Learn Linux TV. If you want some more specific detail, you can check out my .tmux.conf configuration file here.\n\n\ngit\nI use git for version control. Homebrew can be used to install git:\nbrew install git\nSome additional configuration is needed for the local setup of git. Run the following code in the terminal. Make sure to replace what is in quotations with your information.\ngit config --global user.name \"&lt;full-name&gt;\"\ngit config --global user.email \"&lt;email&gt;\"\ngit config --global core.editor \"nvim\"\nThe user.name and user.email variables are required. You can exclude the core.editor configuration if you want to use the default editor. However, I like to use Neovim (more on Neovim in a later section) as my text editor, so I make it my default when working with git.\nAlong with git, I use GitHub for remote repositories. Some additional steps are needed to authenticate with this service. The GitHub CLI tool simplifies these steps.\n\n\nGitHub’s CLI tool\nBring GitHub to your command line with the GitHub CLI. This tool provides commands to do many of the same things you do on GitHub, but with terminal commands. Need to create an issue in a repo, run the following in your terminal:\ngh create issue\nWant to see all the pull requests in a repo needing review, run the following in your terminal:\ngh pr list\nYou can also use these commands within aliases to streamline your workflows. I particularly like my custom aliases to list and create issues and PRs.\n# Custom alias to list GitHub issues\n.il\n\n# Custom alias to create an issue\n.ic\nHomebrew, again, is used for the installation.\nbrew install gh\n\nAuthenticate using the GitHub CLI\nOnce installed, run the gh auth login command to walk you through the authentication flow. During the flow, you’ll have to make a few decisions. Your first decision will be the protocol you want to use for git operations. I select HTTPS. Second, you’ll need to decide how you want to authenticate the GitHub CLI. I select the web browser setting out of convenience. If you’re interested in other forms of authentication, I suggest checking out GitHub’s docs.\nOne minor, additional configuration step is to set Neovim as the default editor for use with the GitHub CLI. If you want to use the default editor, then skip this step. To modify the default editor, run the following command in the terminal:\ngh config set editor nvim"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-rig",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-rig",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install rig",
    "text": "Install rig\nHomebrew handles the installation of rig. Run the following in your terminal:\nbrew tap r-lib/rig\nbrew install --cask rig"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-the-most-recent-version-of-r",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-the-most-recent-version-of-r",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install the most recent version of R",
    "text": "Install the most recent version of R\nOnce rig is installed, download the most recent version of R by running the following in the terminal:\nrig add\nOnce the most recent version is downloaded, you can verify the installation was successful by printing out a list of all the R versions installed on your machine. If this is a fresh start on a new machine or it’s your first time downloading R, you should only see one version listed.\nrig list"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#download-rstudio",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#download-rstudio",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Download RStudio",
    "text": "Download RStudio\nAlthough I have made the switch to using a different IDE (more on this in the next section), I still teach classes and present to groups who mainly use RStudio. So to keep everything up to date and in synch, I download the current version of RStudio using Homebrew:\nbrew install --cask rstudio\nrig also makes it easy to open up a new session of RStudio from the terminal. To do this, run the following in the terminal:\nrig rstudio"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-r-packages",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-r-packages",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install R packages",
    "text": "Install R packages\nThis section overviews the installation of R packages I use most often. Indeed, it would be excessive to download and overview all the packages in my workflow. In addition, the following sections contain a brief description of what each package does and how it is used when I work with R.\nThe following code downloads packages I rely on most. If you use R, many of these packages will be familiar.\n\ninstall.packages(c(\n  devtools,\n  usethis,\n  roxygen2,\n  tidyverse,\n  lubridate,\n  testthat,\n  googleAnalyticsR,\n  bigrquery\n))\n\nIf you’re unfamiliar with loading packages in R, you’ll need to run this code an R console. This can be done either in RStudio or via an iTerm2 system terminal. From the system terminal, type the letter R and hit Enter. Doing this should change your terminal prompt, as you are now running in a R session. You’ll then run the code from above. Information will be printed to the terminal during the installation of the packages.\nOnce all these packages have been installed, run the quit() function to return back to the system’s original prompt. When quitting this R session, you may be prompted to save the workspace. Enter no, as there is no need to save this session’s information. The next few sections provide a brief description of how each of the installed packages are used within my workflow.\n\ndevtools\n\nThe aim of devtools is to make your life as a package developer easier by providing R functions that simplify many common tasks.\n– devtools package docs\n\nSimply put, I rely on devtools for package development. This package provides many convenience functions to manage the mundane tasks involved in package development.\n\n\nusethis\nusethis is a workflow package. It automates many tasks involved when setting up a project. It also contains convenience functions to help with other R project workflow tasks. I’m still exploring all the package’s functions, but using the one’s I’ve learned have made me more productive.\n\n\nroxygen2\nPackages need documentation. The roxygen2 package helps with the documentation setup and development process. If you’re familair with comments in R, you’ll find writing package documentation with roxygen2 intuitive.\n\n\ntidyverse\ntidyverse is mainly used for common data wrangling and analysis tasks. Although I use base functions from time-to-time, I learned R by using tidyverse packages; they’re ingrained throughout my workflow.\nIndeed, the tidyverse is not just a single package, but a collection of packages. Some of the tidyverse packages I rely on most often include:\n\nggplot2 for data visualization\ndplyr for manipulating data\ntidyr for common data tidying tasks\npurrr for functional programming\nstringr for working with string data\n\n\n\nlubridate\nlubridate is magic when it comes to working with date-time data. I use this package mostly to handle data with a time dimension, which usually occurs in cases where I’m working with and analyzing time series data. If you work with date time data, look into using lubridate.\n\n\ntestthat\nThe testhat package is used for writing tests (e.g., unit tests) for code, especially when developing a package. To write more robust code, it’s best practice to write tests. testthat provides a framework and several convenience functions to make composing tests more enjoyable.\n\n\ngoogleAnalyticsR\nPart of my work involves the analysis of web analytics data. Much of this data is collected with and made available via Google Analytics. googleAnalyticsR is a package that allows you to authenticate and export web analytics data using the Google Analytics Reporting API.\n\n\nbigrquery\nGoogle BigQuery is a data warehouse and analytics solution. To access data via its API, I rely on the bigrquery package. This package provides multiple convenience functions to extract, transform, and load data from and into BigQuery. bigrquery also provides several functions to perform some BigQuery administrative tasks.\nThe packages highlighted above are ones I rely on most often in my day-to-day workflow. Indeed, others are used less frequently, especially when performing specific analysis tasks. However, the use of some packages is project dependent and describing all the packages I use would be outside the scope of this post."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Google Cloud Command Line Interface (CLI)",
    "text": "Google Cloud Command Line Interface (CLI)\nI mainly use the Google Cloud Platform (GCP) for cloud based project development. Although I’ll use GCP’s web portal occasionally, the command line interface provides some useful utilities to work from the terminal. The Google Cloud CLI is made available by installing the Google Cloud Software Development Kit (SDK).\nGoogle BigQuery, a data warehouse solution, is a GCP service I use quite often. The Google Cloud CLI has the bq command, which is an interface with BigQuery. I also manage some compute instances in the cloud, so I use the gcloud compute instances command as well.\n\nInstalling the Google Cloud SDK\nInstall the GCP SDK with Homebrew. To download, run the following code in your terminal:\nbrew install --cask google-cloud-sdk\n\n\nAuthorizing the Google Cloud CLI\nYou can review Google Cloud CLI’s authentication steps here. I provided the link to these docs because depending on your current setup and needs, you may need to use different steps to authenticate. Most likely, though, if you’re intending to authenticate with a user account, you can run the following command in your terminal to walk through the authentication steps:\ngcloud init\nAgain, it’s best to review the docs linked above, so you’re aware of the steps needed to authenticate with your specific setup."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#neovim",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#neovim",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Neovim",
    "text": "Neovim\nTo be honest, there was no real reason why I chose Neovim. I just saw others using and suggesting to give it a try. I did briefly read some of the arguments for why Neovim is a good choice, though. From my shallow reading of the topic, most of the arguments I came across pertained to Neovim’s use of the lua programming language, a better plugin management experience, and some additional points that made it appealing. In fact, Neovim is considered to be an extension of Vim, rather than its own stand-alone text editor. It aims to be extensible, usable, and retain the good parts of Vim. Now, I haven’t developed a sufficient understanding of these arguments to fully articulate the benefits of using one Vim like editor from another. I just know I’m enjoying it thus far. I suggest giving it a try.\n\nInstalling Neovim\nHomebrew is used to download Neovim.\nbrew install neovim\n\n\nConfiguring Neovim\nAs mentioned in the intro to this section, Neovim’s setup and configuration can be its own series of posts; there are so many options and plugins available. The focus of the following sections is to draw attention to some of the tools I find useful when working in Neovim. Keep in mind, the configuration of Neovim is a bit of a learning curve. It can be frustrating when you first start, but very rewarding at times. You can review my configuration files here.\nMy Neovim setup is based on chris@machine’s Neovim from Scratch YouTube tutorial series. This series does an excellent job overviewing a complete Neovim setup using the Lua programming language. While my setup is mostly based on the one described in this series, I have added some custom configuration for my specific workflow.\n\n\nNeovim package manager\nI use packer for plugin management. Packer simplifies plugin installation. For example, here is the Lua code to install some plugins I highlight in the following sections:\nreturn packer.startup(function(use)\n  use \"wbthomason/packer.nvim\"   -- Have packer manage itself\n  use \"jalvesaq/Nvim-R\"          -- Tools to work with R in nvim\n\n  -- Colorschemes\n  use \"lunarvim/colorschemes\"     -- A selection of various colorschemes\n  use \"tomasiser/vim-code-dark\"\n  use \"EdenEast/nightfox.nvim\"\n  use \"folke/tokyonight.nvim\"\n\n  -- LSP \n  use \"neovim/nvim-lspconfig\"         -- enable LSP\n  use \"williamboman/mason.nvim\"\n  use \"williamboman/mason-lspconfig.nvim\"\n\n\n  -- Telescope\n  use \"nvim-telescope/telescope.nvim\"\n\n  -- Treesitter\n  use {\n    \"nvim-treesitter/nvim-treesitter\",\n    run = \":TSUpdate\",\n  } \n\n  -- Git \n  use \"lewis6991/gitsigns.nvim\"\n  use \"tpope/vim-fugitive\"\n\n  if PACKER_BOOTSTRAP then\n    require(\"packer\").sync()\n  end\nend)\nThis code might not make much sense, as I only included a snippet of the code needed to install plugins I use most often. It’s mainly intended to show with a few lines of code, packer can manage all the plugin installation steps. This example code deviates slightly from the original packer docs on how to install plugins. Check out the previously linked docs if you would like an alternative setup while using Packer.\nHere is a link to a file with all the plugins I use in my setup. Admittedly, some plugins are carry overs from chris@machine’s YouTube series, and I will fully admit I’m still learning the reason why some of these plugins are present within my configuration. Thus, my setup is not as lean as I would like it to be. But hey, I’m still learning.\n\n\nNeovim plugins\n\nNvim-R\nSince I mostly work with R, I use Nvim-R to write code and interact with the R console directly in Neovim. Nvim-R provides utilities to have the Vim experience, while also affording interactive analysis right at your fingertips. Here is what a session using Nvim-R looks like:\n\n\n\nNvim-r running in Neovim\n\n\nThe power of Nvim-R comes from its predefined keybindings keybindings, which allow you to quickly and easily do interactive analysis tasks using just a few keystrokes. I’ve found it’s the best option to work with R in Neovim. A whole blog post could be written about the use of Nvim-R, and I only hit the highlights here. I highly suggest checking it out if you’re looking to write R code with Neovim.\n\n\nvim-devtools-plugin\nAs mentioned above, I use devtools for package development. To leverage its functionality in Neovim, I use the vim-devtools-plugin. This plugin provides several convenient commands to run different devtools functions. This is especially useful as you can configure keymaps to these commands for added convenience and speed.\n\n\nTelescope\nFind, filter, preview, and pick. Telescope is great at these actions. Specifically, Telescope is a fuzzy file finder. However, it provides additional features that go beyond just working with a project’s files. I’m attempting to use it more and more in my workflow, as I mostly use it to find and navigate to specific files. However, I’ve begun to explore more of its functionality and integration with git.\n\n\nvim-fugitive\nDo yourself a favor, use vim-fugitive. Fugitive is a plugin that helps you work with Git while working in Neovim. In the past, my git and GitHub workflow was mainly done from the command line. However, jumping in and out of Neovim back to run this workflow became old quickly. To solve this, Fugitive provides the :Git or :G command to call git commands directly from the editor. Also, since I use Neovim as my editor for commit messages, I’m able to directly compose them without having to leave my current Neovim session.\n\n\nLSP\nNeovim supports the Language Server Protocol (LSP). LSP provides many different features. This includes go-to-definition (a great feature that speeds up editing), find references, hover, completion, and many other types of functionality. Most IDEs have LSP set up out-of-the-box. This is done so you can get started quickly working with any language without too much configuration.\nNeovim does provide an LSP client, but you’ll have to set up the individual servers for each language you would like to work with. This sounds harder then it is, but it does take a few steps to complete. A good rundown can be found in this video here, which is from the Neovim series I linked above. I recently made the switch over to the Mason plugin, which makes LSP server management so much simpler. I would suggest checking it out if you’re intending to work with other languages in Neovim.\nI’m still learning about LSP and how to set it up. I highly suggest reading up on the docs and reviewing other’s setups rather than relying solely on mine. Mine is still a work in progress.\n\n\nnvim-treesitter\nSetting up syntax highlighting is another important step when setting up Neovim. I use nvim-treesitter to improve the syntax highlighting within Neovim. This is another advanced topic I’m still learning about, so I just use a basic setup. You can read more about it in the plugin docs linked earlier.\n\n\nCustom Neovim keymaps\nSince Neovim is all about customization of your development environment, one thing to modify is Neovim’s keymaps. To configure, you just have to define the configuration in your Neovim config files. For example, the following is some code I use to customize my keymap setup.\n-- BigQuery keymappings \nkeymap(\"n\", \"&lt;C-b&gt;\", \":w | :! bq query &lt; % --format pretty &lt;CR&gt;\", opts)\n\n-- R coding keymappings\nkeymap(\"n\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"i\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;ts\", \":RStop&lt;cr&gt;\", opts)\nkeymap(\"n\", \"tt\", \"&lt;Esc&gt;&lt;C-w&gt;&lt;C-w&gt;i\", opts)\n\n-- R devtools keymappings\nkeymap(\"n\", \"&lt;leader&gt;I\", \":RInstallPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;L\", \":RLoadPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;B\", \":RBuildPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;E\", \":RCheckPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;T\", \":RTestPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;D\", \":RDocumentPackage&lt;Esc&gt;\", opts)\nI have additional custom keymappings in my setup, but including the entire file would be too much for this post. Nevertheless, you can access my keymapping configuration files to get a sense of other keymaps I have within my setup."
  },
  {
    "objectID": "blog/posts/2024-03-22-tidytuesday-2024-03-12-fiscal-sponsor-directory/index.html",
    "href": "blog/posts/2024-03-22-tidytuesday-2024-03-12-fiscal-sponsor-directory/index.html",
    "title": "Exploring data from the Fiscal Sponsor Directory",
    "section": "",
    "text": "Image generated using the prompt ‘Hexagonal pattern in a grey palette’ with the Bing Image Creator\n\n\n\nBackground\nI’m a little behind on this submission. My time to focus on #tidytuesday contributions has been limited recently. Nevertheless, here’s my submission for the 2024-03-12 data set.\nThis week’s data comes from the Fiscal Sponsor Directory. In short, this directory is a listing of groups supporting non-profits through the fiscal sponsorship of projects. I was unfamilar with this space, so I found the Fiscal Sponsor Directory’s About Us page helpful.\nWhy the Fiscal Sponsorship Directory this week? Well, the organizer of #tidytuesday is the R4DS Online Learning Community, a group I actively participate in. This group has been on the search for a new fiscal sponsor recently. The aim, thus, was to lean on the community to create data visualizations that may be helpful in identifying another fiscal sponsor for the group. So, below is what I came up with.\nBefore getting to my contribution, let’s take a moment to explore the data.\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(skimr)\nlibrary(tidytext)\nlibrary(plotly)\n\n\ndata_sponsor_dir &lt;- read_csv(\n  here(\n    \"blog\", \n    \"posts\",\n    \"2024-03-22-tidytuesday-2024-03-12-fiscal-sponsor-directory\",\n    \"fiscal_sponsor_directory.csv\"\n  )\n)\n\nRows: 370 Columns: 12\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (9): details_url, name, website, fiscal_sponsorship_fee_description, eligibility_criteria, p...\ndbl (3): year_501c3, year_fiscal_sponsor, n_sponsored\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData description\nThis week was tough. The data was pretty dirty, and I relied heavily on string processing for data wrangling. I think, however, I’ve come up with something that is a little more informative than just a listing of the different sponsorship groups.\n\nglimpse(data_sponsor_dir)\n\nRows: 370\nColumns: 12\n$ details_url                        &lt;chr&gt; \"https://fiscalsponsordirectory.org/?page_id=599\", \"htt…\n$ name                               &lt;chr&gt; \"1st Note Music Foundation\", \"50CAN, Inc.\", \"The Abunda…\n$ website                            &lt;chr&gt; \"www.1stnote.org\", \"50can.org\", \"abundancenc.org\", \"acc…\n$ year_501c3                         &lt;dbl&gt; 2012, 2011, 2006, 2014, 2007, 1992, 2008, 2002, 1989, 1…\n$ year_fiscal_sponsor                &lt;dbl&gt; 2012, 2016, 2007, 2017, 2013, 1997, 2009, 2018, 2004, 1…\n$ n_sponsored                        &lt;dbl&gt; 2, 10, 20, 6, 2, 1, NA, 7, 1, 15, 130, 60, 5, 13, 20, 1…\n$ fiscal_sponsorship_fee_description &lt;chr&gt; \"We charge a 7% administrative fee for most grants and …\n$ eligibility_criteria               &lt;chr&gt; \"Type of service: Music related projects\", \"Aligned mis…\n$ project_types                      &lt;chr&gt; \"Arts and culture: Music Instruments to kids\", \"Educati…\n$ services                           &lt;chr&gt; \"Auditing: Grants\", \"Auditing|Bill paying|Bookkeeping/a…\n$ fiscal_sponsorship_model           &lt;chr&gt; \"Model C, Preapproved Grant Relationship\", \"Model A, Di…\n$ description                        &lt;chr&gt; \"1st Note Music Foundation Inc. is a nonprofit public s…\n\n\n\nskim(data_sponsor_dir)\n\n\nData summary\n\n\nName\ndata_sponsor_dir\n\n\nNumber of rows\n370\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndetails_url\n0\n1.00\n47\n49\n0\n370\n0\n\n\nname\n0\n1.00\n3\n64\n0\n368\n0\n\n\nwebsite\n7\n0.98\n7\n70\n0\n362\n0\n\n\nfiscal_sponsorship_fee_description\n20\n0.95\n2\n901\n0\n324\n0\n\n\neligibility_criteria\n7\n0.98\n21\n2039\n0\n303\n0\n\n\nproject_types\n9\n0.98\n9\n1244\n0\n324\n0\n\n\nservices\n14\n0.96\n9\n1852\n0\n307\n0\n\n\nfiscal_sponsorship_model\n86\n0.77\n7\n499\n0\n64\n0\n\n\ndescription\n31\n0.92\n70\n1665\n0\n337\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear_501c3\n6\n0.98\n1997.62\n17.18\n1903\n1985.75\n2001\n2012\n2022\n▁▁▂▅▇\n\n\nyear_fiscal_sponsor\n15\n0.96\n2005.15\n13.68\n1957\n1998.00\n2009\n2016\n2023\n▁▂▂▅▇\n\n\nn_sponsored\n13\n0.96\n42.68\n90.87\n0\n4.00\n12\n45\n850\n▇▁▁▁▁\n\n\n\n\n\n\nhead(data_sponsor_dir$project_types)\n\n[1] \"Arts and culture: Music Instruments to kids\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n[2] \"Education\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n[3] \"Children, youth and families|Economic development|Education|Environment/sustainable growth\"                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n[4] \"Arts and culture|Children, youth and families|Education|Festivals and events|Health|GBTQ|Mental health|Open space/Parks|People or communities of color/minorities| Social services|Other: Do you have a project idea that increases access to music and recreation? Our mission is to foster connection and understanding through pleasurable experiences like music and recreation to inspire the creation of safe, inclusive, equitable communities.\"                                                                                         \n[5] \"Arts and culture|Children, youth and families: Mentor Develop Programs, Youth Empowerment Programs, Grant-funded Programs. Family Empowerment and Educational Programs.|Education: Student Success Strategy Programs; Adult Re-Entry Into Education Projects.|Festivals and events: African Diaspora History Festivals and Events|People or communities of color/minorities: Diversity Impact Programs. African-American and African Diaspora Immigrant Projects|Youth development: Youth Empowerment, Job Readiness, Early Career Development,\"\n[6] \"Arts and culture|Children, youth and families|Disaster relief|Education|Festivals and events|People or communities of color/minorities|Women|Youth development\"                                                                                                                                                                                                                                                                                                                                                                                 \n\n\n\n\nData wrangling\nThe first step in the data wrangling process was to clean up the string data in the project_types column. I wanted to use this as a dimension to filter out fiscal sponsor potentially relevant to the the R4DS community. Take note, I used a regular expression to remove string values after the other in the column. These free text responses would have made it harder to filter data on this dimension.\n\n# Remove any string text after 'other' \ndata_sponsor_dir &lt;- data_sponsor_dir |&gt;\n  select(\n    details_url, \n    name, \n    year_fiscal_sponsor,\n    n_sponsored,\n    project_types,\n    website\n  ) |&gt;\n  mutate(\n    project_types = str_to_lower(project_types),\n    project_types = str_remove(project_types, \":.*\")\n  ) \n\nThe next step was to tokenize project_types’ categories into it’s own rows. I did this by using the unnest_tokens() function from the tidytext package.\n\n# What are the unique categories?\ndata_sponsor_cat &lt;- data_sponsor_dir |&gt;\n  unnest_tokens(cat, project_types, token = 'regex', pattern = \"\\\\|\") \n\nhead(unique(data_sponsor_cat$cat), n = 10)\n\n [1] \"arts and culture\"               \"education\"                     \n [3] \"children, youth and families\"   \"economic development\"          \n [5] \"environment/sustainable growth\" \"festivals and events\"          \n [7] \"health\"                         \"gbtq\"                          \n [9] \"mental health\"                  \"open space/parks\"              \n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile wrangling this data, I learned you can pass a regex pattern to tokenize when using unnest_tokens(). I found this to be useful in this case.\n\n\nOutputted above are 10 example categories found in the project_types column. Looking through all of these categories, the following might be fiscal sponsors whose purpose aligns with that of the R4DS community. These include the following categories:\n\nconference/event management\neducation environment/sustainable growth\neducation\neconomic development\norganizational development\n\nHowever, before I go about creating a visualization using these categories, let’s see if there’s enough data within each category to make the visualization informative.\n\ncat_filter &lt;- c(\n  \"education\",\n  \"education environment/sustainable growth\",\n  \"economic development\",\n  \"conference/event management\",\n  \"organizational development\"\n)\n\ndata_sponsor_cat |&gt;\n  filter(cat %in% cat_filter) |&gt;\n  count(cat, sort = TRUE)\n\n# A tibble: 5 × 2\n  cat                                          n\n  &lt;chr&gt;                                    &lt;int&gt;\n1 education                                  186\n2 economic development                       144\n3 organizational development                   6\n4 conference/event management                  1\n5 education environment/sustainable growth     1\n\n\nIndeed, some categories don’t have enough data. Really, the only two categories worth plotting would be ‘economic development’ and ‘education’. So, let’s filter for just these two categories. Let’s also drop NA values for simplicity sake.\n\ncat_filter &lt;- c(\n  \"education\",\n  \"economic development\"\n)\n\ndata_cat_filter &lt;- data_sponsor_cat |&gt;\n  filter(cat %in% cat_filter) |&gt;\n  mutate(cat = str_to_title(cat)) |&gt;\n  arrange(name, cat) |&gt;\n  drop_na() |&gt;\n  select(-details_url)\n\n\n\nCreating a Box and Whisker plot\nGiven I had a numeric variable, n_sponsored, I thought a Box and Whisker plot split by the two categories would be informative. It would certainly help identify fiscal sponsors who support many or very little projects based on the types of projects they support. Another thing I had to do was log the n_sponsored column. When I first plotted the untransformed variables, it was challenging to see the distribution of values. Logging n_sponsored made it easier to see the values. However, the hover tool provides the untransformed value for each fiscal sponsor in the data set.\n\n\n\n\n\n\nWarning\n\n\n\nThere will be duplicates in this visualization, as some sponsors will support both education and economic development focused projects.\n\n\n\nplot_ly(type = \"box\") |&gt;\n  add_boxplot(\n    data = data_cat_filter |&gt; filter(cat == \"Education\"),\n    x = ~log(n_sponsored),\n    y = ~cat,\n    boxpoints = \"all\",\n    name = \"Education\",\n    color = I(\"#189AB4\"),\n    marker = list(color = \"#189AB4\"),\n    line = list(color = \"#000000\"),\n    text = ~paste(\n      \"Sponsor: \", name,\n      \"&lt;br&gt;Projects: \", n_sponsored,\n      \"&lt;br&gt;Website: \", website\n    ),\n    hoverinfo = \"text\"\n  ) |&gt;\n  add_boxplot(\n    data = data_cat_filter |&gt; \n      filter(cat == \"Economic Development\"),\n    x = ~log(n_sponsored),\n    y = ~cat,\n    boxpoints = \"all\",\n    name = \"Economic development\",\n    color = I(\"#191970\"),\n    marker = list(color = \"#191970\"),\n    line = list(color = \"#000000\"),\n    text = ~paste(\n      \"Sponsor: \", name,\n      \"&lt;br&gt;Projects: \", n_sponsored,\n      \"&lt;br&gt;Website: \", website\n    ),\n    hoverinfo = \"text\"\n  ) |&gt;\nlayout(\n  title = \"&lt;b&gt;Distribution of the number of projects (logged) supported by fiscal sponsors\",\n  yaxis = list(title = \"\"),\n  xaxis = list(title = \"Projects sponsored on log scale\")\n)\n\nWarning: Can't display both discrete & non-discrete data on same axis\n\n\n\n\n\n\nNot bad. The only thing I ran out of time on was related to the hover tool. I really wanted separate hovers, one for the five number summary in the box and whisker plot and one for the individual data points. Unfortunately, I wasn’t able to figure out how to do this with the time I had. Oh well, what resulted was still a useful data visualization, given where we started with the data.\nSo there you have it. Not the cleanest data to work with. Nonetheless, we came up with a visualization we could still learn something from.\n\n\nAn attempt using Tableau\nTo continue developing my skills and to practice using other data visualization tools, I created this same visualization using Tableau. You can check out this version of the visualization here.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring Data from the {Fiscal} {Sponsor} {Directory}},\n  date = {2024-03-22},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Exploring Data from the Fiscal Sponsor\nDirectory.” March 22, 2024."
  },
  {
    "objectID": "blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/index.html",
    "href": "blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/index.html",
    "title": "30 day tidymodels recipes challenge",
    "section": "",
    "text": "Photo by Nicolas Gras\n\n\n\nBackground\nBefore the holidays, I came across Emil Hvitfeldt’s #adventofsteps LinkedIn posts. Following a model popularized by advent of code–an annual tradition of online programming puzzles based on the theme of an advent calendar–these posts provided daily examples on the use of various step_* functions from the tidymodels’ recipes package. This post, with a slight spin, is inspired by these posts.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(textrecipes)\nlibrary(themis)\nlibrary(timetk)\ntidymodels_prefer()\n\n\n\nMy spin on this\nOne of my personal goals this coming year is to learn and practice using the different tidymodels’ packages. To complete this goal, I thought a 30 day recipes challenge would be a good start. Each day during this 30 day personal challenge, I will focus on learning and creating some daily notes about one functionality of the recipes package. First, I start with the basics (e.g., how to create a recipe object). Then, I’ll focus on describing the various step_* functions.\nTo keep me on track, while also avoiding making this a chore, I’m going to place a 1-hour a day stopgap on studying, practicing, and documenting what I’ve learned. Depending on my schedule and motivation, I may work ahead on some material, but I will strive to update this post once a day.\nGiven the time constraint I’m imposing on myself, some of my daily notes or examples may result in an incomplete description of functionality. In cases like this, I’ll try to link to relevant documentation for you to follow up and learn more. Please be flexible with any grammar and spelling errors during this challenge, as I’ll likely edit very little until the end of the 30 days, if at all.\nSince the aim of this post is to document what I’m learning, all errors are completely mine. I highly suggest following up with the recipes package’s documentation and the Tidy Modeling with R book following a review of these notes. Both do a more thorough job overviewing the package’s functionality.\n\n\nWhat I intend to get out of this challenge\nBy the end of this challenge, I hope to have pushed myself to learn more about how to use tidymodels’s recipe package, and to create several example use cases of different functionality.\n\n\nDay 01 - Create a recipe\nFirst off, what is a recipe? According to the docs:\n\nA recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.\n\nSo, I start this personal challenge by overviewing how to create a recipe object with the recipes package. The recipe() function is used to create a recipe object.\nWhen creating a recipe, we need to consider what roles variables take. In simple modeling tasks, you’ll just have outcomes and predictors. However, variables may take on other roles (i.e., IDs). As such, the recipe() function provides multiple means for specifying the role of a variable:\n\nThe formula\nManually updating roles using the update_role() function.\n\nLet’s use the credit_data from tidymodels’ modeldata package. You can get more information about this data by running ?credit_data in your console.\n\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\n\n# Create splits for examples\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n\n# No outcome variable, `.` is a shortcut for **all** variables\ncredit_rec &lt;- recipe(~., data = credit_train)\n\n# Outcome with specific variables to be included within model\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)\n\n# Recipe uses `data` only as a template, all the data is not needed\n# Useful in cases when you're working with large data\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = head(credit_train)\n)\n\n\n# Use `update_role()` to specify variable roles\ncredit_rec_update &lt;- recipe(credit_train) |&gt;\n  update_role(Status, new_role = \"outcome\") |&gt;\n  update_role(\n    Seniority, Home, Time, Age, Marital, Records, \n    Job, Expenses, Income, Assets, Debt, Amount, \n    Price, new_role = \"predictor\"\n  )\n\ncredit_rec_update\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\n\n\nThe update_role() function is useful in cases where you might have an ID variable you don’t want to include within your model.\n\ncredit_data_id &lt;- credit_data |&gt;\n  mutate(id = 1:n(), .before = 1)\n\nset.seed(2)\ncredit_id_split &lt;- \n  initial_split(credit_data_id, prop = 0.8, strata = Status)\ncredit_id_train &lt;- training(credit_id_split)\ncredit_id_test &lt;- testing(credit_id_split)\n\n\n# Manually add an 'id' role to a variable\ncredit_id_rec &lt;- recipe(credit_id_train) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  update_role(Status, new_role = \"outcome\") |&gt;\n  update_role(\n    Seniority, Home, Time, Age, Marital, Records, \n    Job, Expenses, Income, Assets, Debt, Amount, \n    Price, new_role = \"predictor\"\n  ) \n\ncredit_id_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\nid:         1\n\n\nIn case you ever need to remove a role, you can use remove_role().\n\ncredit_no_id_rec &lt;- credit_id_rec |&gt;\n  remove_role(id, old_role = \"id\")\n\n# id will be assigned and 'undeclared' role\ncredit_no_id_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:          1\npredictor:       13\nundeclared role:  1\n\n\nEach recipe has its own summary method. We can wrap the recipe object within summary() to output more information about each variable and its assigned role.\n\n# Formula specified recipe\nsummary(credit_rec)\n\n# A tibble: 4 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 Debt     &lt;chr [2]&gt; predictor original\n2 Income   &lt;chr [2]&gt; predictor original\n3 Assets   &lt;chr [2]&gt; predictor original\n4 Status   &lt;chr [3]&gt; outcome   original\n\n# Manually specified using `update_role()`\nsummary(credit_rec_update)\n\n# A tibble: 14 × 4\n   variable  type      role      source  \n   &lt;chr&gt;     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Status    &lt;chr [3]&gt; outcome   original\n 2 Seniority &lt;chr [2]&gt; predictor original\n 3 Home      &lt;chr [3]&gt; predictor original\n 4 Time      &lt;chr [2]&gt; predictor original\n 5 Age       &lt;chr [2]&gt; predictor original\n 6 Marital   &lt;chr [3]&gt; predictor original\n 7 Records   &lt;chr [3]&gt; predictor original\n 8 Job       &lt;chr [3]&gt; predictor original\n 9 Expenses  &lt;chr [2]&gt; predictor original\n10 Income    &lt;chr [2]&gt; predictor original\n11 Assets    &lt;chr [2]&gt; predictor original\n12 Debt      &lt;chr [2]&gt; predictor original\n13 Amount    &lt;chr [2]&gt; predictor original\n14 Price     &lt;chr [2]&gt; predictor original\n\n# Recipe with a variable holding the 'id' role\nsummary(credit_id_rec)\n\n# A tibble: 15 × 4\n   variable  type      role      source  \n   &lt;chr&gt;     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 id        &lt;chr [2]&gt; id        original\n 2 Status    &lt;chr [3]&gt; outcome   original\n 3 Seniority &lt;chr [2]&gt; predictor original\n 4 Home      &lt;chr [3]&gt; predictor original\n 5 Time      &lt;chr [2]&gt; predictor original\n 6 Age       &lt;chr [2]&gt; predictor original\n 7 Marital   &lt;chr [3]&gt; predictor original\n 8 Records   &lt;chr [3]&gt; predictor original\n 9 Job       &lt;chr [3]&gt; predictor original\n10 Expenses  &lt;chr [2]&gt; predictor original\n11 Income    &lt;chr [2]&gt; predictor original\n12 Assets    &lt;chr [2]&gt; predictor original\n13 Debt      &lt;chr [2]&gt; predictor original\n14 Amount    &lt;chr [2]&gt; predictor original\n15 Price     &lt;chr [2]&gt; predictor original\n\n\n\n\nDay 02 - How to use prep() and bake()\nLet’s stick with the credit data for today’s examples.\n\n# Same code from day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\n\n# Create splits for our day 2 examples\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\nWe’re going to continue to use the previously specified limited model from day 01 for our examples.\n\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)\n\nNow that we know how to specify a recipe, we need to learn how to use recipes’ prep() and bake() functions. prep() calculates any intermediate values required for preprocessing. bake() applies the preprocessing steps–using any intermediate values–to our testing and training data.\nprep() and bake() can be confusing at first. However, I like the following analogy from the R4DS learning community’s Q&A with the authors of the Tidy Modeling with R book:\n\nThey’re analogous to fit() and predict() … prep() is like fitting where you’re estimating stuff and bake() is like you’re applying it.\n- Max Kuhn\n\nFor a more formal treatment, the prep() docs state:\n\nFor a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets.\n\nThe bake() docs state:\n\nFor a recipe with at least one preprocessing operation that has been trained by prep(), apply the computations to new data.\n\nWhy two separate functions? Some preprocessing steps need an intermediate calculation step to be performed before applying the recipe to the data (e.g., step_normalize() and step_center(); more on this later). To better articulate this point, I’m going to fast-forward a bit in our challenge and apply the step_center() function to our recipe. step_center() is used to center variables.\nWhen centering a variable, we need to make an intermediate calculation (i.e., prep()) before applying the calculation to perform the centering to our data (i.e., bake()).\nFor our example, say we want to center the Debt variable. To do this, we can simply add step_center(Debt) to our recipe. When we pipe the recipe object to prep(), the mean is calculated in the background to perform the preprocessing step.\n\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n) |&gt;\n  step_center(Debt) |&gt;\n  prep() \n\nWe can see this calculated value by using the number argument in the tidy.recipe() method.\n\n# Print a summary of the recipe steps to be performed\ntidy(credit_rec)\n\n# A tibble: 1 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_lw98c\n\n# Print additional information about the first recipe step\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms value id          \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 Debt   337. center_lw98c\n\n\nTake note, though, the Debt variable has not been centered yet, and we are still working with a recipe object.\nWe then apply the centering transformation to the data by piping the prepped recipe to bake(). We can apply the preprocessing to the training data by passing the NULL to the new_data argument. bake() returns a tibble with our transformed variable using our training data.\n\ncredit_baked &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n) |&gt;\n  step_center(Debt) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_baked\n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt; &lt;fct&gt; \n 1 -337.     80      0 bad   \n 2 -337.     50      0 bad   \n 3 -337.    107      0 bad   \n 4  163.    112   2000 bad   \n 5 -337.     85   5000 bad   \n 6   NA      NA     NA bad   \n 7 -337.     90      0 bad   \n 8 -337.     71   3000 bad   \n 9 -337.    128      0 bad   \n10 -337.    100      0 bad   \n# ℹ 3,553 more rows\n\n\nMost likely, you won’t use prep() and bake() for other modeling tasks. However, they’ll be important as we continue exploring the recipes package in the coming days.\n\n\nDay 03 - Selector functions\nRemaining consistent, let’s continue using the credit_data data for some of today’s examples. We’ll also use the Chicago data set for a couple additional examples. You can read more about this data by running ?Chicago in your console.\nHere we’ll get our data and split it into training and testing for both data sets.\n\n# Same code from day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n\ndata(Chicago, package = \"modeldata\")\nglimpse(Chicago)\n\nRows: 5,698\nColumns: 50\n$ ridership        &lt;dbl&gt; 15.732, 15.762, 15.872, 15.874, 15.423, 2.425, 1.467, 15.511, 15.927, 15.…\n$ Austin           &lt;dbl&gt; 1.463, 1.505, 1.519, 1.490, 1.496, 0.693, 0.408, 0.987, 1.551, 1.588, 1.5…\n$ Quincy_Wells     &lt;dbl&gt; 8.371, 8.351, 8.359, 7.852, 7.621, 0.911, 0.414, 4.807, 8.227, 8.246, 8.0…\n$ Belmont          &lt;dbl&gt; 4.599, 4.725, 4.684, 4.769, 4.720, 2.274, 1.631, 3.517, 4.707, 4.774, 4.8…\n$ Archer_35th      &lt;dbl&gt; 2.009, 2.088, 2.108, 2.166, 2.058, 0.624, 0.378, 1.339, 2.221, 2.227, 2.1…\n$ Oak_Park         &lt;dbl&gt; 1.421, 1.429, 1.488, 1.445, 1.415, 0.426, 0.225, 0.879, 1.457, 1.475, 1.4…\n$ Western          &lt;dbl&gt; 3.319, 3.344, 3.363, 3.359, 3.271, 1.111, 0.567, 1.937, 3.457, 3.511, 3.4…\n$ Clark_Lake       &lt;dbl&gt; 15.561, 15.720, 15.558, 15.745, 15.602, 2.413, 1.374, 9.017, 16.003, 15.8…\n$ Clinton          &lt;dbl&gt; 2.403, 2.402, 2.367, 2.415, 2.416, 0.814, 0.583, 1.501, 2.437, 2.457, 2.4…\n$ Merchandise_Mart &lt;dbl&gt; 6.481, 6.477, 6.405, 6.489, 5.798, 0.858, 0.268, 4.193, 6.378, 6.458, 6.2…\n$ Irving_Park      &lt;dbl&gt; 3.744, 3.853, 3.861, 3.843, 3.878, 1.735, 1.164, 2.903, 3.828, 3.869, 3.8…\n$ Washington_Wells &lt;dbl&gt; 7.560, 7.576, 7.620, 7.364, 7.089, 0.786, 0.298, 4.731, 7.479, 7.547, 7.2…\n$ Harlem           &lt;dbl&gt; 2.655, 2.760, 2.789, 2.812, 2.732, 1.034, 0.642, 1.958, 2.742, 2.753, 2.7…\n$ Monroe           &lt;dbl&gt; 5.672, 6.013, 5.786, 5.959, 5.769, 1.044, 0.530, 3.165, 5.935, 5.829, 5.9…\n$ Polk             &lt;dbl&gt; 2.481, 2.436, 2.526, 2.450, 2.573, 0.006, 0.000, 1.065, 2.533, 2.566, 2.4…\n$ Ashland          &lt;dbl&gt; 1.319, 1.314, 1.324, 1.350, 1.355, 0.566, 0.347, 0.852, 1.400, 1.358, 1.4…\n$ Kedzie           &lt;dbl&gt; 3.013, 3.020, 2.982, 3.013, 3.085, 1.130, 0.635, 1.969, 3.149, 3.099, 3.1…\n$ Addison          &lt;dbl&gt; 2.500, 2.570, 2.587, 2.528, 2.557, 0.800, 0.487, 1.560, 2.574, 2.618, 2.5…\n$ Jefferson_Park   &lt;dbl&gt; 6.595, 6.750, 6.967, 7.013, 6.922, 2.765, 1.856, 4.928, 6.817, 6.853, 6.8…\n$ Montrose         &lt;dbl&gt; 1.836, 1.915, 1.977, 1.979, 1.953, 0.772, 0.475, 1.325, 2.040, 2.038, 2.0…\n$ California       &lt;dbl&gt; 0.756, 0.781, 0.812, 0.776, 0.789, 0.370, 0.274, 0.473, 0.844, 0.835, 0.8…\n$ temp_min         &lt;dbl&gt; 15.1, 25.0, 19.0, 15.1, 21.0, 19.0, 15.1, 26.6, 34.0, 33.1, 23.0, 0.0, 10…\n$ temp             &lt;dbl&gt; 19.45, 30.45, 25.00, 22.45, 27.00, 24.80, 18.00, 32.00, 37.40, 34.00, 28.…\n$ temp_max         &lt;dbl&gt; 30.0, 36.0, 28.9, 27.0, 32.0, 30.0, 28.9, 41.0, 43.0, 36.0, 33.1, 21.2, 3…\n$ temp_change      &lt;dbl&gt; 14.9, 11.0, 9.9, 11.9, 11.0, 11.0, 13.8, 14.4, 9.0, 2.9, 10.1, 21.2, 20.0…\n$ dew              &lt;dbl&gt; 13.45, 25.00, 18.00, 10.90, 21.90, 15.10, 10.90, 30.20, 35.60, 30.90, 21.…\n$ humidity         &lt;dbl&gt; 78.0, 79.0, 81.0, 66.5, 84.0, 71.0, 74.0, 93.0, 93.0, 89.0, 80.0, 66.5, 7…\n$ pressure         &lt;dbl&gt; 30.430, 30.190, 30.160, 30.440, 29.910, 30.280, 30.330, 30.040, 29.400, 2…\n$ pressure_change  &lt;dbl&gt; 0.12, 0.18, 0.23, 0.16, 0.65, 0.49, 0.10, 0.78, 0.16, 0.48, 0.23, 0.28, 0…\n$ wind             &lt;dbl&gt; 5.20, 8.10, 10.40, 9.80, 12.70, 12.70, 8.10, 8.10, 9.20, 11.50, 11.50, 12…\n$ wind_max         &lt;dbl&gt; 10.4, 11.5, 19.6, 16.1, 19.6, 17.3, 13.8, 17.3, 23.0, 16.1, 16.1, 19.6, 1…\n$ gust             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ gust_max         &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 25.3, 26.5, 0.0, 26.5, 31.1, 0.0, 0.0, 23.0, 26.5, 0.…\n$ percip           &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ percip_max       &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.11, 0.01, 0.00, 0.00, 0…\n$ weather_rain     &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ weather_snow     &lt;dbl&gt; 0.00000000, 0.00000000, 0.21428571, 0.00000000, 0.51612903, 0.04000000, 0…\n$ weather_cloud    &lt;dbl&gt; 0.7083333, 1.0000000, 0.3571429, 0.2916667, 0.4516129, 0.6400000, 0.52000…\n$ weather_storm    &lt;dbl&gt; 0.00000000, 0.20833333, 0.07142857, 0.04166667, 0.45161290, 0.24000000, 0…\n$ Blackhawks_Away  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Blackhawks_Home  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Bulls_Away       &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Bulls_Home       &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ Bears_Away       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Bears_Home       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ WhiteSox_Away    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ WhiteSox_Home    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Cubs_Away        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Cubs_Home        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ date             &lt;date&gt; 2001-01-22, 2001-01-23, 2001-01-24, 2001-01-25, 2001-01-26, 2001-01-27, …\n\n# For reproducibility\nset.seed(2)\nchicago_split &lt;- initial_split(Chicago, prop = 0.8)\nchicago_train &lt;- training(chicago_split)\nchicago_test &lt;- testing(chicago_split)\n\nWhen using recipes, we often need to select a group of variables (e.g., all predictors, all numeric variables, all categorical variables, etc.) to apply preprocessing steps. Indeed, we certainly could just explicitly specify each variable by name within our recipe. There’s a better way, though. Use selector functions.\nSelector functions can be used to choose variables based on:\n\nVariable names\nCurrent role\nData type\nAny combination of the above three\n\nThe first set of selectors comes from the tidyselect package, which allows you to make selections based on variable names. Some common ones include:\n\ntidyselect::starts_with()\ntidyselect::ends_with()\ntidyselect::contains()\ntidyselect::everything()\n\nCheck out recipes’ ?selections and the tidyselect docs for a more exhaustive list of available selection functions. Included above are the ones I commonly use. Here are a few examples of how to use these selector functions to center variables.\n\n# Apply the centering to variables that start with the *weather* prefix\nchicago_rec &lt;- \n  recipe(ridership ~ ., data = chicago_train) |&gt;\n  step_center(starts_with(\"weather\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nchicago_rec |&gt; select(starts_with(\"weather\"))\n\n# A tibble: 4,558 × 4\n   weather_rain weather_snow weather_cloud weather_storm\n          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1      -0.0803      -0.0529       0.0973        -0.0518\n 2      -0.0386       0.572       -0.403          0.0315\n 3      -0.0803       0.147       -0.00272        0.0398\n 4      -0.0803      -0.0529       0.264          0.198 \n 5      -0.0803      -0.0529       0.193          0.0612\n 6      -0.0803      -0.0529       0.264          0.323 \n 7      -0.0803      -0.0529       0.264          0.201 \n 8      -0.0803       0.614       -0.403         -0.236 \n 9      -0.0803      -0.0529       0.144         -0.100 \n10       0.0678      -0.0529      -0.0694        -0.112 \n# ℹ 4,548 more rows\n\n\nSelections also allows us to use the - to exclude specific variables or groupings of variables while using selector functions.\n\nchicago_rec &lt;- \n  recipe(ridership ~ ., data = chicago_train) |&gt;\n  step_center(-date, -starts_with(\"weather\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nchicago_rec\n\n# A tibble: 4,558 × 50\n     Austin Quincy_Wells Belmont Archer_35th Oak_Park Western Clark_Lake Clinton Merchandise_Mart\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1  0.756          2.49    1.44        0.914   0.645    0.903       6.57   0.972            1.29 \n 2  0.279          0.222   1.54        0.577   0.316    1.35        3.80   1.34             0.379\n 3  0.579          2.96    1.64        0.949   0.537    1.20        6.12   1.45             3.52 \n 4  0.584          1.91    0.686       0.768   0.470    0.849       5.65   0.504            1.21 \n 5  0.616          2.41    1.47        0.964   0.569    0.831       6.15   0.995            1.92 \n 6  0.660          2.68    1.42        0.982   0.532    0.981       6.02   1.20             2.32 \n 7 -1.04          -5.56   -2.33       -1.61   -0.852   -2.25      -10.7   -1.55            -3.60 \n 8 -0.00927        0.699   0.483       0.217   0.0441   0.209       1.63   0.999            0.262\n 9 -1.06          -4.55   -2.45       -1.50   -1.05    -2.00      -10.9   -1.58            -4.26 \n10  0.536          2.03    0.500       0.372   0.560    0.345       5.71   0.557            1.36 \n# ℹ 4,548 more rows\n# ℹ 41 more variables: Irving_Park &lt;dbl&gt;, Washington_Wells &lt;dbl&gt;, Harlem &lt;dbl&gt;, Monroe &lt;dbl&gt;,\n#   Polk &lt;dbl&gt;, Ashland &lt;dbl&gt;, Kedzie &lt;dbl&gt;, Addison &lt;dbl&gt;, Jefferson_Park &lt;dbl&gt;, Montrose &lt;dbl&gt;,\n#   California &lt;dbl&gt;, temp_min &lt;dbl&gt;, temp &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_change &lt;dbl&gt;, dew &lt;dbl&gt;,\n#   humidity &lt;dbl&gt;, pressure &lt;dbl&gt;, pressure_change &lt;dbl&gt;, wind &lt;dbl&gt;, wind_max &lt;dbl&gt;, gust &lt;dbl&gt;,\n#   gust_max &lt;dbl&gt;, percip &lt;dbl&gt;, percip_max &lt;dbl&gt;, weather_rain &lt;dbl&gt;, weather_snow &lt;dbl&gt;,\n#   weather_cloud &lt;dbl&gt;, weather_storm &lt;dbl&gt;, Blackhawks_Away &lt;dbl&gt;, Blackhawks_Home &lt;dbl&gt;, …\n\n# To show centering was not applied to variables with the *weather* prefix\nchicago_rec |&gt; select(starts_with(\"weather\"))\n\n# A tibble: 4,558 × 4\n   weather_rain weather_snow weather_cloud weather_storm\n          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1       0             0             0.833        0.208 \n 2       0.0417        0.625         0.333        0.292 \n 3       0             0.2           0.733        0.3   \n 4       0             0             1            0.458 \n 5       0             0             0.929        0.321 \n 6       0             0             1            0.583 \n 7       0             0             1            0.462 \n 8       0             0.667         0.333        0.0238\n 9       0             0             0.88         0.16  \n10       0.148         0             0.667        0.148 \n# ℹ 4,548 more rows\n\n\nrecipes provides functions to select variables based on role and type. This includes the has_role() and has_type() functions.\n\n# Simplified recipe, applying centering to variables with predictor role \ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)  |&gt;\n  step_center(has_role(\"predictor\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec\n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ℹ 3,553 more rows\n\n\n\n# Applying centering to variables with type numeric\ncredit_rec_type &lt;- recipe(Status ~ ., data = credit_train) |&gt;\n  step_center(has_type(match = \"numeric\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_type\n\n# A tibble: 3,563 × 14\n   Seniority Home    Time      Age Marital Records Job   Expenses Income Assets  Debt Amount   Price\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     -7.91 pare…   1.54   3.94   married no      part…    34.6   -60.7 -5233. -337.  159.    -2.20\n 2     -7.91 other -28.5  -16.1    single  yes     part…   -20.4   -90.7 -5233. -337. -641.  -970.  \n 3     -5.91 rent   13.5  -12.1    single  no      fixed    -9.42  -33.7 -5233. -337.  459.   719.  \n 4     -6.91 owner  13.5    7.94   married no      part…    49.6   -28.7 -3233.  163. -441.  -138.  \n 5     -4.91 owner -22.5  -14.1    married no      fixed    19.6   -55.7  -233. -337. -441.   130.  \n 6     -7.91 &lt;NA&gt;    1.54  -0.0589 single  no      &lt;NA&gt;    -20.4    NA      NA    NA   459.   380.  \n 7     -2.91 rent    1.54  -6.06   single  no      fixed   -11.4   -50.7 -5233. -337.  259.   230.  \n 8     -5.91 owner  13.5    5.94   married no      part…    19.6   -69.7 -2233. -337.  459.    81.8 \n 9     -5.91 rent  -10.5  -10.1    separa… no      fixed    -7.42  -12.7 -5233. -337. -591.  -925.  \n10     -6.91 rent    1.54  -8.06   married yes     free…    29.6   -40.7 -5233. -337.  -41.1 -125.  \n# ℹ 3,553 more rows\n# ℹ 1 more variable: Status &lt;fct&gt;\n\n\nAlthough has_role() and has_type() are available, you’ll most likely rely on functions that are more specific. The docs state (?has_role):\n\nIn most cases, the right approach for users will be to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\nThese include functions to select variables based on type:\n\nall_numeric() - includes all numeric variables.\nall_nominal() - includes both character and factor variables.\n\n\n# Center **all** numeric variables\ncredit_rec_type &lt;- recipe(\n  Status ~ Debt + Income + Assets,\n  data = credit_train\n) |&gt;\n  step_center(all_numeric()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_type \n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ℹ 3,553 more rows\n\n\nFunctions to select by role:\n\nall_predictors()\nall_outcomes()\n\n\n# Center all predictors\ncredit_rec_role &lt;- \n  recipe(\n    Status ~ Debt + Income + Assets, \n    data = credit_train\n  ) |&gt;\n  step_center(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_role\n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ℹ 3,553 more rows\n\n\nFunctions to select variables that intersect by role and type:\n\nall_numeric_predictors()\nall_nominal_predictors()\n\n\ncredit_rec_num_pred &lt;- \n  recipe(Status ~ ., data = credit_train) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_num_pred\n\n# A tibble: 3,563 × 14\n   Seniority Home    Time      Age Marital Records Job   Expenses Income Assets  Debt Amount   Price\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     -7.91 pare…   1.54   3.94   married no      part…    34.6   -60.7 -5233. -337.  159.    -2.20\n 2     -7.91 other -28.5  -16.1    single  yes     part…   -20.4   -90.7 -5233. -337. -641.  -970.  \n 3     -5.91 rent   13.5  -12.1    single  no      fixed    -9.42  -33.7 -5233. -337.  459.   719.  \n 4     -6.91 owner  13.5    7.94   married no      part…    49.6   -28.7 -3233.  163. -441.  -138.  \n 5     -4.91 owner -22.5  -14.1    married no      fixed    19.6   -55.7  -233. -337. -441.   130.  \n 6     -7.91 &lt;NA&gt;    1.54  -0.0589 single  no      &lt;NA&gt;    -20.4    NA      NA    NA   459.   380.  \n 7     -2.91 rent    1.54  -6.06   single  no      fixed   -11.4   -50.7 -5233. -337.  259.   230.  \n 8     -5.91 owner  13.5    5.94   married no      part…    19.6   -69.7 -2233. -337.  459.    81.8 \n 9     -5.91 rent  -10.5  -10.1    separa… no      fixed    -7.42  -12.7 -5233. -337. -591.  -925.  \n10     -6.91 rent    1.54  -8.06   married yes     free…    29.6   -40.7 -5233. -337.  -41.1 -125.  \n# ℹ 3,553 more rows\n# ℹ 1 more variable: Status &lt;fct&gt;\n\n\nSelector functions will become useful as we continue to explore the step_* functions within the recipes package.\n\n\nDay 04 - Create dummy variables using step_dummy()\nBefore starting our overview of recipes’ step_* functions, we need a bit of direction on what preprocessing steps might be required or beneficial to apply. The type of data preprocessing is determined by the model being fit. As a starting point, the Tidy Modeling with R book provides an appendix with a table of preprocessing recommendations based on the types of models being used. This table is separate from the types of feature engineering that may be applied, but it’s a good baseline for determining the initial step_* functions to be included within a recipe.\nDummy variables is the first preprocessing method highlighted in this appendix. That is, the encoding of qualitative predictors into numeric predictors. Closely related is one-hot encoding. When dummy variables are created, most commonly, nominal variable columns are converted into separate columns of 1’s and 0’s. recipes’ step_dummy() function performs these preprocessing operations.\nLet’s continue using the credit_data for today’s examples. Take note, this data contains some NA‘s. To address this issue, I’m just going to drop any cases with a missing value using dplyr’s drop_na() function. Indeed, this issue could be addressed with imputation through the use of recipes’ step_impute_* functions (more on this in the coming days).\n\n# Same code as day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\ncredit_data &lt;- credit_data |&gt;\n  drop_na()\n\n\n# Create the split, training and testing data\nset.seed(20230104)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\nHere’s the recipe we’ll use. I’m gonna keep it simple, so it’s easier to observe the results of adding step_dummy() to our recipe.\n\ncredit_rec &lt;- \n  recipe(\n    Status ~ Job + Home + Marital, \n    data = credit_train\n  ) \n\nLet’s create dummy variables from the Job column. But first, let’s take a look at how many different variable levels there are.\n\nunique(credit_data$Job)\n\n[1] freelance fixed     partime   others   \nLevels: fixed freelance others partime\n\n\nSince we have four levels (freelance, fixed, partime, others), the step_dummy() function will create three columns. The fixed Job level will be the reference group, since it’s the first level specified for the factor.\n\ncredit_rec |&gt;\n  step_dummy(Job) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 6\n   Home    Marital Status Job_freelance Job_others Job_partime\n   &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 owner   married good               0          0           0\n 2 other   married bad                0          1           0\n 3 owner   married good               0          0           0\n 4 owner   married good               1          0           0\n 5 parents single  good               0          0           0\n 6 rent    single  good               0          0           0\n 7 parents single  good               0          0           0\n 8 other   widow   good               0          0           0\n 9 priv    single  good               1          0           0\n10 owner   married bad                0          0           0\n# ℹ 3,221 more rows\n\n\nTake note of the naming conventions applied to the new dummy columns. step_dummy() uses the following naming convention variable-name_variable-level. This makes it easier to know what variable the dummy variables originated.\nSay you don’t want to drop the original column when the dummy variables are created. We can pass TRUE to the keep_original_cols argument. This will retain the original column, while also creating the dummy variables.\n\ncredit_rec |&gt;\n  step_dummy(Job, keep_original_cols = TRUE) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 7\n   Job       Home    Marital Status Job_freelance Job_others Job_partime\n   &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 fixed     owner   married good               0          0           0\n 2 others    other   married bad                0          1           0\n 3 fixed     owner   married good               0          0           0\n 4 freelance owner   married good               1          0           0\n 5 fixed     parents single  good               0          0           0\n 6 fixed     rent    single  good               0          0           0\n 7 fixed     parents single  good               0          0           0\n 8 fixed     other   widow   good               0          0           0\n 9 freelance priv    single  good               1          0           0\n10 fixed     owner   married bad                0          0           0\n# ℹ 3,221 more rows\n\n\nWhat about one-hot encoding? To apply one-hot encoding we specify FALSE to the one_hot argument within the function. The preprocessed, baked data will now contain four columns. One column for each level of the source column.\n\ncredit_rec |&gt;\n  step_dummy(Job, one_hot = TRUE) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 7\n   Home    Marital Status Job_fixed Job_freelance Job_others Job_partime\n   &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 owner   married good           1             0          0           0\n 2 other   married bad            0             0          1           0\n 3 owner   married good           1             0          0           0\n 4 owner   married good           0             1          0           0\n 5 parents single  good           1             0          0           0\n 6 rent    single  good           1             0          0           0\n 7 parents single  good           1             0          0           0\n 8 other   widow   good           1             0          0           0\n 9 priv    single  good           0             1          0           0\n10 owner   married bad            1             0          0           0\n# ℹ 3,221 more rows\n\n\nWe can scale this preprocessing to all nominal predictors by using, you guessed it, selector functions.\n\ncredit_rec |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 13\n   Status Job_freelance Job_others Job_partime Home_other Home_owner Home_parents Home_priv\n   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 good               0          0           0          0          1            0         0\n 2 bad                0          1           0          1          0            0         0\n 3 good               0          0           0          0          1            0         0\n 4 good               1          0           0          0          1            0         0\n 5 good               0          0           0          0          0            1         0\n 6 good               0          0           0          0          0            0         0\n 7 good               0          0           0          0          0            1         0\n 8 good               0          0           0          1          0            0         0\n 9 good               1          0           0          0          0            0         1\n10 bad                0          0           0          0          1            0         0\n# ℹ 3,221 more rows\n# ℹ 5 more variables: Home_rent &lt;dbl&gt;, Marital_married &lt;dbl&gt;, Marital_separated &lt;dbl&gt;,\n#   Marital_single &lt;dbl&gt;, Marital_widow &lt;dbl&gt;\n\n\nThat’s a lot of additional columns. How can we keep track of all these additional columns and how they were preprocessed? We can summary and tidy our prepped recipe. Summarizing the prepped recipe is useful because of the source column that gets outputted. In our example, the source column of the returned tibble contains two values: original (i.e., the column was an original column in the data set) and derived (i.e., a column created from the preprocessing step). When we tidy() the recipe object returned from step_dummy(), a tibble with two columns is returned: terms and columns. terms represents the original variable the dummy variables were created from. columns represents the newly preprocessed dummy variable.\n\n# Prep our dummy variables\ncredit_rec &lt;- \n  credit_rec |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep()\n\nsummary(credit_rec)\n\n# A tibble: 13 × 4\n   variable          type      role      source  \n   &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Status            &lt;chr [3]&gt; outcome   original\n 2 Job_freelance     &lt;chr [2]&gt; predictor derived \n 3 Job_others        &lt;chr [2]&gt; predictor derived \n 4 Job_partime       &lt;chr [2]&gt; predictor derived \n 5 Home_other        &lt;chr [2]&gt; predictor derived \n 6 Home_owner        &lt;chr [2]&gt; predictor derived \n 7 Home_parents      &lt;chr [2]&gt; predictor derived \n 8 Home_priv         &lt;chr [2]&gt; predictor derived \n 9 Home_rent         &lt;chr [2]&gt; predictor derived \n10 Marital_married   &lt;chr [2]&gt; predictor derived \n11 Marital_separated &lt;chr [2]&gt; predictor derived \n12 Marital_single    &lt;chr [2]&gt; predictor derived \n13 Marital_widow     &lt;chr [2]&gt; predictor derived \n\n# View what preprocessing steps are applied\ntidy(credit_rec)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id         \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;      \n1      1 step      dummy TRUE    FALSE dummy_9a72e\n\n# Drill down and view what was done in during this specific step \ntidy(credit_rec, number = 1)\n\n# A tibble: 12 × 3\n   terms   columns   id         \n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;      \n 1 Job     freelance dummy_9a72e\n 2 Job     others    dummy_9a72e\n 3 Job     partime   dummy_9a72e\n 4 Home    other     dummy_9a72e\n 5 Home    owner     dummy_9a72e\n 6 Home    parents   dummy_9a72e\n 7 Home    priv      dummy_9a72e\n 8 Home    rent      dummy_9a72e\n 9 Marital married   dummy_9a72e\n10 Marital separated dummy_9a72e\n11 Marital single    dummy_9a72e\n12 Marital widow     dummy_9a72e\n\n\nWhen it comes to specifying interactions within a model, there are some special considerations when using dummy variables. I don’t have much time to discuss this today, but I hope to address it on a future day of this challenge. I suggest reviewing the ‘Interactions with Dummy Variables’ section from the ‘Dummies’ vignette (vignettes(\"Dummies\", package = \"recipes\")) for more information.\nOne more thing, step_dummy() is useful for straight forward dummy variable creation. However, recipes also has some other closely related step_* functions. Here is a list of a few from the ‘Dummies’ vignette:\n\nstep_other() - collapses infrequently occurring levels into an ‘other’ category.\nstep_holiday() - creates dummy variables from dates to capture holidays. Useful when working with time series data.\nstep_zv() - removes dummy variables that are zero-variance.\n\nI look to highlight the use of some of these step_* functions in the coming days.\n\n\nDay 05 - Create a binary indicator variable for holidays using step_holiday()\nStaying on the topic of dummy variables, I wanted to take a day to focus on the use of recipes’ step_holiday() function. It seems to be pretty useful when working with time series data.\nFor today’s example, I’m going to use some obfuscated, simulated Google Analytics ecommerce data. This emulates data closely related to what would be collected for the Google Merchandise Store. You can learn more about this data by clicking on the previously linked docs. Let’s do some data wrangling.\nSome notes about what wrangling was done:\n\nParse the event_date column into a date variable.\nCalculate the revenue generated from the purchase of items based on quantity.\nRetain only relevant columns.\n\nFor simplicity, I’m not going to create a testing training split for this data.\n\ndata_ga &lt;- \n  read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  ) |&gt;\n  mutate(\n    event_date = ymd(event_date),\n    revenue = price_in_usd * quantity\n  ) |&gt;\n  select(event_date, transaction_id, item_category, revenue)\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s start on our recipe. Since we have an id variable, transaction_id, let’s update the recipe to change it’s role to id. Once we do that, we can pass the event_date to the step_holiday() function. Before we bake our recipe, I wanna prep() and summarise the preprocessing to see what columns will get added.\n\nga_rec &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  update_role(transaction_id, new_role = \"id\") |&gt;\n  step_holiday(event_date) |&gt;\n  prep() \n\nsummary(ga_rec)\n\n# A tibble: 7 × 4\n  variable                type      role      source  \n  &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 event_date              &lt;chr [1]&gt; predictor original\n2 transaction_id          &lt;chr [2]&gt; id        original\n3 item_category           &lt;chr [3]&gt; predictor original\n4 revenue                 &lt;chr [2]&gt; outcome   original\n5 event_date_LaborDay     &lt;chr [2]&gt; predictor derived \n6 event_date_NewYearsDay  &lt;chr [2]&gt; predictor derived \n7 event_date_ChristmasDay &lt;chr [2]&gt; predictor derived \n\n\nNote, three new columns will be added once the recipe is baked. This includes:\n\nevent_date_LaborDay - a dummy variable to represent an item purchases on Labor Day.\nevent_date_NewYearsDay - a dummy variable to represent item purchases on New Years Day.\nevent_date_ChristmasDay - a dummy variable to represent item purchases made on Christmas Day.\n\nYou can see the variables that get added by baking the recipe.\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 9,365 × 7\n   event_date transaction_id item_category       revenue event_date_LaborDay event_date_NewYearsDay\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt;               &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance                12                   0                      0\n 2 2020-12-01          10648 Accessories               2                   0                      0\n 3 2020-12-01          10648 Drinkware                 4                   0                      0\n 4 2020-12-01          10648 Small Goods               2                   0                      0\n 5 2020-12-01          10648 Office                    3                   0                      0\n 6 2020-12-01          10648 Accessories               3                   0                      0\n 7 2020-12-01          10648 Apparel                  14                   0                      0\n 8 2020-12-01         171491 Apparel                  48                   0                      0\n 9 2020-12-01         171491 Drinkware                14                   0                      0\n10 2020-12-01         174748 Uncategorized Items      44                   0                      0\n# ℹ 9,355 more rows\n# ℹ 1 more variable: event_date_ChristmasDay &lt;int&gt;\n\n\nLabor Day, New Years Day, and Christmas Day are the default holidays preprocessed by the function. You can modify this by passing a character vector of holidays to step_holiday()’s holidays argument. For instance, say we wanted to create dummy variables for Boxing Day and the United State’s Thanksgiving Day holiday, while excluding Labor Day. The following code will specify this preprocessing step for us:\n\nga_rec_holidays &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  update_role(transaction_id, new_role = \"transaction_id\") |&gt;\n  step_holiday(\n    event_date, \n    holidays = c(\"USThanksgivingDay\", \"ChristmasDay\", \"BoxingDay\", \"NewYearsDay\")\n  ) |&gt;\n  prep()\n\nsummary(ga_rec_holidays)\n\n# A tibble: 8 × 4\n  variable                     type      role           source  \n  &lt;chr&gt;                        &lt;list&gt;    &lt;chr&gt;          &lt;chr&gt;   \n1 event_date                   &lt;chr [1]&gt; predictor      original\n2 transaction_id               &lt;chr [2]&gt; transaction_id original\n3 item_category                &lt;chr [3]&gt; predictor      original\n4 revenue                      &lt;chr [2]&gt; outcome        original\n5 event_date_USThanksgivingDay &lt;chr [2]&gt; predictor      derived \n6 event_date_ChristmasDay      &lt;chr [2]&gt; predictor      derived \n7 event_date_BoxingDay         &lt;chr [2]&gt; predictor      derived \n8 event_date_NewYearsDay       &lt;chr [2]&gt; predictor      derived \n\n\nNow we have a dummy variable for all four of these holidays. Let’s bake our recipe and see the final result.\n\nbake(ga_rec_holidays, new_data = NULL)\n\n# A tibble: 9,365 × 8\n   event_date transaction_id item_category     revenue event_date_USThanksg…¹ event_date_Christmas…²\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;                  &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance              12                      0                      0\n 2 2020-12-01          10648 Accessories             2                      0                      0\n 3 2020-12-01          10648 Drinkware               4                      0                      0\n 4 2020-12-01          10648 Small Goods             2                      0                      0\n 5 2020-12-01          10648 Office                  3                      0                      0\n 6 2020-12-01          10648 Accessories             3                      0                      0\n 7 2020-12-01          10648 Apparel                14                      0                      0\n 8 2020-12-01         171491 Apparel                48                      0                      0\n 9 2020-12-01         171491 Drinkware              14                      0                      0\n10 2020-12-01         174748 Uncategorized It…      44                      0                      0\n# ℹ 9,355 more rows\n# ℹ abbreviated names: ¹​event_date_USThanksgivingDay, ²​event_date_ChristmasDay\n# ℹ 2 more variables: event_date_BoxingDay &lt;int&gt;, event_date_NewYearsDay &lt;int&gt;\n\n\nIndeed, there are many holidays that could be specified for dummy variable creation. All the available holidays can be seen by running timeDate::listHolidays() in your console. Last time I checked, there were 118 available holidays.\n\n\nDay 06 - Use step_zv() to drop variables with one value\nFor today, I’m focusing on recipes’ step_zv() function. This function is a filter function, which drops variables that only contain one value.\nAt first, I didn’t really understand why step_zv() was made available. Why would you want a step to drop variables within a recipe? Then it clicked working on yesterday’s example using the obfuscated Google Analytics data for the Google Merchandise store.\nBut first, let’s get our data again and specify our recipe. I’m going to keep things simple here. First, I’m just going to use data_ga, which was previously wrangled in yesterday’s post (check it out if you want more info). Second, I’m going to skip creating a testing and training split. Lastly, I’m going to create dummy variables using step_holiday(), just to show how step_zv() can be useful.\n\nga_rec &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  step_holiday(event_date)\n\nsummary(ga_rec)\n\n# A tibble: 4 × 4\n  variable       type      role      source  \n  &lt;chr&gt;          &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 event_date     &lt;chr [1]&gt; predictor original\n2 transaction_id &lt;chr [2]&gt; predictor original\n3 item_category  &lt;chr [3]&gt; predictor original\n4 revenue        &lt;chr [2]&gt; outcome   original\n\n\nLet’s take a closer look at our data. You’ll notice the range of the event_date is a subset of data. data_ga’s event_date ranges between the US holiday season. It starts right before Christmas and moves into the first month of the new year.\n\nc(\n  min_date = min(data_ga$event_date), \n  max_date = max(data_ga$event_date)\n)\n\n    min_date     max_date \n\"2020-12-01\" \"2021-01-30\" \n\n\nIf you remember from yesterday’s post, one of the default holidays for step_holiday() is Labor Day. As such, a dummy variable with all 0’s will be created for the Labor Day holiday. Purchases made on these dates were not included within this data.\n\nga_prep &lt;- prep(ga_rec)\n\ntidy(ga_prep, number = 1)\n\n# A tibble: 3 × 3\n  terms      holiday      id           \n  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;        \n1 event_date LaborDay     holiday_cClNx\n2 event_date NewYearsDay  holiday_cClNx\n3 event_date ChristmasDay holiday_cClNx\n\n# Check the unique values\nbake(ga_prep, new_data = NULL) |&gt; \n  select(event_date_LaborDay) |&gt;\n  distinct(event_date_LaborDay)\n\n# A tibble: 1 × 1\n  event_date_LaborDay\n                &lt;int&gt;\n1                   0\n\n\nAs such, this variable is not very useful and should be dropped before being applied within our model. This is why step_zv() can be handy, especially in situations where you have a lot of variables that could only have one value. step_zv() makes it easy to drop all unnecessary variables in one step, while allowing you to continue working with a recipe object.\nIndeed, keen observers might note this step could be mitigated by modifying the holiday argument in step_holiday(). However, the function’s utility extends beyond just step_holiday(). You might even consider useful as a final step you apply to every recipe.\n\nga_rec_drop &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  step_holiday(event_date) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep()\n\nga_rec_drop\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 9365 data points and 164 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Holiday features from: event_date | Trained\n\n\n• Zero variance filter removed: event_date_LaborDay | Trained\n\ntidy(ga_rec_drop)\n\n# A tibble: 2 × 6\n  number operation type    trained skip  id           \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;        \n1      1 step      holiday TRUE    FALSE holiday_FpWeG\n2      2 step      zv      TRUE    FALSE zv_3cDmc     \n\n\nTake note, the prep() output informs us of the variables that were dropped when the step was applied. This is something to keep an eye on, just in case you need to explore situations where many variables are dropped, and you need to explore what your recipe is actually doing.\nFor completeness, lets bake() our final recipe.\n\nga_rec &lt;- recipe(revenue ~., data = data_ga) |&gt;\n  step_holiday(event_date) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec\n\n# A tibble: 9,365 × 6\n   event_date transaction_id item_category     revenue event_date_NewYearsDay event_date_Christmas…¹\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;                  &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance              12                      0                      0\n 2 2020-12-01          10648 Accessories             2                      0                      0\n 3 2020-12-01          10648 Drinkware               4                      0                      0\n 4 2020-12-01          10648 Small Goods             2                      0                      0\n 5 2020-12-01          10648 Office                  3                      0                      0\n 6 2020-12-01          10648 Accessories             3                      0                      0\n 7 2020-12-01          10648 Apparel                14                      0                      0\n 8 2020-12-01         171491 Apparel                48                      0                      0\n 9 2020-12-01         171491 Drinkware              14                      0                      0\n10 2020-12-01         174748 Uncategorized It…      44                      0                      0\n# ℹ 9,355 more rows\n# ℹ abbreviated name: ¹​event_date_ChristmasDay\n\nglimpse(ga_rec)\n\nRows: 9,365\nColumns: 6\n$ event_date              &lt;date&gt; 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-…\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1…\n$ item_category           &lt;fct&gt; Clearance, Accessories, Drinkware, Small Goods, Office, Accessorie…\n$ revenue                 &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, …\n$ event_date_NewYearsDay  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ event_date_ChristmasDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\nDay 07 - Use step_impute_*() functions for imputation\nThe recipes package makes it easy to perform imputation tasks. As of this writing, recipes had the following functions to perform different methods of imputation:\n\nstep_impute_bag()\nstep_impute_knn()\nstep_impute_linear()\nstep_impute_lower()\nstep_impute_mean()\nstep_impute_median()\nstep_impute_mode()\nstep_impute_roll()\n\nFor today’s examples, I’m going to highlight the use of step_impute_mean(), step_input_median(), and step_input_mode(). First, though, we need some data with missing values. Let’s switch it up a bit and use the Palmer Station penguin data (run ?penguins in your console to get more information about the data). In brief, these data represent different measurements of various penguin species in Antarctica.\n\ndata(penguins, package = \"modeldata\") \n\n# Add an id column\npenguins &lt;- \n  penguins |&gt; mutate(id = 1:n(), .before = everything())\n\nThis data set contains some missing values that could be addressed using imputation methods. Let’s take a moment and explore the data a little further.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\n# What columns have missing data?\nmap_df(penguins, \\(x) any(is.na(x)))\n\n# A tibble: 1 × 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  TRUE           TRUE          TRUE              TRUE        TRUE \n\n# What percentage of data is missing in each column?\nmap(penguins, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$species\n[1] 0\n\n$island\n[1] 0\n\n$bill_length_mm\n[1] 0.005813953\n\n$bill_depth_mm\n[1] 0.005813953\n\n$flipper_length_mm\n[1] 0.005813953\n\n$body_mass_g\n[1] 0.005813953\n\n$sex\n[1] 0.03197674\n\n# Missing data examples\nmissing_examples &lt;- c(4, 12, 69, 272)\npenguins |&gt; slice(missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nThe following columns contain missing data (included are the variable types):\n\nbill_length_mm - double\nbill_depth_mm - double\nflipper_length_mm - integer\nbody_mass_g - integer\nsex - factor\n\nYou’ll also notice some of these variables are of various types (i.e. factor, double, or integer). Indeed, the variable type will determine the method of imputation applied.\n\nset.seed(20240107)\npenguins_split &lt;- initial_split(penguins, prop = .8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nLet’s start by highlighting how to apply mean substitution as our imputation method. Specifically, let’s apply this step to our first numeric variable with missing values, bill_length_mm.\n\n\n\n\n\n\nNote\n\n\n\nTake note of the importance of the use of prep() here. Remember, some recipe steps need to calculate an intermediate value before applying it to the final baked data. This is highlighted with the tidy(penquin_rec, number = 1) in the code below.\n\n\n\npenguins_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_mean(bill_length_mm) |&gt;\n  prep()\n\nsummary(penguins_rec)\n\n# A tibble: 8 × 4\n  variable          type      role      source  \n  &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 id                &lt;chr [2]&gt; id        original\n2 species           &lt;chr [3]&gt; predictor original\n3 island            &lt;chr [3]&gt; predictor original\n4 bill_length_mm    &lt;chr [2]&gt; predictor original\n5 bill_depth_mm     &lt;chr [2]&gt; predictor original\n6 flipper_length_mm &lt;chr [2]&gt; predictor original\n7 body_mass_g       &lt;chr [2]&gt; predictor original\n8 sex               &lt;chr [3]&gt; predictor original\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm  43.7 impute_mean_yJPI2\n\n\n\npenguins_baked &lt;- bake(penguins_rec, new_data = NULL)\n\npenguins_baked\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\n# Imputation should result in a complete column of data\nany(is.na(penguins_baked$bill_length_mm))\n\n[1] FALSE\n\n# The missing values have now been substituted\npenguins_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n1   272 Gentoo  Biscoe              43.7          NA                  NA          NA &lt;NA&gt; \n2     4 Adelie  Torgersen           43.7          NA                  NA          NA &lt;NA&gt; \n3    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt; \n\n\nstep_impute_mean() also includes a trim argument, which trims observations from the end of the variable before the mean is computed. This is also a tuning parameter, which can be used in any hyperparameter tuning applied within your modeling. I would like to explore this more, but it’s outside the scope of this post. Just to highlight the use of the trim argument, here’s some example code:\n\npenguin_mean_trim_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  step_impute_mean(bill_length_mm, trim = .5) |&gt;\n  prep()\n\n# Notice how the intermediate calculation changed because\n# we trimmed the observations used to make the mean calculation\ntidy(penguin_mean_trim_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm    44 impute_mean_sGprM\n\n\nLet’s bake this recipe for completeness.\n\npenguins_mean_trim_baked &lt;- \n  bake(penguin_mean_trim_rec, new_data = NULL)\n\npenguins_mean_trim_baked\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\n# The missing values have now been imputed\npenguins_mean_trim_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n1   272 Gentoo  Biscoe              44            NA                  NA          NA &lt;NA&gt; \n2     4 Adelie  Torgersen           44            NA                  NA          NA &lt;NA&gt; \n3    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt; \n\n\nMean substitution is just one imputation step. The recipes package also includes the step_impute_median() and step_impute_mode(). These step functions have similar syntax, just a different calculated metric is applied in the background. Let’s apply step_impute_median() to bill_depth_mm, flipper_length_mm, and body_mass_g.\nIn addition, we’ll apply step_impute_mode() to impute values for the missing data within the sex variable. Take note, the docs for this function state:\n\nImpute nominal data using the most common value.\n\nSo, it only seems step_impute_mode() can only be used to impute missing values for nominal variables.\n\npenguin_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  step_impute_mean(bill_length_mm) |&gt;\n  step_impute_median(\n    bill_depth_mm, \n    flipper_length_mm, \n    body_mass_g\n  ) |&gt;\n  step_impute_mode(sex) |&gt;\n  prep()\n\npenguin_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 8\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 11 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: bill_length_mm | Trained\n\n\n• Median imputation for: bill_depth_mm, flipper_length_mm, body_mass_g | Trained\n\n\n• Mode imputation for: sex | Trained\n\ntidy(penguin_rec)\n\n# A tibble: 3 × 6\n  number operation type          trained skip  id                 \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;              \n1      1 step      impute_mean   TRUE    FALSE impute_mean_fOycb  \n2      2 step      impute_median TRUE    FALSE impute_median_zocWX\n3      3 step      impute_mode   TRUE    FALSE impute_mode_PGohX  \n\n\nLet’s take a look at the calculated values for all these steps.\n\nmap(1:3, \\(x) tidy(penguin_rec, number = x))\n\n[[1]]\n# A tibble: 1 × 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm  43.7 impute_mean_fOycb\n\n[[2]]\n# A tibble: 3 × 3\n  terms              value id                 \n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;              \n1 bill_depth_mm       17.3 impute_median_zocWX\n2 flipper_length_mm  196   impute_median_zocWX\n3 body_mass_g       4050   impute_median_zocWX\n\n[[3]]\n# A tibble: 1 × 3\n  terms value  id               \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;            \n1 sex   female impute_mode_PGohX\n\n\nAs always, let’s bake this recipe and look at the final data, which should now contain no missing data.\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\nmap_df(baked_penguin, \\(x) any(is.na(x)))\n\n# A tibble: 1 × 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  FALSE          FALSE         FALSE             FALSE       FALSE\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              43.7          17.3               196        4050 female\n2     4 Adelie  Torgersen           43.7          17.3               196        4050 female\n3    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n\n\nThat’s all the time I have for today. Tomorrow I’ll pick up exploring some more of the other step_impute_* functions.\n\n\nDay 08 - Use bagged tree models to impute missing data with step_impute_bag()\nTo start, I wanted to highlight a really good, simplified definition of imputation from the Feature Engineering and Selection: A Practical Approach for Predictive Models book by Max Kuhn and Kjell Johnson.\n\nImputation uses information and relationships among the non-missing predictors to provide an estimate to fill in the missing values.\n\nYesterday we used the step_impute_mean(), step_impute_median(), and step_impute_mode() functions to calculate missing values. However, we can also use tree-based methods, which uses information from different variables rather than just values in rows, to perform our imputation step.\nTo be honest, this imputation method was beyond my current knowledge set. Thus, my explanation of what is happening on the backend may be quite general. However, check out the ‘Trees’ section from the Feature Engineering and Selection: A Practical Approach for Predictive Models book for a good starting point to learn more. The book does suggest using bagged models can produce reasonable outputs, which results in values to be produced within the range of the training data. Such methods also retains all predictors, unlike when case-wise deletion is used to manage missing data.\nrecipes’ step_impute_bag() function is used to impute missing data using bagged tree models. To highlight the use of this step, let’s go back to using the penguins data from yesterday.\n\ndata(penguins, package = \"modeldata\")\nmissing_examples &lt;- c(4, 12, 69, 272)\n\n# Create an id variable\npenguins &lt;- \n  penguins |&gt; \n  mutate(id = 1:n(), .before = everything())\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\n# Print the missing examples\npenguins |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nWe’ll now create our training and testing split.\n\nset.seed(20240108)\npenguins_split &lt;- initial_split(penguins, prop = 0.8)\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nTo start small, let’s use a bagged tree model to impute values for the missing data in the bill_length_mm variable. The syntax is pretty straightforward:\n\npenguins_rec &lt;- recipe (~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(bill_length_mm) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 9 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Bagged tree imputation for: bill_length_mm | Trained\n\n\nBefore we bake() our recipe, let’s tidy() our prepped recipe a bit to see what’s happening under the hood.\n\ntidy(penguins_rec)\n\n# A tibble: 1 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_bag TRUE    FALSE impute_bag_OQalP\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms          model     id              \n  &lt;chr&gt;          &lt;list&gt;    &lt;chr&gt;           \n1 bill_length_mm &lt;regbagg&gt; impute_bag_OQalP\n\n\nTidying down to the bagging step, you’ll see this step outputs a tibble with three columns:\n\nterms - the selectors or variables selected.\nmodel - the bagged tree model object.\nid - a unique id for the step being applied in the recipe.\n\nLet’s bake the recipe and see the result of our imputation step.\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              43.7          17.3               196        4050 female\n2     4 Adelie  Torgersen           43.7          17.3               196        4050 female\n3    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n\n\nstep_impute_bagged() also has several options to modify the imputation method. First, it has an impute_with argument that allows you to be selective about what variables are used as predictors in the bagged tree model. We’ll specify these variables by passing them into the imp_vars() function to the argument.\nThis argument accepts the various selector functions as well. For instance, the default for the argument is the all_predictors() function. The following code uses this argument to limit the imputation to the bill_depth_mm and sex variables (I’m not a biologist, so I have no idea if this is actually a good approach).\nI did come across a cryptic warning when first doing this, though. This warning also resulted in the imputation step to not be applied.\n\npenguin_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    impute_with = imp_vars(bill_depth_mm, sex)\n  ) |&gt;\n  prep()\n\nWarning: All predictors are missing; cannot impute.\n\n\nI assumed this was because all the predictors used to create the model for imputation had missing values. So, I applied some imputation to these first before applying the step_impute_bag() and the warning went away. However, I’m unsure if this was the initial problem. I might submit an issue to the recipes GitHub repo to which I’ll link later. Nevertheless, I got the example to work. Here’s the code:\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_mean(bill_depth_mm) |&gt;\n  step_impute_mode(sex) |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    impute_with = imp_vars(bill_depth_mm)\n  ) |&gt;\n  prep() \n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           41.0          17.2                NA          NA male  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 male  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              41.0          17.2                NA          NA male  \n\n\nGiven we’re using a bagged tree model to perform imputation, we can modify the number of bagged trees used in each model in the step_impute_bag() function. To do this, we just pass a value to the trees argument. Indeed, its suggested to keep this value between 25 - 50 trees.\n\n# The default is 25 trees\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(bill_length_mm, trees = 50) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           38.3          NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              47.8          NA                  NA          NA &lt;NA&gt;  \n\n\nThe last step_impute_bag() argument I’ll highlight is options. ipred::ipredbagg() implements the bagged model used for this imputation step. Thus, the options argument is used to pass arguments to this function. For example, if we want to speed up execution, we can lower the nbagg argument, the number of bootstrap replications applied, and indicate we don’t want to return a data frame of predictors by setting keepX = FALSE.\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    options = list(nbagg = 2, keepX = FALSE)\n  ) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   280 Chinstrap Dream               45.4          18.7               188        3525 female\n 2   160 Gentoo    Biscoe              46.7          15.3               219        5200 male  \n 3    27 Adelie    Biscoe              40.6          18.6               183        3550 male  \n 4   274 Gentoo    Biscoe              50.4          15.7               222        5750 male  \n 5   288 Chinstrap Dream               51.7          20.3               194        3775 male  \n 6    46 Adelie    Dream               39.6          18.8               190        4600 male  \n 7   316 Chinstrap Dream               53.5          19.9               205        4500 male  \n 8   286 Chinstrap Dream               51.3          19.9               198        3700 male  \n 9   164 Gentoo    Biscoe              49            16.1               216        5550 male  \n10    79 Adelie    Torgersen           36.2          16.1               187        3550 female\n# ℹ 265 more rows\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           38.1          NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              48.0          NA                  NA          NA &lt;NA&gt;  \n\n\nJust for the heck of it, let’s apply step_impute_bag() to all predictor variables in our recipe.\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(all_predictors()) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   280 Chinstrap Dream               45.4          18.7               188        3525 female\n 2   160 Gentoo    Biscoe              46.7          15.3               219        5200 male  \n 3    27 Adelie    Biscoe              40.6          18.6               183        3550 male  \n 4   274 Gentoo    Biscoe              50.4          15.7               222        5750 male  \n 5   288 Chinstrap Dream               51.7          20.3               194        3775 male  \n 6    46 Adelie    Dream               39.6          18.8               190        4600 male  \n 7   316 Chinstrap Dream               53.5          19.9               205        4500 male  \n 8   286 Chinstrap Dream               51.3          19.9               198        3700 male  \n 9   164 Gentoo    Biscoe              49            16.1               216        5550 male  \n10    79 Adelie    Torgersen           36.2          16.1               187        3550 female\n# ℹ 265 more rows\n\n# There should now be no missing data\nmap_df(baked_penguin, \\(x) any(is.na(x)))\n\n# A tibble: 1 × 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  FALSE          FALSE         FALSE             FALSE       FALSE\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           37.9          17.8               188        3546 male  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              47.9          14.8               214        5028 female\n\n\nThat’s it for today. Tomorrow I’ll focus on the use of step_impute_knn().\n\n\nDay 09 - Impute missing values using step_impute_knn()\nToday, we’re focusing on imputing missing data using recipes’ step_impute_knn() function. In short, this function uses a k-nearest neighbors approach to impute missing values.\nFor today’s examples, I’m going to stick with the penguins data we’ve been using the past few days. Given this data is relatively small (n = 344), it’s a good candidate for using a k-nearest neighbor approach to imputation.\n\ndata(penguins, package = \"modeldata\")\n\n# Create a row id\npenguins &lt;- penguins |&gt;\n  mutate(id = 1:n(), .before = everything())\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\n# Percent missing for each column\nmap(penguins, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$species\n[1] 0\n\n$island\n[1] 0\n\n$bill_length_mm\n[1] 0.005813953\n\n$bill_depth_mm\n[1] 0.005813953\n\n$flipper_length_mm\n[1] 0.005813953\n\n$body_mass_g\n[1] 0.005813953\n\n$sex\n[1] 0.03197674\n\n\nJust for a refresher, let’s peek at a few of the missing values.\n\nmissing_examples &lt;- c(4, 12, 69, 272)\n\npenguins |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nLet’s create our training and testing split.\n\nset.seed(20240109)\npenguins_split &lt;- initial_split(penguins, prop = 0.8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nSince imputation can be applied to either numeric or nominal data, step_impute_knn() uses Gower’s distance for calculating nearest neighbors (you can learn more by running ?step_impute_knn in your console). Once the neighbors are calculated, nominal variables are predicted using the mean, and numeric data is predicted using the mode. The number of neighbors can be set by specifying the neighbors argument of the function, which can also be used for hyperparameter tuning.\nLet’s start by imputing values for our missing data in the sex column. Here’s the code for this initial recipe.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(sex) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• K-nearest neighbor imputation for: sex | Trained\n\n\nBefore we bake and examine what the imputation step does, let’s drill down and see what’s occurring at the prep stage. tidy() will be used to do this. Similar to step_impute_bag(), a tibble of terms, predictors, neighbors (specific to k-nearest neighbors), and an id is returned.\n\ntidy(penguins_rec)\n\n# A tibble: 1 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_knn TRUE    FALSE impute_knn_dK6OX\n\n# Drill down into the specific impute_knn step\ntidy(penguins_rec, number = 1)\n\n# A tibble: 6 × 4\n  terms predictors        neighbors id              \n  &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;           \n1 sex   species                   5 impute_knn_dK6OX\n2 sex   island                    5 impute_knn_dK6OX\n3 sex   bill_length_mm            5 impute_knn_dK6OX\n4 sex   bill_depth_mm             5 impute_knn_dK6OX\n5 sex   flipper_length_mm         5 impute_knn_dK6OX\n6 sex   body_mass_g               5 impute_knn_dK6OX\n\n\nLet’s bake our recipe and examine the result of our imputation step.\n\nbaked_penguins &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguins\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   168 Gentoo    Biscoe              49.3          15.7               217        5850 male  \n 2   196 Gentoo    Biscoe              49.6          15                 216        4750 male  \n 3   117 Adelie    Torgersen           38.6          17                 188        2900 female\n 4    39 Adelie    Dream               37.6          19.3               181        3300 female\n 5   299 Chinstrap Dream               43.2          16.6               187        2900 female\n 6   207 Gentoo    Biscoe              46.5          14.4               217        4900 female\n 7   167 Gentoo    Biscoe              45.8          14.6               210        4200 female\n 8   101 Adelie    Biscoe              35            17.9               192        3725 female\n 9   339 Chinstrap Dream               45.7          17                 195        3650 female\n10   267 Gentoo    Biscoe              46.2          14.1               217        4375 female\n# ℹ 265 more rows\n\nbaked_penguins |&gt; \n  filter(id %in% missing_examples) |&gt; \n  relocate(sex, .after = 1) \n\n# A tibble: 3 × 8\n     id sex    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1   272 male   Gentoo  Biscoe              NA            NA                  NA          NA\n2    69 female Adelie  Torgersen           35.9          16.6               190        3050\n3     4 male   Adelie  Torgersen           NA            NA                  NA          NA\n\n\nAs mentioned before, neighbors is an argument to set the number of neighbors to use in our estimation. The function defaults to five, but we can modify this to any integer value. It is suggested that 5 - 10 neighbors is a sensible default. However, this is dependent on the data you are working with. For our next example, let’s constrain this parameter to 3, while also applying our imputation step to all numeric predictors.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(all_numeric_predictors(), neighbors = 3) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• K-nearest neighbor imputation for: bill_length_mm and bill_depth_mm, ... | Trained\n\n\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              47.2          15.4               214        5217 &lt;NA&gt;  \n2    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n3     4 Adelie  Torgersen           36.6          18.3               184        3658 &lt;NA&gt;  \n\n\nJust like step_impute_bag(), step_impute_knn() provides both an impute_with and options argument. We can be explicit about the variables to use with our knn calculations by passing a comma-separated list of names to the imp_vars() function to the impute_with arugment. options accepts a list of arguments. These get passed along to the underlying gower::gower_topn() function running under the hood, which performs the k-nearest neighbors calculation using Gower’s distance. According to the docs, the only two options accepted are:\n\nnthread - specify the number of threads to use for parallelization.\neps - optional option for variable ranges (I’m not quite sure what this does).\n\nMy assumption is these options can be used to optimize the run-time for our calculations. However, I would consult the documentation for the gower::gower_topn() function to verify. The key takeaway here is that step_impute_knn() provides an interface to configure options for the function it wraps.\nHere’s an example constraining our sex imputation to the bill_length and bill_depth_mm variables.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(\n    sex, \n    impute_with = imp_vars(bill_depth_mm, bill_length_mm)\n  ) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• K-nearest neighbor imputation for: sex | Trained\n\n\nLet’s bake our final example and examine what happened.\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin |&gt; \n  filter(id %in% missing_examples) |&gt;\n  relocate(sex, .after = 1)\n\n# A tibble: 3 × 8\n     id sex    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1   272 female Gentoo  Biscoe              NA            NA                  NA          NA\n2    69 female Adelie  Torgersen           35.9          16.6               190        3050\n3     4 female Adelie  Torgersen           NA            NA                  NA          NA\n\n\nSo there you have it, another example of a step_impute_* function. Tomorrow I’ll continue exploring imputation steps by highlighting the use of the step_impute_linear() function.\n\n\nDay 10 - Impute missing values using a linear model with step_impute_linear()\nSo here we are, day 10. We continue our overview of recipes’ imputation steps. Specifically, I’m going to highlight the use of step_impute_linear() for today. step_impute_linear() uses linear regression models to impute missing data. Indeed, when there is a strong, linear relationship between a complete predictor variable and one that requires imputation (i.e., contains missing data), linear methods for imputation may be a good approach. Such a method is also really quick and requires few computational resources to calculate.\nFor today’s examples, we’re going back to our credit_data data. You can read more about this data by running ?credit_data in your console.\n\ndata(credit_data, package = \"modeldata\")\n\ncredit_data &lt;- credit_data |&gt;\n  mutate(id = seq_len(nrow(credit_data)), .before = everything()) |&gt;\n  as_tibble()\n\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 15\n$ id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\nmap(credit_data, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$Status\n[1] 0\n\n$Seniority\n[1] 0\n\n$Home\n[1] 0.001347104\n\n$Time\n[1] 0\n\n$Age\n[1] 0\n\n$Marital\n[1] 0.0002245173\n\n$Records\n[1] 0\n\n$Job\n[1] 0.0004490346\n\n$Expenses\n[1] 0\n\n$Income\n[1] 0.08554109\n\n$Assets\n[1] 0.01055231\n\n$Debt\n[1] 0.004041311\n\n$Amount\n[1] 0\n\n$Price\n[1] 0\n\n\nLet’s peek at some missing data examples.\n\nmissing_examples &lt;- c(114, 195, 206, 242, 496)\n\ncredit_data |&gt; filter(id %in% missing_examples) \n\n# A tibble: 5 × 15\n     id Status Seniority Home   Time   Age Marital Records Job   Expenses Income Assets  Debt Amount\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1   114 bad            0 owner    36    39 single  no      free…       35     NA   4000     0   1000\n2   195 bad            0 other    36    48 married yes     free…       45     NA      0     0   1600\n3   206 good          10 owner    36    45 married yes     free…       60     NA   9500   250    750\n4   242 bad           10 rent     60    43 married no      free…       90     NA      0     0   1350\n5   496 bad            3 owner    60    33 separa… no      free…       35     NA   6000     0    950\n# ℹ 1 more variable: Price &lt;int&gt;\n\n\nAs mentioned before, linear imputation methods are useful in cases where you have a strong, linear relationship between a complete variable (i.e., contains no missing data) and one where imputation is to be applied. For our example, let’s use linear methods to impute values for missing data in the Income variable. The Senority variable is complete, so we’ll use it for our imputation step. Although the relationship between these two variables isn’t a strong, linear one, let’s use it for example sake.\n\n# Use corrr::correlate() to examine correlation among numeric variables\ncorrelate(credit_data)\n\nNon-numeric variables removed from input: `Status`, `Home`, `Marital`, `Records`, and `Job`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 10 × 11\n   term            id Seniority     Time     Age Expenses  Income   Assets     Debt   Amount   Price\n   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 id        NA        -0.00362  8.19e-3 -0.0165 -2.71e-1 -0.116  -0.00533 -0.00578  0.0264   0.0266\n 2 Seniority -0.00362  NA       -2.14e-2  0.506   1.26e-1  0.122   0.127   -0.0191  -0.00791  0.0409\n 3 Time       0.00819  -0.0214  NA       -0.0517 -8.40e-4 -0.0430 -0.0848   0.0577   0.431    0.130 \n 4 Age       -0.0165    0.506   -5.17e-2 NA       2.48e-1  0.147   0.185   -0.0459   0.0292   0.0489\n 5 Expenses  -0.271     0.126   -8.40e-4  0.248  NA        0.258   0.0184   0.0148   0.0492   0.0403\n 6 Income    -0.116     0.122   -4.30e-2  0.147   2.58e-1 NA       0.237    0.151    0.192    0.227 \n 7 Assets    -0.00533   0.127   -8.48e-2  0.185   1.84e-2  0.237  NA        0.191    0.147    0.200 \n 8 Debt      -0.00578  -0.0191   5.77e-2 -0.0459  1.48e-2  0.151   0.191   NA        0.0525   0.0456\n 9 Amount     0.0264   -0.00791  4.31e-1  0.0292  4.92e-2  0.192   0.147    0.0525  NA        0.725 \n10 Price      0.0266    0.0409   1.30e-1  0.0489  4.03e-2  0.227   0.200    0.0456   0.725   NA     \n\n\nWe start off by creating our testing and training split.\n\nset.seed(20240110)\ncredit_split &lt;- initial_split(credit_data, prop = .80)\n\ncredit_tr &lt;- training(credit_split)\ncredit_te &lt;- testing(credit_split)\n\n\n\n\n\n\n\nNote\n\n\n\nThe docs mention imputed variables must be of type numeric. Also, this method requires predictors to be complete cases. As such, the imputation model will only use training set predictors that don’t have any missing values.\n\n\nJust like we did with other imputation methods that use a modeling approach, we can be specific about what variables to use as predictors. We do this by passing them to the impute_with argument, which are wrapped in the imp_vars() function. Here’s what this looks like:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_linear(Income, impute_with = imp_vars(Seniority)) |&gt;\n  prep(credit_tr)\n\ncredit_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\nid:         1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3563 data points and 319 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Linear regression imputation for: Income | Trained\n\n\nBefore we bake our recipe, let’s take a look at what’s happening under the hood with tidy().\n\ntidy(credit_rec)\n\n# A tibble: 1 × 6\n  number operation type          trained skip  id                 \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;              \n1      1 step      impute_linear TRUE    FALSE impute_linear_5l9Ot\n\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms  model  id                 \n  &lt;chr&gt;  &lt;list&gt; &lt;chr&gt;              \n1 Income &lt;lm&gt;   impute_linear_5l9Ot\n\n\nA tibble is outputted with three columns:\n\nterms - the variable we’re seeking to replace missing values with imputed values.\nmodel - the model object used to calculate the imputed value. Note, the model object is lm.\nid - a unique id for the step being performed.\n\nReady, get set, bake.\n\ncredit_baked &lt;- bake(credit_rec, new_data = NULL)\n\n# View some of the imputed values\ncredit_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 15\n     id Status Seniority Home   Time   Age Marital Records Job   Expenses Income Assets  Debt Amount\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1   496 bad            3 owner    60    33 separa… no      free…       35    135   6000     0    950\n2   242 bad           10 rent     60    43 married no      free…       90    143      0     0   1350\n3   206 good          10 owner    36    45 married yes     free…       60    143   9500   250    750\n# ℹ 1 more variable: Price &lt;int&gt;\n\n\nThe recipes’ step_impute_linear() documentation also has a really good example of how to visualize the imputed data, using a dataset with complete values. It’s purpose is to show a comparison between the original values and the newly imputed values.\nThe following code is adapted from this example. Here I show the regression line used for the imputed Income values based on Seniority. It’s just a linear regression. If you’re interested in seeing the full example, run ?step_impute_linear() in your console, and scroll down to the examples section.\n\nggplot(credit_baked, aes(x = Seniority, y = Income)) +\n  geom_abline(col = \"green\") +\n  geom_point(alpha = .3) +\n  labs(title = \"Imputed Values\")\n\n\n\n\n\n\n\n\nAgain, this relationship is not a strong, linear one. Therefore, a linear imputation method of estimation may not be the best approach for this data. Nevertheless, it serves as an example of how to do it using the recipes package.\nDay 10, check. Another 20 to go. Tomorrow I’ll continue my exploration of step_impute_*() functions by highlighting the use of the step_impute_lower() function.\n\n\nDay 11 - Impute values using step_impute_lower()\nstep_impute_lower() is our focus for today. According to the docs, step_impute_lower() calculates a variable’s minimum, simulates a value between zero and the minimum, and imputes this value for any cases at the minimum value. This imputation method is useful when we have non-negative numeric data, where values cannot be measured below a known value.\nFor today’s examples, I’m going to use the modeldata package’s crickets data. This data comes from a study examining the relationship between chirp rates and temperature for two different cricket species (run ?crickets in your console to learn more).\n\ndata(crickets, package = \"modeldata\")\n\nglimpse(crickets)\n\nRows: 31\nColumns: 3\n$ species &lt;fct&gt; O. exclamationis, O. exclamationis, O. exclamationis, O. exclamationis, O. exclama…\n$ temp    &lt;dbl&gt; 20.8, 20.8, 24.0, 24.0, 24.0, 24.0, 26.2, 26.2, 26.2, 26.2, 28.4, 29.0, 30.4, 30.4…\n$ rate    &lt;dbl&gt; 67.9, 65.1, 77.3, 78.7, 79.4, 80.4, 85.8, 86.6, 87.5, 89.1, 98.6, 100.8, 99.3, 101…\n\nskim(crickets)\n\n\nData summary\n\n\nName\ncrickets\n\n\nNumber of rows\n31\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1\nFALSE\n2\nO. : 17, O. : 14\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntemp\n0\n1\n23.76\n3.82\n17.2\n20.80\n24.0\n26.35\n30.4\n▆▆▆▇▅\n\n\nrate\n0\n1\n72.89\n16.91\n44.3\n59.45\n76.2\n85.25\n101.7\n▅▅▇▆▃\n\n\n\n\n\nWe’re also going to modify the data a bit to better highlight what step_impute_lower() is doing. Here I’m just truncating all temp values less than or equal to 21 to 21.\n\n# Create a floor at temp = 21\ncrickets &lt;- crickets |&gt; \n  mutate(temp = case_when(\n    temp &lt;= 21 ~ 21,\n    TRUE ~ as.double(temp)\n  ))\n\nprint(crickets, n = 50)\n\n# A tibble: 31 × 3\n   species           temp  rate\n   &lt;fct&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 O. exclamationis  21    67.9\n 2 O. exclamationis  21    65.1\n 3 O. exclamationis  24    77.3\n 4 O. exclamationis  24    78.7\n 5 O. exclamationis  24    79.4\n 6 O. exclamationis  24    80.4\n 7 O. exclamationis  26.2  85.8\n 8 O. exclamationis  26.2  86.6\n 9 O. exclamationis  26.2  87.5\n10 O. exclamationis  26.2  89.1\n11 O. exclamationis  28.4  98.6\n12 O. exclamationis  29   101. \n13 O. exclamationis  30.4  99.3\n14 O. exclamationis  30.4 102. \n15 O. niveus         21    44.3\n16 O. niveus         21    47.2\n17 O. niveus         21    47.6\n18 O. niveus         21    49.6\n19 O. niveus         21    50.3\n20 O. niveus         21    51.8\n21 O. niveus         21    60  \n22 O. niveus         21    58.5\n23 O. niveus         21    58.9\n24 O. niveus         22.1  60.7\n25 O. niveus         23.5  69.8\n26 O. niveus         24.2  70.9\n27 O. niveus         25.9  76.2\n28 O. niveus         26.5  76.1\n29 O. niveus         26.5  77  \n30 O. niveus         26.5  77.7\n31 O. niveus         28.6  84.7\n\n\nFor simplicity, I’m not going to create a testing and training split and will use the full data for our example. Let’s start with our recipe.\n\ncrickets_rec &lt;- recipe(~., data = crickets) |&gt;\n  step_impute_lower(temp) |&gt;\n  prep()\n\ncrickets_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 31 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Lower bound imputation for: temp | Trained\n\n\nLet’s take a quick peek at what’s happening under the hood by tidying our recipe object.\n\ntidy(crickets_rec)\n\n# A tibble: 1 × 6\n  number operation type         trained skip  id                \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;             \n1      1 step      impute_lower TRUE    FALSE impute_lower_h7llz\n\ntidy(crickets_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms value id                \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;             \n1 temp     21 impute_lower_h7llz\n\n\nYou’ll notice the value column in the tibble represents the minimum value within the column. Using this value, step_impute_lower() will replace these values with any value between 0 and 21. Let’s bake this recipe and verify this is the case.\n\nbaked_crickets &lt;- bake(crickets_rec, new_data = NULL)\n\nprint(baked_crickets, n = 50)\n\n# A tibble: 31 × 3\n   species             temp  rate\n   &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 O. exclamationis  7.84    67.9\n 2 O. exclamationis 20.1     65.1\n 3 O. exclamationis 24       77.3\n 4 O. exclamationis 24       78.7\n 5 O. exclamationis 24       79.4\n 6 O. exclamationis 24       80.4\n 7 O. exclamationis 26.2     85.8\n 8 O. exclamationis 26.2     86.6\n 9 O. exclamationis 26.2     87.5\n10 O. exclamationis 26.2     89.1\n11 O. exclamationis 28.4     98.6\n12 O. exclamationis 29      101. \n13 O. exclamationis 30.4     99.3\n14 O. exclamationis 30.4    102. \n15 O. niveus         5.51    44.3\n16 O. niveus         7.75    47.2\n17 O. niveus         6.99    47.6\n18 O. niveus         8.92    49.6\n19 O. niveus        15.2     50.3\n20 O. niveus         0.0919  51.8\n21 O. niveus        13.6     60  \n22 O. niveus        12.0     58.5\n23 O. niveus        10.0     58.9\n24 O. niveus        22.1     60.7\n25 O. niveus        23.5     69.8\n26 O. niveus        24.2     70.9\n27 O. niveus        25.9     76.2\n28 O. niveus        26.5     76.1\n29 O. niveus        26.5     77  \n30 O. niveus        26.5     77.7\n31 O. niveus        28.6     84.7\n\n\nGreat! All values of 21 have now been imputed with a value between 0 and the minimum value for the column, 21.\nWe can also create a plot to highlight what step_impute_lower() is doing. This approach is adapted from the example in the docs (?step_impute_lower()).\n\nplot(baked_crickets$temp, crickets$temp,\n  ylab = \"pre-imputation\", xlab = \"imputed\"\n)\n\n\n\n\n\n\n\n\nstep_impute_lower() is pretty straightforward in my opinion. Give it a try. That’s it for today. A short one. Tomorrow is our final step_impute_*() function, step_impute_roll().\n\n\nDay 12 - Impute values using a rolling window via step_impute_roll()\nWe’re going to round out our discussion of step_impute_*() functions by highlighting the use of step_impute_roll(). step_impute_roll() utilizes window functions to calculate missing values. At first, I had trouble understanding how window functions are utilized. However, seeing a simplified example helped me better understand what was occurring.\nLet’s get some data for today’s examples. I’m going back to our obfuscated Google Analytics data from the Google Merchandise store for today’s examples.\n\ndata_ga &lt;- \n  read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  ) |&gt;\n  mutate(\n    event_date = ymd(event_date),\n    revenue = price_in_usd * quantity\n  ) \n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data_ga)\n\nRows: 9,365\nColumns: 15\n$ event_date              &lt;date&gt; 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-…\n$ purchase_revenue_in_usd &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 62, 62, 44, 28, 28, 36, 36, 36, 36, 92…\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1…\n$ item_name               &lt;chr&gt; \"Google Hemp Tote\", \"Android SM S/F18 Sticker Sheet\", \"Android Buo…\n$ item_category           &lt;chr&gt; \"Clearance\", \"Accessories\", \"Drinkware\", \"Small Goods\", \"Office\", …\n$ price_in_usd            &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 7, 92, 7, 14,…\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 53, 1, 1, 1, 1,…\n$ item_revenue_in_usd     &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, …\n$ shipping_tier           &lt;chr&gt; \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"F…\n$ payment_type            &lt;chr&gt; \"Pay with credit card\", \"Pay with credit card\", \"Pay with credit c…\n$ category                &lt;chr&gt; \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobil…\n$ country                 &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United States\"…\n$ region                  &lt;chr&gt; \"California\", \"California\", \"California\", \"California\", \"Californi…\n$ city                    &lt;chr&gt; \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San J…\n$ revenue                 &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, …\n\nskim(data_ga)\n\n\nData summary\n\n\nName\ndata_ga\n\n\nNumber of rows\n9365\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nDate\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n385\n0\n\n\nitem_category\n164\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n109\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n96\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n289\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n434\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nevent_date\n0\n1\n2020-12-01\n2021-01-30\n2020-12-16\n61\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npurchase_revenue_in_usd\n0\n1\n101.84\n118.06\n2\n39\n72\n125\n1530\n▇▁▁▁▁\n\n\ntransaction_id\n0\n1\n487977.44\n282873.71\n546\n249662\n479506\n724658\n999850\n▇▇▇▇▇\n\n\nprice_in_usd\n0\n1\n19.52\n18.74\n1\n7\n14\n24\n120\n▇▂▁▁▁\n\n\nquantity\n0\n1\n1.45\n2.77\n1\n1\n1\n1\n160\n▇▁▁▁▁\n\n\nitem_revenue_in_usd\n0\n1\n23.26\n28.61\n1\n8\n15\n30\n704\n▇▁▁▁▁\n\n\nrevenue\n0\n1\n23.26\n28.58\n1\n8\n15\n30\n704\n▇▁▁▁▁\n\n\n\n\n\nTo make it clear what step_impute_roll() is doing, I’m going to wrangle the data to only look at total revenue for Clearance item purchases for a specific date range (2 weeks). Since this data is complete, I will introduce some missing values into the data.\n\ndata_ga &lt;- data_ga |&gt;\n  select(event_date, transaction_id, item_category, revenue) |&gt;\n  filter(item_category == \"Clearance\") |&gt;\n  group_by(event_date) |&gt;\n  summarise(total_rev = sum(revenue))  |&gt;\n  filter(\n    event_date &gt;= as_date('2020-12-14') & \n    event_date &lt;= as_date('2020-12-27')\n  )\n\n# Introduce some NAs for the example\ndata_ga$total_rev[c(1, 6, 7, 8, 14)] &lt;- NA\n\nLet’s get our recipe set up to the point of prepping it.\n\nga_rec &lt;- recipe(~., data = data_ga) |&gt;\n  update_role(event_date, new_role = \"data_ref\") |&gt;\n  step_impute_roll(total_rev, window = 3) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\ndata_ref:  1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 14 data points and 5 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Rolling imputation for: total_rev | Trained\n\ntidy(ga_rec)\n\n# A tibble: 1 × 6\n  number operation type        trained skip  id               \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;            \n1      1 step      impute_roll TRUE    FALSE impute_roll_6HJg3\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms     window id               \n  &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;            \n1 total_rev      3 impute_roll_6HJg3\n\n\nNot too much useful information here. Let’s move forward with bake and see the imputed values. Take note, we will still have a missing value (more on this later).\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 14 × 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14      324.\n 2 2020-12-15      323 \n 3 2020-12-16      324 \n 4 2020-12-17      203 \n 5 2020-12-18      215 \n 6 2020-12-19      215 \n 7 2020-12-20       NA \n 8 2020-12-21       55 \n 9 2020-12-22       55 \n10 2020-12-23      256 \n11 2020-12-24       32 \n12 2020-12-25       73 \n13 2020-12-26       19 \n14 2020-12-27       46 \n\n\nWe now have a complete data set. But, how were these imputed values calculated? If you peek at step_impute_roll()’s arguments, you’ll notice it contains a statistic argument set to median. It should be pretty intuitive that we are calculating the median here, but the median of what? It’s the median of our window we set in the function, which was window = 3.\n\nargs(step_impute_roll)\n\nfunction (recipe, ..., role = NA, trained = FALSE, columns = NULL, \n    statistic = median, window = 5, skip = FALSE, id = rand_id(\"impute_roll\")) \nNULL\n\n\nWe need to be aware of some important notes regarding our calcuation of the median here. Let’s put our baked data side-by-side with our old data, just so it’s easier to see what was imputed and how it was calculated.\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL) |&gt;\n  rename(new_rev = total_rev) |&gt;\n  left_join(\n    data_ga |&gt; rename(old_rev = total_rev)\n  )\n\nJoining with `by = join_by(event_date)`\n\n\nLet’s start with the tails, the missing values at row 1 and 14. Since these values lack a value above and below, they default to using the series 1:3 and 12:14 for imputation. When making the calculation for the window, only complete values are passed to the calculation. Take for example the missing value at row 1. The 324 imputed value comes from calculating the median between 323 and 324.\n\n# We're rounding up here for the imputed value \nmedian(c(323, 324))\n\n[1] 323.5\n\n\nThe same calculation is being done for the 14th value, which looks like this:\n\nmedian(c(73, 19))\n\n[1] 46\n\n\nThis brings up the interesting case for the imputed NA value at row 7, or the lack there of. Since all the values within the window are NA, an NA is imputed (i.e., you can’t calculate a median with no known values). To fix this, we would need to expand our window to include more values. We do this by setting window = 5 within the function.\n\nbaked_ga &lt;- recipe(~., data = data_ga) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_impute_roll(total_rev, window = 5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# Now we have complete values, since we expanded the window\nbaked_ga\n\n# A tibble: 14 × 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14     269  \n 2 2020-12-15     323  \n 3 2020-12-16     324  \n 4 2020-12-17     203  \n 5 2020-12-18     215  \n 6 2020-12-19     209  \n 7 2020-12-20     135  \n 8 2020-12-21     156. \n 9 2020-12-22      55  \n10 2020-12-23     256  \n11 2020-12-24      32  \n12 2020-12-25      73  \n13 2020-12-26      19  \n14 2020-12-27      52.5\n\n\nWhat about other functions to calculate values? To do this, step_impute_roll() has the statistic argument. Say instead of the median we want to calculate our imputed value using the mean. We just do the following:\n\nga_mean_rec &lt;- recipe(~., data_ga) |&gt;\n  update_role(event_date, new_role = \"ref_date\") |&gt;\n  step_impute_roll(total_rev, window = 5, statistic = mean) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_mean_rec\n\n# A tibble: 14 × 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14      266.\n 2 2020-12-15      323 \n 3 2020-12-16      324 \n 4 2020-12-17      203 \n 5 2020-12-18      215 \n 6 2020-12-19      209 \n 7 2020-12-20      135 \n 8 2020-12-21      156.\n 9 2020-12-22       55 \n10 2020-12-23      256 \n11 2020-12-24       32 \n12 2020-12-25       73 \n13 2020-12-26       19 \n14 2020-12-27       95 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAccording to the docs, the statistic function you use should:\n\ncontain a single argument for the data to compute the imputed value.\nreturn a double precision value.\n\nThey also note only complete values will be passed to the functions specified with the statistic argument.\n\n\nThat’s a wrap for step_imputation_*() functions. Next we’re going to focus on step functions to decorrelate predictors.\n\n\nDay 13 - Use step_corr() to remove highly correlated predictors\nStarting today, I’m going to begin highlighting step_*() functions useful for performing decorrelation preprocessing steps. These include steps to filter out highly correlated predictors, using principal component analysis, or other model-based methods.\nI’m starting out simple here by focusing on the use of step_corr(). According to the docs, step_corr() will\n\npotentially remove variables that have large absolute correations with other variables.\n\nUsing some threshold value, step_corr() will identify column combinations where a minimum number of columns are removed and all the absolute correlations between columns are below a specified threshold.\n\n\n\n\n\n\nImportant\n\n\n\nstep_corr() will potentially remove columns.\n\n\nBefore using step_corr() let’s highlight some of the important arguments of the function.\n\nthreshold - used as the absolute correlation cut off value. The function defaults to .9. This is the only tuning parameter for the function.\nmethod - the method used to make correlation calculations, defaults to pearson.\n\nLet’s get our data together. For today’s example, I’m going back to our penguins data. You can learn more about this data by running ?penguins in your console or by reviewing my past examples using this data.\n\ndata(penguins, package = \"modeldata\")\n\npenguins\n\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n 2 Adelie  Torgersen           39.5          17.4               186        3800 female\n 3 Adelie  Torgersen           40.3          18                 195        3250 female\n 4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193        3450 female\n 6 Adelie  Torgersen           39.3          20.6               190        3650 male  \n 7 Adelie  Torgersen           38.9          17.8               181        3625 female\n 8 Adelie  Torgersen           39.2          19.6               195        4675 male  \n 9 Adelie  Torgersen           34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\nglimpse(penguins)\n\nRows: 344\nColumns: 7\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\n\n\n\nNow we create our testing training split.\n\nset.seed(20240113)\npenguins_split &lt;- initial_split(penguins, prop = .8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nQuickly, let’s use corrr::correlate() to explore correlations between variables in our training data. The code looks like this:\n\ncorrelate(penguins_tr)\n\nNon-numeric variables removed from input: `species`, `island`, and `sex`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 4 × 5\n  term              bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 bill_length_mm            NA            -0.229             0.646       0.591\n2 bill_depth_mm             -0.229        NA                -0.582      -0.477\n3 flipper_length_mm          0.646        -0.582            NA           0.872\n4 body_mass_g                0.591        -0.477             0.872      NA    \n\n\nYou’ll notice a highly positive correlation between flipper_length_mm and body_mass_g (.872). Although I’m not making a causal argument here, it would seem feasible to assume that as a penguin’s flippers get longer, their body mass would increase. The question now is, if we were using this data to train a model, would the inclusion of both these variables improve our model? Or, could we simplify model estimation by eliminating one of these variables? Perhaps we can achieve the same amount of accuracy while also specifying the most parsimonious model.\nHere we’ll create our recipe to address these two, highly correlated variables within our data.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_corr(all_numeric_predictors(), threshold = .8) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 11 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: flipper_length_mm | Trained\n\n\nThe prepped recipe now tells us the flipper_length_mm variable will be removed once we bake the data. This is a good thing to keep an eye on, verifying the step removed expected columns.\nWhen you drill down into the step using tidy(), you’ll notice a tibble that highlights what column will be removed once we bake() the recipe.\n\ntidy(penguins_rec)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id        \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;     \n1      1 step      corr  TRUE    FALSE corr_q9cC1\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 × 2\n  terms             id        \n  &lt;chr&gt;             &lt;chr&gt;     \n1 flipper_length_mm corr_q9cC1\n\n\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 6\n   species island    bill_length_mm bill_depth_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Gentoo  Biscoe              50.5          15.9        5400 male  \n 2 Gentoo  Biscoe              NA            NA            NA &lt;NA&gt;  \n 3 Gentoo  Biscoe              50            15.3        5550 male  \n 4 Adelie  Torgersen           40.2          17          3450 female\n 5 Adelie  Biscoe              40.5          17.9        3200 female\n 6 Adelie  Torgersen           37.8          17.1        3300 &lt;NA&gt;  \n 7 Gentoo  Biscoe              48.2          14.3        4600 female\n 8 Gentoo  Biscoe              55.1          16          5850 male  \n 9 Adelie  Dream               37.2          18.1        3900 male  \n10 Gentoo  Biscoe              48.7          15.7        5350 male  \n# ℹ 265 more rows\n\n\nYou can verify this highly correlated variable is mitigated by checking out the correlations between variables in our baked data set:\n\ncorrelate(baked_penguin)\n\nNon-numeric variables removed from input: `species`, `island`, and `sex`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 4\n  term           bill_length_mm bill_depth_mm body_mass_g\n  &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 bill_length_mm         NA            -0.229       0.591\n2 bill_depth_mm          -0.229        NA          -0.477\n3 body_mass_g             0.591        -0.477      NA    \n\n\nSo there you have it, our first step_*() function to perform decorrelation preprocessing. Tomorrow I’ll be focusing on step_pca(), which will use principal components analysis to manage correlations among predictors.\n\n\nDay 14 - Perform dimension reduction with step_pca()\nFor today, I’m overviewing step_pca(). step_pca() performs dimension reduction using principal components analysis. Dimension reduction seeks to combine predictors into latent predictors, while also retaining as much information from the full data set as possible. Principal component analysis can seem like an advanced topic, but the @statquest YouTube channel has a good step-by-step video on how it’s performed, in general terms.\nWe’ll use the mlc_churn data from the ‘modeldata’ package for today’s examples. This data is an artificial data set representing customer churn (i.e., the loss of customers to a service). You can learn more about this data by running ?mlc_churn within your console. But here’s some basic data exploration code to get a sense of what’s included within the data.\n\ndata(mlc_churn, package = \"modeldata\")\n\nglimpse(mlc_churn)\n\nRows: 5,000\nColumns: 20\n$ state                         &lt;fct&gt; KS, OH, NJ, OH, OK, AL, MA, MO, LA, WV, IN, RI, IA, MT, IA, …\n$ account_length                &lt;int&gt; 128, 107, 137, 84, 75, 118, 121, 147, 117, 141, 65, 74, 168,…\n$ area_code                     &lt;fct&gt; area_code_415, area_code_415, area_code_415, area_code_408, …\n$ international_plan            &lt;fct&gt; no, no, no, yes, yes, yes, no, yes, no, yes, no, no, no, no,…\n$ voice_mail_plan               &lt;fct&gt; yes, yes, no, no, no, no, yes, no, no, yes, no, no, no, no, …\n$ number_vmail_messages         &lt;int&gt; 25, 26, 0, 0, 0, 0, 24, 0, 0, 37, 0, 0, 0, 0, 0, 0, 27, 0, 3…\n$ total_day_minutes             &lt;dbl&gt; 265.1, 161.6, 243.4, 299.4, 166.7, 223.4, 218.2, 157.0, 184.…\n$ total_day_calls               &lt;int&gt; 110, 123, 114, 71, 113, 98, 88, 79, 97, 84, 137, 127, 96, 88…\n$ total_day_charge              &lt;dbl&gt; 45.07, 27.47, 41.38, 50.90, 28.34, 37.98, 37.09, 26.69, 31.3…\n$ total_eve_minutes             &lt;dbl&gt; 197.4, 195.5, 121.2, 61.9, 148.3, 220.6, 348.5, 103.1, 351.6…\n$ total_eve_calls               &lt;int&gt; 99, 103, 110, 88, 122, 101, 108, 94, 80, 111, 83, 148, 71, 7…\n$ total_eve_charge              &lt;dbl&gt; 16.78, 16.62, 10.30, 5.26, 12.61, 18.75, 29.62, 8.76, 29.89,…\n$ total_night_minutes           &lt;dbl&gt; 244.7, 254.4, 162.6, 196.9, 186.9, 203.9, 212.6, 211.8, 215.…\n$ total_night_calls             &lt;int&gt; 91, 103, 104, 89, 121, 118, 118, 96, 90, 97, 111, 94, 128, 1…\n$ total_night_charge            &lt;dbl&gt; 11.01, 11.45, 7.32, 8.86, 8.41, 9.18, 9.57, 9.53, 9.71, 14.6…\n$ total_intl_minutes            &lt;dbl&gt; 10.0, 13.7, 12.2, 6.6, 10.1, 6.3, 7.5, 7.1, 8.7, 11.2, 12.7,…\n$ total_intl_calls              &lt;int&gt; 3, 3, 5, 7, 3, 6, 7, 6, 4, 5, 6, 5, 2, 5, 6, 9, 4, 3, 5, 2, …\n$ total_intl_charge             &lt;dbl&gt; 2.70, 3.70, 3.29, 1.78, 2.73, 1.70, 2.03, 1.92, 2.35, 3.02, …\n$ number_customer_service_calls &lt;int&gt; 1, 1, 0, 2, 3, 0, 3, 0, 1, 0, 4, 0, 1, 3, 4, 4, 1, 3, 1, 1, …\n$ churn                         &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no,…\n\nskim(mlc_churn)\n\n\nData summary\n\n\nName\nmlc_churn\n\n\nNumber of rows\n5000\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nstate\n0\n1\nFALSE\n51\nWV: 158, MN: 125, AL: 124, ID: 119\n\n\narea_code\n0\n1\nFALSE\n3\nare: 2495, are: 1259, are: 1246\n\n\ninternational_plan\n0\n1\nFALSE\n2\nno: 4527, yes: 473\n\n\nvoice_mail_plan\n0\n1\nFALSE\n2\nno: 3677, yes: 1323\n\n\nchurn\n0\n1\nFALSE\n2\nno: 4293, yes: 707\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naccount_length\n0\n1\n100.26\n39.69\n1\n73.00\n100.00\n127.00\n243.00\n▂▇▇▂▁\n\n\nnumber_vmail_messages\n0\n1\n7.76\n13.55\n0\n0.00\n0.00\n17.00\n52.00\n▇▁▂▁▁\n\n\ntotal_day_minutes\n0\n1\n180.29\n53.89\n0\n143.70\n180.10\n216.20\n351.50\n▁▃▇▅▁\n\n\ntotal_day_calls\n0\n1\n100.03\n19.83\n0\n87.00\n100.00\n113.00\n165.00\n▁▁▇▇▁\n\n\ntotal_day_charge\n0\n1\n30.65\n9.16\n0\n24.43\n30.62\n36.75\n59.76\n▁▃▇▅▁\n\n\ntotal_eve_minutes\n0\n1\n200.64\n50.55\n0\n166.38\n201.00\n234.10\n363.70\n▁▂▇▅▁\n\n\ntotal_eve_calls\n0\n1\n100.19\n19.83\n0\n87.00\n100.00\n114.00\n170.00\n▁▁▇▇▁\n\n\ntotal_eve_charge\n0\n1\n17.05\n4.30\n0\n14.14\n17.09\n19.90\n30.91\n▁▂▇▅▁\n\n\ntotal_night_minutes\n0\n1\n200.39\n50.53\n0\n166.90\n200.40\n234.70\n395.00\n▁▃▇▃▁\n\n\ntotal_night_calls\n0\n1\n99.92\n19.96\n0\n87.00\n100.00\n113.00\n175.00\n▁▁▇▆▁\n\n\ntotal_night_charge\n0\n1\n9.02\n2.27\n0\n7.51\n9.02\n10.56\n17.77\n▁▃▇▃▁\n\n\ntotal_intl_minutes\n0\n1\n10.26\n2.76\n0\n8.50\n10.30\n12.00\n20.00\n▁▃▇▃▁\n\n\ntotal_intl_calls\n0\n1\n4.44\n2.46\n0\n3.00\n4.00\n6.00\n20.00\n▇▅▁▁▁\n\n\ntotal_intl_charge\n0\n1\n2.77\n0.75\n0\n2.30\n2.78\n3.24\n5.40\n▁▃▇▃▁\n\n\nnumber_customer_service_calls\n0\n1\n1.57\n1.31\n0\n1.00\n1.00\n2.00\n9.00\n▇▅▁▁▁\n\n\n\n\n\nLet’s create our training and testing split.\n\nset.seed(20240114)\nchurn_split &lt;- initial_split(mlc_churn, prop = .8)\n\nchurn_tr &lt;- training(churn_split)\nchurn_te &lt;- testing(churn_split)\n\nBefore we apply the step_pca() function, we need to first center and scale our data. To do this, we’ll first use step_normalize() on all numeric variables within the data set. Normalizing will place all numeric data on the same scale, which is required step to complete before performing principal components analysis.\nPost center and scaling of our variables, we’ll use step_pca() on all_numeric() predictors. We use the num_comp = 3 argument to specify how many components will be outputted from our principal components analysis.\nLet’s prep() and tidy() our recipe to peek under the hood to get a better sense of what’s happening with this recipe step.\n\nchurn_rec &lt;- recipe(churn ~ ., data = churn_tr) |&gt;\n  update_role(state, area_code, new_role = \"ref_var\") |&gt;\n  update_role(ends_with(\"plan\"), new_role = \"plan_var\") |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric_predictors(), num_comp = 3) |&gt;\n  prep()\n\nchurn_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 15\nplan_var:   2\nref_var:    2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4000 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: account_length and number_vmail_messages, ... | Trained\n\n\n• PCA extraction with: account_length, number_vmail_messages, total_day_minutes, ... | Trained\n\n\nIt’s also important to point out that the tidy() method for step_pca() has two type options:\n\ntype = \"coef\"\ntype = \"variance\"\n\nEach of these options modify what gets printed to the console. I’ve added some comments below on what gets outputted for each.\n\ntidy(churn_rec)\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_7hokR\n2      2 step      pca       TRUE    FALSE pca_BXk23      \n\n# Output the variable loadings for each component\ntidy(churn_rec, number = 2, type = \"coef\")\n\n# A tibble: 225 × 4\n   terms                     value component id       \n   &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 account_length         0.0192   PC1       pca_BXk23\n 2 number_vmail_messages -0.00296  PC1       pca_BXk23\n 3 total_day_minutes      0.312    PC1       pca_BXk23\n 4 total_day_calls       -0.000110 PC1       pca_BXk23\n 5 total_day_charge       0.312    PC1       pca_BXk23\n 6 total_eve_minutes     -0.462    PC1       pca_BXk23\n 7 total_eve_calls        0.0230   PC1       pca_BXk23\n 8 total_eve_charge      -0.462    PC1       pca_BXk23\n 9 total_night_minutes    0.426    PC1       pca_BXk23\n10 total_night_calls      0.00724  PC1       pca_BXk23\n# ℹ 215 more rows\n\n# Output the variance each component accounts for\ntidy(churn_rec, number = 2, type = \"variance\")\n\n# A tibble: 60 × 4\n   terms    value component id       \n   &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 2.07          1 pca_BXk23\n 2 variance 2.02          2 pca_BXk23\n 3 variance 1.98          3 pca_BXk23\n 4 variance 1.94          4 pca_BXk23\n 5 variance 1.06          5 pca_BXk23\n 6 variance 1.04          6 pca_BXk23\n 7 variance 1.01          7 pca_BXk23\n 8 variance 0.988         8 pca_BXk23\n 9 variance 0.977         9 pca_BXk23\n10 variance 0.965        10 pca_BXk23\n# ℹ 50 more rows\n\n\nGreat, let’s bake our recipe. You’ll notice step_pca() reduced all numeric data into our three components, PC1, PC2, and PC3.\n\nbake(churn_rec, new_data = NULL)\n\n# A tibble: 4,000 × 8\n   state area_code     international_plan voice_mail_plan churn    PC1     PC2    PC3\n   &lt;fct&gt; &lt;fct&gt;         &lt;fct&gt;              &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 IN    area_code_408 no                 no              no    -0.358  1.03   -1.30 \n 2 AZ    area_code_415 no                 yes             no     3.04   2.20   -1.28 \n 3 IL    area_code_415 no                 no              no     1.17  -2.36    0.697\n 4 IN    area_code_415 no                 no              no    -0.127  0.811   1.14 \n 5 FL    area_code_415 no                 no              no     0.248  2.06   -0.311\n 6 NM    area_code_510 yes                no              no    -1.28   0.0720  1.02 \n 7 DE    area_code_415 no                 no              no    -1.95   0.250  -0.553\n 8 SD    area_code_408 no                 no              no     0.624  1.36   -0.344\n 9 DE    area_code_510 no                 no              no    -1.13  -0.663  -0.804\n10 CT    area_code_415 no                 yes             no    -2.60   0.817   0.939\n# ℹ 3,990 more rows\n\n\nConstraining by component works, but step_pca() also provides a threshold argument. In other words, we can specify the total variance that should be covered by components, and step_pca() will create the required number of components based on this threshold. Let’s say we want our PCA to create a number of components to cover 70% of the variability in our variables. We can do this by using the following recipe code:\n\nchurn_rec_thresh &lt;- recipe(churn ~ ., data = churn_tr) |&gt;\n  update_role(state, area_code, new_role = \"ref_var\") |&gt;\n  update_role(ends_with(\"plan\"), new_role = \"plan_var\") |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric_predictors(), threshold = .7) |&gt;\n  prep()\n\nchurn_rec_thresh\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 15\nplan_var:   2\nref_var:    2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4000 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: account_length and number_vmail_messages, ... | Trained\n\n\n• PCA extraction with: account_length, number_vmail_messages, total_day_minutes, ... | Trained\n\n\nLet’s tidy() our recipe to see what’s happening here.\n\ntidy(churn_rec_thresh)\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_8WvPy\n2      2 step      pca       TRUE    FALSE pca_Aj4UJ      \n\n# variable loadings\ntidy(churn_rec_thresh, number = 2, type = \"coef\")\n\n# A tibble: 225 × 4\n   terms                     value component id       \n   &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 account_length         0.0192   PC1       pca_Aj4UJ\n 2 number_vmail_messages -0.00296  PC1       pca_Aj4UJ\n 3 total_day_minutes      0.312    PC1       pca_Aj4UJ\n 4 total_day_calls       -0.000110 PC1       pca_Aj4UJ\n 5 total_day_charge       0.312    PC1       pca_Aj4UJ\n 6 total_eve_minutes     -0.462    PC1       pca_Aj4UJ\n 7 total_eve_calls        0.0230   PC1       pca_Aj4UJ\n 8 total_eve_charge      -0.462    PC1       pca_Aj4UJ\n 9 total_night_minutes    0.426    PC1       pca_Aj4UJ\n10 total_night_calls      0.00724  PC1       pca_Aj4UJ\n# ℹ 215 more rows\n\n# variance accounted for\ntidy(churn_rec_thresh, number = 2, type = \"variance\")\n\n# A tibble: 60 × 4\n   terms    value component id       \n   &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 2.07          1 pca_Aj4UJ\n 2 variance 2.02          2 pca_Aj4UJ\n 3 variance 1.98          3 pca_Aj4UJ\n 4 variance 1.94          4 pca_Aj4UJ\n 5 variance 1.06          5 pca_Aj4UJ\n 6 variance 1.04          6 pca_Aj4UJ\n 7 variance 1.01          7 pca_Aj4UJ\n 8 variance 0.988         8 pca_Aj4UJ\n 9 variance 0.977         9 pca_Aj4UJ\n10 variance 0.965        10 pca_Aj4UJ\n# ℹ 50 more rows\n\n\nNow, we’ll bake our churn_rec_thresh recipe.\n\nbake(churn_rec_thresh, new_data = NULL)\n\n# A tibble: 4,000 × 12\n   state area_code     international_plan voice_mail_plan churn    PC1     PC2    PC3    PC4    PC5\n   &lt;fct&gt; &lt;fct&gt;         &lt;fct&gt;              &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 IN    area_code_408 no                 no              no    -0.358  1.03   -1.30   0.613  0.402\n 2 AZ    area_code_415 no                 yes             no     3.04   2.20   -1.28   1.06   1.04 \n 3 IL    area_code_415 no                 no              no     1.17  -2.36    0.697  1.05   0.218\n 4 IN    area_code_415 no                 no              no    -0.127  0.811   1.14   1.14   0.177\n 5 FL    area_code_415 no                 no              no     0.248  2.06   -0.311 -2.28  -1.20 \n 6 NM    area_code_510 yes                no              no    -1.28   0.0720  1.02   1.05  -1.12 \n 7 DE    area_code_415 no                 no              no    -1.95   0.250  -0.553  1.80  -0.712\n 8 SD    area_code_408 no                 no              no     0.624  1.36   -0.344 -0.717 -1.42 \n 9 DE    area_code_510 no                 no              no    -1.13  -0.663  -0.804 -0.993 -0.159\n10 CT    area_code_415 no                 yes             no    -2.60   0.817   0.939 -1.09   2.53 \n# ℹ 3,990 more rows\n# ℹ 2 more variables: PC6 &lt;dbl&gt;, PC7 &lt;dbl&gt;\n\n\nTo meet our threshold, the step_pca() recipes step calculated and returned seven principal components, PC1 - PC7.\nstep_pca() makes it pretty easy to do dimension reduction utilizing principal components analysis. Give it a try.\nAnother day down. See you tomorrow.\n\n\nDay 15 - Use step_ica() for signal extraction\nHere we are, the halfway point 🎉.\nToday, I’m overviewing the use of step_ica(). This step is used to make transformations in signal processing. In general terms, independent component analysis (ICA) is a dimensionality reduction technique that attempts to isolate signal from noise.\nBefore reviewing this step, I had to do some research on what ICA is and how it is used. Here are a few sources I found useful to gain an intuitive sense of this step:\n\nMaking sense of independent component analysis Cross Validated post.\nIntroduction to ICA: Independent Component Analysis by Jonas Dieckmann\nIndependent Component Analysis (ICA) and Automated Component Labeling — EEG Example by Bartek Kulas\n\nAcross many of these sources, two examples are commonly presented to more clearly explain the purpose of ICA. First, the cocktail party problem, where we can isolate individual conversations among many during a party. The second comes from audio recording. Take for example a situation where you have multiple microphones. These microphones are being used to capture the sound of several audio sources (e.g., various instruments), and you’re attempting to isolate the sound from a single source. Both are situations where you’re trying to isolate the signal from surrounding noise. Indeed, these are simplified examples. The linked sources above provide more sophisticated explanations.\nBefore we use step_ica(), we’ll need some data. I’m using a portion of the data from the Independent Component Analysis (ICA) and Automated Component Labeling — EEG Example. This data comes from the use of Electroencephalography (EEG). It was collected to examine EEG correlates of a genetic predisposition to alcoholism, and it was made available via Kaggle.\n\n\n\n\n\n\nWarning\n\n\n\nI am not an EEG professional, and I rarely work with this type of data. The approach I highlight here may not be best practice.\n\n\nThis data represents EEG sensor measurements of electrical activity during an experimental session for one participant. There are multiple sensors, which collect data intended to observe six known brain wave patterns (again, I’m not an expert here, so this is certainly a more complex topic then I’m making it out to be here). Let’s use step_ica() to see if we can transform this data into six different signals.\n\ndata_eeg &lt;- \n  read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_eeg.csv\")\n  ) |&gt;\n  clean_names() |&gt;\n  rename(id = x1)\n\nNew names:\nRows: 16384 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): sensor position, subject identifier, matching condition, name dbl (6): ...1, trial number,\nsample num, sensor value, channel, time\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types\nor set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nglimpse(data_eeg)\n\nRows: 16,384\nColumns: 10\n$ id                 &lt;dbl&gt; 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, …\n$ trial_number       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sensor_position    &lt;chr&gt; \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"…\n$ sample_num         &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2…\n$ sensor_value       &lt;dbl&gt; -8.921, -8.433, -2.574, 5.239, 11.587, 14.028, 11.587, 6.704, 1.821, -1…\n$ subject_identifier &lt;chr&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"…\n$ matching_condition &lt;chr&gt; \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"…\n$ channel            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ name               &lt;chr&gt; \"co2a0000364\", \"co2a0000364\", \"co2a0000364\", \"co2a0000364\", \"co2a000036…\n$ time               &lt;dbl&gt; 0.00000000, 0.00390625, 0.00781250, 0.01171875, 0.01562500, 0.01953125,…\n\nskim(data_eeg)\n\n\nData summary\n\n\nName\ndata_eeg\n\n\nNumber of rows\n16384\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsensor_position\n0\n1\n1\n3\n0\n64\n0\n\n\nsubject_identifier\n0\n1\n1\n1\n0\n1\n0\n\n\nmatching_condition\n0\n1\n6\n6\n0\n1\n0\n\n\nname\n0\n1\n11\n11\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n8228.00\n4748.27\n5.00\n4116.50\n8228.00\n12339.50\n16451.0\n▇▇▇▇▇\n\n\ntrial_number\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.0\n▁▁▇▁▁\n\n\nsample_num\n0\n1\n127.50\n73.90\n0.00\n63.75\n127.50\n191.25\n255.0\n▇▇▇▇▇\n\n\nsensor_value\n0\n1\n1.99\n7.51\n-39.83\n-2.23\n1.22\n5.40\n51.9\n▁▂▇▁▁\n\n\nchannel\n0\n1\n31.50\n18.47\n0.00\n15.75\n31.50\n47.25\n63.0\n▇▇▇▇▇\n\n\ntime\n0\n1\n0.50\n0.29\n0.00\n0.25\n0.50\n0.75\n1.0\n▇▇▇▇▇\n\n\n\n\n\nI’ll have to do some data wrangling here to work with the data first, though. Specifically, I need to go from long to wide data. This will give each EEG sensor measurement its own column.\n\ndata_eeg &lt;- data_eeg |&gt;\n  select(sensor_position, sensor_value, time) |&gt;\n  pivot_wider(\n    names_from = sensor_position, \n    values_from = sensor_value\n  )\n\ndata_eeg\n\n# A tibble: 256 × 65\n      time   FP1    FP2      F7     F8    AF1    AF2     FZ    F4     F3   FC6    FC5    FC2    FC1\n     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 0       -8.92  0.834 -19.8    8.15  -2.15   1.13  -0.071 3.41  -0.092 4.83  -2.43   0.488  0.824\n 2 0.00391 -8.43  3.28  -12.5    1.80  -2.15   0.641 -0.559 1.46   0.397 6.30  -4.38  -0.977  0.824\n 3 0.00781 -2.57  5.72    1.15  -2.59  -1.66  -0.336 -1.05  0.478 -1.07  5.81  -5.36  -1.46   0.336\n 4 0.0117   5.24  7.67   14.8   -4.55  -0.682 -0.824 -0.559 0.966 -3.51  3.37  -5.36   0     -0.641\n 5 0.0156  11.6   9.62   20.7   -5.04   2.25   0.641  0.905 1.94  -5.46  1.41  -0.966  1.95  -1.13 \n 6 0.0195  14.0   9.62   17.3   -5.52   5.18   3.57   2.37  2.92  -4.49  0.437  6.85   3.42  -1.62 \n 7 0.0234  11.6   8.65    8.96  -4.55   6.64   6.01   3.84  1.94  -1.07  0.926 13.7    3.42  -1.13 \n 8 0.0273   6.70  5.23    0.173 -0.641  5.18   6.99   4.32  0.478  3.33  1.90  15.6    1.95   0.824\n 9 0.0312   1.82  1.32   -3.73   5.71   1.76   5.52   3.35  0.478  6.74  1.90  11.7    0.977  3.75 \n10 0.0352  -1.11 -2.10   -2.27  10.6   -1.66   2.59   1.88  1.94   7.23  1.90   3.92   0.977  5.22 \n# ℹ 246 more rows\n# ℹ 51 more variables: T8 &lt;dbl&gt;, T7 &lt;dbl&gt;, CZ &lt;dbl&gt;, C3 &lt;dbl&gt;, C4 &lt;dbl&gt;, CP5 &lt;dbl&gt;, CP6 &lt;dbl&gt;,\n#   CP1 &lt;dbl&gt;, CP2 &lt;dbl&gt;, P3 &lt;dbl&gt;, P4 &lt;dbl&gt;, PZ &lt;dbl&gt;, P8 &lt;dbl&gt;, P7 &lt;dbl&gt;, PO2 &lt;dbl&gt;, PO1 &lt;dbl&gt;,\n#   O2 &lt;dbl&gt;, O1 &lt;dbl&gt;, X &lt;dbl&gt;, AF7 &lt;dbl&gt;, AF8 &lt;dbl&gt;, F5 &lt;dbl&gt;, F6 &lt;dbl&gt;, FT7 &lt;dbl&gt;, FT8 &lt;dbl&gt;,\n#   FPZ &lt;dbl&gt;, FC4 &lt;dbl&gt;, FC3 &lt;dbl&gt;, C6 &lt;dbl&gt;, C5 &lt;dbl&gt;, F2 &lt;dbl&gt;, F1 &lt;dbl&gt;, TP8 &lt;dbl&gt;, TP7 &lt;dbl&gt;,\n#   AFZ &lt;dbl&gt;, CP3 &lt;dbl&gt;, CP4 &lt;dbl&gt;, P5 &lt;dbl&gt;, P6 &lt;dbl&gt;, C1 &lt;dbl&gt;, C2 &lt;dbl&gt;, PO7 &lt;dbl&gt;, PO8 &lt;dbl&gt;,\n#   FCZ &lt;dbl&gt;, POZ &lt;dbl&gt;, OZ &lt;dbl&gt;, P2 &lt;dbl&gt;, P1 &lt;dbl&gt;, CPZ &lt;dbl&gt;, nd &lt;dbl&gt;, Y &lt;dbl&gt;\n\n\nIn addition, step_ica() requires a few packages to be installed. This includes dimRed and fastICA. If these are not installed beforehand, an error will be returned.\nNow we create our recipe. Just like principal components analysis, we need to center and scale our data. We achieve this by applying step_center() and step_scale() to all_numeric() variables. Since we’re attempting to create columns to represent the six brain waves, we’ll set num_comp to 6.\n\neeg_rec &lt;- recipe(~., data = data_eeg) |&gt;\n  update_role(time, new_role = \"time_ref\") |&gt;\n  step_center(all_numeric()) |&gt;\n  step_scale(all_numeric()) |&gt;\n  step_ica(all_numeric(), num_comp = 6) |&gt;\n  prep()\n\nLet’s tidy() our recipe to get a better sense of what’s happening here. You’ll notice it’s very similar to principal components analysis.\n\ntidy(eeg_rec, number = 3)\n\n# A tibble: 390 × 4\n   terms component     value id       \n   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    \n 1 AF1   IC1       -0.0446   ica_xymfX\n 2 AF1   IC2        0.0123   ica_xymfX\n 3 AF1   IC3        0.00928  ica_xymfX\n 4 AF1   IC4        0.000524 ica_xymfX\n 5 AF1   IC5       -0.0280   ica_xymfX\n 6 AF1   IC6       -0.0415   ica_xymfX\n 7 AF2   IC1       -0.0489   ica_xymfX\n 8 AF2   IC2        0.0667   ica_xymfX\n 9 AF2   IC3       -0.00168  ica_xymfX\n10 AF2   IC4       -0.0385   ica_xymfX\n# ℹ 380 more rows\n\n\nNow let’s bake our data and see what the result of the step_ica() function.\n\nbaked_eeg &lt;- bake(eeg_rec, new_data = NULL) |&gt;\n  bind_cols(data_eeg |&gt; select(time))\n\nFor the heck of it, let’s plot our baked data. Do you see any of the common brain wave types in these plots? Remember, this is only one participant, so if additional data were included, the patterns might become more apparent.\n\nwalk(\n  baked_eeg |&gt; select(-time), \n  ~plot(baked_eeg$time, .x, type = \"line\") \n)\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\n\n\n\n\nThere you have it, another step_*() function to try out. See you tomorrow.\n\n\nDay 16 - Use step_other() to collapse low occurring categorical levels into other\nFor today, I’m going to highlight the use of step_other(). step_other() is used to collapse infrequent categorical values into an other category. To do this, the function has a threshold argument to modify the cutoff for determining values within this category. Let’s highlight its use with an example.\nFor data, I’m going back to our Google Analytics data. I’ve used this data for several examples already, so I’m not going to detail it too much here. However, here is some quick data exploration code for you to review.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data_ga)\n\nRows: 9,365\nColumns: 14\n$ event_date              &lt;dbl&gt; 20201201, 20201201, 20201201, 20201201, 20201201, 20201201, 202012…\n$ purchase_revenue_in_usd &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 62, 62, 44, 28, 28, 36, 36, 36, 36, 92…\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1…\n$ item_name               &lt;chr&gt; \"Google Hemp Tote\", \"Android SM S/F18 Sticker Sheet\", \"Android Buo…\n$ item_category           &lt;chr&gt; \"Clearance\", \"Accessories\", \"Drinkware\", \"Small Goods\", \"Office\", …\n$ price_in_usd            &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 7, 92, 7, 14,…\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 53, 1, 1, 1, 1,…\n$ item_revenue_in_usd     &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, …\n$ shipping_tier           &lt;chr&gt; \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"F…\n$ payment_type            &lt;chr&gt; \"Pay with credit card\", \"Pay with credit card\", \"Pay with credit c…\n$ category                &lt;chr&gt; \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobil…\n$ country                 &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United States\"…\n$ region                  &lt;chr&gt; \"California\", \"California\", \"California\", \"California\", \"Californi…\n$ city                    &lt;chr&gt; \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San J…\n\nskim(data_ga)\n\n\nData summary\n\n\nName\ndata_ga\n\n\nNumber of rows\n9365\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n385\n0\n\n\nitem_category\n164\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n109\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n96\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n289\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n434\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1\n20203675.03\n3983.09\n20201201\n20201209\n20201216\n20210106\n20210130\n▇▁▁▁▃\n\n\npurchase_revenue_in_usd\n0\n1\n101.84\n118.06\n2\n39\n72\n125\n1530\n▇▁▁▁▁\n\n\ntransaction_id\n0\n1\n487977.44\n282873.71\n546\n249662\n479506\n724658\n999850\n▇▇▇▇▇\n\n\nprice_in_usd\n0\n1\n19.52\n18.74\n1\n7\n14\n24\n120\n▇▂▁▁▁\n\n\nquantity\n0\n1\n1.45\n2.77\n1\n1\n1\n1\n160\n▇▁▁▁▁\n\n\nitem_revenue_in_usd\n0\n1\n23.26\n28.61\n1\n8\n15\n30\n704\n▇▁▁▁▁\n\n\n\n\n\nFirst, let’s create a split.\n\nsplit_ga &lt;- initial_split(data_ga, prop = .8)\n\ndata_ga_tr &lt;- training(split_ga)\ndata_ga_te &lt;- testing(split_ga)\n\nI’m interested in the item_category variable here. Let’s get a sense of the unique values and counts of each value within the data.\n\ndata_ga_tr |&gt;\n  count(item_category, sort = TRUE) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  print(n = 25) \n\n# A tibble: 21 × 3\n   item_category               n     prop\n   &lt;chr&gt;                   &lt;int&gt;    &lt;dbl&gt;\n 1 Apparel                  2199 0.294   \n 2 Campus Collection         755 0.101   \n 3 New                       724 0.0966  \n 4 Accessories               632 0.0844  \n 5 Shop by Brand             463 0.0618  \n 6 Office                    408 0.0545  \n 7 Bags                      372 0.0497  \n 8 Drinkware                 343 0.0458  \n 9 Clearance                 336 0.0448  \n10 Lifestyle                 263 0.0351  \n11 Uncategorized Items       242 0.0323  \n12 Google                    148 0.0198  \n13 Stationery                146 0.0195  \n14 &lt;NA&gt;                      140 0.0187  \n15 Writing Instruments       121 0.0162  \n16 Small Goods               106 0.0141  \n17 Gift Cards                 48 0.00641 \n18 Electronics Accessories    20 0.00267 \n19 Notebooks & Journals       16 0.00214 \n20 Fun                         9 0.00120 \n21 Black Lives Matter          1 0.000133\n\n\nIndeed, there are certainly some item categories that are purchased less than 5% of the time in our training data. Let’s create our recipe, where we focus on using step_other() to create an other category.\n\n# threshold defaults to .05\nga_rec &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 230 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Collapsing factor levels for: item_category | Trained\n\n\nWe’ll now tidy() our recipe to drill down and get more information on what’s happening. When we look deeper into the first step, a tibble describing what columns will be affected by the step and what variables will not be converted into an other category is returned. In our case, Accessories, Apparel, Bags, Campus Collection, New, Office, and Shop by Brand will remain as factor levels. All others will be converted to the other category.\n\ntidy(ga_rec)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id         \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;      \n1      1 step      other TRUE    FALSE other_MsyPy\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 7 × 3\n  terms         retained          id         \n  &lt;chr&gt;         &lt;chr&gt;             &lt;chr&gt;      \n1 item_category Accessories       other_MsyPy\n2 item_category Apparel           other_MsyPy\n3 item_category Bags              other_MsyPy\n4 item_category Campus Collection other_MsyPy\n5 item_category New               other_MsyPy\n6 item_category Office            other_MsyPy\n7 item_category Shop by Brand     other_MsyPy\n\n\nWe can bake to see what happens as a result.\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 7,492 × 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic… New                      4        1\n 2   20201208                      63         776080 Android SM… Accessories              2        2\n 3   20201209                     163         517829 Womens Goo… Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An… Accessories             13       15\n 5   20201210                     275         104071 Gift Card … other                   25        1\n 6   20210113                      18         865692 Google Met… Office                   6        1\n 7   20201203                     311         652984 Android Bu… other                    4        5\n 8   20210120                      84         482111 Google See… other                   10        1\n 9   20201202                      45         541131 Google Chr… Accessories             30        1\n10   20201222                      54         125445 Google Tot… New                     19        1\n# ℹ 7,482 more rows\n# ℹ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nbaked_ga |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 9 × 2\n  item_category         n\n  &lt;fct&gt;             &lt;int&gt;\n1 Apparel            2199\n2 other              1799\n3 Campus Collection   755\n4 New                 724\n5 Accessories         632\n6 Shop by Brand       463\n7 Office              408\n8 Bags                372\n9 &lt;NA&gt;                140\n\n\nSay we want to modify the threshold to be 10%. All we need to do is pass this value to the function via the threshold argument in step_other(). This looks like this:\n\nga_rec_thresh &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category, threshold = .1) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec_thresh \n\n# A tibble: 7,492 × 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic… other                    4        1\n 2   20201208                      63         776080 Android SM… other                    2        2\n 3   20201209                     163         517829 Womens Goo… Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An… other                   13       15\n 5   20201210                     275         104071 Gift Card … other                   25        1\n 6   20210113                      18         865692 Google Met… other                    6        1\n 7   20201203                     311         652984 Android Bu… other                    4        5\n 8   20210120                      84         482111 Google See… other                   10        1\n 9   20201202                      45         541131 Google Chr… other                   30        1\n10   20201222                      54         125445 Google Tot… other                   19        1\n# ℹ 7,482 more rows\n# ℹ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nga_rec_thresh |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 4 × 2\n  item_category         n\n  &lt;fct&gt;             &lt;int&gt;\n1 other              4398\n2 Apparel            2199\n3 Campus Collection   755\n4 &lt;NA&gt;                140\n\n\nThis might be too restrictive. You can also pass an integer to threshold to use a frequency as a cutoff. This looks like:\n\nga_rec_thresh &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category, threshold = 300) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec_thresh\n\n# A tibble: 7,492 × 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic… New                      4        1\n 2   20201208                      63         776080 Android SM… Accessories              2        2\n 3   20201209                     163         517829 Womens Goo… Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An… Accessories             13       15\n 5   20201210                     275         104071 Gift Card … other                   25        1\n 6   20210113                      18         865692 Google Met… Office                   6        1\n 7   20201203                     311         652984 Android Bu… Drinkware                4        5\n 8   20210120                      84         482111 Google See… other                   10        1\n 9   20201202                      45         541131 Google Chr… Accessories             30        1\n10   20201222                      54         125445 Google Tot… New                     19        1\n# ℹ 7,482 more rows\n# ℹ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nga_rec_thresh |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 11 × 2\n   item_category         n\n   &lt;fct&gt;             &lt;int&gt;\n 1 Apparel            2199\n 2 other              1120\n 3 Campus Collection   755\n 4 New                 724\n 5 Accessories         632\n 6 Shop by Brand       463\n 7 Office              408\n 8 Bags                372\n 9 Drinkware           343\n10 Clearance           336\n11 &lt;NA&gt;                140\n\n\nIf you don’t like the other category label, you can modify it by passing a string to the otherargument.\n\nga_rec_thresh &lt;- recipe(~., data = data_ga_tr) |&gt;\n  step_other(item_category, threshold = 300, other = \"Other items\") |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec_thresh\n\n# A tibble: 7,492 × 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201210                      49         166937 Android Ic… New                      4        1\n 2   20201208                      63         776080 Android SM… Accessories              2        2\n 3   20201209                     163         517829 Womens Goo… Apparel                 16        1\n 4   20201214                    1530         396355 Noogler An… Accessories             13       15\n 5   20201210                     275         104071 Gift Card … Other items             25        1\n 6   20210113                      18         865692 Google Met… Office                   6        1\n 7   20201203                     311         652984 Android Bu… Drinkware                4        5\n 8   20210120                      84         482111 Google See… Other items             10        1\n 9   20201202                      45         541131 Google Chr… Accessories             30        1\n10   20201222                      54         125445 Google Tot… New                     19        1\n# ℹ 7,482 more rows\n# ℹ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\nga_rec_thresh |&gt;\n  count(item_category, sort = TRUE)\n\n# A tibble: 11 × 2\n   item_category         n\n   &lt;fct&gt;             &lt;int&gt;\n 1 Apparel            2199\n 2 Other items        1120\n 3 Campus Collection   755\n 4 New                 724\n 5 Accessories         632\n 6 Shop by Brand       463\n 7 Office              408\n 8 Bags                372\n 9 Drinkware           343\n10 Clearance           336\n11 &lt;NA&gt;                140\n\n\nCheck mark next to another day. step_other() is pretty useful in cases where you have many factor levels in a categorical variable. Take it for a test drive. See you tomorrow.\n\n\nDay 17 - Convert date data into factor or numeric variables with step_date()\nDo you ever work with date variables? Do these tasks often require you to parse date variables into various components (e.g., month, year, day of the week, etc.)? recipes’ step_date() simplifies these operations.\nLet’s continue using our Google Analytics ecommerce data from yesterday. I’ve overviewed this data a few times now, so I’m not going to discuss it in depth here. Rather, I’m going to jump right into today’s examples.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240117)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nWithin this data is an event_date column. Let’s say I want to create two new columns from this data: month and year. First, we need to utilize step_mutate() to convert event_date to type date (i.e., 20201202 to 2020-12-02). lubridate’s ymd() function is used to do this. We do this by doing the following:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_date(event_date, features = c(\"month\", \"year\")) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~ymd(event_date) | Trained\n\n\n• Date features from: event_date | Trained\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms      value           id          \n  &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;       \n1 event_date ymd(event_date) mutate_1gITK\n\n\nLet’s bake our recipe and take a look at what’s happening. You should notice two new variables are created, each prefixed with the original variable name event_date_. The data now contains event_date_month and event_date_year.\n\nbake(ga_rec, new_data = NULL) |&gt;\n  relocate(starts_with(\"event_\"), .after = 1)\n\n# A tibble: 7,492 × 16\n   event_date event_date_month event_date_year purchase_revenue_in_usd transaction_id item_name     \n   &lt;date&gt;     &lt;fct&gt;                      &lt;int&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;         \n 1 2020-12-02 Dec                         2020                      41          11395 Google Navy S…\n 2 2020-12-07 Dec                         2020                      22         190059 Google Super …\n 3 2020-12-08 Dec                         2020                      29         147010 Google SF Cam…\n 4 2021-01-20 Jan                         2021                      86         101328 Google Speckl…\n 5 2020-12-04 Dec                         2020                     142         469624 Google Men's …\n 6 2020-12-28 Dec                         2020                      79         797936 Google Kirkla…\n 7 2020-12-28 Dec                         2020                      79         797936 Google Kirkla…\n 8 2020-12-17 Dec                         2020                     139         439292 Google Speckl…\n 9 2021-01-24 Jan                         2021                      55         172750 Google Black …\n10 2020-12-18 Dec                         2020                     128         878199 #IamRemarkabl…\n# ℹ 7,482 more rows\n# ℹ 10 more variables: item_category &lt;fct&gt;, price_in_usd &lt;dbl&gt;, quantity &lt;dbl&gt;,\n#   item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;, category &lt;fct&gt;,\n#   country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\nSay we don’t like the use of the abbreviation for event_date_month, we can change this by setting the abbr argument to FALSE in our recipe.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_date(event_date, features = c(\"month\", \"year\"), abbr = FALSE) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~ymd(event_date) | Trained\n\n\n• Date features from: event_date | Trained\n\nbake(ga_rec, new_data = NULL) |&gt;\n  relocate(starts_with(\"event_\"), .after = 1)\n\n# A tibble: 7,492 × 16\n   event_date event_date_month event_date_year purchase_revenue_in_usd transaction_id item_name     \n   &lt;date&gt;     &lt;fct&gt;                      &lt;int&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;         \n 1 2020-12-02 December                    2020                      41          11395 Google Navy S…\n 2 2020-12-07 December                    2020                      22         190059 Google Super …\n 3 2020-12-08 December                    2020                      29         147010 Google SF Cam…\n 4 2021-01-20 January                     2021                      86         101328 Google Speckl…\n 5 2020-12-04 December                    2020                     142         469624 Google Men's …\n 6 2020-12-28 December                    2020                      79         797936 Google Kirkla…\n 7 2020-12-28 December                    2020                      79         797936 Google Kirkla…\n 8 2020-12-17 December                    2020                     139         439292 Google Speckl…\n 9 2021-01-24 January                     2021                      55         172750 Google Black …\n10 2020-12-18 December                    2020                     128         878199 #IamRemarkabl…\n# ℹ 7,482 more rows\n# ℹ 10 more variables: item_category &lt;fct&gt;, price_in_usd &lt;dbl&gt;, quantity &lt;dbl&gt;,\n#   item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;, category &lt;fct&gt;,\n#   country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\nAlso of note is step_date() converted the event_date_month column into an ordered factor for us.\n\nbake(ga_rec, new_data = NULL) |&gt;\n  pull(event_date_month) |&gt;\n  levels()\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"      \"July\"      \"August\"   \n [9] \"September\" \"October\"   \"November\"  \"December\" \n\n\nstep_date()’s feature argument has many different options. This includes:\n\nmonth\ndow (day of week)\ndoy (day of year)\nweek\nmonth\ndecimal (decimal date)\nquarter\nsemester\nyear\n\nJust for the heck of it, let’s create features using all of the available options.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_date(\n    event_date, \n    features = c(\"month\", \"dow\", \"doy\", \"week\", \"decimal\", \"quarter\", \"semester\", \"year\")\n  ) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~ymd(event_date) | Trained\n\n\n• Date features from: event_date | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL) |&gt;\n  relocate(starts_with(\"event_\"), .after = 1)\n\nbaked_ga\n\n# A tibble: 7,492 × 22\n   event_date event_date_month event_date_dow event_date_doy event_date_week event_date_decimal\n   &lt;date&gt;     &lt;fct&gt;            &lt;fct&gt;                   &lt;int&gt;           &lt;int&gt;              &lt;dbl&gt;\n 1 2020-12-02 Dec              Wed                       337              49              2021.\n 2 2020-12-07 Dec              Mon                       342              49              2021.\n 3 2020-12-08 Dec              Tue                       343              49              2021.\n 4 2021-01-20 Jan              Wed                        20               3              2021.\n 5 2020-12-04 Dec              Fri                       339              49              2021.\n 6 2020-12-28 Dec              Mon                       363              52              2021.\n 7 2020-12-28 Dec              Mon                       363              52              2021.\n 8 2020-12-17 Dec              Thu                       352              51              2021.\n 9 2021-01-24 Jan              Sun                        24               4              2021.\n10 2020-12-18 Dec              Fri                       353              51              2021.\n# ℹ 7,482 more rows\n# ℹ 16 more variables: event_date_quarter &lt;int&gt;, event_date_semester &lt;int&gt;, event_date_year &lt;int&gt;,\n#   purchase_revenue_in_usd &lt;dbl&gt;, transaction_id &lt;dbl&gt;, item_name &lt;fct&gt;, item_category &lt;fct&gt;,\n#   price_in_usd &lt;dbl&gt;, quantity &lt;dbl&gt;, item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;,\n#   payment_type &lt;fct&gt;, category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnlike other step_*() functions, step_date does not remove the original column. If needed you can modify this with the keep_original_columns argument.\n\n\n\nglimpse(baked_ga)\n\nRows: 7,492\nColumns: 22\n$ event_date              &lt;date&gt; 2020-12-02, 2020-12-07, 2020-12-08, 2021-01-20, 2020-12-04, 2020-…\n$ event_date_month        &lt;fct&gt; Dec, Dec, Dec, Jan, Dec, Dec, Dec, Dec, Jan, Dec, Dec, Dec, Jan, D…\n$ event_date_dow          &lt;fct&gt; Wed, Mon, Tue, Wed, Fri, Mon, Mon, Thu, Sun, Fri, Thu, Thu, Fri, T…\n$ event_date_doy          &lt;int&gt; 337, 342, 343, 20, 339, 363, 363, 352, 24, 353, 345, 352, 22, 352,…\n$ event_date_week         &lt;int&gt; 49, 49, 49, 3, 49, 52, 52, 51, 4, 51, 50, 51, 4, 51, 50, 49, 51, 5…\n$ event_date_decimal      &lt;dbl&gt; 2020.918, 2020.932, 2020.934, 2021.052, 2020.923, 2020.989, 2020.9…\n$ event_date_quarter      &lt;int&gt; 4, 4, 4, 1, 4, 4, 4, 4, 1, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 1, …\n$ event_date_semester     &lt;int&gt; 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, …\n$ event_date_year         &lt;int&gt; 2020, 2020, 2020, 2021, 2020, 2020, 2020, 2020, 2021, 2020, 2020, …\n$ purchase_revenue_in_usd &lt;dbl&gt; 41, 22, 29, 86, 142, 79, 79, 139, 55, 128, 23, 41, 20, 15, 223, 18…\n$ transaction_id          &lt;dbl&gt; 11395, 190059, 147010, 101328, 469624, 797936, 797936, 439292, 172…\n$ item_name               &lt;fct&gt; Google Navy Speckled Tee, Google Super G Tumbler (Red Lid), Google…\n$ item_category           &lt;fct&gt; Apparel, Lifestyle, Campus Collection, New, Apparel, Clearance, Cl…\n$ price_in_usd            &lt;dbl&gt; 24, 22, 6, 16, 71, 14, 14, 16, 55, 10, 16, 11, 10, 14, 3, 71, 11, …\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, …\n$ item_revenue_in_usd     &lt;dbl&gt; 24, 22, 6, 16, 71, 14, 14, 16, 55, 19, 16, 11, 10, 14, 6, 71, 11, …\n$ shipping_tier           &lt;fct&gt; FedEx Ground, FedEx Ground, UPS Ground, FedEx Ground, FedEx Ground…\n$ payment_type            &lt;fct&gt; Pay with credit card, Pay with credit card, Pay with credit card, …\n$ category                &lt;fct&gt; desktop, desktop, mobile, desktop, desktop, desktop, desktop, mobi…\n$ country                 &lt;fct&gt; France, United States, Thailand, United Kingdom, United States, Un…\n$ region                  &lt;fct&gt; Nouvelle-Aquitaine, Missouri, Bangkok, England, Washington, Califo…\n$ city                    &lt;fct&gt; (not set), (not set), Bangkok, (not set), (not set), (not set), (n…\n\n\nPretty neat! step_date() provides some pretty good utility, especially if you tend to work with a lot of dates needing to be transformed into different representations. We’ll see you tomorrow everybody.\n\n\nDay 18 - Create a lagged variable using step_lag()\nWelcome back, fellow readers! Another day, another step_*() function.\nCurrently, I tend to work with timeseries data. As such, I need to create lagged variables from time-to-time. recipes’ step_lag() function is handy in these situations. Let’s highlight an example using this function. But first, we need some data.\nOnce again, I’m going to use the obfuscated Google Analytics ecommerce data. You can read more about this data in my previous posts. I’m not going to detail it much here, other than importing it, doing a little wrangling to calculate total revenue and total number of items purchased, and creating a training testing split. Here’s all the code to do this:\n\ndata_ga &lt;- \n  read_csv(here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\"))\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    items_purchased = sum(quantity),\n    revenue = sum(item_revenue_in_usd)\n  )\n\n\nset.seed(20240118)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nLet’s create our recipe, prep, and bake it. The tidy() method doesn’t provide too much useful information about our step, other than what columns lagged columns will be created from, so no use in looking at it here. The important argument is lag. This argument is where we specify how much we want to lag our variable by.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1) |&gt;\n  prep()\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 48 × 5\n   event_date items_purchased revenue lag_1_items_purchased lag_1_revenue\n   &lt;date&gt;               &lt;dbl&gt;   &lt;dbl&gt;                 &lt;dbl&gt;         &lt;dbl&gt;\n 1 2020-12-28             187    2097                    NA            NA\n 2 2021-01-27              51     457                   187          2097\n 3 2020-12-30             317    1606                    51           457\n 4 2020-12-18             398    6617                   317          1606\n 5 2021-01-09              68    1362                   398          6617\n 6 2021-01-24             133    2406                    68          1362\n 7 2020-12-06             140    2328                   133          2406\n 8 2020-12-14             527    8498                   140          2328\n 9 2021-01-04              63     835                   527          8498\n10 2020-12-01             356    6260                    63           835\n# ℹ 38 more rows\n\n\nThe lag argument also makes it convenient to create multiple lagged variables in one step. We just need to pass along a vector of positive vectors (e.g., 1:3) to the lag argument. This looks like this:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1:3) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~ymd(event_date) | Trained\n\n\n• Lagging: items_purchased and revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date            &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-01…\n$ items_purchased       &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156, …\n$ revenue               &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551,…\n$ lag_1_items_purchased &lt;dbl&gt; NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 1…\n$ lag_2_items_purchased &lt;dbl&gt; NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 6…\n$ lag_3_items_purchased &lt;dbl&gt; NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 2…\n$ lag_1_revenue         &lt;dbl&gt; NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, …\n$ lag_2_revenue         &lt;dbl&gt; NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 12…\n$ lag_3_revenue         &lt;dbl&gt; NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260…\n\n\nSay we want a variable lagged by 1, 3, and 5 values. We can do this by doing the following:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = c(1, 3, 5)) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~ymd(event_date) | Trained\n\n\n• Lagging: items_purchased and revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date            &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-01…\n$ items_purchased       &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156, …\n$ revenue               &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551,…\n$ lag_1_items_purchased &lt;dbl&gt; NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 1…\n$ lag_3_items_purchased &lt;dbl&gt; NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 2…\n$ lag_5_items_purchased &lt;dbl&gt; NA, NA, NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 9…\n$ lag_1_revenue         &lt;dbl&gt; NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, …\n$ lag_3_revenue         &lt;dbl&gt; NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260…\n$ lag_5_revenue         &lt;dbl&gt; NA, NA, NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 8…\n\n\nNot happy with the default NA value. We can change that by passing a value to the default argument.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1:3, default = 0) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~ymd(event_date) | Trained\n\n\n• Lagging: items_purchased and revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date            &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-01…\n$ items_purchased       &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156, …\n$ revenue               &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551,…\n$ lag_1_items_purchased &lt;dbl&gt; 0, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 15…\n$ lag_2_items_purchased &lt;dbl&gt; 0, 0, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64,…\n$ lag_3_items_purchased &lt;dbl&gt; 0, 0, 0, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, …\n$ lag_1_revenue         &lt;dbl&gt; 0, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 5…\n$ lag_2_revenue         &lt;dbl&gt; 0, 0, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224…\n$ lag_3_revenue         &lt;dbl&gt; 0, 0, 0, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1…\n\n\nThe prefix argument also makes it easy to modify the prefix of the outputted column names.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_mutate(event_date = ymd(event_date)) |&gt;\n  step_lag(items_purchased, revenue, lag = 1:3, prefix = \"diff_\") |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 2\ndate_ref:  1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~ymd(event_date) | Trained\n\n\n• Lagging: items_purchased and revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\nglimpse(baked_ga)\n\nRows: 48\nColumns: 9\n$ event_date             &lt;date&gt; 2020-12-28, 2021-01-27, 2020-12-30, 2020-12-18, 2021-01-09, 2021-0…\n$ items_purchased        &lt;dbl&gt; 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, 156,…\n$ revenue                &lt;dbl&gt; 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224, 551…\n$ diff_1_items_purchased &lt;dbl&gt; NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, 64, …\n$ diff_2_items_purchased &lt;dbl&gt; NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, 26, …\n$ diff_3_items_purchased &lt;dbl&gt; NA, NA, NA, 187, 51, 317, 398, 68, 133, 140, 527, 63, 356, 96, 40, …\n$ diff_1_revenue         &lt;dbl&gt; NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1224,…\n$ diff_2_revenue         &lt;dbl&gt; NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 6260, 1…\n$ diff_3_revenue         &lt;dbl&gt; NA, NA, NA, 2097, 457, 1606, 6617, 1362, 2406, 2328, 8498, 835, 626…\n\n\nThere you have it, another useful step_*() function. step_lag() is pretty straightforward, but the lag argument makes it easy to create many different lagged variables. See you tomorrow.\n\n\nDay 19 - Use step_log() to log transform variables\nLet’s pivot topics at this point in the post. Another type of preprocessing step recipes can perform is data transformations. For today’s example, I’ll spend some time learning about step_log(). step_log() creates a recipe step that will log transform data.\nBefore highlighting the use of this step function, I wanted to revisit this topic from my Stats 101 course. To refresh my memory, I went to find explanations on what a log transformation is, while also looking for views on why you might employ it as a data preprocessing step in machine learning contexts.\nWhat is a log transformation? In simple terms, it’s using a mathematical function (i.e., a logarithim) to transform variables from one representation to another. The @statquest YouTube channel has a pretty good video explaining what a log transformation does when applied to a variable.\nWhy use a log transformation? In most of what I’ve read, a log transformation is used to address skewed distributions, where it seeks to make the distribution more normal. This normality is important because some models (e.g., linear regression) assume variables are normally distributed. In addition, when employed in various modeling contexts, its application may lead to more accurate model predictions. Several of the articles I read stated log transformations are useful when working in domains that tend to have variables with skewed distributions, like financial data (i.e., salaries, housing prices, etc.). This Stack Exchange post provides a pretty good explanation on the use of a log transformation.\nTo highlight the use of this step function, let’s continue using our obfuscated Google Analytics data. Here’s the code to get started with this data.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240117)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nskim(ga_tr)\n\n\nData summary\n\n\nName\nga_tr\n\n\nNumber of rows\n7492\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n384\n0\n\n\nitem_category\n132\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n85\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n94\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n285\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n420\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1\n20203701.15\n3996.13\n20201201\n20201209\n20201216\n20210106.0\n20210130\n▇▁▁▁▃\n\n\npurchase_revenue_in_usd\n0\n1\n101.74\n119.22\n2\n39\n72\n124.0\n1530\n▇▁▁▁▁\n\n\ntransaction_id\n0\n1\n487314.88\n283530.55\n546\n246420\n479575\n724942.5\n999850\n▇▇▇▇▇\n\n\nprice_in_usd\n0\n1\n19.57\n18.85\n1\n7\n14\n24.0\n120\n▇▂▁▁▁\n\n\nquantity\n0\n1\n1.41\n2.13\n1\n1\n1\n1.0\n53\n▇▁▁▁▁\n\n\nitem_revenue_in_usd\n0\n1\n23.26\n28.89\n1\n8\n15\n30.0\n704\n▇▁▁▁▁\n\n\n\n\n\nLet’s take a quick look at item_revenue_in_usd. Using skim(), this variable ranges from $1 to $704. Now let’s peek at the distribution of the item_revenue_in_usd using base R plotting.\n\nhist(ga_tr$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nIndeed, this variable is skewed to the right. A log transformation would be appropriate here. Let’s do this by using step_log().\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\nNow that we transformed the variable, let’s explore the histogram of our baked variable and see its effect on the distribution.\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nNot bad. The transformed item_revenue_in_usd now exhibits a more normal distribution.\nstep_log() also has a few useful arguments. The first useful argument is base. When taking the log, R defaults to using the natural log, or exp(1). Say we want to log using base 10? This is certainly useful when dealing with money, since money makes more since in multiples of 10 (e.g., $100, $1,000, $10,000, etc.). Lets change the base of our recipe and see what happens.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd, base = 10) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nThe second useful argument is the offset argument. This argument accepts a value to offset the data prior to logging, which helps us avoid log(0). Here’s some example code:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd, offset = 5) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nThe final useful argument is signed, which accepts a boolean. Setting this argument to TRUE will make step_log() calculate the signed log. This is useful in cases where you have negative values, since a standard log transformation can’t be used to transform negative values. I found this article useful for getting an intuitive sense of what the signed log is and how it can useful for modeling. Here’s the code to serve as an example, even though we don’t have any negative values in our data:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_log(item_revenue_in_usd, signed = TRUE) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 13\ndate_ref:   1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 216 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Signed log transformation on: item_revenue_in_usd | Trained\n\nbake_ga &lt;- bake(ga_rec, new_data = NULL)\n\n\nhist(bake_ga$item_revenue_in_usd)\n\n\n\n\n\n\n\n\nThat’s it for today. step_log() is a great first step in learning all the transformations recipes can do. I’m looking forward to tomorrow.\n\n\nDay 20 - Use step_sqrt() to apply a square root transformation\nWelcome back fellow learners. I had to take a couple days off because my schedule didn’t allow me the time to focus on this post. Nevertheless, let’s get back to it.\nToday I’m focusing on another transformation function, step_sqrt(). step_sqrt() applies a square root transformation to variables. This function is similair to step_log()–it’s just applying a different function to the variable.\nThe square root transformation is pretty straightforward, but I did some digging to more deeply understand what it is and why it’s useful. Here’s what I came up with:\n\nWhat could be the reason for using square root transformation on data? - Cross Validated post\nWhy is the square root transformation recommended for count data? - Cross Validated post\n\nLet’s get our data. I’m going to continue using the obfuscated Google Analytics ecommerce data we’ve been working with.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240122)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nskim(ga_tr)\n\n\nData summary\n\n\nName\nga_tr\n\n\nNumber of rows\n7492\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n382\n0\n\n\nitem_category\n138\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n84\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n93\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n282\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n419\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1\n20203682.14\n3986.72\n20201201\n20201209\n20201216\n20210106.0\n20210130\n▇▁▁▁▃\n\n\npurchase_revenue_in_usd\n0\n1\n101.68\n118.25\n3\n39\n72\n124.0\n1530\n▇▁▁▁▁\n\n\ntransaction_id\n0\n1\n487567.11\n282529.44\n546\n250363\n477087\n723799.8\n999850\n▇▇▇▇▇\n\n\nprice_in_usd\n0\n1\n19.55\n18.80\n1\n7\n14\n24.0\n120\n▇▂▁▁▁\n\n\nquantity\n0\n1\n1.45\n2.89\n1\n1\n1\n1.0\n160\n▇▁▁▁▁\n\n\nitem_revenue_in_usd\n0\n1\n23.40\n29.67\n1\n8\n15\n30.0\n704\n▇▁▁▁▁\n\n\n\n\n\nWe do have some count data, quantity. This is the number of times a specific item was purchased. Let’s take a look at its distribution before we apply our transformation.\n\nhist(ga_tr$quantity)\n\n\n\n\n\n\n\n\nIndeed, this data seems to follow a Poisson Distribution. Let’s see if we can transform it to be more normal (Gaussian) with a square root transformation.\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_sqrt(quantity) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 7492 data points and 221 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Square root transformation on: quantity | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 7,492 × 14\n   event_date purchase_revenue_in_usd transaction_id item_name   item_category price_in_usd quantity\n        &lt;dbl&gt;                   &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1   20201211                      60         411613 Supernatur… Bags                    46     1   \n 2   20210126                      68         315802 Google Sea… Campus Colle…            7     1   \n 3   20201217                      75         940161 Google Inc… Bags                    75     1   \n 4   20210114                      83          22807 Google Pen… Office                   1     1.41\n 5   20201215                      95         892183 Google Fel… Clearance               16     1.41\n 6   20210102                     183         548270 Google Wom… Apparel                 50     1   \n 7   20201217                      16         199350 Google Per… Lifestyle               16     1   \n 8   20201228                      44         302288 Google Lap… New                      8     1   \n 9   20210124                     104          19596 Google Key… New                      6     1.41\n10   20201212                      74         539684 Google Bou… Clearance               14     1   \n# ℹ 7,482 more rows\n# ℹ 7 more variables: item_revenue_in_usd &lt;dbl&gt;, shipping_tier &lt;fct&gt;, payment_type &lt;fct&gt;,\n#   category &lt;fct&gt;, country &lt;fct&gt;, region &lt;fct&gt;, city &lt;fct&gt;\n\n\nNow we’ll check out the effect of the transformation by looking at a histogram of the baked data’s quantity variable.\n\nhist(baked_ga$quantity)\n\n\n\n\n\n\n\n\nWe can further explore the effect of this transformation by plotting the un-transformed variable on a scatter plot with the baked variable.\n\nplot(ga_tr$quantity, baked_ga$quantity)\n\n\n\n\n\n\n\n\nEh, looking at the histogram it seems this transformation didn’t really help very much. My assumption is this is due to many items only being purchased once. Thus, no transformation would likely be helpful in this case. Nonetheless, step_sqrt() is a pretty simple transformation function. You might find it useful when working with your data.\n\n\nDay 21 - Apply the Box-Cox transformation using step_BoxCox()\nHere we are, another day of transformations. To get us started, I turned to ChatGPT for a joke about data transformations.\nHey ChatGPT, tell me a joke about data transformations (prompt).\n\nWhy do data transformations never get invited to parties?\nBecause they always skew the conversation and standard deviations can be quite awkward!\n\nNot bad … Okay, let’s talk about step_BoxCox(). This function applies a Box Cox Transformation to our variables. I haven’t used this transformation in some time, so I scoured the internet to get back up to speed. I found these sources helpful in reminding me what this transformation is and how it can be useful:\n\nTransforming the response(Y) in regression: Box Cox transformation (7 mins) from the @PhilChanstats YouTube channel\nBox Cox transformation formula in regression analysis from the @PhilChanstats YouTube channel\nBox-Cox Transformation + R Demo from the @mathetal YouTube channel\n\nThough the following is a simplified explanation, many of these sources mention the Box Cox transformation attempts to make our distribution more normal. It does this by focusing on the tails of the distribution. Box Cox can be applied in situations where variance is not equal (i.e., heteroscedasticity). This transformation also allows us to better meet the assumptions of our models, and its application can result in a better predictive model. Lastly, many of these sources suggest this transformation is applied to the outcome variable of our model.\nLet’s go back to our Google Analytics obfuscated ecommerce data for today’s examples.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    items_purchased = sum(quantity),\n    revenue = sum(item_revenue_in_usd)\n  )\n\nset.seed(20240122)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nSince we can use the Box Cox transformation to rescale a variable to be more normal, let’s see if the revenue variable will be a good candidate. You’ll notice the application of the Box Cox transformation is pretty straightforward (like most of recipes’ transformation functions) with step_BoxCox().\n\nhist(ga_tr$revenue)\n\n\n\n\n\n\n\n\nIndeed, this varible exhibits a slight right skew. Let’s use the step_BoxCox() function here to perform the transformation.\n\nga_rec &lt;- recipe(revenue ~ ., data = ga_tr) |&gt;\n  step_BoxCox(revenue) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 48 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Box-Cox transformation on: revenue | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 48 × 3\n   event_date items_purchased revenue\n        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1   20210101              97    14.2\n 2   20210126              61    15.2\n 3   20201225              40    12.2\n 4   20210125             199    19.4\n 5   20201211             617    25.3\n 6   20210115             119    17.2\n 7   20210120             400    22.8\n 8   20201201             356    22.5\n 9   20201205             333    22.6\n10   20201224              69    15.0\n# ℹ 38 more rows\n\n\nNot too bad. The distribution has been rescaled to resemble a slightly more normal distribution. The following histogram shows the effect of this transformation on the distribution.\n\nhist(baked_ga$revenue)\n\n\n\n\n\n\n\n\nIf for some reason you wanted to inspect the lambda value used by the transformation, you can inspect it by tidying the recipe. The outputted table shows lambda in the value column.\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms   value id          \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 revenue 0.190 BoxCox_GMKqI\n\n\nstep_BoxCox() is pretty simple, but useful. You might give it a try if you have some data of positive values exihibiting skewness. Until tomorrow, keep having fun with recipes.\n\n\nDay 22 - Apply an inverse transformation using step_inverse()\nI’m keeping things simple today; step_inverse() is my focus. This step function will inverse transform our data. Just like the other transformation functions, recipes makes it pretty easy to perform.\nLet’s continue using our Google Analytics ecommerce data for today’s example.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    items_purchased = sum(quantity),\n    revenue = sum(item_revenue_in_usd)\n  )\n\nset.seed(20240122)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nJust as a quick reminder, let’s take a look at the histogram of items_purchased.\n\nhist(ga_tr$items_purchased)\n\n\n\n\n\n\n\n\nTo apply an inverse transformation to our variable, we do the following in our recipe.\n\nga_rec &lt;- recipe(~., data = ga_te) |&gt;\n  step_inverse(items_purchased) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 13 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Inverse transformation on: items_purchased | Trained\n\n\nNow bake.\n\nga_baked &lt;- bake(ga_rec, new_data = NULL)\n\nga_baked\n\n# A tibble: 13 × 3\n   event_date items_purchased revenue\n        &lt;dbl&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1   20201202         0.00366    5202\n 2   20201203         0.00319    4313\n 3   20201207         0.00215    8466\n 4   20201208         0.00161    8691\n 5   20201214         0.00190    8498\n 6   20201219         0.00714    2778\n 7   20201222         0.00474    3226\n 8   20201230         0.00315    1606\n 9   20201231         0.0106     2595\n10   20210102         0.00990    1025\n11   20210113         0.00990    1859\n12   20210116         0.0139     1462\n13   20210123         0.00524    2744\n\n\nHere’s the effect of the transformation on our variable in two plots.\n\nhist(ga_baked$items_purchased)\n\n\n\n\n\n\n\nplot(ga_te$items_purchased, ga_baked$items_purchased)\n\n\n\n\n\n\n\n\nThe inverse did a pretty good job of making this distribution more normal.\nLike I said, keeping things simple today with a straight forward step_*() function. recipes makes it pretty easy to perform the inverse transformation with the step_inverse() function.\n\n\nDay 23 - Use extension packages to get even more steps\nRecently, I’ve felt these posts have become a little stale and templated. So today, I’m switching it up a bit. I’m highlighting recipes extension packages. I’m also using this post for a bit of inspiration; I’m exploring other step_* functions to highlight. As such, I most likely won’t describe specific examples from each package in today’s post (there’s too many to cover). However, I’m aiming to list them here and highlight some packages that would be useful for the work I do.\nSeveral packages are available that provide additional step_*() functions, which are outside the core functionality of the recipes package. At the time of this writing, these include but are not limited to:\n\nactxps\nbestNormalize\ncustomsteps\nembed\nextrasteps\ntfhub\nhealthcareai\nhealthyR.ai\nhealthyR.ts\nhydrorecipes\nMachineShop\nmeasure\nnestedmodels\nsparseR\ntextrecipes\nthemis\ntimetk\n\nThe tidymodels site provides a table of a good majority of step_*() functions made available via these extension packages. Shout out to @jonthegeek from the R4DS Online Learning Community for pointing me to this table, and much praise to @Emil Hvitfeldt for pointing me in the direction of additional recipes extension packages I wasn’t aware. Keep in mind, not all of these extension packages are made available on CRAN.\nTaking a moment to read the descriptions of each extension package, I found textrecipes, themis, and timetk to be the most applicable to my work. Here’s a brief description of what each does. I also included some functions that could be interesting to explore in future posts. Note, some of these packages are meant to be applied in a specific domain, so their purpose may not solely be to be an extension of recipes (e.g., timetk).\n\nThe textrecipes package provides extra step_*() functions to preprocess text data. Looking through the package’s reference section, the step_tokenize() family of functions look useful. I also find the token modification (e.g., step_stem() and step_stopwords()) and text cleaning functions (e.g. step_clean_levels()) to be intriguing and worth further exploration.\nThe themis package provides additional step_*() functions to assist in the preprocessing of unbalanced data. Specifically, this package provides functions to apply over-sampling or under-sampling methods to unbalance the data used for modeling. There’s several functions in here that seem to be useful when dealing with unbalanced data. I highly suggest checking out its reference page to get an idea of what all is available.\nThe timetk package does more than extend recipes. It’s main focus is to assist in the analysis of timeseries data. The package contains some similar functions we’ve highlighted before (e.g., step_holiday_signature() and step_box_cox()). However, there’s some additional functions of interest. step_ts_pad() seems useful in cases where you need to add rows to fill in gaps. step_ts_clean() helps to clean outliers and missing data for timeseries analysis. step_diff() also looks helpful if you need to create a differenced predictor. If you’re working with timeseries, you might want to explore this package some more.\n\nIndeed, the tidymodels ecosystem is quite vast. This is especially true in the area of preprocessing steps achieved by extension packages for the recipes package.\nI wanted to shake it up a little bit. Today’s post was a little different, but in a good, refreshing way. See you tomorrow.\n\n\nDay 24 - Use step_tokenize() from textrecipes to convert a character predictor into a token variable\nBuilding on yesterday’s post, I’ll begin exploring recipes extension packages’ functions. step_tokenize() from textrecipes is first up. This function is useful when we need to convert a character variable into a token variable. If you’re unfamiliar with the concept of tokenization, the Text Mining with R: A Tidy Approach provides an overview.\nI’ll keep things simple here by using a pared down data set. This will make it easier to see what this step_function() is doing. How about some quotes about statistics?\n\nquotes &lt;- tibble(\n  text = c(\n    \"All models are wrong, but some are useful.\",\n    \"Statisticians, like artists, have the bad habit of falling in love with their models.\",\n    \"If you torture the data enough, nature will always confess.\",\n    \"All generalizations are false, including this one.\"\n  )\n)\n\nLet’s create our recipe to tokenize by word.\n\nquotes_rec &lt;- recipe(~ text, data = quotes) |&gt;\n  step_tokenize(text) |&gt;\n  prep() \n\nquotes_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Tokenization for: text | Trained\n\nbaked_quotes &lt;- bake(quotes_rec, new_data = NULL)\n\nbaked_quotes\n\n# A tibble: 4 × 1\n         text\n    &lt;tknlist&gt;\n1  [8 tokens]\n2 [14 tokens]\n3 [10 tokens]\n4  [7 tokens]\n\n\nOne thing I didn’t expect, but learned, was step_tokenize() creates a tknlist column. Indeed, I expected the column to still be a character where every token was placed on it’s own row, like what happens when you use tidytext functions. This certainly makes the object more compact and easier to work with, an excellent design decision in my opinion. To see the tokens, though, we need to use recipes show_tokens() function.\n\nrecipe(~ text, data = quotes) |&gt;\n  step_tokenize(text) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all\"    \"models\" \"are\"    \"wrong\"  \"but\"    \"some\"   \"are\"    \"useful\"\n\n[[2]]\n [1] \"statisticians\" \"like\"          \"artists\"       \"have\"          \"the\"           \"bad\"          \n [7] \"habit\"         \"of\"            \"falling\"       \"in\"            \"love\"          \"with\"         \n[13] \"their\"         \"models\"       \n\n[[3]]\n [1] \"if\"      \"you\"     \"torture\" \"the\"     \"data\"    \"enough\"  \"nature\"  \"will\"    \"always\" \n[10] \"confess\"\n\n[[4]]\n[1] \"all\"             \"generalizations\" \"are\"             \"false\"           \"including\"      \n[6] \"this\"            \"one\"            \n\n\nstep_tokenize() defaults to tokenizing by word. However, there are several ways we can tokenize by. To do this, we modify the tokenizers argument. For example, say we want to tokenize by character instead of word. We do the following:\n\nrecipe(~ text, data = quotes) |&gt;\n  step_tokenize(text, token = \"characters\") |&gt;\n  show_tokens(text)\n\n[[1]]\n [1] \"a\" \"l\" \"l\" \"m\" \"o\" \"d\" \"e\" \"l\" \"s\" \"a\" \"r\" \"e\" \"w\" \"r\" \"o\" \"n\" \"g\" \"b\" \"u\" \"t\" \"s\" \"o\" \"m\" \"e\"\n[25] \"a\" \"r\" \"e\" \"u\" \"s\" \"e\" \"f\" \"u\" \"l\"\n\n[[2]]\n [1] \"s\" \"t\" \"a\" \"t\" \"i\" \"s\" \"t\" \"i\" \"c\" \"i\" \"a\" \"n\" \"s\" \"l\" \"i\" \"k\" \"e\" \"a\" \"r\" \"t\" \"i\" \"s\" \"t\" \"s\"\n[25] \"h\" \"a\" \"v\" \"e\" \"t\" \"h\" \"e\" \"b\" \"a\" \"d\" \"h\" \"a\" \"b\" \"i\" \"t\" \"o\" \"f\" \"f\" \"a\" \"l\" \"l\" \"i\" \"n\" \"g\"\n[49] \"i\" \"n\" \"l\" \"o\" \"v\" \"e\" \"w\" \"i\" \"t\" \"h\" \"t\" \"h\" \"e\" \"i\" \"r\" \"m\" \"o\" \"d\" \"e\" \"l\" \"s\"\n\n[[3]]\n [1] \"i\" \"f\" \"y\" \"o\" \"u\" \"t\" \"o\" \"r\" \"t\" \"u\" \"r\" \"e\" \"t\" \"h\" \"e\" \"d\" \"a\" \"t\" \"a\" \"e\" \"n\" \"o\" \"u\" \"g\"\n[25] \"h\" \"n\" \"a\" \"t\" \"u\" \"r\" \"e\" \"w\" \"i\" \"l\" \"l\" \"a\" \"l\" \"w\" \"a\" \"y\" \"s\" \"c\" \"o\" \"n\" \"f\" \"e\" \"s\" \"s\"\n\n[[4]]\n [1] \"a\" \"l\" \"l\" \"g\" \"e\" \"n\" \"e\" \"r\" \"a\" \"l\" \"i\" \"z\" \"a\" \"t\" \"i\" \"o\" \"n\" \"s\" \"a\" \"r\" \"e\" \"f\" \"a\" \"l\"\n[25] \"s\" \"e\" \"i\" \"n\" \"c\" \"l\" \"u\" \"d\" \"i\" \"n\" \"g\" \"t\" \"h\" \"i\" \"s\" \"o\" \"n\" \"e\"\n\n\nHow about by word stems?\n\nrecipe(~ text, data = quotes) |&gt;\n  step_tokenize(text, token = \"word_stems\") |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all\"   \"model\" \"are\"   \"wrong\" \"but\"   \"some\"  \"are\"   \"use\"  \n\n[[2]]\n [1] \"statistician\" \"like\"         \"artist\"       \"have\"         \"the\"          \"bad\"         \n [7] \"habit\"        \"of\"           \"fall\"         \"in\"           \"love\"         \"with\"        \n[13] \"their\"        \"model\"       \n\n[[3]]\n [1] \"if\"      \"you\"     \"tortur\"  \"the\"     \"data\"    \"enough\"  \"natur\"   \"will\"    \"alway\"  \n[10] \"confess\"\n\n[[4]]\n[1] \"all\"     \"general\" \"are\"     \"fals\"    \"includ\"  \"this\"    \"one\"    \n\n\nngrams?\n\nrecipe(~ text, data = quotes) |&gt;\n  step_tokenize(text, token = \"ngrams\") |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all models are\"   \"models are wrong\" \"are wrong but\"    \"wrong but some\"   \"but some are\"    \n[6] \"some are useful\" \n\n[[2]]\n [1] \"statisticians like artists\" \"like artists have\"          \"artists have the\"          \n [4] \"have the bad\"               \"the bad habit\"              \"bad habit of\"              \n [7] \"habit of falling\"           \"of falling in\"              \"falling in love\"           \n[10] \"in love with\"               \"love with their\"            \"with their models\"         \n\n[[3]]\n[1] \"if you torture\"      \"you torture the\"     \"torture the data\"    \"the data enough\"    \n[5] \"data enough nature\"  \"enough nature will\"  \"nature will always\"  \"will always confess\"\n\n[[4]]\n[1] \"all generalizations are\"   \"generalizations are false\" \"are false including\"      \n[4] \"false including this\"      \"including this one\"       \n\n\nSay we only want 2 words in our ngram tokens. In this case, we need to pass argument values to the tokenizers::tokenize_ngrams() function using step_tokenize()’s options argument. Here we pass all the argument values in a list.\n\nrecipe(~ text, data = quotes) |&gt;\n  step_tokenize(\n    text, \n    token = \"ngrams\",\n    options = list(n = 2)\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all models\" \"models are\" \"are wrong\"  \"wrong but\"  \"but some\"   \"some are\"   \"are useful\"\n\n[[2]]\n [1] \"statisticians like\" \"like artists\"       \"artists have\"       \"have the\"          \n [5] \"the bad\"            \"bad habit\"          \"habit of\"           \"of falling\"        \n [9] \"falling in\"         \"in love\"            \"love with\"          \"with their\"        \n[13] \"their models\"      \n\n[[3]]\n[1] \"if you\"         \"you torture\"    \"torture the\"    \"the data\"       \"data enough\"   \n[6] \"enough nature\"  \"nature will\"    \"will always\"    \"always confess\"\n\n[[4]]\n[1] \"all generalizations\" \"generalizations are\" \"are false\"           \"false including\"    \n[5] \"including this\"      \"this one\"           \n\n\nGoing further, say we want to also exclude stop words (e.g., the, is, and) from being included within our ngrams. We just pass this option along in our list of options.\n\nrecipe(~ text, data = quotes) |&gt;\n  step_tokenize(\n    text, \n    token = \"ngrams\",\n    options = list(n = 2, stopwords = c(\"the\", \"is\", \"and\"))\n  ) |&gt;\n  show_tokens(text)\n\n[[1]]\n[1] \"all models\" \"models are\" \"are wrong\"  \"wrong but\"  \"but some\"   \"some are\"   \"are useful\"\n\n[[2]]\n [1] \"statisticians like\" \"like artists\"       \"artists have\"       \"have bad\"          \n [5] \"bad habit\"          \"habit of\"           \"of falling\"         \"falling in\"        \n [9] \"in love\"            \"love with\"          \"with their\"         \"their models\"      \n\n[[3]]\n[1] \"if you\"         \"you torture\"    \"torture data\"   \"data enough\"    \"enough nature\" \n[6] \"nature will\"    \"will always\"    \"always confess\"\n\n[[4]]\n[1] \"all generalizations\" \"generalizations are\" \"are false\"           \"false including\"    \n[5] \"including this\"      \"this one\"           \n\n\nThat’s all the time I have for today. There’s some other things step_tokenize() can do. I highly suggest checking out it’s documentation to see all of its functionality.\n\n\nDay 25 - Use step_clean_level() to clean categorical levels\nKeeping our focus on textrecipes, I’m going to highlight the use of step_clean_levels(). This function is useful for cleaning up the text used within character or factor variables. Specifically, this function will clean text values in our variable to only include:\n\nletters\nnumbers\nunderscores\n\nLet’s apply this text cleaning recipe step to the item_name variable in our obfuscated Google Analytics data. In fact, to make it easier to see what’s happening, I’ll select() just the item_name within our example data set. I’ll also go ahead and just create our training and testing split here as well.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  ) |&gt;\nselect(item_name)\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(20240127)\nga_split &lt;- initial_split(data_ga, prop = .8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nIf you scan through the items, you’ll begin to notice some of the text could be cleaned. For example, the data contains item names like:\n\nGoogle Medium Pet Collar (Red/Yellow)\n#IamRemarkable Unisex T-Shirt\nAndroid SM S/F18 Sticker Sheet\n\nIf you’ve ever worked with ecommerce or website data, these strings are actually not too bad. However, we can use step_clean_levels() to make them easier to compute on.\nHere’s our recipe.\n\nga_rec &lt;- recipe(~., data = ga_te) |&gt;\n  step_clean_levels(item_name) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 1873 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Cleaning factor levels for: item_name | Trained\n\n\nstep_clean_levels()’ tidy method is pretty informative. It provides one column with the original value and a second column with the cleaned value. Take a look at what it will do to our values once baked.\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 332 × 4\n   terms     original                      value                                id                \n   &lt;chr&gt;     &lt;chr&gt;                         &lt;chr&gt;                                &lt;chr&gt;             \n 1 item_name #IamRemarkable Journal        number_iam_remarkable_journal        clean_levels_JONrr\n 2 item_name #IamRemarkable Ladies T-Shirt number_iam_remarkable_ladies_t_shirt clean_levels_JONrr\n 3 item_name #IamRemarkable Lapel Pin      number_iam_remarkable_lapel_pin      clean_levels_JONrr\n 4 item_name #IamRemarkable Pen            number_iam_remarkable_pen            clean_levels_JONrr\n 5 item_name #IamRemarkable Unisex Hoodie  number_iam_remarkable_unisex_hoodie  clean_levels_JONrr\n 6 item_name #IamRemarkable Unisex T-Shirt number_iam_remarkable_unisex_t_shirt clean_levels_JONrr\n 7 item_name #IamRemarkable Water Bottle   number_iam_remarkable_water_bottle   clean_levels_JONrr\n 8 item_name Android Buoy Bottle           android_buoy_bottle                  clean_levels_JONrr\n 9 item_name Android Garden Tee Orange     android_garden_tee_orange            clean_levels_JONrr\n10 item_name Android Geek Pin              android_geek_pin                     clean_levels_JONrr\n# ℹ 322 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s interesting to see that the hashtag (i.e., #) was converted to the value number. I couldn’t find any options in step_clean_levels()to modify this, so you may need to first do some pre-cleaning before applying this function.\n\n\nNow, we bake.\n\n\n\n\n\n\nNote\n\n\n\nstep_clean_levels() will convert character vectors to factors for us.\n\n\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 1,873 × 1\n   item_name                           \n   &lt;fct&gt;                               \n 1 google_thermal_tumbler_navy         \n 2 noogler_android_figure              \n 3 google_leather_strap_hat_blue       \n 4 you_tube_jotter_task_pad            \n 5 google_nyc_campus_mug               \n 6 google_nyc_campus_zip_hoodie        \n 7 number_iam_remarkable_ladies_t_shirt\n 8 number_iam_remarkable_lapel_pin     \n 9 google_leather_strap_hat_blue       \n10 google_f_c_longsleeve_ash           \n# ℹ 1,863 more rows\n\n\nAll in all, a pretty useful step function from textrecipes. Take some time to apply the step_clean_levels() to your data. It might save you a great deal of time if you’re working with some messy text data.\n\n\nDay 26 - Use over-sampling and under-sampling methods with the themis package\nToday I’m going to highlight some basic uses of the themis package. themis makes over- and under-sampling methods available. These methods are useful to address imbalanced class data. You can read more about these methods here and here. In short, these methods allow us to extract more accurate information from imbalanced data.\nTo highlight this, let’s use our obfuscated Google Analytics ecommerce data. Specifically, let’s check out our shipping_tier variable for each transaction. Here’s the code we’ll need to wrangle the data for our example:\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  group_by(transaction_id) |&gt;\n  summarise(\n    revenue = sum(item_revenue_in_usd),\n    shipping_tier = max(shipping_tier)\n  ) |&gt;\n  mutate(shipping_tier = factor(shipping_tier))\n\n\nset.seed(20240128)\nga_split &lt;- initial_split(data_ga, prop = 0.8)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nLet’s take a moment to check for the presence of imbalanced data. We can do this with a simple count() and a quick bar chart using ggplot2.\n\nga_tr |&gt;\n  count(shipping_tier, sort = TRUE)\n\n# A tibble: 14 × 2\n   shipping_tier              n\n   &lt;fct&gt;                  &lt;int&gt;\n 1 FedEx Ground            2048\n 2 UPS Ground               270\n 3 FedEx 2Day               125\n 4 International Shipping    34\n 5 FedEx Overnight           25\n 6 &lt;NA&gt;                      25\n 7 FedEx Ground-T            12\n 8 FedEx Ground-V             7\n 9 UPS 2nd Day Air            5\n10 UPS 3 Day Select           5\n11 FedEx Ground-GI            2\n12 FedEx 2Day-V               1\n13 UPS 2nd Day Air-F-V        1\n14 UPS Next Day Air           1\n\n\n\nggplot(\n  count(ga_tr, shipping_tier, sort = TRUE),\n  aes(reorder(shipping_tier, n), y = n)\n) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Shipping Tier\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou’ll notice the presence of a severe imbalance in the shipping_tier variable, as most transactions selected FedEx ground as the shipping tier (most likely the cheapest and default shipping tier). We can use step_upsample() and step_downsample() to create synthetic samples to address this imbalance with our recipe.\nWhen over- or under-sampling, we need to select an over_ratio or under_raio. These values specify the ratio of the majority-to-minority or minority-to-majority frequencies, which are used to determine how many of the minority samples are added or how many of the majority class are removed.\nFor instance, if we are upsampling, we can set the over_ratio to 1. This will synthetically add points to the minority class so the minority and majority classes are equal.\n\n\n\n\n\n\nNote\n\n\n\nI’m omitting NAs here with step_naomit(), just to make things easier.\n\n\n\nga_rec &lt;- recipe(~., data = data_ga) |&gt;\n  step_naomit(shipping_tier) |&gt;\n  step_upsample(shipping_tier, over_ratio = 1) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3202 data points and 27 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Removing rows with NA values in: shipping_tier | Trained\n\n\n• Up-sampling based on: shipping_tier | Trained\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL)\n\nbaked_ga\n\n# A tibble: 33,436 × 3\n   transaction_id revenue shipping_tier\n            &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;        \n 1         551345     137 FedEx 2Day   \n 2         548006      71 FedEx 2Day   \n 3         843790      40 FedEx 2Day   \n 4         852510      50 FedEx 2Day   \n 5         318033      48 FedEx 2Day   \n 6         392135      57 FedEx 2Day   \n 7         888228      90 FedEx 2Day   \n 8         396838     671 FedEx 2Day   \n 9         975263      24 FedEx 2Day   \n10         859104      14 FedEx 2Day   \n# ℹ 33,426 more rows\n\n\n\nggplot(\n  count(baked_ga, shipping_tier, sort = TRUE),\n  aes(reorder(shipping_tier, n), y = n)\n) +\n  geom_col() +\n  coord_flip() +\n  labs(y = \"Shipping Tier\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSay we only want the minority levels to have about half the amount of values of the majority class. We can do this by passing .5 to the over_ratio argument.\n\nrecipe(~., data = data_ga) |&gt;\n  step_naomit(shipping_tier) |&gt;\n  step_upsample(shipping_tier, over_ratio = .5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  count(shipping_tier, sort = TRUE) |&gt;\n  ggplot(\n    aes(reorder(shipping_tier, n), y = n)\n  ) +\n    geom_col() +\n    coord_flip() +\n    labs(y = \"Shipping Tier\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nUnder-sampling is pretty much the same, but we use step_downsample() and the under_ratio argument. Here’s some example code to get you started.\n\nrecipe(~., data = data_ga) |&gt;\n  step_naomit(shipping_tier) |&gt;\n  step_downsample(shipping_tier, under_ratio = 100) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  count(shipping_tier, sort = TRUE) |&gt;\n  ggplot(\n    aes(reorder(shipping_tier, n), y = n)\n  ) +\n    geom_col() +\n    coord_flip() +\n    labs(y = \"Shipping Tier\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nthemis makes it pretty easy to utilize over- and under-sampling methods. Indeed, this was just a highlight of some of the package’s basics. themis also has implemented some more advanced methods to address imbalanced data (e.g., SMOTE). If you work with imbalanced data, I suggest checking out the themis package.\n\n\nDay 27 - Use step_ts_pad() from timetk to pad timeseries data\nIt’s been a few of days. Work was getting busy, so I didn’t have time to focus on this post as much as I would have liked. So, let’s pick up where we left off, focusing on some step_*() functions from extension packages. Today, I’m focusing on the step_ts_pad() from the timetk package.\ntimetk is it’s own package. It serves as a toolkit for working with timeseries data, so it’s not just an extension package to recipes. However, it contains some useful step_*() functions when modeling timeseries data. step_ts_pad() is highlighted in today’s post.\nstep_ts_pad() fills in the gaps of timeseries data with a specified value. Let’s take a look at some example data. Specifically, we’re returning to our obfuscated Google Analytics ecommerce data. To make it more clear what this function is doing, I’ll remove some values.\n\ndata_ga &lt;- read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  )\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_ga &lt;- data_ga |&gt;\n  mutate(event_date = ymd(event_date)) |&gt;\n  group_by(event_date) |&gt;\n  summarise(\n    revenue = sum(item_revenue_in_usd),\n  ) \n\n# Remove some values to better highlight the example\ndata_ga &lt;- data_ga[1:61 %% 2 == 0, ]\n\ndata_ga\n\n# A tibble: 30 × 2\n   event_date revenue\n   &lt;date&gt;       &lt;dbl&gt;\n 1 2020-12-02    5202\n 2 2020-12-04    6454\n 3 2020-12-06    2328\n 4 2020-12-08    8691\n 5 2020-12-10   10760\n 6 2020-12-12    6280\n 7 2020-12-14    8498\n 8 2020-12-16   11505\n 9 2020-12-18    6617\n10 2020-12-20    1401\n# ℹ 20 more rows\n\n\nWe’re working with timeseries data, so the training and testing split is created using rsamples’ initital_time_split() function. This split function performs the same action as initial_split(), however, it takes the first prop samples for training, instead of a random selection (checkout ?initial_time_split for more info).\n\nset.seed(20240202)\nga_split &lt;- initial_time_split(data_ga)\nga_tr &lt;- training(ga_split)\nga_te &lt;- testing(ga_split)\n\nWe don’t have a complete timeseries here, since this data is aggregated by day and we removed some days in the wrangling step. However, we can use step_ts_pad() to fill in values for missing days. This can be as simple as filling in missing values with NA. Here’s the code to do this:\n\nga_rec &lt;- recipe(~., data = ga_tr) |&gt;\n  step_ts_pad(event_date, by = \"day\", pad_value = NA) |&gt;\n  prep()\n\ntidy(ga_rec)\n\n# A tibble: 1 × 6\n  number operation type   trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      ts_pad TRUE    FALSE ts_padding_KR6BU\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 × 4\n  terms      by    pad_value id              \n  &lt;chr&gt;      &lt;chr&gt; &lt;lgl&gt;     &lt;chr&gt;           \n1 event_date day   NA        ts_padding_KR6BU\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 43 × 2\n   event_date revenue\n   &lt;date&gt;       &lt;dbl&gt;\n 1 2020-12-02    5202\n 2 2020-12-03      NA\n 3 2020-12-04    6454\n 4 2020-12-05      NA\n 5 2020-12-06    2328\n 6 2020-12-07      NA\n 7 2020-12-08    8691\n 8 2020-12-09      NA\n 9 2020-12-10   10760\n10 2020-12-11      NA\n# ℹ 33 more rows\n\n\nNow we have a complete series. However, say we want to fill in this padded value with another value, say the mean of revenue from our training data.\n\nga_rec &lt;- recipe(~., data = data_ga) |&gt;\n  step_ts_pad(event_date, by = \"day\", pad_value = mean(ga_tr$revenue)) |&gt;\n  prep()\n\ntidy(ga_rec)\n\n# A tibble: 1 × 6\n  number operation type   trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      ts_pad TRUE    FALSE ts_padding_wvkNb\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 × 4\n  terms      by    pad_value id              \n  &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;           \n1 event_date day       3883. ts_padding_wvkNb\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 59 × 2\n   event_date revenue\n   &lt;date&gt;       &lt;dbl&gt;\n 1 2020-12-02   5202 \n 2 2020-12-03   3883.\n 3 2020-12-04   6454 \n 4 2020-12-05   3883.\n 5 2020-12-06   2328 \n 6 2020-12-07   3883.\n 7 2020-12-08   8691 \n 8 2020-12-09   3883.\n 9 2020-12-10  10760 \n10 2020-12-11   3883.\n# ℹ 49 more rows\n\n\nThe by argument can be used to modify the padding interval. This argument can take values like:\n\n“auto”\n“year”\n“month”\n“day”\n“hour”\n“7 days”\n\nFor example, let’s use the drinks timeseries data from the modeldata package. This data is a monthly timeseries of drink sales from 1992-01-01 to 2017-09-01. You can get more information about this data by running ?drinks in your console. I’m going to remove some rows again to make it more clear on what this function is doing to our data.\n\ndata(drinks, package = \"modeldata\")\n\n# Remove some values to better highlight the example\ndata_drinks &lt;- drinks[1:nrow(drinks) %% 2 == 0, ]\n\nset.seed(20240202)\ndrinks_split &lt;- initial_time_split(data_drinks)\ndrinks_tr &lt;- training(drinks_split)\ndrinks_te &lt;- testing(drinks_split)\n\nTo pad by month, all we do is pass “month” to the by argument of the function.\n\ndrinks_rec &lt;- recipe(~., data = drinks_tr) |&gt;\n  step_ts_pad(date, by = \"month\") |&gt;\n  prep()\n\nbake(drinks_rec, new_data = NULL)\n\n# A tibble: 229 × 2\n   date       S4248SM144NCEN\n   &lt;date&gt;              &lt;dbl&gt;\n 1 1992-02-01           3458\n 2 1992-03-01             NA\n 3 1992-04-01           4564\n 4 1992-05-01             NA\n 5 1992-06-01           4529\n 6 1992-07-01             NA\n 7 1992-08-01           4137\n 8 1992-09-01             NA\n 9 1992-10-01           4259\n10 1992-11-01             NA\n# ℹ 219 more rows\n\n\nWe can also pad with an alternative value, just like we did above. We just do the following:\n\ndrinks_rec &lt;- recipe(~., data = drinks_tr) |&gt;\n  step_ts_pad(date, by = \"month\", pad_value = mean(drinks_tr$S4248SM144NCEN)) |&gt;\n  prep()\n\nbake(drinks_rec, new_data = NULL)\n\n# A tibble: 229 × 2\n   date       S4248SM144NCEN\n   &lt;date&gt;              &lt;dbl&gt;\n 1 1992-02-01          3458 \n 2 1992-03-01          6628.\n 3 1992-04-01          4564 \n 4 1992-05-01          6628.\n 5 1992-06-01          4529 \n 6 1992-07-01          6628.\n 7 1992-08-01          4137 \n 8 1992-09-01          6628.\n 9 1992-10-01          4259 \n10 1992-11-01          6628.\n# ℹ 219 more rows\n\n\ntimetk’s step_ts_pad() function is pretty useful. If you need to pad values in a set of timeseries data, then check it out.\n\n\nDay 28 - Specify an interaction variable using step_interact()\nToday, I’m taking a step back. I forgot to cover a very important step_*() function. Interaction terms are an essential component to modeling, and so I need to highlight the use of recipes’ step_interact() function.\nrecipes’ step_interact() function specifies new columns of interaction terms between two or more variables within our recipe. Let’s apply this step function to some example data. Going back to a previous data set, let’s use credit_data from the modeldata package for our examples. I’ve used this data several times in previous posts, so I just provide the code to wrangle and create the testing and training split without much exploration or explanation.\n\ndata(\"credit_data\", package = \"modeldata\")\n\n# Remove NAs to make it easier to work with data\ncredit_data &lt;- credit_data |&gt; na.omit()\n\nset.seed(20240203)\ncredit_split &lt;- initial_split(credit_data, prop = .8)\n\ncredit_tr &lt;- training(credit_split)\ncredit_te &lt;- testing(credit_split)\n\nHow about we add a simple interaction between Expenses and Income to our recipe. We can do this by doing the following:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(terms = ~Expenses:Income) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\ncredit_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3231 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Interactions with: Expenses:Income | Trained\n\ntidy(credit_rec)\n\n# A tibble: 1 × 6\n  number operation type     trained skip  id            \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n1      1 step      interact TRUE    FALSE interact_xYaaz\n\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 × 2\n  terms           id            \n  &lt;chr&gt;           &lt;chr&gt;         \n1 Expenses:Income interact_xYaaz\n\n# Select just the interaction term, just to make it easier to see\nbake(credit_rec, new_data = NULL) |&gt;\n  select(Expenses_x_Income)\n\n# A tibble: 3,231 × 1\n   Expenses_x_Income\n               &lt;dbl&gt;\n 1              7200\n 2              4125\n 3              2835\n 4              8100\n 5              2345\n 6              7500\n 7              4500\n 8              7200\n 9              4500\n10              6525\n# ℹ 3,221 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nstep_interact()’s docs mention this function is intended for numeric data, and categorical variables should be converted to dummy variables first. This can be done using step_dummy() (see my previous day 04 post).\n\n\nYou’ll notice step_interact() modifies the interaction variable’s name by using _x_ as the separator. If for some reason you want to change this, you just pass a character string to step_interact()’s sep argument.\nThis naming convention is useful, especially if you intend to specify higher order interaction variables within your model, like a 3-way interaction.\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(terms = ~Expenses:Income:Assets) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\nbake(credit_rec, new_data = NULL) |&gt;\n  select(Expenses_x_Income_x_Assets)\n\n# A tibble: 3,231 × 1\n   Expenses_x_Income_x_Assets\n                        &lt;dbl&gt;\n 1                          0\n 2                   16500000\n 3                          0\n 4                  243000000\n 5                          0\n 6                   60000000\n 7                   15750000\n 8                   21600000\n 9                   24750000\n10                   45675000\n# ℹ 3,221 more rows\n\n\nstep_interact() also allows the use of traditional R model formula when specifying interaction variables. This is really convenient. Say we want to include all the two-way interactions along with our three way interaction, we could do something like this:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(\n    terms = ~Expenses:Income + Expenses:Assets + Income:Assets + Expenses:Income:Assets) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\nbake(credit_rec, new_data = NULL) |&gt;\n  select(starts_with(\"Expenses\"), starts_with(\"Income\"))\n\n# A tibble: 3,231 × 6\n   Expenses Expenses_x_Income Expenses_x_Assets Expenses_x_Income_x_Assets Income Income_x_Assets\n      &lt;int&gt;             &lt;dbl&gt;             &lt;dbl&gt;                      &lt;dbl&gt;  &lt;int&gt;           &lt;dbl&gt;\n 1       60              7200                 0                          0    120               0\n 2       75              4125            300000                   16500000     55          220000\n 3       35              2835                 0                          0     81               0\n 4       90              8100           2700000                  243000000     90         2700000\n 5       35              2345                 0                          0     67               0\n 6       75              7500            600000                   60000000    100          800000\n 7       45              4500            157500                   15750000    100          350000\n 8       60              7200            180000                   21600000    120          360000\n 9       60              4500            330000                   24750000     75          412500\n10       45              6525            315000                   45675000    145         1015000\n# ℹ 3,221 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdd additional interactions using the +\n\n\nHowever, a short-cut would be to do something like this:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  step_interact(\n    terms = ~(Expenses + Income + Assets)^3) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\ncredit_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3231 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Interactions with: (Expenses + Income + Assets)^3 | Trained\n\nbake(credit_rec, new_data = NULL) |&gt;\n  select(starts_with(\"Expenses\"), starts_with(\"Income\"))\n\n# A tibble: 3,231 × 6\n   Expenses Expenses_x_Income Expenses_x_Assets Expenses_x_Income_x_Assets Income Income_x_Assets\n      &lt;int&gt;             &lt;dbl&gt;             &lt;dbl&gt;                      &lt;dbl&gt;  &lt;int&gt;           &lt;dbl&gt;\n 1       60              7200                 0                          0    120               0\n 2       75              4125            300000                   16500000     55          220000\n 3       35              2835                 0                          0     81               0\n 4       90              8100           2700000                  243000000     90         2700000\n 5       35              2345                 0                          0     67               0\n 6       75              7500            600000                   60000000    100          800000\n 7       45              4500            157500                   15750000    100          350000\n 8       60              7200            180000                   21600000    120          360000\n 9       60              4500            330000                   24750000     75          412500\n10       45              6525            315000                   45675000    145         1015000\n# ℹ 3,221 more rows\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAlthough traditional model formula syntax can be used, step_interact()’s docs warn that inline functions (e.g., log) should not be used.\n\n\nIt’s also important to mention, again, categorical variables should be converted into numeric variables before being used within an interaction. Let’s say we want to specify an interaction between Debt and Home variables within our credit_data recipe. We would use step_dummy() first on Home, then specify all the interactions between the dummy variables and Debt. step_interact() makes this very easy with the use of dplyr-like selection functions.\n\ncredit_rec &lt;- recipe(~., data = credit_data) |&gt;\n  step_dummy(Home) |&gt;\n  step_interact(~starts_with(\"Home\"):Debt) |&gt;\n  prep()\n\nWarning in object$object: partial match of 'object' to 'objects'\nWarning in object$object: partial match of 'object' to 'objects'\n\ncredit_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4039 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: Home | Trained\n\n\n• Interactions with: (Home_other + Home_owner + Home_parents + Home_priv + Home_rent):Debt |\n  Trained\n\ntidy(credit_rec, number = 1)\n\n# A tibble: 5 × 3\n  terms columns id         \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      \n1 Home  other   dummy_asve2\n2 Home  owner   dummy_asve2\n3 Home  parents dummy_asve2\n4 Home  priv    dummy_asve2\n5 Home  rent    dummy_asve2\n\ntidy(credit_rec, number = 2)\n\n# A tibble: 5 × 2\n  terms             id            \n  &lt;chr&gt;             &lt;chr&gt;         \n1 Home_other:Debt   interact_hgBQM\n2 Home_owner:Debt   interact_hgBQM\n3 Home_parents:Debt interact_hgBQM\n4 Home_priv:Debt    interact_hgBQM\n5 Home_rent:Debt    interact_hgBQM\n\nbake(credit_rec, new_data = NULL)\n\n# A tibble: 4,039 × 23\n   Status Seniority  Time   Age Marital Records Job       Expenses Income Assets  Debt Amount Price\n   &lt;fct&gt;      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;\n 1 good           9    60    30 married no      freelance       73    129      0     0    800   846\n 2 good          17    60    58 widow   no      fixed           48    131      0     0   1000  1658\n 3 bad           10    36    46 married yes     freelance       90    200   3000     0   2000  2985\n 4 good           0    60    24 single  no      fixed           63    182   2500     0    900  1325\n 5 good           0    36    26 single  no      fixed           46    107      0     0    310   910\n 6 good           1    60    36 married no      fixed           75    214   3500     0    650  1645\n 7 good          29    60    44 married no      fixed           75    125  10000     0   1600  1800\n 8 good           9    12    27 single  no      fixed           35     80      0     0    200  1093\n 9 good           0    60    32 married no      freelance       90    107  15000     0   1200  1957\n10 bad            0    48    41 married no      partime         90     80      0     0   1200  1468\n# ℹ 4,029 more rows\n# ℹ 10 more variables: Home_other &lt;dbl&gt;, Home_owner &lt;dbl&gt;, Home_parents &lt;dbl&gt;, Home_priv &lt;dbl&gt;,\n#   Home_rent &lt;dbl&gt;, Home_other_x_Debt &lt;dbl&gt;, Home_owner_x_Debt &lt;dbl&gt;, Home_parents_x_Debt &lt;dbl&gt;,\n#   Home_priv_x_Debt &lt;dbl&gt;, Home_rent_x_Debt &lt;dbl&gt;\n\n\nThe tidy methods are useful here, especially if you want to know what variables will be created as a result of the recipe.\nTo sum up today’s post, step_interact() has a nice, intuitive interface. It allows for traditional interaction variable specification, while also adding some convenient functionality. A really great step_*() function.\n\n\nDay 29 - Use recipes’ check_*() functions to validate steps\nrecipes has several check_*() functions. These functions are useful for validating data returned from recipe steps. Specifically, recipes includes the following check_*() functions:\n\ncheck_class() - checks variable classes\ncheck_cols() - checks if all columns are present\ncheck_missing() - checks for any missing values\ncheck_new_values() - checks for the presence of any new values\ncheck_range() - checks for whether a variable’s range changes\n\nToday, I’m going to highlight a couple of these check_* functions: check_class(), check_cols(), and check_missing(). Let’s start with check_class().\nWe’ll continue to use our credit_data for some of today’s examples. Towards the end, I’ll use the penguins data for the final example.\n\ndata(\"credit_data\", package = \"modeldata\")\n\ncredit_data &lt;- credit_data |&gt; na.omit()\n\nset.seed(20240204)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8)\ncredit_tr &lt;- training(credit_split)\ncredit_te &lt;- testing(credit_split)\n\ncheck_class() is useful for verifying variables are expected classes. Two methods are used to check variable classes. First, this check function uses values provided to the class argument. If NULL, the check will then learn the classes from the prep. A variable can have multiple classes, which will be used within the check.\nHere’s how to use the training set to learn variable classes for the check:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_class(everything()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec\n\n# A tibble: 3,231 × 14\n   Status Seniority Home     Time   Age Marital   Records Job    Expenses Income Assets  Debt Amount\n   &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 good           1 other      30    26 single    no      fixed        35    110      0     0    700\n 2 good           1 parents    48    22 married   no      fixed        45    117      0     0    768\n 3 good          25 priv       48    55 married   no      fixed        60    169   4000     0   1000\n 4 good          34 owner      60    50 married   yes     fixed        60    150   9000     0   1300\n 5 good           0 parents    60    21 single    no      parti…       45    312  10000     0   1033\n 6 good          15 owner      60    35 single    yes     fixed        35    150   5000  2800   1300\n 7 good           3 other      12    30 single    no      freel…       35    150      0     0    920\n 8 good          23 other      54    38 separated no      fixed        60    178      0     0    740\n 9 bad            2 owner      60    38 married   yes     fixed        75     74   3500   500   1700\n10 good           0 parents    36    22 single    no      parti…       35    105   3000     0    750\n# ℹ 3,221 more rows\n# ℹ 1 more variable: Price &lt;int&gt;\n\n\nIndeed, nothing happens because our recipe resulted in variables to not change their class. If any unexpected changes did occur, then the bake would be broken with an error.\nWe can manually specify our variable class expectations using the class_nm argument:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_class(\n    Status, Home, Marital, Records, Job, class_nm = \"factor\"\n  ) |&gt;\n  check_class(\n    Seniority, Time, Age, Expenses, Income, \n    Assets, Debt, Amount, Price, class_nm = \"integer\"\n  ) |&gt;\n  prep(strings_as_factors = FALSE) |&gt;\n  bake(new_data = NULL)\n\ncredit_rec\n\n# A tibble: 3,231 × 14\n   Status Seniority Home     Time   Age Marital   Records Job    Expenses Income Assets  Debt Amount\n   &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 good           1 other      30    26 single    no      fixed        35    110      0     0    700\n 2 good           1 parents    48    22 married   no      fixed        45    117      0     0    768\n 3 good          25 priv       48    55 married   no      fixed        60    169   4000     0   1000\n 4 good          34 owner      60    50 married   yes     fixed        60    150   9000     0   1300\n 5 good           0 parents    60    21 single    no      parti…       45    312  10000     0   1033\n 6 good          15 owner      60    35 single    yes     fixed        35    150   5000  2800   1300\n 7 good           3 other      12    30 single    no      freel…       35    150      0     0    920\n 8 good          23 other      54    38 separated no      fixed        60    178      0     0    740\n 9 bad            2 owner      60    38 married   yes     fixed        75     74   3500   500   1700\n10 good           0 parents    36    22 single    no      parti…       35    105   3000     0    750\n# ℹ 3,221 more rows\n# ℹ 1 more variable: Price &lt;int&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you intend to have a variable with multiple classes, you can specify this with allow_additional = TRUE in the check_class() function. Check out the function’s examples section for more details (run ?check_class in your console).\n\n\nJust to highlight what happens when the check fails, let’s set the second check to expect a numeric rather than an integer class for the variable.\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_class(\n    Status, Home, Marital, Records, Job, class_nm = \"factor\"\n  ) |&gt;\n  check_class(\n    Seniority, Time, Age, Expenses, Income, \n    Assets, Debt, Amount, Price, class_nm = \"numeric\"\n  ) |&gt;\n  prep(strings_as_factors = FALSE) |&gt;\n  bake(new_data = NULL)\n\nError in `check_class()`:\nCaused by error:\n! `Seniority` should have the class &lt;numeric&gt; but has the class &lt;integer&gt;.\n\ncredit_rec\n\n# A tibble: 3,231 × 14\n   Status Seniority Home     Time   Age Marital   Records Job    Expenses Income Assets  Debt Amount\n   &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n 1 good           1 other      30    26 single    no      fixed        35    110      0     0    700\n 2 good           1 parents    48    22 married   no      fixed        45    117      0     0    768\n 3 good          25 priv       48    55 married   no      fixed        60    169   4000     0   1000\n 4 good          34 owner      60    50 married   yes     fixed        60    150   9000     0   1300\n 5 good           0 parents    60    21 single    no      parti…       45    312  10000     0   1033\n 6 good          15 owner      60    35 single    yes     fixed        35    150   5000  2800   1300\n 7 good           3 other      12    30 single    no      freel…       35    150      0     0    920\n 8 good          23 other      54    38 separated no      fixed        60    178      0     0    740\n 9 bad            2 owner      60    38 married   yes     fixed        75     74   3500   500   1700\n10 good           0 parents    36    22 single    no      parti…       35    105   3000     0    750\n# ℹ 3,221 more rows\n# ℹ 1 more variable: Price &lt;int&gt;\n\n\nAnother useful check is check_cols(). This function checks if all the columns from the training frame are also present within the new data. Here’s what this looks like:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  check_cols(everything()) |&gt;\n  step_dummy(Job) |&gt;\n  prep()\n\n# Omit Job variable in the new data to show the error\nbake(credit_rec, new_data = credit_te[, -8])\n\nError in `bake()`:\n✖ The following required columns are missing from `new_data`: `Job`.\nℹ These columns have one of the following roles, which are required at `bake()` time: `predictor`.\n\n\nYou’ll notice an error stops our bake. This is because the variable used to make our dummy variables, Job, was omitted from the new data. Indeed, this is a very useful check, especially to verify if variables needed in our new data are available when the recipe is baked.\nOkay, I’ll highlight one more useful check_* function for today, check_missing(). This function’s purpose is simple–check if variables contain any missing values. check_missing() can fail either on prep or bake. Both help catch if missing values are present in the training or new data.\nThe check_missing() documentation (run ?check_missing in your console), uses the credit_data data for its examples. So, let’s switch it up a bit here. Instead, I’ll use the penguins data for this final example. Here’s the code to get us up to the specification of the recipe.\n\ndata(\"penguins\", package = \"modeldata\")\n\nset.seed(20240204)\npenguins_split &lt;- initial_split(penguins, prop = .8)\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\n\nrecipe(~., data = penguins_tr) |&gt;\n  check_missing(everything()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nError in `check_missing()`:\nCaused by error in `bake()`:\n! The following columns contains missing values: bill_length_mm, bill_depth_mm,\n  flipper_length_mm, body_mass_g, and sex.\n\n\nYou’ll notice the above code errors because all the columns listed in the error contain a missing value(s).\n\nis.na(penguins_tr) |&gt; colSums()\n\n          species            island    bill_length_mm     bill_depth_mm flipper_length_mm \n                0                 0                 1                 1                 1 \n      body_mass_g               sex \n                1                 9 \n\n\nIf for some reason your training data doesn’t contain missing values, but your new data does, then the check will push an error during the bake.\nHere I’ll use step_naomit() to remove missing values from the training set to show how check_missing() throws an error during the bake.\n\n# No error\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_naomit(everything()) |&gt;\n  check_missing(everything()) |&gt;\n  prep()\n\n\n# Error\nbake(penguin_rec, new_data = penguins_te)\n\nError in `bake()`:\n! The following columns contains missing values: bill_length_mm, bill_depth_mm,\n  flipper_length_mm, body_mass_g, and sex.\n\n\nA pretty useful check_*() function, especially if you’re concerned data might contain missing values.\nIndeed, recipes provides some other check_*() functions. I highly suggest looking over recipes’ reference page to see the other functions that are provided.\nThat’s all for day 29. Tomorrow is day 30 of this challenge. I’m excited to wrap this post up.\n\n\nDay 30 - Use step_cut() to turn a numeric variable into a factor\nHere we are, day 30. We made it! 🎉\nstep_cut() is my focus for our final day. This step function creates factor variables from numeric variables. If you’re familiar with base::cut(), you’ll have a pretty good idea of what step_cut() does within a recipe.\nLet’s use our penguins data for our final day of examples. Here’s the code to split the data into our training and testing sets.\n\ndata(penguins, package = \"modeldata\")\n\nset.seed(20240206)\npenguins_split &lt;- initial_split(penguins, prop = .8)\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nI want to split our penguins into three categories: small, medium, and large. We’ll do this by cutting the body_mass_g column into three categories. Let’s first obtain some summary statistics to inform us on the cut values we should use.\n\nskim(penguins_tr, body_mass_g)\n\n\nData summary\n\n\nName\npenguins_tr\n\n\nNumber of rows\n275\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbody_mass_g\n2\n0.99\n4185.07\n789.06\n2850\n3550\n4000\n4750\n6300\n▅▇▅▃▁\n\n\n\n\n\nGreat, let’s use body_mass_g’s 25th and 75th percentiles as the cut offs in our recipe. You’ll notice I use step_naomit() here to remove rows with missing values. If I didn’t do this, step_cut() would error.\n\n\n\n\n\n\nNote\n\n\n\nstep_naomit()’s skip argument is set to TRUE by default. This will cause the bake step to skip this step, and NA values will still be present within our data.\n\n\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_naomit(body_mass_g) |&gt;\n  step_cut(body_mass_g, breaks = c(3550, 4750)) |&gt;\n  prep() \n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Removing rows with NA values in: body_mass_g | Trained\n\n\n• Cut numeric for: body_mass_g | Trained\n\nbake(penguins_rec, penguins_tr)\n\n# A tibble: 275 × 7\n   species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g         sex   \n   &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;fct&gt;               &lt;fct&gt; \n 1 Adelie    Biscoe              43.2          19                 197 (4.75e+03,6.3e+03]  male  \n 2 Chinstrap Dream               45.7          17.3               193 (3.55e+03,4.75e+03] female\n 3 Gentoo    Biscoe              48.4          16.3               220 (4.75e+03,6.3e+03]  male  \n 4 Adelie    Torgersen           34.6          21.1               198 (3.55e+03,4.75e+03] male  \n 5 Gentoo    Biscoe              45.2          15.8               215 (4.75e+03,6.3e+03]  male  \n 6 Gentoo    Biscoe              49.2          15.2               221 (4.75e+03,6.3e+03]  male  \n 7 Adelie    Torgersen           38.5          17.9               190 [2.85e+03,3.55e+03] female\n 8 Gentoo    Biscoe              45.1          14.5               215 (4.75e+03,6.3e+03]  female\n 9 Adelie    Torgersen           46            21.5               194 (3.55e+03,4.75e+03] male  \n10 Chinstrap Dream               46.4          17.8               191 (3.55e+03,4.75e+03] female\n# ℹ 265 more rows\n\n\nSay we wanted to keep the original variable along with the newly created factor variable. We need to first use step_mutate() to retain the original before cutting. Here’s the code to do this:\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_naomit(body_mass_g) |&gt;\n  step_mutate(body_mass_g_orig = body_mass_g) |&gt;\n  step_cut(body_mass_g, breaks = c(3550, 4750)) |&gt;\n  prep()\n\nbake(penguins_rec, penguins_tr) |&gt;\n  select(starts_with(\"body\"))\n\n# A tibble: 275 × 2\n   body_mass_g         body_mass_g_orig\n   &lt;fct&gt;                          &lt;int&gt;\n 1 (4.75e+03,6.3e+03]              4775\n 2 (3.55e+03,4.75e+03]             3600\n 3 (4.75e+03,6.3e+03]              5400\n 4 (3.55e+03,4.75e+03]             4400\n 5 (4.75e+03,6.3e+03]              5300\n 6 (4.75e+03,6.3e+03]              6300\n 7 [2.85e+03,3.55e+03]             3325\n 8 (4.75e+03,6.3e+03]              5000\n 9 (3.55e+03,4.75e+03]             4200\n10 (3.55e+03,4.75e+03]             3700\n# ℹ 265 more rows\n\n\nOne other note about step_cut(), it also has an include_outside_range argument. This argument accepts a boolean value (TRUE / FALSE), which specifies what you want to do with ranges outside of the cut values learned during the prep phase. TRUE will include values outside of the range. FALSE will exclude them and apply an NA as the value.\nI’m ending today’s post on an easy one. step_cut() is pretty straightforward, but it is very useful for creating factor variables out of numeric variables within our recipe.\nAlthough this is the end of the 30ish days, I’m going to take some time–hopefully tomorrow–to draft up a quick summary about what I’ve learned during this process. Until then, keep working on those recipes.\n\n\nDay 31 and beyond\nFor the past 30ish days (I surely wasn’t perfect and missed some days), I devoted time to learning more about tidymodels’ recipes package. Being the end of this challenge, I thought a post reflecting on what I’ve learned would be valuable. Here’s a grab bag of things I’ve learned throughout this process. Some are related to the specific focus of this post, the use of the recipes package. Others are things I learned while taking on this personal challenge.\nrecipes makes it really easy to perform most data preprocessing steps for modelling tasks. Most step_function()s I came across made intuitive sense, and when a function had various options and functionality, the interfaces made them easy to work with. Most of the time I recognized you just needed to pass a variable name to perform the intended step.\nrecipes’ summary() and tidy() methods are great for understanding what’s happening under the hood. At times, I wasn’t exactly sure what was occurring in the background with each step. However, taking the prep()ped recipe and throwing it into a summary() or tidy() function helped provide more information on what was taking place in the background.\nI certainly got some things wrong, but that’s okay. This post was intended to be a form of ‘learning out loud’. Indeed, getting things wrong was great for multiple reasons. First, it challenged me to go deeper. If I didn’t know how something worked or what concepts I needed to know to understand what each step_*() function was doing, I read further and wider. Doing this additional reading introduced me to many other topics, some I’ve been introduced to, and others I didn’t know existed. Second, it allowed me to experiment with how functions worked. As a result, allowing me to better understand how a function worked. If a recipe errored or pushed a warning, I was forced to ask: ‘Why is this failing?’, ‘What is wrong with my recipe?’, and ‘Am I specifying the arguments correctly?’. Answering these questions made me really ‘read the docs’ to understand what is happening.\nReading the docs closely also led me to identify areas I could contribute to open-source software. Indeed, I wasn’t adding features or fixing critical bugs, but I was able to provide little fixes, hopefully making the documentation more clear for the next person. Even though my contributions were small, I was elated to see my name was added to the tidyverse’s, Q4 2023 tidymodels digest blog post. Having my name highlighted has been a catalyst to find other areas to contribute.\nWriting’s tough. This especially becomes apparent when you’re forced to get something on the page every day. Some days I just wasn’t into it. Other days I didn’t know what to cover. From time-to-time, I was just busy and didn’t have the time to write. During these times, I found taking a break to be best. Taking breaks usually resulted in the reset I needed, allowing me to be even more focused the next day.\nAside from learning the above, here’s a brief list of additional things I learned that were helpful in getting me to finish writing this post (future-self, take note):\n\nHave a plan for what you’re going to write next.\nSet a timer and just write. You’ll be amazed how much you can output in 50 - 60 minutes.\nDon’t care about quality at first, you can always revise later (there are certainly revisions to be made here).\n\n\n\nWrap up\nSo there you have it. 30ish days of learning how to use tidymodels’ recipes package. I started off by discussing what a recipe is and how to specify it. I highlighted various step_*() functions that get added to recipes. Then, I covered some recipes’ extension packages I’ve found to be useful. Finally, I highlighted the use of some of the recipes’ check functions.\nThere’s certainly more to explore with this package and others in the tidymodels ecosystem. I’m excited to explore what its other packages have to offer.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {30 Day Tidymodels `Recipes` Challenge},\n  date = {2024-01-01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “30 Day Tidymodels `Recipes`\nChallenge.” January 1, 2024."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "",
    "text": "Photo by Markus Winkler"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The purpose of this blog series",
    "text": "The purpose of this blog series\nThis series of blogposts will focus on creating forecasts using Google Analytics 4 data. Specifically, this series overviews the steps and methods involved when developing forecasts of time series data. This blog series begins with a post overviewing the wrangling, visualization, and exploratory analysis involved when working with time series data. The primary focus of the exploratory analysis will be to identify interesting trends for further analysis and application within forecasting models. Then, subsequent posts will focus on developing different forecasting models. The primary goal of this series is to generate a forecast of online order completions on the Google Merchandise store."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "A quick disclaimer",
    "text": "A quick disclaimer\nAnother intention of this series is to document and organize my learning and practice of time series analysis. Although I try my best to perform and report valid and accurate analysis, I will most likely get something wrong at some point in this series. I’m not an expert in this area. However, it’s my hope that this series can be a supplement to others who may be learning and practicing time series analysis. In fact, seeing somebody (i.e., myself) do something wrong might be a valuable learning experience for someone else, even if that someone is my future self. If I got something wrong, I would greatly appreciate the feedback and will make the necessary changes.\nThroughout the series, I will document the resources I used to learn the process involved when generating forecasting models. I highly suggest using these as the primary source to learn this subject, especially if you intend to use this type of analysis in your own work. Specifically, the process and methods detailed in this series are mostly inspired by the Forecasting: Principles and Practice online textbook by Rob J Hyndman and George Athanasopoulos, and it utilizes several packages to wrangle, visualize, and forecast time series data (e.g., tsibble; fable; and feasts). I am very thankful to the authors and contributors for making these materials open-source."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Setup",
    "text": "Setup\nThe following is the setup steps needed to perform this exploratory analysis.\n\n# Libraries needed\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(fable)\nlibrary(feasts)\nlibrary(fuzzyjoin)\nlibrary(bigrquery)\nlibrary(glue)\nlibrary(GGally)\nlibrary(scales)\nbq_auth()\n\n## Replace with your Google Cloud `project ID`\nproject_id &lt;- 'your.google.project.id'\n\n\n## Configure the plot theme\ntheme_set(\n  theme_minimal() +\n    theme(\n       plot.title = element_text(size = 14, face = \"bold\"),\n       plot.subtitle = element_text(size = 12),\n       panel.grid.minor = element_blank()\n    )\n)"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The data",
    "text": "The data\nAs was used in previous posts, Google Analytics 4 data for the Google Merchandise Store are used for the examples below. Data represents website usage from 2020-11-01 to 2021-12-31. Google’s Public Datasets initiative makes this data open and available for anyone to use (as long as you have a Google account and have access to Google Cloud resources). Data are stored in Google BigQuery, a data analytics warehouse solution, and are exported using a SQL like syntax. Details on how this data were exported can be found in this GitHub repository. More about the data can be found here."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Export the data",
    "text": "Export the data\nThe first step in the process was to export all page_view events. To do this, the following SQL code was submitted to BigQuery using the bigrquery package. Keep in mind Google charges for data processing performed by BigQuery. Each Google account–at least since the writing of this post–had a free tier of usage. If you’re following along and you don’t have any current Google Cloud projects attached to your billing account, this query should be well within the free usage quota. However, terms of service may change at anytime, so this might not always be the case. Nevertheless, it is best to keep informed about the data processing pricing rates before submitting any query to BigQuery.\nselect \n    event_date,\n    user_pseudo_id,\n    event_name,\n    key,\n    value.string_value\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nCROSS JOIN UNNEST(event_params)\nwhere \n   event_name = 'page_view' and \n   _TABLE_SUFFIX between '20201101' and '20211231' and \n   key = 'page_location'\nThe query returns a data set with 1,350,428 rows and the following columns:\n\nevent_date - represents the date the website event took place.\nuser_pseudo_id - represents a unique User ID.\nevent_name - The name of the event specified by Google Analytics. In our case this will just be page_view given the filtering criteria.\nkey - represents the page_location dimension from the data. This column should only contain page_location.\nstring_value - represents the page to which the event took place. In other words, the page path a page_view event was counted.\n\nThis query returns a lot of data. Thus, the analysis’ scope needed to be narrowed to make the exploratory analysis more manageable. To do this, top-level pages were identified and data wrangling procedures were performed to reduce the data down to pages relevant to the exploratory analysis.\n\nNarrowing the analysis’ scope to relevant pages\nThe overall aim of the series is to forecast Order Completed page views. As part of this, relevant pages that could be used to improve forecasting models needed to be a part of the exploratory analysis. However, this is challenging given the sheer amount of pages being represented within the data. Some pages relevant to the analysis, others, not so much. Given the number of possible pages, a decision was made to only examine key, top-level pages. The question is, then, what pages should be considered relevant to the analysis?\n\n\nDetermining top-level pages\nThe navigation bar of the Google Merchandise Store was used to determine the top-level pages. Indeed, it’s reasonable to expect the navigation bar is designed to drive users to key areas of the site. Developers won’t waste valuable navbar real-estate for content users would consider useless and/or irrelevant (i.e., these are developers at Google, so they mostly likely have a good idea of how users use a website). With this in mind, the following pages were identified as potential candidates for further analysis.\n\nNew products\nApparel\nLifestyle\nStationery\nCollections\nShop by Brand\nSale (i.e., Clearance)\nCampus Collection\n\nThe checkout flow is another key component of any e-commerce website. Indeed, a main goal of the site is to convert visits into order completions. As such, pages related to the checkout flow might be another area of interest in the analysis. It’s challenging to piece together the checkout flow by just looking at the data. So, I purchased a few products to observe the checkout flow and track the different pages that came up. The checkout flow–at least when I made my purchase–went in this specific order:\n\nReview basket\nSign in (I wasn’t signed into my Google account)\nReview my information\nPayment info\nOrder review\nOrder completed\n\nAlthough these pages were identified as potential candidates for further analysis, it’s important to recognize the Google Merchandise store is not static, and thus the design and layout may have changed from the dates the data represents vs. when I went through the checkout flow. Regardless, these initial observations provided a starting point to help narrow the analysis’ focus."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Filtering out homepage events",
    "text": "Filtering out homepage events\nNow that the analysis’ scope had been narrowed to top-level pages, events associated with homepage views were filtered out to reduce the number of events within the data. To do this, the regex_filter variable was created using the glue() function from the glue package, which was then applied within a filter statement.\n\nregex_page_filter &lt;- glue(\n  \"(\",\n  \"^https://www.googlemerchandisestore.com/$|\",\n  \"^https://googlemerchandisestore.com/$|\",\n  \"^https://shop.googlemerchandisestore.com/$\",\n  \")\"\n  )\n\nThe variable contained multiple regex expressions, as several page paths in the data represented home page visits. Defining the variable in this way ensured the filter excluded all data associated with homepage visits.\nOnce the filter statement was set up, the str_to_lower() function from the stringr package was used to convert all the page paths to lower case. The following code chunk demonstrates how these operations were performed.\n\nga4_pagepaths &lt;- ga4_pageviews %&gt;% \n  filter(!str_detect(string_value, regex_page_filter)) %&gt;% \n  mutate(string_value = str_to_lower(string_value))\n\nThe filtering resulted in a reduced data set (i.e., ~1 million rows). Since the intent was to further narrow the analysis’ scope, additional filtering was performed. Specifically, the data were filtered to return a data set containing the top-level pages identified previously.\nAnother variable–similar to regex_filter–was created and used to filter the data further. Given the number of pages, though, a filtering join would be more appropriate (e.g., semi-join). The problem is the join operation needed to filter the data needed to be based on several regular expressions.\nA semi-join using a regular expression is not supported with dplyr’s joins, so the regex_semi_join() function from David Robinson’s {fuzzyjoin} package was used. This package provides a set of join operations based on inexact matching. A separate data set (tracked_data), containing the regular expressions was then created, imported into the session, and used within the join operation. A dplyr::left_join() was then used to include this data within a tidy dataset. The following chunk provides example code to perform these operations.\n\ntracked_pages &lt;- read_csv(\"tracked_pages.csv\")\n\ntop_pages_data &lt;- ga4_pagepaths %&gt;% \n  mutate(\n    string_value = str_remove(\n      string_value,'https://shop.googlemerchandisestore.com/')) %&gt;% \n  regex_semi_join(tracked_pages, by = c(\"string_value\" = \"page_path\")) %&gt;% \n  regex_left_join(tracked_pages, by = c(\"string_value\" = \"page_path\"))\n\nAt this point, the data is more manageable and easier to work with. At the start, the initial export contained around 1.6 million rows. By narrowing the focus of the analysis and performing several data wrangling steps to filter the data, the final tidy data set contained around 320,000 rows.\nGiven the limited amount of storage available and how this post is hosted makes authentication into BigQuery challenging, I opted to not integrate the extraction steps into the rendering steps and to exclude the full data with this post. However, I included the filtered data set in a .rds file to conserve space. I imported this file by running the following code chunk to continue the exploratory analysis. I would skip this step and just directly export the data from BigQuery if this analysis was performed outside the forum of a blog post.\n\ntop_pages_data &lt;- readRDS(\"top_pages_data.rds\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Data exploration",
    "text": "Data exploration\nWith data in a tidy format, the exploratory analysis and further identification of relevant series for forecasts of Order Completed page views can take place. One area of possible exploration is to identify which pages generate a significant amount of traffic. Indeed, it’s possible that pages with a lot of traffic might also result in more order completions: more traffic indicates more interest; more interest could mean more orders.\nA few questions to answer:\n\nWhich top-level pages have the most unique users?\nWhat pages get the most traffic (i.e., page views)?\n\nA simple bar plot is created to answer these questions. Here’s the code to create these plots, using the ggplot2 package.\n\npage_summary &lt;- top_pages_data %&gt;% \n  group_by(page) %&gt;% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n()\n  ) %&gt;% \n  arrange(desc(unique_users))\n\n\nggplot(page_summary, aes(x = unique_users, y = reorder(page, unique_users))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by users\",\n       subtitle = \"Apparel page viewed by a significant amount of users\",\n       y = \"\",\n       x = \"Users\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       )\n\n\n\n\n\n\n\n\n\nggplot(page_summary, aes(x = views, y = reorder(page, views))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by views\",\n       subtitle = \"Apparel and basket pages generate significant amount of views\",\n       y = \"\", \n       x = \"Views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\nApparel clearly seems to not only have received a significant amount of users, but a high number of page views as well. It’s also interesting to note that the basket had nearly half the amount of users compared to apparel, but the amount of page views was similar. It’s also apparent, at least with the data available, that more users browsed clearance then they did new items during this period. Just looking at the current summary for the period, apparel might be a potential time series to include within forecasting models of order completions.\nAlthough the apparel page is a likely candidate for the forecasting models, supplemental data should be examined to justify its inclusion. For instance, actual purchase/financial data could provide further justification of the business case and value of focusing on this specific area within future analyses. For instance, apparel may drive a lot of traffic, but it may not be an area where much revenue or profit is generated. Thus, the focus on more money generating/profitable products may be better candidates to improve the accuracy of our forecasting models. Despite this, actual purchase and financial data are not available. As a result, this is not explored further."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Create time series data visualizations",
    "text": "Create time series data visualizations\nVisualizing the time series is the next step in the exploratory analysis. This step will be further helpful in identifying potential time series that may be of value in creating a forecast of Order Completed page views.\n\nConvert the tibble into a tsibble\nThe top_pages_data tibble is now converted to an object that contains temporal structure. To do this, the as_tsibble() function from the {tsibble} package is used. This package provides a set of tools to create and wrangle tidy temporal data. Before the temporal structure could be mapped to the data set, a few wrangling steps were performed: 1). the event_date column was converted into a date variable; and 2). data were aggregated to count the number of unique_users and views. The following code chunk contains an example of these steps.\n\npages_of_interest &lt;- c(\"Apparel\", \n                       \"Campus Collection\", \n                       \"Clearance\", \n                       \"Lifestyle\", \n                       \"New\", \n                       \"Order Completed\", \n                       \"Shop by Brand\")\n\ntidy_trend_data &lt;- top_pages_data %&gt;% \n  mutate(event_date = parse_date(event_date, \"%Y%m%d\")) %&gt;% \n  group_by(event_date, page) %&gt;% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n(), \n    .groups = \"drop\"\n  ) %&gt;% \n  as_tsibble(index = event_date, key = c(\"page\")) %&gt;% \n  filter(page %in% pages_of_interest)\n\nAt this point, several trend plots could be created using the ggplot2 package. However, the feasts package provides a convenient wrapper function to quickly make trend visualizations of tsibble objects, autoplot(). The outputted plot was then formatted to improve readability.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of page views\", \n       subtitle = \"Apparel drove a significant amount of views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nggplot2’s facet_wrap() function was used to create a plot for each series. Splitting the plots into separate entities allowed for a clearer view of the characteristics within each series.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(.~page, scales = \"free_y\")  +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plots of page views\", \n       subtitle = \"Various characteristics are present within each series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Identify notable features using time plots",
    "text": "Identify notable features using time plots\nApparel again emerges as a potential series to include within the forecasting models, as this page generates a significant amount of traffic. Despite the sheer amount of traffic to the apparel page, though, other time series peak interest. Specifically, the Campus Collection, Clearance, Lifestyle, and New pages all have some interesting characteristics that could be used to improve forecasting models. The following plots contain the isolated trends. A description of the characteristics within each trend is provided.\n\nApparel page’s characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Apparel\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Apparel page views\", \n       subtitle = \"Series exhibits positive trend; slight cyclic patterns; no seasonal patterns present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\n\nA clear positive trend.\nThe series contains some cyclic elements and very little indication of a seasonal pattern. However, with a greater amount of points, a seasonal pattern might be revealed (e.g., holiday season shopping).\n\n\n\nCampus Collection page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Campus Collection\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Campus Collection page views\", \n       subtitle = \"Cyclic behavior present within the series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\n\nA slight positive trend is present up until the middle of the series. Towards the middle of the series, no real trend is present.\nGiven the variation is not of a fixed frequency, this series exhibits some cyclical behavior.\n\n\n\nClearance page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Clearance\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Clearance page views\", \n       subtitle = \"Slight trend components are present; weekly seasonality is also present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\n\nA slight upward trend towards the middle of the series, followed by a steep downward trend, and then a slight upward trend towards the end of the series is present.\nThe series also has a clear seasonal pattern, which seems to be weekly in nature. Perhaps products are moved to clearance on a weekly basis.\n\n\n\nLifestyle page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Lifestyle\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Lifestyle page views\", \n       subtitle = \"Trend not clear in this series; some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\n\nTrend is not clear in this series.\nThere is some strong cyclic behavior being exhibited with limited seasonality with the time frame available.\n\n\n\nNew page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"New\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of New page views\", \n       subtitle = \"No trend present; some some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\n\nNo real trend is present.\nStrong cyclic behavior is present within the series. Some seasonality is present. Indeed, this is similar to the Clearance series, as the seasonality seems to be weekly. Perhaps new products are released each week.\n\n\n\nShop by brand characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Shop by Brand\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Shop by Brand page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\n\nThe trend here seems to be positive from the start, then declines sharply, and then exhibits a slight positive trend towards the end of the series.\nThere also seems to be some slight cyclicity with very little seasonality.\n\n\n\nOrder completed characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Order Completed\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Order Completed page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\n\n\n\n\nThe trend from the start seems to be positive, up until the middle of the series. From there, a steep decline is present. Towards the end of the series, there is a subtle positive trend.\nTowards the beginning of the series, there seems to be some strong cyclicity. Towards the end of the series, there seems to be more of a seasonal pattern within the data. This cyclicity may be due to the time of year which this data represents, the holiday season.\n\nSince this analysis is focused on creating a forecast for order completions, additional work needed to be done to identify potential series that may improve the forecasts. To do this, several scatter plots were created to help identify variables that relate to order completions.\nBefore additional exploratory plots can be created, though, additional data wrangling steps needed to be taken. Specifically, the data was transformed from a long format to a wide format, where the page variable is turned into several numeric columns within the transformed data set. The following code chunk was used to perform this task.\n\ntidy_trend_wide &lt;- tidy_trend_data %&gt;% \n  select(-unique_users) %&gt;% \n  mutate(page = str_replace(str_to_lower(page), \" \", \"_\")) %&gt;% \n  pivot_wider(names_from = page, \n              values_from = views, \n              names_glue = \"{page}_views\")\n\nWith data in a wide format, the ggpairs() function from the GGally package was used to create a matrix of scatterplots and correlation estimates for the various series within the dataset.\nHere is the code to perform this analysis and output the matrix of plots.\n\ntidy_trend_wide %&gt;% \n  ggpairs(columns = 2:8)\n\n\n\n\n\n\n\n\nThe scatterplots and correlations output revealed some interesting relationships. For one, although previous exploratory analysis revealed apparel generated high volumes of views, the correlation analysis revealed a slight negative relationship with order completions. However, five variables seem to be highly correlated with order completions: Clearance (.877), Campus Collection (.846), Lifestyle (.769), New (.753), and Shop by Brand (.659). Evidence points to these series as being potentially valuable components of a forecasting model of Order Completed page views."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Narrow the focus further",
    "text": "Narrow the focus further\nAt this point, this post transitions into examining just the Order Completed page views, as this is the time series intended to be forecasted within future analyses done in this series of blog posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Lag plots",
    "text": "Lag plots\nIt’s time to shift focus onto exploring the characteristics of the outcome variable of future forecasting models, order completions, in more depth. The next step, then, is to examine for any lagged relationships present within the Order Completed page views time series.\nThe gg_lag function from the feats package makes it easy to produce the lag plots. Here the tidy_trend_wide data will be used.\n\ntidy_trend_wide %&gt;% \n  gg_lag(order_completed_views, geom = \"point\")\n\n\n\n\n\n\n\n\nThe plots provide little evidence that any lagged relationships–positive or negative–are present within this time series. Thus, no further steps were taken to account for lagged relationships at this point in the analysis."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nExploring for autocorrelation is the next step. A correlogram is created to explore for this characteristic within the series. A correlogram “measures the linear relationship between lagged values of a time series” (Hyndman and Athanasopoulos 2021). The ACF is first calculated for each value within the series. This value is then plotted according to it’s corresponding lag values. The ACF() function from the feasts package was used to calculate these values. The resulting data object is then passed along to the autoplot() function, which creates the correlogram for the data. Here is what the code looks like, along with the outputted plot.\n\ntidy_trend_wide %&gt;% \n  ACF(order_completed_views, lag_max = 28) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Inspect the correlogram",
    "text": "Inspect the correlogram\nThe correlogram clearly shows the data is not a white noise series. Moreover, the plot reveals several structural characteristics within the time series.\n\nThe correlogram, given the smaller lags are large, positive, and seem to decrease with each subsequent lag, which suggests the series contains some type of trend.\nThe plot also reveals a slight scalloped shape (i.e., peaks and valleys at specific intervals), which suggests some seasonality occurring within the process. Indeed, it seems peaks occur every seven days (e.g., lags 7 and 14). Thus, a slight weekly seasonality may be present within the time series.\n\nGiven these structural characteristics of the series, future forecasting steps will need to account for these issues. This topic will be further discussed in future posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Trend Decomposition",
    "text": "Trend Decomposition\nThe final exploratory analysis step is to split the series into its several components. This includes the seasonal, trend, and remainder components. Here an additive decomposition is performed. Transformations were not applied to this data before decomposition was performed.\nAn argument could be made to transform this time series using some mathematical operation. Indeed, transforming the series may improve forecasts generated from the data (Hyndman and Athanasopoulos 2021). However, this analysis doesn’t have access to a complete series of data. Having more data could lead to more informed decisions on the appropriate application of transformations. A full year or multiple years worth of data would be preferred. Interpretability is also a concern, as transformations would need to be converted back to the original scale once the forecast was created. Thus, it was decided that transformations were not going to be applied to the data. More about transforming the series can be referenced here.\nThe series was broken down into its multiple components: seasonal, trend-cycle, and remainder (Hyndman and Athanasopoulos 2021). Decomposing the series allows for more to be learned about the underlying structure of the time series. As a result, allowing for structural components of the time series that could improve forecasting models to be identified. Several functions from the {feasts} and {fabletools} packages simplified the decomposition process.\nFirst, the trend components are calculated using the STL() and model() functions. STL() decomposes the trend. The model() function creates a mabel object of estimates. The components() function is then used to view the model object.\n\norder_views_dcmp &lt;- tidy_trend_wide %&gt;% \n  model(stl = STL(order_completed_views))\n\ncomponents(order_views_dcmp)\n\nWarning in as_dable.tbl_ts(object, method = attrs[[\"method\"]], resp = !!attrs[[\"response\"]], :\npartial argument match of 'resp' to 'response'\n\n\n# A dable: 92 x 7 [1D]\n# Key:     .model [1]\n# :        order_completed_views = trend + season_week + remainder\n   .model event_date order_completed_views trend season_week remainder season_adjust\n   &lt;chr&gt;  &lt;date&gt;                     &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 stl    2020-11-01                    14  47.8      -37.6      3.80           51.6\n 2 stl    2020-11-02                    73  47.9       12.9     12.2            60.1\n 3 stl    2020-11-03                    69  47.9       23.1     -1.99           45.9\n 4 stl    2020-11-04                    47  48.2       11.8    -13.0            35.2\n 5 stl    2020-11-05                    28  48.5       -4.93   -15.5            32.9\n 6 stl    2020-11-06                    62  48.7       13.0      0.285          49.0\n 7 stl    2020-11-07                    34  48.9      -17.9      3.00           51.9\n 8 stl    2020-11-08                    32  51.7      -37.9     18.2            69.9\n 9 stl    2020-11-09                    58  54.5       12.4     -8.95           45.6\n10 stl    2020-11-10                    70  56.6       22.7     -9.33           47.3\n# ℹ 82 more rows\n\n\nPlotting the decomposition is done by piping the output from the components() function to autoplot(). The visualization will contain the original trend, the trend component, the seasonal component, and the remainder of the series after the trend and seasonal components are removed.\n\ncomponents(order_views_dcmp) %&gt;% \n  autoplot() +\n  labs(x = \"Event Date\")\n\nWarning in as_dable.tbl_ts(object, method = attrs[[\"method\"]], resp = !!attrs[[\"response\"]], :\npartial argument match of 'resp' to 'response'\n\n\n\n\n\n\n\n\n\nScanning the components outputted by the decomposition, a few conclusions were drawn. Looking at the trend component, it seems a steady upward trend takes place from the start to the middle of the series. Then, a sharp negative trend followed by a slight increase towards the tail end of the series is present. Indeed, this might be additional seasonality that might become more apparent if additional data were available.\nThe seasonal component is also interesting here, as some type of cyclic weekly pattern seems to be present. This includes less traffic around the beginning of the week and weekends, where the majority of this cyclic pattern occurs during the week. It’s also interesting to note a consistent drop occured on most Thursdays of the week."
  },
  {
    "objectID": "blog/posts/2024-02-23-tidytuesday-2024-02-20-r-consortium-grants/index.html",
    "href": "blog/posts/2024-02-23-tidytuesday-2024-02-20-r-consortium-grants/index.html",
    "title": "Exploring R Consortium ISC Grants",
    "section": "",
    "text": "Photo by Markus Winkler\n\n\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(skimr)\nlibrary(tidytext)\nlibrary(here)\nlibrary(scales)\n\n\nBackground\nI’ve never really contributed to tidytuesday. Recently, I’ve been trying to spark some inspiration, so I thought contributing to this social data project would be a good start. I used this post as an opportunity to get more comfortble using plotly and Tableau for creating data visualizations.\n\ndata_isc_grants &lt;- \n  read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-20/isc_grants.csv')\n\n\n\nData description\nThe data represents information about past projects funded by the R Consortium Infrastructure Committee (ISC) Grant Program. The purpose of these grants is to support projects contributing to the R community. Learn more about the most recent round of funding by checking out their blog post announcing this round of grants.\nThe data includes columns like: year, group (i.e., funding cycle), title, funded (i.e., funding amount), and summary. Before creating some data visualizations, let’s do some quick exploratory analysis.\n\nglimpse(data_isc_grants)\n\nRows: 85\nColumns: 7\n$ year        &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, …\n$ group       &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ title       &lt;chr&gt; \"The future of DBI (extension 1)\", \"Secure TLS Communications for R\", \"volcalc…\n$ funded      &lt;dbl&gt; 10000, 10000, 12265, 3000, 15750, 8000, 8000, 22000, 6000, 25000, 15000, 20000…\n$ proposed_by &lt;chr&gt; \"Kirill Müller\", \"Charlie Gao\", \"Kristina Riemer\", \"Mark Padgham\", \"Jon Harmon…\n$ summary     &lt;chr&gt; \"This proposal mostly focuses on the maintenance and support for {DBI}, the {D…\n$ website     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nskim(data_isc_grants)\n\n\nData summary\n\n\nName\ndata_isc_grants\n\n\nNumber of rows\n85\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntitle\n0\n1.00\n4\n120\n0\n85\n0\n\n\nproposed_by\n0\n1.00\n8\n63\n0\n66\n0\n\n\nsummary\n0\n1.00\n31\n2210\n0\n85\n0\n\n\nwebsite\n33\n0.61\n21\n224\n0\n48\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2019.14\n2.08\n2016\n2017\n2019\n2021\n2023\n▇▆▇▅▅\n\n\ngroup\n0\n1\n1.40\n0.49\n1\n1\n1\n2\n2\n▇▁▁▁▅\n\n\nfunded\n0\n1\n13781.14\n11325.80\n0\n6000\n10000\n16000\n62400\n▇▂▁▁▁\n\n\n\n\n\n\n\nWhat’s the trend for grant funding?\nLet’s take a look at the funding trend by funding cycle (i.e., fall and spring).\n\n\nCode\ndata_by_year_grp &lt;- data_isc_grants |&gt;\n  mutate(group = case_when(\n    group == 1 ~ \"Spring\", \n    group == 2 ~ \"Fall\")\n  ) |&gt;\n  group_by(year, group) |&gt;\n  summarise(funded = sum(funded), .groups = \"drop\") |&gt;\n  arrange(group, year) |&gt; \n  pivot_wider(names_from = group, values_from = funded)\n\n\n\n\nCode\nplot_ly(\n  data_by_year_grp, \n  x = ~year, \n  y = ~Fall, \n  name = \"Fall\", \n  type = 'scatter', \n  mode = 'lines',\n  line = list(width = 5),\n  text = ~paste(\n    \"Funding awarded: $\", comma(Fall),\n    \"&lt;br&gt;Year: \", year\n  ),\n  hoverinfo = \"text\"\n) |&gt;\nadd_trace(\n  y = ~Spring,\n  name = \"Spring\",\n  text = ~paste(\n    \"Funding awarded: $\", comma(Spring),\n    \"&lt;br&gt;Year: \", year\n  ),\n  hoverinfo = \"text\"\n) |&gt;\nlayout(\n  title = list(\n    text = \"&lt;b&gt;Funding trend for R Consortium ISC grants by funding round&lt;/b&gt;\",\n    xanchor = \"center\",\n    yanchor = \"top\",\n    font = list(family = \"arial\", size = 24)\n  ),\n  xaxis = list(title = \"\"),\n  yaxis = list(title = \"Funding amount ($US)\")\n)\n\n\n\n\n\n\n\n\nWhat words are used most often within descriptions of funded projects?\nNow, let’s explore the words used within descriptions most often in awarded grant applications.\n\n\nCode\ndata_word_fund_trend &lt;- data_isc_grants |&gt;\n  mutate(\n    summary = str_remove_all(str_to_lower(summary), \"[[:punct:]]\"),\n    summary = str_remove_all(summary, \"[0-9]\"),\n  ) |&gt;\n  unnest_tokens(word, summary) |&gt;\n  anti_join(get_stopwords()) |&gt;\n  group_by(year) |&gt;\n  count(word) |&gt;\n  arrange(word, year) |&gt;\n  group_by(word) |&gt;\n  mutate(\n    n_cume = cumsum(n)\n  )\n\n\n\n\nCode\ntop_words &lt;- data_word_fund_trend |&gt;\n  ungroup() |&gt;\n  summarise(top = quantile(n_cume, .99)) |&gt;\n  pull(top)\n\ndata_top_words &lt;- data_word_fund_trend |&gt;\n  filter(n_cume &gt;= top_words) |&gt;\n  distinct(word)\n\nplot_ly(\n  data = data_word_fund_trend, \n  x = ~year,\n  y = ~n_cume,\n  mode = \"lines\",\n  line = list(color = \"#d3d3d3\", width = 3),\n  type = \"scatter\",\n  mode = \"lines\",\n  name = \"\",\n  text = ~paste(\n    \"Word: \", word,\n    \"&lt;br&gt;Cumulative mentions: \", n_cume,\n    \"&lt;br&gt;Year: \", year\n  ),\n  hoverinfo = \"text\"\n) |&gt;\nadd_lines(\n  data = data_word_fund_trend |&gt; semi_join(data_top_words),\n  x = ~year,\n  y = ~n_cume,\n  line = list(color = \"#0C2D48\", width = 3),\n  type = \"scatter\",\n  mode = \"lines\",\n  name = \"\"\n) |&gt;\nlayout(\n  title = list(\n    text = \"&lt;b&gt;Aiming for RConsortium grant funding? Consider using these words&lt;/b&gt;\",\n    xanchor = \"center\",\n    yanchor = \"top\",\n    font = list(family = \"arial\", size = 24)\n  ),\n  xaxis = list(title = \"\"),\n  yaxis = list(title = \"Cumulative mentions\"),\n  showlegend = FALSE\n)\n\n\n\n\n\n\n\n\nAn attempt using Tableau\nTo learn more about using Tableau, I took this week’s data as an opportunity to learn more. Here’s what I came up with.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring {R} {Consortium} {ISC} {Grants}},\n  date = {2024-02-26},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Exploring R Consortium ISC Grants.”\nFebruary 26, 2024."
  },
  {
    "objectID": "blog/posts/2024-09-14-google-analytics-machine-learning-kmeans-clustering/index.html",
    "href": "blog/posts/2024-09-14-google-analytics-machine-learning-kmeans-clustering/index.html",
    "title": "Messing with models: k-means clustering of Google Analytics 4 data",
    "section": "",
    "text": "Photo by Daniel Fazio\n\n\nMarketing campaigns target specific audiences. Some form of customer segmentation is performed to identify these audiences. These segments can be identified using several approaches. This post describes one such approach: the use of data and machine learning to specify clustering models present within Google Analytics data for an e-commerce store.\nThe goal is simple: create customer segments a marketing team could use to craft and target specific marketing campaigns. To achieve this goal, this post overviews the use of a straightforward, useful machine learning algorithm called k-means clustering. Packages and functions from R, a statistical programming language, are used for the segmentation process.\n\nlibrary(tidyverse)\nlibrary(bigrquery)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(factoextra)\nlibrary(ids)\nlibrary(here)\nlibrary(psych)\nlibrary(gt)\n\n\nggplot_theme &lt;- \n  theme_bw() +\n  theme(\n    text = element_text(size = 12),\n    title = element_text(size = 16, face = \"bold\")\n  )\n# Override the default ggplot2 theme\ntheme_set(ggplot_theme)\n\n\nExtract and explore data\nFirst, we need some data. This section briefly discusses the data extraction process. I’ve detailed the extraction of Google Analytics data in a previous post. For the sake of time, then, this topic won’t be discussed in-depth. If you’re already familiar with how to access this data, feel free to skip this section.\nThe data used here is obfuscated Google Analytics data for the Google Merchandise Store. This data is stored and accessed via BigQuery, a cloud-based data warehouse useful for analytics purposes.\n\n\n\n\n\n\nWarning\n\n\n\nThis data is useful for example tutorials, so the conclusions drawn here should not be used to infer anything about true purchasing behavior. The purpose of this post is to be a tutorial on how to perform k-means clustering, rather than about deriving true conclusions about Google Merchandise store customers.\n\n\nBefore data extraction, let’s do some exploration of the source data. The focus here is to get a sense of what’s in the data, while also creating an understanding of the source data’s structure. This mostly involves identifying the available fields and data types.\nThe bigrquery package provides the bq_tables_fields() function to retrieve this information. The following code example shows how to use this function to return field names within the dataset’s tables:\n\ntable_ecommerce &lt;-\n  bq_table(\n    \"bigquery-public-data\",\n    \"ga4_obfuscated_sample_ecommerce\",\n    \"events_20210101\"\n  )\n\nbq_table_fields(table_ecommerce)\n\nThen, the following code submits a SQL query to return the ga_obfuscated_sample_ecommerce data from BigQuery. It’s important to note, similar to what was done in past posts, I’m querying data associated with transactions occurring over the U.S. Christmas holiday season. Here’s the query string if you want specific details:\n\nquery &lt;- \"\n  select \n    event_date,\n    user_pseudo_id,\n    ecommerce.transaction_id,\n    items.item_category,\n    items.quantity,\n    ecommerce.purchase_revenue_in_usd\n  from `&lt;your-project-name&gt;.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(items) as items\n  where _table_suffix between '20201101' and '20201231' and \n  event_name = 'purchase'\n  order by user_pseudo_id, transaction_id\n\"\n\n\ndata_ga_transactions &lt;- bq_project_query(\n  \"&lt;your-project-name&gt;\",\n  query\n) |&gt;\nbq_table_download()\n\nBefore moving forward, some data validation is in order. Columns containing missing values are the biggest concern.\n\n# Verify if there are any missing values\nmap(data_ga_transactions, \\(x) any(is.na(x)))\n\n$event_date\n[1] FALSE\n\n$user_pseudo_id\n[1] FALSE\n\n$transaction_id\n[1] TRUE\n\n$item_category\n[1] FALSE\n\n$quantity\n[1] TRUE\n\n$purchase_revenue_in_usd\n[1] FALSE\n\n\nNo surprise: some features contain missing values. transaction_id and quantity both contain missing values. Our data wrangling step will need to address these issues. Let’s look closer and explore why missing values might be present within the data. This code can be used to do this:\n\n# Examples with a missing `transaction_id`\ndata_ga_transactions |&gt; filter(is.na(transaction_id))\n\n# A tibble: 59 × 6\n   event_date user_pseudo_id transaction_id item_category quantity purchase_revenue_in_usd\n        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;                   &lt;dbl&gt;\n 1   20201101       1494019. &lt;NA&gt;           New                  1                      25\n 2   20201101       2422026. &lt;NA&gt;           New                  2                      72\n 3   20201101       2422026. &lt;NA&gt;           Fun                  1                      72\n 4   20201101      29640693. &lt;NA&gt;           Apparel              1                      55\n 5   20201101      29640693. &lt;NA&gt;           Shop by Brand        2                      59\n 6   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 7   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 8   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n 9   20201101       3297047. &lt;NA&gt;           Apparel              1                      87\n10   20201101      33027284. &lt;NA&gt;           Accessories          1                      63\n# ℹ 49 more rows\n\n# Examples where a `quantity` is missing\ndata_ga_transactions |&gt; filter(is.na(quantity))\n\n# A tibble: 148 × 6\n   event_date user_pseudo_id transaction_id item_category quantity purchase_revenue_in_usd\n        &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;            &lt;dbl&gt;                   &lt;dbl&gt;\n 1   20201217       1086663. (not set)      (not set)           NA                       0\n 2   20201120      11080045. (not set)      (not set)           NA                       0\n 3   20201204      12422651. (not set)      (not set)           NA                       0\n 4   20201213      13309292. (not set)      (not set)           NA                       0\n 5   20201126      13429550. (not set)      (not set)           NA                       0\n 6   20201213       1396855. (not set)      (not set)           NA                       0\n 7   20201127       1423291. (not set)      (not set)           NA                       0\n 8   20201210    1540308157. (not set)      (not set)           NA                       0\n 9   20201201      15915163. (not set)      (not set)           NA                       0\n10   20201120      15980073. (not set)      (not set)           NA                       0\n# ℹ 138 more rows\n\n\nFor the transaction_id column, 59 rows contain missing values. If you dig a little further, you’ll notice the majority of these missing transaction_ids occur near the beginning of data collection (i.e., 2020-11-01). This was likely a measurement issue with the Google Analytics setup, which would warrant further exploration. Since access to information about how Google Analytics was set up for the Google Merchandise Store (i.e., I can’t speak with the developers), this issue can’t be further explored. The only option, then, is to simply drop these rows.\nMissing quantity values seem to be associated with examples where transaction_id and item_category both contain a (not set) character string. Again, this could be a measurement issue worth further exploration. Given the available information, these examples will also be dropped.\nAt this point, dplyr’s glimpse() and skimr’s skim() functions are used for some additional exploratory analysis. glimpse() is great to get info about the data’s structure. skim() provides summary statistics for each variable within the dataset, regardless of type.\n\nglimpse(data_ga_transactions)\n\nRows: 13,113\nColumns: 6\n$ event_date              &lt;dbl&gt; 20201210, 20201210, 20201210, 20201103, 20201103, 20201103, 202011…\n$ user_pseudo_id          &lt;dbl&gt; 10111056, 10111056, 10111056, 1014825, 1014825, 1014825, 1014825, …\n$ transaction_id          &lt;chr&gt; \"741471\", \"741471\", \"741471\", \"(not set)\", \"(not set)\", \"(not set)…\n$ item_category           &lt;chr&gt; \"Apparel\", \"Apparel\", \"Apparel\", \"Shop by Brand\", \"Office\", \"Shop …\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ purchase_revenue_in_usd &lt;dbl&gt; 94, 94, 94, 183, 183, 183, 183, 183, 183, 86, 86, 86, 86, 86, 86, …\n\n\n\nskim(data_ga_transactions)\n\n\nData summary\n\n\nName\ndata_ga_transactions\n\n\nNumber of rows\n13113\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntransaction_id\n59\n1\n2\n9\n0\n3563\n0\n\n\nitem_category\n0\n1\n0\n23\n189\n22\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nevent_date\n0\n1.00\n20201167.86\n4.740000e+01\n20201101\n20201121\n20201202\n20201212\n20201231\n▇▁▁▂▇\n\n\nuser_pseudo_id\n0\n1.00\n289012023.38\n1.274012e+09\n1014825\n5706716\n25060616\n62921408\n9202815833\n▇▁▁▁▁\n\n\nquantity\n148\n0.99\n1.49\n2.910000e+00\n1\n1\n1\n1\n160\n▇▁▁▁▁\n\n\npurchase_revenue_in_usd\n0\n1.00\n106.68\n1.171100e+02\n0\n40\n71\n137\n1530\n▇▁▁▁▁\n\n\n\n\n\nThe data contains a total of 13,113 rows with six features. After reviewing the data types and comparing some of the example values outputted from glimpse(), it’s concerning that some variables are of type character rather than type numeric. Values of type integer would be best in this case. Let’s do some more digging.\n\n# Only printing the first 10 values for brevity\nunique(data_ga_transactions$transaction_id) |&gt;\n  head(n = 10)\n\n [1] \"741471\"    \"(not set)\" \"983645\"    \"406646\"    \"2105\"      \"886501\"    \"76937\"     \"29460\"    \n [9] \"339943\"    \"614046\"   \n\n\nAs expected, some transaction_ids are of type character, which is keeping the column from being a true numeric variable. This is due to some rows containing the (not set) character string. Since the transaction_id is critical for our analysis, these rows will need to be filtered out during data wrangling.\nInitial data exploration is complete. We now have enough information to begin data wrangling. The wrangling step will strive to format the data into a structure necessary for performing k-means clustering. The following code chunk contains the code to do this. After, there’s a step-by-step explanation of what this code is doing.\n\ndata_user_items &lt;- \n  data_ga_transactions |&gt;\n  filter(transaction_id != \"(not set)\") |&gt;\n  drop_na() |&gt;\n  mutate(\n    user_id = random_id(), \n    .by = user_pseudo_id, \n    .after = user_pseudo_id\n  ) |&gt;\n  select(-user_pseudo_id) |&gt;\n  summarise(\n    quantity = sum(quantity), \n    .by = c(user_id, item_category)\n  ) |&gt;\n  mutate(\n    item_category = case_when(\n      item_category == \"\" ~ \"Unknown\",\n      TRUE ~ as.character(item_category)\n    )\n  ) |&gt;\n  pivot_wider(\n    names_from = item_category, \n    values_from = quantity,\n    values_fill = 0\n  ) |&gt;\n  clean_names()\n\nIn the code above, the (not set) issue in the transaction_id feature is addressed first. Any row with the (not set) value is simply filtered from the data. drop_na() follows. This function drops any rows that contain a NA value. As a result of dropping NAs, we’re left with 11,615 rows: a loss of 1,498 examples (~11.4%).\n\n\n\n\n\n\nNote\n\n\n\nMerely dropping examples is convenient in this case, but it may not be ideal or even valid in every context. You’ll want to explore what is appropriate for the data you’re working with before applying this strategy.\n\n\nSome mutating and summarization of the data is next. The mutation step applies a random id in place of the user_pseudo_id. This provides an extra layer of privacy for users, since the data is being used outside of its original source system.\n\n\n\n\n\n\nNote\n\n\n\nThis may be a little inordinate, but I argue it’s important. It’s best to do what we can to create another layer of privacy to the information we’re using outside of the original source system. Since this data is public, it’s not too much of a concern here. However, this may be an important consideration when you’re working with ‘real world’ Google Analytics data.\nI’m not a privacy expert, so you’ll want to identify best practice for the context you work in.\n\n\nWith the modelling goal top-of-mind, aggregation will sum values to individual users rather than transactions. This way, we’ll be able to use k-means clustering to identify customer cohorts, rather than transaction cohorts. To do this, the data is grouped by user_id and item_category and the quantity feature is summed.\nPost aggregation, item_categorys that don’t contain any values need to be addressed. To address this, any missing string is replaced with ‘Unknown’ using the case_when() function. Finally, tidyr’s pivot_wider() is used to transform the data from long format to wide format. When taken together, the transformation results in a data set where each feature, other than user_id, is a sum of the item categories purchased by each user during the period. Each example, then, could comprise of multiple transactions that occur over the period.\nPost wrangling, readr’s write_csv() function is used to save the data to disk. This way a request isn’t sent to BigQuery every time the post is built. You don’t have to do this, but it’s useful for limiting query costs associated with the service, though it’s pretty economical to query data in this way.\n\nwrite_csv(data_user_items, \"data_user_items.csv\")\n\n\ndata_user_items &lt;- read_csv(\"data_user_items.csv\")\n\nRows: 2941 Columns: 22\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): user_id\ndbl (21): apparel, bags, shop_by_brand, drinkware, new, clearance, accessories, campus_collectio...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData exploration for modeling\nBefore modeling, let’s do some data exploration. The goal during exploration is to ensure data meets the assumptions of k-means clustering. First, let’s get a general sense of the structure of our wrangled data. dplyr’s glimpse() is once again useful here.\n\nglimpse(data_user_items)\n\nRows: 2,941\nColumns: 22\n$ user_id                 &lt;chr&gt; \"vqnzdyytdx\", \"lhkctjeylp\", \"fkgfswdpur\", \"bycrrxogpw\", \"xdecczvgb…\n$ apparel                 &lt;dbl&gt; 3, 0, 3, 0, 0, 0, 4, 15, 1, 1, 0, 1, 2, 0, 2, 3, 0, 0, 0, 0, 1, 6,…\n$ bags                    &lt;dbl&gt; 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ shop_by_brand           &lt;dbl&gt; 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, …\n$ drinkware               &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, …\n$ new                     &lt;dbl&gt; 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 6, 2, 0, 0, 0, 0, 1, 1, 0, 4, 0, 0, …\n$ clearance               &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, …\n$ accessories             &lt;dbl&gt; 0, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, …\n$ campus_collection       &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0,…\n$ office                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ lifestyle               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ small_goods             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ uncategorized_items     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ stationery              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ google                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ writing_instruments     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ fun                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ unknown                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ electronics_accessories &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ notebooks_journals      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ gift_cards              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ black_lives_matter      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nFor this time period, the data contains over 2,941 customers and 21 features available for the k-means clustering model. The user_id feature will be excluded from the modeling. Features represent counts of items purchased within each item category for each user during the period. skimr::skim() is again handy for getting a sense of the shape of the data.\n\nskim(data_user_items)\n\n\nData summary\n\n\nName\ndata_user_items\n\n\nNumber of rows\n2941\n\n\nNumber of columns\n22\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nuser_id\n0\n1\n10\n10\n0\n2941\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\napparel\n0\n1\n1.42\n2.13\n0\n0\n1\n2\n28\n▇▁▁▁▁\n\n\nbags\n0\n1\n0.28\n2.02\n0\n0\n0\n0\n75\n▇▁▁▁▁\n\n\nshop_by_brand\n0\n1\n0.30\n1.32\n0\n0\n0\n0\n40\n▇▁▁▁▁\n\n\ndrinkware\n0\n1\n0.30\n1.49\n0\n0\n0\n0\n30\n▇▁▁▁▁\n\n\nnew\n0\n1\n0.56\n2.32\n0\n0\n0\n0\n87\n▇▁▁▁▁\n\n\nclearance\n0\n1\n0.23\n1.03\n0\n0\n0\n0\n20\n▇▁▁▁▁\n\n\naccessories\n0\n1\n0.49\n2.23\n0\n0\n0\n0\n48\n▇▁▁▁▁\n\n\ncampus_collection\n0\n1\n0.58\n3.40\n0\n0\n0\n0\n128\n▇▁▁▁▁\n\n\noffice\n0\n1\n0.56\n3.21\n0\n0\n0\n0\n80\n▇▁▁▁▁\n\n\nlifestyle\n0\n1\n0.21\n0.97\n0\n0\n0\n0\n20\n▇▁▁▁▁\n\n\nsmall_goods\n0\n1\n0.05\n0.28\n0\n0\n0\n0\n4\n▇▁▁▁▁\n\n\nuncategorized_items\n0\n1\n0.21\n1.32\n0\n0\n0\n0\n48\n▇▁▁▁▁\n\n\nstationery\n0\n1\n0.19\n2.16\n0\n0\n0\n0\n100\n▇▁▁▁▁\n\n\ngoogle\n0\n1\n0.19\n3.39\n0\n0\n0\n0\n160\n▇▁▁▁▁\n\n\nwriting_instruments\n0\n1\n0.10\n0.99\n0\n0\n0\n0\n40\n▇▁▁▁▁\n\n\nfun\n0\n1\n0.04\n1.40\n0\n0\n0\n0\n75\n▇▁▁▁▁\n\n\nunknown\n0\n1\n0.16\n1.08\n0\n0\n0\n0\n40\n▇▁▁▁▁\n\n\nelectronics_accessories\n0\n1\n0.01\n0.24\n0\n0\n0\n0\n12\n▇▁▁▁▁\n\n\nnotebooks_journals\n0\n1\n0.01\n0.30\n0\n0\n0\n0\n12\n▇▁▁▁▁\n\n\ngift_cards\n0\n1\n0.02\n0.40\n0\n0\n0\n0\n15\n▇▁▁▁▁\n\n\nblack_lives_matter\n0\n1\n0.00\n0.02\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\n\n\n\nUpon visual inspection, the shape of the distributions are concerning. Each histogram exhibits the presence of highly skewed data. Outliers are most likely present, and they will need to be addressed. Otherwise, they’ll negatively impact the k-means clustering algorithm. The mean values for the features also indicate a likely issue: limited purchase frequency for certain item categories.\nBefore moving ahead with removing examples, dropping some features might be worth exploring. The objective here is to identify item categories with limited purchase frequency. Any item category with a limited purchase frequency could likely be dropped.\n\nsummarise(\n  data_user_items, \n  across(apparel:black_lives_matter, sum)\n) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 21\n$ apparel                 &lt;dbl&gt; 4169\n$ bags                    &lt;dbl&gt; 832\n$ shop_by_brand           &lt;dbl&gt; 889\n$ drinkware               &lt;dbl&gt; 893\n$ new                     &lt;dbl&gt; 1634\n$ clearance               &lt;dbl&gt; 684\n$ accessories             &lt;dbl&gt; 1438\n$ campus_collection       &lt;dbl&gt; 1700\n$ office                  &lt;dbl&gt; 1633\n$ lifestyle               &lt;dbl&gt; 607\n$ small_goods             &lt;dbl&gt; 143\n$ uncategorized_items     &lt;dbl&gt; 608\n$ stationery              &lt;dbl&gt; 554\n$ google                  &lt;dbl&gt; 572\n$ writing_instruments     &lt;dbl&gt; 291\n$ fun                     &lt;dbl&gt; 122\n$ unknown                 &lt;dbl&gt; 463\n$ electronics_accessories &lt;dbl&gt; 35\n$ notebooks_journals      &lt;dbl&gt; 43\n$ gift_cards              &lt;dbl&gt; 46\n$ black_lives_matter      &lt;dbl&gt; 1\n\n\nReviewing the output, small_goods; fun; electronics_accessories; notebooks_journals; gift_cards; and black_lives_matter have a small enough purcharse frequency to be dropped. Since we don’t have information on what is being purchased, the unknown feature is also another feature that could be dropped. Here’s the code to drop these features:\n\ndata_items &lt;- \n  data_user_items |&gt; \n  select(\n    -c(\n      small_goods,\n      fun,\n      electronics_accessories,\n      notebooks_journals,\n      gift_cards,\n      black_lives_matter,\n      unknown\n    )\n  )\n\nWhile the skimr::skim() output includes histograms, we’ll use ggplot2 to examine the distributions in more detail.\n\ndata_item_hist &lt;- data_items |&gt;\n  pivot_longer(\n    cols = apparel:stationery,\n    values_to = \"items\"\n  ) \n\nggplot(data_item_hist, aes(x = items)) +\n  geom_histogram(binwidth = 1) +\n  facet_wrap(~name, ncol = 4, scales = \"free\") +\n  labs(\n    title = \"Distribution of purchases by item category\",\n    y = \"\",\n    x = \"\"\n  )\n\n\n\n\n\n\n\n\nThe following histograms further confirm the presence of skewed data. It also further provides evidence of another characteristic of concern: low item purchase frequency. Both of these issues will need to be addressed before applying k-means clustering.\nLet’s first normalize the data, so we can get values across features to be within a similar range. To do this, we’ll transform features into z-scores. The summary() function can be used to confirm the transformation was applied. This step can be done by utilizing the following code:\n\n\n\n\n\n\nNote\n\n\n\nscale() accepts a dataframe with only numeric features. So, I have to remove it, then add it back.\n\n\n\ndata_items_stnd &lt;-\n  as_tibble(scale(select(data_items, -user_id))) |&gt;\n  mutate(user_id = data_items$user_id, .before = 1)\n\n# Verify the standarization was applied\nsummary(data_items_stnd)\n\n   user_id             apparel             bags         shop_by_brand       drinkware      \n Length:2941        Min.   :-0.6655   Min.   :-0.1399   Min.   :-0.2297   Min.   :-0.2032  \n Class :character   1st Qu.:-0.6655   1st Qu.:-0.1399   1st Qu.:-0.2297   1st Qu.:-0.2032  \n Mode  :character   Median :-0.1960   Median :-0.1399   Median :-0.2297   Median :-0.2032  \n                    Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n                    3rd Qu.: 0.2734   3rd Qu.:-0.1399   3rd Qu.:-0.2297   3rd Qu.:-0.2032  \n                    Max.   :12.4797   Max.   :36.9390   Max.   :30.1695   Max.   :19.8776  \n      new            clearance        accessories     campus_collection     office       \n Min.   :-0.2391   Min.   :-0.2252   Min.   :-0.219   Min.   :-0.1701   Min.   :-0.1728  \n 1st Qu.:-0.2391   1st Qu.:-0.2252   1st Qu.:-0.219   1st Qu.:-0.1701   1st Qu.:-0.1728  \n Median :-0.2391   Median :-0.2252   Median :-0.219   Median :-0.1701   Median :-0.1728  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.:-0.2391   3rd Qu.:-0.2252   3rd Qu.:-0.219   3rd Qu.:-0.1701   3rd Qu.:-0.1728  \n Max.   :37.2019   Max.   :19.1369   Max.   :21.277   Max.   :37.4975   Max.   :24.7254  \n   lifestyle       uncategorized_items   stationery           google         writing_instruments\n Min.   :-0.2135   Min.   :-0.1566     Min.   :-0.08721   Min.   :-0.05737   Min.   :-0.1002    \n 1st Qu.:-0.2135   1st Qu.:-0.1566     1st Qu.:-0.08721   1st Qu.:-0.05737   1st Qu.:-0.1002    \n Median :-0.2135   Median :-0.1566     Median :-0.08721   Median :-0.05737   Median :-0.1002    \n Mean   : 0.0000   Mean   : 0.0000     Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.0000    \n 3rd Qu.:-0.2135   3rd Qu.:-0.1566     3rd Qu.:-0.08721   3rd Qu.:-0.05737   3rd Qu.:-0.1002    \n Max.   :20.4745   Max.   :36.2012     Max.   :46.21115   Max.   :47.13470   Max.   :40.4114    \n\n\nReviewing the summary() output post normalization, the maximum values are concerning. There are some users who purchased items within categories at a very significant rate. Although this is a nice problem to have for the merchandise store (i.e., big purchases are good for the bottom line), it may cause problems when specifying the clustering algorithm.\nIndeed, a couple of approaches could be taken here. For one, the outliers could be retained. This will likely highly affect the k-means algorithms’ ability to effectively identify useful clusters, though. The second option is to drop examples that we consider to be outliers. Let’s think this through a bit.\nMore than likely, any example with the number of items purchased above 3 standard deviations beyond the mean should be dropped. The merchandise store is likely meant for business-to-consumer (B2C) sales, rather than business-to-business (B2B) sales. As such, the amount of items purchased during a typical customer transaction will likely be of a volume that is reasonable for consumer purchases (e.g., who needs more than 50 items of stationery?). Such large purchases are likely a B2B transaction, where large volumes of items are being bought. Given the intent of the store to be B2C, then examples exhibiting such large purchase volumes should be dropped.\nAdditional information could be used to further verify this assumption. We likely have a Customer Relationship Management (CRM) system with information about who the customers of these purchases are, and thus we could use this information to confirm if a purchase was for a business or individual. Since the ability to obtain this additional information is not possible, dropping these outliers before clustering is the best option. With all that said, here’s the code to further explore customers considered to be outliers. head() is used here to limit the output.\n\ndata_items_stnd |&gt;\n  filter(if_any(-c(user_id), ~ . &gt; 3)) |&gt;\n  head(n = 10)\n\n# A tibble: 10 × 15\n   user_id    apparel   bags shop_by_brand drinkware    new clearance accessories campus_collection\n   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;             &lt;dbl&gt;\n 1 qifwtgogoz   6.38  -0.140        -0.230    -0.203 -0.239    -0.225      -0.219             7.78 \n 2 dootkphyen  -0.665 -0.140         4.33     -0.203  2.34     -0.225      -0.219            -0.170\n 3 qksqqiqfjm  -0.665 -0.140         1.29     -0.203  0.191     0.743       0.677             0.124\n 4 cdykzwveuk  -0.665 -0.140        -0.230    10.5   -0.239    -0.225      -0.219            -0.170\n 5 yigpconufy  -0.665 -0.140        -0.230    -0.203  6.65     -0.225      -0.219            -0.170\n 6 ubblgupnrd   4.97  -0.140        -0.230     5.82  -0.239    -0.225      -0.219             1.30 \n 7 shyjuzbuci  -0.665 36.9          -0.230    -0.203 -0.239    -0.225      -0.219            -0.170\n 8 cbpqtertvd  -0.665 -0.140         4.33     -0.203 -0.239    -0.225      -0.219            -0.170\n 9 djoibydgth   0.743 -0.140         0.530     2.47   1.05     -0.225      -0.219            -0.170\n10 zqgnutolrn   3.09  -0.140        -0.230    -0.203 12.7      -0.225      -0.219            -0.170\n# ℹ 6 more variables: office &lt;dbl&gt;, lifestyle &lt;dbl&gt;, uncategorized_items &lt;dbl&gt;, stationery &lt;dbl&gt;,\n#   google &lt;dbl&gt;, writing_instruments &lt;dbl&gt;\n\n\nHere’s the code to filter out customers considered to be outliers:\n\ndata_items_stnd &lt;- data_items_stnd |&gt;\n  select(-user_id) |&gt;\n  filter(!if_any(everything(), ~ . &gt; 3)) \n\nsummary(data_items_stnd)\n\n    apparel              bags          shop_by_brand        drinkware             new          \n Min.   :-0.66549   Min.   :-0.13986   Min.   :-0.22973   Min.   :-0.20324   Min.   :-0.23910  \n 1st Qu.:-0.66549   1st Qu.:-0.13986   1st Qu.:-0.22973   1st Qu.:-0.20324   1st Qu.:-0.23910  \n Median :-0.19602   Median :-0.13986   Median :-0.22973   Median :-0.20324   Median :-0.23910  \n Mean   :-0.09975   Mean   :-0.05728   Mean   :-0.07586   Mean   :-0.08145   Mean   :-0.08105  \n 3rd Qu.: 0.27345   3rd Qu.:-0.13986   3rd Qu.:-0.22973   3rd Qu.:-0.20324   3rd Qu.:-0.23910  \n Max.   : 2.62080   Max.   : 2.82645   Max.   : 2.81020   Max.   : 2.47420   Max.   : 2.77339  \n   clearance         accessories       campus_collection      office           lifestyle       \n Min.   :-0.22516   Min.   :-0.21897   Min.   :-0.17010   Min.   :-0.17281   Min.   :-0.21349  \n 1st Qu.:-0.22516   1st Qu.:-0.21897   1st Qu.:-0.17010   1st Qu.:-0.17281   1st Qu.:-0.21349  \n Median :-0.22516   Median :-0.21897   Median :-0.17010   Median :-0.17281   Median :-0.21349  \n Mean   :-0.07933   Mean   :-0.08889   Mean   :-0.05533   Mean   :-0.09425   Mean   :-0.07426  \n 3rd Qu.:-0.22516   3rd Qu.:-0.21897   3rd Qu.:-0.17010   3rd Qu.:-0.17281   3rd Qu.:-0.21349  \n Max.   : 2.67916   Max.   : 2.91591   Max.   : 2.77268   Max.   : 2.93947   Max.   : 2.88970  \n uncategorized_items   stationery           google         writing_instruments\n Min.   :-0.15659    Min.   :-0.08721   Min.   :-0.05737   Min.   :-0.10021   \n 1st Qu.:-0.15659    1st Qu.:-0.08721   1st Qu.:-0.05737   1st Qu.:-0.10021   \n Median :-0.15659    Median :-0.08721   Median :-0.05737   Median :-0.10021   \n Mean   :-0.04983    Mean   :-0.05096   Mean   :-0.03856   Mean   :-0.05943   \n 3rd Qu.:-0.15659    3rd Qu.:-0.08721   3rd Qu.:-0.05737   3rd Qu.:-0.10021   \n Max.   : 2.87322    Max.   : 2.69069   Max.   : 1.71234   Max.   : 2.93816   \n\n\nLet’s take a look at the histograms again, just to get a sense if dropping outliers helped.\n\ndata_items_stnd |&gt;\n  pivot_longer(\n    cols = apparel:stationery,\n    values_to = \"items\"\n  ) |&gt; \n  ggplot(aes(x = items)) +\n    geom_histogram(binwidth = 1) +\n    facet_wrap(~name, ncol = 4, scales = \"free\") +\n    labs(\n      title = \"Distribution of purchases by item category post wrangling\",\n      y = \"\",\n      x = \"\"\n    )\n\n\n\n\n\n\n\n\nAlthough this helped with the issues caused by outliers, we still have to contend with the fact that some customers just don’t buy certain items. We’ll want to keep this in mind when drawing conclusions from the final clusters.\n\n\nDetermine a k-value\nNow that the data is in a format acceptable for modeling, we need to explore a value for the number of cluster centers, the k-value. Various methods can be used to determine this value. I’ll rely on three methods here: the elbow method; the average silhouette method; and using the realities imposed by the business case (Lantz 2023). Each will be discussed more in-depth in the following sections.\nThe first method is the elbow method, where we visually examine an elbow plot of the total within sum of squares based on the number of potential clusters used for the model. The fviz_nbclust() function from the factoextra package is useful here. We first pass in the data we intend to use for modeling, then base R’s stats kmeans() function. We’re also interested in creating the plot using the within sum of squares method, so we specifiy that using the method argument.\n\nfviz_nbclust(data_items_stnd, kmeans, method = \"wss\")\n\n\n\n\n\n\n\n\nGiven visual examination of the plot, six clusters seems to be a good starting point. The average silhouette method is another visulization useful for confirming the number of cluster groups for our cluster modelling.\n\nfviz_nbclust(data_items_stnd, kmeans, method = \"silhouette\")\n\n\n\n\n\n\n\n\nJust as was expected, the silhouette method provides additional evidence for 6 clusters.\nConsidering the business case is another useful method for determining the k-value. For instance, say our goal is to cluster the data based on the number of campaigns our marketing team has capacity to manage. The number used here is completely arbitrary (i.e., I don’t have a marketing team to confirm capacity). Thus, for the sake of example, let’s say we have a marketing team capable of managing 3 campaigns over the holiday season.\nSo, based on the information above, let’s examine k-means clustering using 3 and 6 groups.\n\n\nSpecify the model\nNow the modeling step. We’ll use base R’s kmeans() function from the stats package. Since a clustering model with 3 or 6 clusters is being explored here, purrr’s map() function is used to iterate the model specification. map() returns a list object, which each element of the list is a model output. One for the three and six cluster model. set.seed() is also used for the reproducibility of the results.\n\nset.seed(20240722)\nmdl_clusters &lt;- \n  map(c(3, 6), \\(x) kmeans(data_items_stnd, centers = x))\n\n\n\nEvaluating model performance\nTo assess model fit, we’ll look to the cluster sizes for both clustering models. purrr’s map function makes this easy. Use the following code to return the size element of kmeans output:\n\nmap(mdl_clusters, \"size\")\n\n[[1]]\n[1] 1642  762  278\n\n[[2]]\n[1]  223  296 1284  660   80  139\n\n\nIdentifying imbalanced groups is priority here. Some imbalance is tolerable, but major imbalances might indicate the presence of model fit issues. The six cluster model looks fairly balanced, where only one group includes a small subset of customers (~80 customers). The three cluster model has one larger group followed by ever decreasing sized groups. Overall, the balance across the different groups seems to be acceptable here.\nVisualizing the clusters is also useful for model assessment. The factoextra package is once again helpful. The fviz_cluster() function from the package can be used to visualize the clusters. The function first takes our model output (mdl_clsuters[[1]]) and the initial data object (data_item_stnd) as arguments. Both the three and six cluster model are visualized using the example code below.\n\nfviz_cluster(mdl_clusters[[1]], data = data_items_stnd, pointsize = 3) +\n  labs(\n    title = \"Three-cluster model of Google Merchandise Store customers\",\n    subtitle = \"View of cluster groupings during the U.S. holiday season\"\n  ) +\n  theme_replace(ggplot_theme)\n\n\n\n\n\n\n\n\n\nfviz_cluster(mdl_clusters[[2]], data = data_items_stnd, pointsize = 3) +\n  labs(\n    title = \"Six-cluster model of Google Merchandise Store customers\",\n    subtitle = \"View of cluster groupings during the U.S. holiday season\"\n  ) +\n  theme_replace(ggplot_theme)\n\n\n\n\n\n\n\n\nfviz_cluster() uses dimension reduction methods to allow for plotting of a multi-dimensional dataset into a two-dimensional representation. The output is useful for identifying general clustering patterns within the data, which could provide additional information about the shape of the clusters. For instance, this type of visualization can be used to visually identify any outliers which may be influencing the shape clusters take.\nIndeed, the plot for both models shows some cluster overlap. This is an indication that the clusters for this data may not be as distinct as we would like. There might be some common products every customer buys, and a few peripheral products that other clusters purchase beyond these common products. These initial results indicate the presence of ‘upsell’ opportunities for the marketing team. You have your core items that most customers purchase, but some clusters seem to purchase items beyond the core products. Thus, some of the marketing campaigns might strategise ways to highlight the upselling of some products.\n\n\nDraw conclusions about clusters\nThe next step is to examine the cluster centers and factor loadings. The goal is to derive conclusions about each cluster from this information. Let’s first draw conclusions using our three-cluster model. We’ll look to identify specific audience segments based on item category loadings.\nThe kmeans()’s output has an object labelled centers, which is a matrix of cluster centers. We then inspect these center values for each cluster, which are on the left, and the values associated within each column.\n\nmdl_clusters[[1]]$centers \n\n     apparel        bags shop_by_brand   drinkware         new  clearance  accessories\n1 -0.4476288 -0.06097525   -0.06495514 -0.08339455 -0.09966977 -0.2251558 -0.096510610\n2  0.7127262 -0.06395056   -0.09907281 -0.07850703 -0.07644876 -0.1933938 -0.103778685\n3 -0.2720182 -0.01715296   -0.07663600 -0.07803938  0.01632421  1.0946694 -0.003105749\n  campus_collection        office   lifestyle uncategorized_items  stationery      google\n1       -0.05701557 -0.1077973177 -0.09379899        -0.073095026 -0.06268202 -0.04012104\n2       -0.09325080 -0.0992918176 -0.05330947        -0.005497098 -0.05926368 -0.04110829\n3        0.05854479 -0.0004035005 -0.01628649        -0.033980814  0.04102364 -0.02235330\n  writing_instruments\n1        -0.062586565\n2        -0.072300004\n3        -0.005490095\n\n\n\nmdl_clusters[[1]]$centers |&gt;\n  as_tibble() |&gt;\n  mutate(\n    cluster = 1:3,\n    across(where(is.double), \\(x) round(x, digits = 4)),\n    .before = 1\n  ) |&gt;\n  gt() |&gt;\n  data_color(\n    columns = !(\"cluster\"),\n    rows = 1:3,\n    direction = \"row\",\n    method = \"numeric\",\n    palette = \"Blues\"\n  ) |&gt;\n  tab_header(\n    title = md(\n      \"**Three-cluster k-means model of Google Merchandise store customers factor loadings**\"\n    )\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"Source: Google Analytics data for the Google Merchandise Store\"\n    )\n  ) |&gt;\n  opt_interactive()\n\n\n\nTable 1: Three-cluster model factor loadings\n\n\n\n\n\n\nThree-cluster k-means model of Google Merchandise store customers factor loadings\n\n\n\n\n\n\nSource: Google Analytics data for the Google Merchandise Store\n\n\n\n\n\n\n\n\n\n\n\nAfter inspecting the three-cluster k-means model factor loadings, a few groups emerge. Cluster 1 (n = 1,642) seems to be the ‘anything but apparel’ customers. This customer segment really doesn’t purchase any specific item, but when they shop, they’re purchasing items other than apparel. Perhaps a campaign could be created to improve customer’s familiarity with products other than apparel that are available in the merchandise store.\nCluster 2 (n = 762) are the ‘fashion fanatics’. In fact, it seems this group is mainly purchasing apparel. Maybe our product and marketing teams could consider releasing a new apparel line around the holiday season. A campaign focused on highlighting different apparel pieces could also be explored.\nCluster 3 (n = 278) are ‘discount diggers’. Indeed, this data covers the holiday season, so maybe some customers around this time are trying to find a unique gift, but want to do so on a budget. Perhaps a campaign focused on ‘holiday gift deals’ might appeal to these types of customers.\nIf more nuance is required for the segmentation discussion, the factor loadings for the six-cluster model can be examined. Again, these results suggest the presence of ‘anything but apparel customers’; ‘fashion fanatics’; and ‘discount diggers’. However, three additional groups emerge from the six cluster model.\n\nmdl_clusters[[2]]$centers |&gt;\n  as_tibble() |&gt;\n  mutate(\n    cluster = 1:6,\n    across(where(is.double), \\(x) round(x, digits = 4)),\n    .before = 1\n  ) |&gt;\n  gt() |&gt;\n  data_color(\n    columns = !(\"cluster\"),\n    rows = 1:6,\n    direction = \"row\",\n    method = \"numeric\",\n    palette = \"Blues\"\n  ) |&gt;\n  tab_header(\n    title = md(\n      \"**Six-cluster k-means model of Google Merchandise store customers factor loadings**\"\n    )\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"Source: Google Analytics data for the Google Merchandise Store\"\n    )\n  ) |&gt;\n  opt_interactive()\n\n\n\nTable 2: Six-cluster model factor loadings\n\n\n\n\n\n\nSix-cluster k-means model of Google Merchandise store customers factor loadings\n\n\n\n\n\n\nSource: Google Analytics data for the Google Merchandise Store\n\n\n\n\n\n\n\n\n\n\n\nThe first additional segment emerging from the six-cluster model includes ‘lifestyle looters’ (n = 223): the customers who purchase products that fit or enhance their lifestyle. Perhaps there’s room for a campaign focused on highlighting how the Google Merchandise Store’s products fit within the lives of its customers: most likely people who work in tech.\nThe second segment are the ‘brand buyers’ (n = 296). These customers are mostly interested in purchasing branded items. Thus, a campaign highlighting the various branded items that are available might be explored.\nThe final group to emerge is our ‘accessory enthusiasts’ (n = 139). These are customers most interested in purchasing accessories. Perhaps a focus on accessories could be another campaign our marketing team might look at to create.\nDepending on the model reviewed, clustering resulted in the identification of three customer segments campaigns could be targeted: ‘anything but apparel’, ‘fashion fanatics’, and ‘discount diggers’. If an expanded list of segments was required, the six-cluster model provides additional information. This includes segments like ‘lifestyle looters’, ‘brand buyers’, and ‘accessory enthusiasts’. Indeed, the segment names are up for debate. I would lean on my marketing team to workshop them some more. Analysts are poor at naming things.\n\n\nWrap up\nThis post was a tutorial on how to perform clustering using Google Analytics e-commerce data. The k-means algorithm was used to identify clusters within obfuscated analytics data from the Google Merchandise store. Information about the clusters was used to generate various customer segments. The intent was to use these clusters to better inform future marketing campaigns and targeting strategies.\nThe kmeans() function from the base R stats package was used to specify two clustering models. The elbow method, silhouette method, and information about the business case were used to determine the k-values for the clustering models. The fviz_nbclust() function from the factoextra package was useful for creating visualizations to further confirm the selected k-values. It was determined both a three and six cluster model would be effective to meet our modelling goal for this data. Lastly, the fviz_cluster() function from the factoextra package was used to create visualizations of each model’s clusters.\nIn truth, this dataset lacked the presence of any interesting cluster groups. I was hoping for some more cross-product segments that could be used for customer segmentation identification. Unfortunately, this wasn’t the case. Many of the Google Merchandise Store’s customers fell within a single item-category. This was likely due to low purchase frequency for item categories, which likely is due to customers only buying one or two products with each purchase. Nonetheless, we were able to still identify various customer segments useful for targeting purposes and provide some additional support for potential marketing campaigns.\nSo there you have it, another messing with models post. I hope you found something useful. If not, I hope it was somewhat informative and you found a few takeaways.\nUntil next time, keep messing with models.\n\n\n\n\n\nReferences\n\nLantz, Brett. 2023. Machine Learning with R. 4th ed. packt. https://www.oreilly.com/library/view/machine-learning-with/9781801071321/.\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Messing with Models: K-Means Clustering of {Google}\n    {Analytics} 4 Data},\n  date = {2024-09-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Messing with Models: K-Means Clustering of\nGoogle Analytics 4 Data.” September 14, 2024."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "",
    "text": "Photo by T K\nI had the great fortune of being a presenter at this year’s PBS TechCon conference. The focus of my talk was to introduce attendees to the principles of tidy data and discuss a data pipeline project my team has been working on at Nebraska Public Media. Here’s the session description:\nAs part of my talk, I mentioned having put together a curated list of resources others could use to learn more about the topics covered. This list can be found in the following section of this blog post. If you’re interested in discussing these topics further, please reach out."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nTidy Data paper published in the Journal of Statistical Software written by Hadley Wickham\nTidy Data Chapter published in the open source R for Data Science book written by Hadley Wickham and Garrett Grolemun\nData Organization: Organizing Data in Spreadsheets post by Karl Broman\nData Organization: Organizing Data in Spreadsheets paper published in The American Statistician written by Karl Broman and Kara Woo\nTidy data section in Data Management in Large-Scale Education Research training modules written by Crystal Lewis\nTidy Data presented by Hadley Wickham\nTowards Data Science post published on Medium summarizing tidy data written by Benedict Neo"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Join a Community",
    "text": "Join a Community\n\nR for Data Science Online Learning Community\n\nJoin the Slack workspace\n@Collin Berke to get a hold of me in the workspace"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Open source workflow management tool",
    "text": "Open source workflow management tool\n\nApache Airflow"
  },
  {
    "objectID": "blog/posts/2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel/index.html",
    "href": "blog/posts/2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel/index.html",
    "title": "Exploring the relationship between trash processed by Mr. Trash Wheel and precipitation",
    "section": "",
    "text": "Photo by Nareeta Martin\n\n\n\n👋 Say hello to Mr. Trash Wheel and friends\nThis week’s #tidytuesday we’re looking into data related to Mr. Trash Wheel and friends. Mr. Trash Wheel is a semi-autonomous trash interceptor, who’s main purpose is to collect trash floating into the Baltimore Inner Harbor. Mr. Trash Wheel is a pretty neat invention. If you’re interested in how it works, check out the information found here.\nMy curiosity peaked when I came across the statement that most of the trash collected by Mr. Trash wheel is the result of water runoff, and not from people disposing trash directly into the habor. So, I wanted to explore the relationship between precipitation and the amount of trash being collected by Mr. Trash Wheel and friends for my contribution this week.\nIn this post, I created my visualizations using plotly and Tableau.\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(plotly)\nlibrary(here)\nlibrary(janitor)\nlibrary(tidymodels)\ntidymodels_prefer()\n\n\ndata_trash &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel\", \n    \"trashwheel.csv\"\n  )\n)\n\nRows: 993 Columns: 16\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): ID, Name, Month, Date\ndbl (12): Dumpster, Year, Weight, Volume, PlasticBottles, Polystyrene, CigaretteButts, GlassBott...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata_balt_precip &lt;- read_csv(\n  here(\n    \"blog\",\n    \"posts\",\n    \"2024-03-12-tidytuesday-2024-03-05-mr-trash-wheel\", \n    \"balt_precip.csv\"\n  )\n)\n\nRows: 10 Columns: 13\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (13): year, january, february, march, april, may, june, july, august, september, october, no...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nData description\nThe data contains observations related to trash collected from 2014 to 2023 by multiple trash wheels. The Baltimore precipitation data came from a tool found here. I simply just copy pasted this data into a Google sheet and saved it as a .csv file. Further wrangling steps for both data sets are included below.\nTo get a better sense of what’s in the data, I did a quick glimpse() and skim() of both the data_trash and data_balt_precip data sets.\n\nglimpse(data_trash)\n\nRows: 993\nColumns: 16\n$ ID             &lt;chr&gt; \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mister\", \"mist…\n$ Name           &lt;chr&gt; \"Mister Trash Wheel\", \"Mister Trash Wheel\", \"Mister Trash Wheel\", \"Mister T…\n$ Dumpster       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, …\n$ Month          &lt;chr&gt; \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"May\", \"June\", \"June\", \"Ju…\n$ Year           &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 201…\n$ Date           &lt;chr&gt; \"5/16/2014\", \"5/16/2014\", \"5/16/2014\", \"5/17/2014\", \"5/17/2014\", \"5/20/2014…\n$ Weight         &lt;dbl&gt; 4.31, 2.74, 3.45, 3.10, 4.06, 2.71, 1.91, 3.70, 2.52, 3.76, 3.43, 4.17, 5.1…\n$ Volume         &lt;dbl&gt; 18, 13, 15, 15, 18, 13, 8, 16, 14, 18, 15, 19, 15, 15, 15, 15, 13, 15, 15, …\n$ PlasticBottles &lt;dbl&gt; 1450, 1120, 2450, 2380, 980, 1430, 910, 3580, 2400, 1340, 740, 950, 530, 84…\n$ Polystyrene    &lt;dbl&gt; 1820, 1030, 3100, 2730, 870, 2140, 1090, 4310, 2790, 1730, 869, 1140, 630, …\n$ CigaretteButts &lt;dbl&gt; 126000, 91000, 105000, 100000, 120000, 90000, 56000, 112000, 98000, 130000,…\n$ GlassBottles   &lt;dbl&gt; 72, 42, 50, 52, 72, 46, 32, 58, 49, 75, 38, 45, 58, 62, 64, 56, 47, 65, 63,…\n$ PlasticBags    &lt;dbl&gt; 584, 496, 1080, 896, 368, 672, 416, 1552, 984, 448, 344, 520, 224, 344, 432…\n$ Wrappers       &lt;dbl&gt; 1162, 874, 2032, 1971, 753, 1144, 692, 3015, 1988, 1066, 544, 727, 361, 631…\n$ SportsBalls    &lt;dbl&gt; 7, 5, 6, 6, 7, 5, 3, 6, 6, 7, 6, 8, 6, 6, 6, 6, 5, 6, 6, 7, 6, 6, 6, 5, 6, …\n$ HomesPowered   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\nglimpse(data_balt_precip)\n\nRows: 10\nColumns: 13\n$ year      &lt;dbl&gt; 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023\n$ january   &lt;dbl&gt; 2.71, 3.89, 3.50, 2.69, 1.00, 3.15, 3.11, 2.15, 4.27, 1.68\n$ february  &lt;dbl&gt; 4.58, 2.24, 5.70, 1.46, 5.30, 3.64, 2.98, 4.85, 2.31, 2.18\n$ march     &lt;dbl&gt; 4.38, 4.67, 2.10, 3.82, 2.25, 4.14, 3.05, 3.90, 3.13, 1.49\n$ april     &lt;dbl&gt; 8.60, 4.30, 1.31, 3.52, 3.20, 1.46, 5.52, 2.07, 3.92, 4.12\n$ may       &lt;dbl&gt; 3.35, 2.10, 5.24, 5.64, 8.17, 5.51, 1.76, 3.63, 5.39, 0.55\n$ june      &lt;dbl&gt; 3.95, 13.09, 3.20, 1.40, 4.77, 2.95, 5.95, 2.75, 2.95, 4.31\n$ july      &lt;dbl&gt; 2.80, 3.49, 6.09, 7.11, 16.73, 3.85, 3.43, 3.65, 6.25, 6.84\n$ august    &lt;dbl&gt; 7.90, 2.46, 3.96, 4.60, 3.84, 2.39, 11.81, 4.36, 3.71, 3.73\n$ september &lt;dbl&gt; 3.21, 3.25, 4.36, 1.95, 9.19, 0.16, 4.48, 6.04, 3.35, 6.27\n$ october   &lt;dbl&gt; 4.16, 3.40, 0.78, 2.99, 2.69, 6.21, 4.36, 5.24, 4.66, 1.13\n$ november  &lt;dbl&gt; 3.36, 2.42, 1.51, 2.15, 8.14, 1.10, 6.35, 1.33, 2.44, 2.80\n$ december  &lt;dbl&gt; 3.58, 5.85, 2.77, 0.95, 6.54, 3.57, 4.58, 0.82, 4.80, 7.16\n\n\n\nskim(data_trash)\n\n\nData summary\n\n\nName\ndata_trash\n\n\nNumber of rows\n993\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nID\n0\n1\n6\n9\n0\n4\n0\n\n\nName\n0\n1\n18\n21\n0\n4\n0\n\n\nMonth\n0\n1\n3\n9\n0\n14\n0\n\n\nDate\n0\n1\n6\n10\n0\n623\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nDumpster\n0\n1.00\n230.88\n185.82\n1.00\n73.00\n176.00\n381.00\n629.00\n▇▅▂▂▂\n\n\nYear\n0\n1.00\n2019.57\n2.75\n2014.00\n2018.00\n2020.00\n2022.00\n2023.00\n▃▃▅▆▇\n\n\nWeight\n0\n1.00\n2.97\n0.84\n0.61\n2.45\n3.04\n3.53\n5.62\n▁▅▇▃▁\n\n\nVolume\n0\n1.00\n14.92\n1.61\n5.00\n15.00\n15.00\n15.00\n20.00\n▁▁▁▇▁\n\n\nPlasticBottles\n1\n1.00\n2219.33\n1650.45\n0.00\n987.50\n1900.00\n2900.00\n9830.00\n▇▆▁▁▁\n\n\nPolystyrene\n1\n1.00\n1436.87\n1832.43\n0.00\n240.00\n750.00\n2130.00\n11528.00\n▇▂▁▁▁\n\n\nCigaretteButts\n1\n1.00\n13728.12\n24049.61\n0.00\n2900.00\n4900.00\n12000.00\n310000.00\n▇▁▁▁▁\n\n\nGlassBottles\n251\n0.75\n20.96\n15.26\n0.00\n10.00\n18.00\n28.00\n110.00\n▇▃▁▁▁\n\n\nPlasticBags\n1\n1.00\n984.00\n1412.34\n0.00\n240.00\n540.00\n1210.00\n13450.00\n▇▁▁▁▁\n\n\nWrappers\n144\n0.85\n2238.76\n2712.85\n0.00\n880.00\n1400.00\n2490.00\n20100.00\n▇▁▁▁▁\n\n\nSportsBalls\n364\n0.63\n13.59\n9.74\n0.00\n6.00\n12.00\n20.00\n56.00\n▇▆▂▁▁\n\n\nHomesPowered\n0\n1.00\n45.85\n18.23\n0.00\n38.00\n49.00\n58.00\n94.00\n▂▂▇▅▁\n\n\n\n\nskim(data_balt_precip)\n\n\nData summary\n\n\nName\ndata_balt_precip\n\n\nNumber of rows\n10\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n13\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2018.50\n3.03\n2014.00\n2016.25\n2018.50\n2020.75\n2023.00\n▇▇▇▇▇\n\n\njanuary\n0\n1\n2.82\n1.00\n1.00\n2.28\n2.91\n3.41\n4.27\n▂▅▅▇▅\n\n\nfebruary\n0\n1\n3.52\n1.50\n1.46\n2.26\n3.31\n4.78\n5.70\n▇▅▂▅▅\n\n\nmarch\n0\n1\n3.29\n1.07\n1.49\n2.45\n3.47\n4.08\n4.67\n▅▂▅▅▇\n\n\napril\n0\n1\n3.80\n2.15\n1.31\n2.35\n3.72\n4.26\n8.60\n▆▇▃▁▂\n\n\nmay\n0\n1\n4.13\n2.28\n0.55\n2.41\n4.44\n5.48\n8.17\n▃▃▂▇▂\n\n\njune\n0\n1\n4.53\n3.26\n1.40\n2.95\n3.58\n4.65\n13.09\n▇▆▁▁▂\n\n\njuly\n0\n1\n6.02\n4.09\n2.80\n3.53\n4.97\n6.69\n16.73\n▇▆▁▁▂\n\n\naugust\n0\n1\n4.88\n2.87\n2.39\n3.71\n3.90\n4.54\n11.81\n▇▂▁▁▁\n\n\nseptember\n0\n1\n4.23\n2.51\n0.16\n3.22\n3.86\n5.65\n9.19\n▅▇▅▅▂\n\n\noctober\n0\n1\n3.56\n1.73\n0.78\n2.77\n3.78\n4.58\n6.21\n▅▂▅▇▅\n\n\nnovember\n0\n1\n3.16\n2.30\n1.10\n1.67\n2.43\n3.22\n8.14\n▇▂▁▁▁\n\n\ndecember\n0\n1\n4.06\n2.16\n0.82\n2.97\n4.08\n5.59\n7.16\n▅▂▇▅▅\n\n\n\n\n\nLooking further into the data, I noticed a few things of note. Here’s some things to keep in mind:\n\nThere are missing data (e.g., NAs) within several variables: PlasticBottles, Polystyrene, CigaretteButts, GlassBottles, PlasticBags, Wrappers, and SportsBalls. The documentation didn’t reference why these were missing and since I wasn’t using these for my contribution, I didn’t dig any further.\nThe month has an issue with capitalization. Some string formatting should fix this issue, though I’m not using this column for my contribution.\nThe Date column needed to be transformed into a date. This can be addressed by using some functions from the lubridate package.\n\n\n\nData wrangling\nNow that we have a better sense of the data, let’s wrangle it. Below is the code to wrangle both the data_balt_precip and data_trash data sets. Since my precipitation data was aggregated by month, I decided to aggregate the trash data by month.\n\n\n\n\n\n\nNote\n\n\n\nWhile working on my contribution, I learned dplyr’s transmute function is superseded, and it’s now suggested to use mutate()’s .keep = \"none' argument.\n\n\n\ndata_balt_precip &lt;- data_balt_precip |&gt;\n  pivot_longer(cols = january:december, names_to = \"month\", values_to = \"precip\") |&gt;\n  mutate(\n    month = match(month, str_to_lower(month.name)),\n    day = 1,\n    month_date = ymd(str_c(year, month, day, sep = \"-\"))\n  ) |&gt;\n  select(\n    month_date, \n    precip\n  )\n\n\ndata_trash &lt;- data_trash |&gt;\n  clean_names() |&gt;\n  mutate(\n    id,\n    name,\n    date = mdy(date),\n    month_date = floor_date(date, \"month\"),\n    dumpster,\n    name = str_to_lower(name),\n    weight,\n    volume,\n    .keep = \"none\"\n  ) \n\n\ndata_trash_summ &lt;- data_trash |&gt;\n  group_by(month_date) |&gt;\n  summarise(\n    total_weight = sum(weight),\n    total_volume = sum(volume) \n  ) |&gt;\n  left_join(data_balt_precip)\n\nJoining with `by = join_by(month_date)`\n\nmin(data_trash_summ$month_date)\n\n[1] \"2014-05-01\"\n\nmax(data_trash_summ$month_date)\n\n[1] \"2023-12-01\"\n\n\n\n\nWhat is the relationship between rainfall and the weight and volume of trash processed by the trash wheels?\nTo explore this relationship, I created two scatter plots. The first plot included precipitation and total weight. The second included volume and precipitation. I did this because weight and volume represent different things. Here’s the code to create the two scatter plots using plotly:\n\nplot_ly(\n  data = data_trash_summ,\n  x = ~precip,\n  y = ~total_weight,\n  type = \"scatter\", \n  mode = \"markers\",\n  marker = list(\n    size = 10, \n    color = \"#6495ED\",\n    line = list(\n      color = \"#151B54\",\n      width = 2\n    )\n  ),\n  text = ~paste(\n    month_date,\n    \"&lt;br&gt;Precipitation (inches): \", precip, \n    \"&lt;br&gt;Weight (tons): \", total_weight\n  ),\n  hoverinfo = \"text\"\n) |&gt;\nplotly::layout(\n  title = list(\n    text = \"&lt;b&gt;More precipitation is related to heavier amounts of trash for Mr. Trash Wheel and friends to process &lt;/b&gt;\",\n    font = list(size = 18),\n    xanchor = \"center\"\n  ),\n  yaxis = list(\n    title = \"Total weight of trash (tons)/month\",\n    titlefont = list(size = 14)\n  ),\n  xaxis = list(\n    title = \"Total precipitation in Baltimore (inches)/month\",\n    titlefont = list(size = 14)\n  ),\n  font  = list(family = \"arial\", size = 18, face = \"bold\")\n)\n\n\n\n\n\n\nplot_ly(\n  data = data_trash_summ,\n  x = ~precip,\n  y = ~total_volume,\n  type = \"scatter\", \n  mode = \"markers\",\n  marker = list(\n    size = 10, \n    color = \"#FFAA33\",\n    line = list(\n      color = \"#151B54\",\n      width = 2\n    )\n  ),\n  text = ~paste(\n    month_date,\n    \"&lt;br&gt;Precipitation (inches): \", precip, \n    \"&lt;br&gt;Volume (cubic yards): \", total_volume \n  ),\n  hoverinfo = \"text\"\n) |&gt;\nplotly::layout(\n  title = list(\n    text = \"&lt;b&gt;More precipitation is related to a greater volume of trash for Mr. Trash Wheel and friends to process&lt;/b&gt;\",\n    font = list(size = 18),\n    xanchor = \"center\"\n  ),\n  yaxis = list(\n    title = \"Total volume of trash (cubic yards)/month\",\n    titlefont = list(size = 14)\n  ),\n  xaxis = list(\n    title = \"Total precipitation in Baltimore (inches)/month\",\n    titlefont = list(size = 14)\n  ),\n  font  = list(family = \"arial\", size = 18, face = \"bold\")\n)\n\n\n\n\n\nLooking at the individual observations, I had a hard time fathoming how much trash Mr. Trash Wheel and friends were processing. So, here’s a video giving you a sense of dimension of how much trash is really being collected–it’s a lot once you put it into perspective. I mean, in one month, the trash wheels processed nearly 25 of these 20 cubic yard dumpsters worth of trash. If you’ve ever seen these dumpters in real-life, they’re huge.\nAlthough upon visual inspection it seems a positive relationship is present for both weight and volume of trash, I wanted to further quantify this relationship using a linear model. To do this, I utilized tidymodels to create two simple linear models, one for volume and the other for weight of trash.\n\nlm_mdl &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n\n\nvolume_mdl &lt;- \n  lm_mdl |&gt;\n  fit(total_volume ~ precip, data = data_trash_summ)\n\ntidy(volume_mdl)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     65.4     16.8       3.90 0.000167 \n2 precip          16.0      3.59      4.47 0.0000186\n\n\n\nweight_mdl &lt;- \n  lm_mdl |&gt;\n  fit(total_weight ~ precip, data = data_trash_summ)\n\ntidy(weight_mdl)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    11.6      3.68       3.16 0.00200  \n2 precip          3.53     0.785      4.49 0.0000172\n\n\nBoth models indicate a statistically significant positive relationship between precipitation, volume, and weight of trash processed. In fact, for every additional inch of precipitation a month in Baltimore, the volume of trash processed increases by 16 cubic yards and the weight of trash increases by 3.53 tons.\nThe bottom line, throw your trash away properly. It has down stream effects, literally … no pun intended.\n\n\nAn attempt using Tableau\nTo further practice my data visualization tool skills, I recreated these plots using Tableau. You can view this version by clicking here.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Exploring the Relationship Between Trash Processed by {Mr.}\n    {Trash} {Wheel} and Precipitation},\n  date = {2024-03-12},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Exploring the Relationship Between Trash\nProcessed by Mr. Trash Wheel and Precipitation.” March 12, 2024."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "The Hex Update: October, 2024\n\n\n\n\n\n\nthe hex update\n\n\nmedia\n\n\n\nKey insights and what I learned about the media industry as of October 2024\n\n\n\n\n\nNov 2, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow walkthrough: Interacting with Google BigQuery in R\n\n\n\n\n\n\nworkflow\n\n\ntutorial\n\n\nproductivity\n\n\nbigquery\n\n\nsql\n\n\n\nA tutorial on how to use the bigrquery package\n\n\n\n\n\nOct 5, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: k-means clustering of Google Analytics 4 data\n\n\n\n\n\n\nmachine learning\n\n\nunsupervised learning\n\n\nk-means clustering\n\n\n\nA tutorial on how to perform k-means clustering using Google Analytics data\n\n\n\n\n\nSep 14, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hex Update: June, 2024\n\n\n\n\n\n\nthe hex update\n\n\nmedia\n\n\n\nKey insights and what I learned about the media industry in June 2024\n\n\n\n\n\nJul 10, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: Market basket analysis of online merchandise store data\n\n\n\n\n\n\nmachine learning\n\n\nunsupervised learning\n\n\nassociation rules\n\n\nmarket basket analysis\n\n\n\nA tutorial on how to perform a market basket analysis using Google Analytics data\n\n\n\n\n\nJun 11, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring objects launched into space and gross domestic product\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nregression\n\n\n\nA contribution to the 2024-04-23 #tidytuesday social data project\n\n\n\n\n\nMay 3, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring data from the Fiscal Sponsor Directory\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-03-12 #tidytuesday social data project\n\n\n\n\n\nMar 22, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the relationship between trash processed by Mr. Trash Wheel and precipitation\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-03-05 #tidytuesday social data project\n\n\n\n\n\nMar 12, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the lifespans of historical figures born on a Leap Day\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-02-27 #tidytuesday social data project\n\n\n\n\n\nMar 5, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring R Consortium ISC Grants\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-02-20 #tidytuesday social data project\n\n\n\n\n\nFeb 26, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n30 day tidymodels recipes challenge\n\n\n\n\n\n\nmachine learning\n\n\nfeature engineering\n\n\ntidymodels\n\n\ndata wrangling\n\n\n\nLearning how to use the recipes package, one day at a time\n\n\n\n\n\nJan 1, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data\n\n\n\n\n\n\ntutorial\n\n\ntidymodels\n\n\nclassification\n\n\ndecision tree\n\n\nlogistic regression\n\n\n\nUsing tidymodels to predict wins and losses for volleyball matches\n\n\n\n\n\nDec 7, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n2023 data science rig: Set up and configuration\n\n\n\n\n\n\ntutorial\n\n\n\nOverviewing and reflecting on my current data science setup.\n\n\n\n\n\nJan 29, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nFlattening Google Analytics 4 data\n\n\n\n\n\n\nBigQuery\n\n\nsql\n\n\ndata wrangling\n\n\n\nLet’s deep dive into working with Google Analytics data stored in BigQuery.\n\n\n\n\n\nSep 20, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory analysis of Google Analytics 4 data for forecasting models\n\n\n\n\n\n\nforecasting\n\n\n\nExploring Google Analytics 4 data for forecasting models.\n\n\n\n\n\nMar 3, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nShiny summary tiles\n\n\n\n\n\n\nshiny\n\n\n\nBuilding custom metric summary tiles for Shiny.\n\n\n\n\n\nDec 30, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n2021 PBS TechCon: Your Data is Disgusting!\n\n\n\n\n\n\ntalks\n\n\n\nI was fortunate to be invited to present about topics I’m passionate about: tidy data and data pipelines.\n\n\n\n\n\nOct 19, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a next and back button in Shiny\n\n\n\n\n\n\nshiny\n\n\n\nTaking the time to understand a challenging question from Mastering Shiny.\n\n\n\n\n\nSep 12, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nIntro Post\n\n\n\n\n\n\npersonal\n\n\n\nHello World!, my name is Collin, and this is my blog.\n\n\n\n\n\nApr 2, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/posts/2024-02-10-til-r-dput-store-objects/index.html",
    "href": "til/posts/2024-02-10-til-r-dput-store-objects/index.html",
    "title": "Use base::dput() to easily create and save objects",
    "section": "",
    "text": "Image generated using the prompt ‘Robot manufacturing several widgets on a conveyor belt in a pop art style’ with the Bing Image Creator\n\n\n\nlibrary(tidyverse)\nlibrary(testthat)\n\n\nBackground\nLately, I’ve been doing a lot of data validation tests for a package I’m working on. Using testthat for the testing framework, some of the tests I’m writing verify dataset column names. For instance, these tests tend to look something like this:\n\ntest_that(\"column names are as expected\", {\n  expect_named(\n    mtcars, \n    c(\"mpg\", \"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \"gear\", \"carb\")\n  )\n})\n\nTest passed 🥇\n\n\nSince mtcars only has 11 columns, the character vector used for the column name test is pretty small. Creating this by hand isn’t too bad. However, what if we need to create a character vector for a dataset much larger than this. Say a dataset with 150+ columns–soul crushing. I don’t know about you, but I would hate to hand key a character vector this long (I’m sad to report I’ve done this more times than I would like to admit). Of course, there’s a better way. Use base::dput().\n\n\nTIL: Use dput()\nAccording to the docs, dput:\n\nWrites an ASCII text representation of an R object to a file, the R console, or a connection, or uses one to recreate the object.\n\nNow that we have a tool to make this easier, all we need to do is pass the data to names(), and then wrap dput() around the return value of names(). What results is a character vector that gets printed to the console. All we need to do now is copy and paste this output into our file. This is what this looks like:\n\ndput(names(mtcars))\n\nc(\"mpg\", \"cyl\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\", \"vs\", \"am\", \n\"gear\", \"carb\")\n\n\nPretty neat.\ndput() also has a file argument, so you can pass along a file string the object will be written. Since I tend to save multiple objects in one file from time to time in a tests fixtures file, I rarely output to a file. Here’s the code to output the object to a file if your interested, though:\n\ndput(\n  names(mtcars), \n  here::here(\"til/posts/2024-02-10-til-r-dput-store-objects/mtcars-names.R\")\n)\n\n\n\nOne more tip, if you use vim or nvim\nI’m particular with how I style long character vectors within a file. If the objects can’t fit on one line, each will be placed on their own line. So you can output your object and use the following substitution command to place each object on it’s own line.\n:.,+1s/, /,\\r/g\nThis command will make our object look like this:\n\n\n\nUse substitution to finish cleaning up the character vector\n\n\nIndeed, it’s not perfect, but it’s close. We only needed to make some minor edits to finish it. But in the end, we’ve saved so much time, and we have a well formatted character vector.\n\n\nWrap up\nI wish I came across dput() much earlier. Not only is it one of those entire workflow changing tips, it’s one that would have saved me so much time. Hopefully if you’re reading this post, you avoid hand creating large character vectors and just use base::dput().\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Use `Base::dput()` to Easily Create and Save Objects},\n  date = {2024-02-10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Use `Base::dput()` to Easily Create and\nSave Objects.” February 10, 2024."
  },
  {
    "objectID": "til/posts/2023-10-24-temp-directories/index.html",
    "href": "til/posts/2023-10-24-temp-directories/index.html",
    "title": "Using base::tempdir() for temporary data storage",
    "section": "",
    "text": "Photo by Jesse Orrico\n\n\nToday I learned how to store data in R’s per-session temporary directory.\nRecently, I’ve been working on an R package for a project. This package contains some internal data, which is intended to be updated from time-to-time. As part of the data update process, I’m required to download a set of .zip files from cloud storage, unzip, wrangle, and make the data available in the package via the data folder.\nGiven the data I’m working with, I wanted to avoid storing pre-wrangled data in the data-raw directory of the package. My main concern was an accidental check-in of pre-proccessed data into version control. So, I sought out a means to solve this problem.\nThis post aims to overview an approach using R’s per-session temporary directory to store data temporarily. Specifically, this post will discuss the use of base::tempdir() and other system file management functions made available in R to store data in this directory.\n\n\n\n\n\n\nWarning\n\n\n\nUsing R’s per-session temporary directory may not be the right solution for your specific situation. If you’re working with sensitive data, make sure you follow your organization’s guidelines on where to store, access, and properly use your data.\nI am not a security expert.\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat are temporary directories?\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, I drafted this post on a computer running a Mac operating system. Some of what gets discussed here may not apply to Windows or Linux systems. The ideas and application should be similar, though I haven’t fully explored the differences.\n\n\nThe temporary directory, simply, is a location on your system. You can store files in this location just like any other directory. The difference is data stored within a temporary directory are not meant to be persistent, and your system will delete them automatically. File deletion either occurs when the system is shut down or after a set amount of time.\nIf you’re working on a Mac operating system, you can get the path to the temporary directory by running the following in your terminal:\necho $TMPDIR\nWhen I last ran this command on my system, echo returned the following path (later we’ll use base::tempdir() to get and use this path in R).\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/\nThis directory is located at the system level. The cd command can be used to navigate to it from the terminal. You may have to back up a few directories if your root starts at the user level, though. This is pretty standard, especially if you’re working on a Mac.\n\n\n\n\n\n\nNote\n\n\n\nSince I’m drafting this post on my personal machine, I’m not aware if you need admin privileges to access this folder. As such, you may run into issues if you’re not an admin on your machine.\n\n\nWith my curiosity peaked, I sought more information about what this directory was used for on a MacOS. Oddly enough, there is very little about this directory online. From what I can deduce, the /var directory is mainly a per-user cache for temporary files, and it provides security benefits beyond other cache locations on a Mac system (again, I’m not a security expert, so my previous statement may be inaccurate). Being that this location is temporary, this cache gets cleared every time the system restarts or every three days.\nAlthough there’s a lack of information about this directory online, I did come across a few blog posts and a Stack Overflow answer that were helpful in understanding this temporary directory in more depth: post 1; post 2; post 3. You might find these useful if you want to learn more. However, for me, the above is as far as I wanted to go to understand its purpose.\n\n\nAccess the temporary directory using base::tempdir()\nAt the start of every session, R creates a temporary per-session directory, and it removes this temporary directory when the session ends.. This temporary directory is stored in the system’s temporary directory location (e.g., /var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/). R also provides several functions to work with the temporary directory, create, and interact with files within it.\nbase::tempdir() can be used to print the file path of the temporary directory on your system. Let’s run it and take a look at what happens.\n\ntempdir()\n\nOutputted to the terminal is the path to the R session’s temporary directory. When I ran it, the returned path looked like this:\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T//RtmpaYxspA\nThe temporary directory R uses for the current session is labeled using the RtmpXXXXXX pattern. The final six characters of the path (i.e., the Xs) are determined by the system. Note, tempdir() doesn’t create this directory, it just prints the temporary directory’s path to the console. This directory is created every time a R session begins.\nSince the temporary directory is just like any location on your computer, you can navigate to it from your terminal during an active R session. With your terminal pointing to the temporary directory, you can use the following code to find R’s per-session temporary directory:\nla | grep \"Rtmp\"\nLet’s take a peak at what’s in this directory. R’s list.files() function can be helpful in this case.\n\nlist.files(tempdir())\n\ncharacter(0)\n\n\nMost R setups should start with an empty per-session directory. So the above should return character(0). Despite being empty now, list.files() will become handy again once we start to write files to this location.\n\n\nWriting files to the temporary directory\nNow that we know a little more about this temp directory and where it is located on our system, let’s write some data to it. We can do this by doing something like the following.\n\nwrite_csv(mtcars, file = paste0(tempdir(), \"/mtcars.csv\"))\n\nNow when we list the files in the temporary directory (e.g., list.files(tempdir())), you should see the mtcars.csv file.\nIf you’re looking to create files with unique names, you can pass the tempfile() function to the file argument. This looks something like this:\n\nwrite_csv(\n  mtcars, \n  file = tempfile(pattern = \"mtcars\", fileext = \".csv\")\n)\n\ntempfile() creates unique file names, which concatenates together the file path, the character vector passed to the pattern argument, a random string in hex, and the character vector inputed to the fileext argument. When you list the files in the temporary directory now, you’ll see the initial mtcars.csv file along with a file that looks something like this: mtcars7eb3503ac74c.csv. The random hex string ensures files remain unique.\nIndeed, the above is just one way to write files to the temporary directory. You can use other methods to read and write files at this location. However, you now know what is needed to interact with this directory, read and write files to and from it. At this point you can do any data wrangling steps your project requires. After which, we can go about deleting our files from this directory.\n\n\nDeleting files with file.remove()\nAlthough these files will eventually be removed by the system, we should be proactive and clean up after ourselves.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using this approach within functions, especially if their intended to be used by other users, you’ll want to be clear they will write data to and remove data from the user’s system.\nIndeed, it’s considered poor practice to change the R landscape on a user’s computer without good reason. So the least we can do here is clean up after ourselves.\n\n\nTo delete our files we wrote to the temporary directory, run the following in the console:\n\nfile.remove(list.files(tempdir(), full.names = TRUE, pattern = \".csv\"))\n\n[1] TRUE TRUE\n\n\nThe arguments of the list.files() function should be pretty straightforward. We want file paths to be full length (i.e., full.names = TRUE) and to list only files with the .csv extension (i.e., pattern = \".csv\"). Then, we use these full file paths within the file.remove() function, which will remove the files from R’s temporary directory.\n\n\nWrap-up\nToday I learned more about R’s per-session temporary directory, and how it can be used to write files not intended for persistent storage. I also learned how to use several base R functions to create files within this temporary directory by using tempfile() and tempdir(). I also demonstrated how the list.files() function can be used to list files within any directory on your system, specifically using it to list files in R’s temporary directory. Finally, I highlighted how files in the temporary directory can be deleted using the file.remove() function.\nHave fun using R’s per-session temporary directory. Cheers 🎉!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Using `Base::tempdir()` for Temporary Data Storage},\n  date = {2023-11-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Using `Base::tempdir()` for Temporary Data\nStorage.” November 3, 2023."
  },
  {
    "objectID": "til/posts/2023-10-14-edit-old-commit/index.html",
    "href": "til/posts/2023-10-14-edit-old-commit/index.html",
    "title": "Edit an older unpushed commit",
    "section": "",
    "text": "Photo by Yancy Min\n\n\nToday I learned how to edit older unpushed commit messages using git rebase.\nI’ve been attempting to be better about linking git commits to specific GitHub issues. Although I try to be disciplined, I forget to reference the issue in the commit message from time-to-time. Luckily, I researched and came upon a solution. The purpose of this post is to briefly document what I’ve learned.\nA quick note: I am not a Git Fu master. The approach I share here (which I learned from a Stack Overflow post) worked for a small project not intended to be in production. In fact, there may be better approaches to solve this problem given your specific situation. I for sure want to avoid receiving angry messages where someone applied what is discussed, and it took down a critical, in production system. Thus, make sure you are aware of what these commands will do to your commit history before applying them.\n\nThe problem\nLet’s take a look at a log from a practice repo I created. I’m using git’s --pretty=format flag here to simplify the printed output for this post; a simple git log will also return the same information but in a more verbose way.\ngit log --pretty=format:\"%h %s %n%b\"\nThis returns the following log information. Printed to the console is a log containing the various commit’s abbreviated SHA-1 values, subjects, and message bodies.\n31964b0 fix-found_bug\n- #1\n\nf8256d6 feat-you_get_the_point\n- #1\n\nb1b99e9 feat-another_awesome_new_feat\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nShoot! I forgot to tag the b1b99e9 commit as being related to issue #1. How can I edit before I push?\n\n\nThe solution\ngit rebase can be used here to edit the past commit message. Again, keep in mind these commits have not been pushed to the remote repository.\nFirst, we need to target the commit we want to edit. git rebase, with the --interactive flag, and the abbreviated SHA-1 value of the commit to be edited is used to do this:\ngit rebase --interactive b1b99e9~\nThis command will open our system’s default text editor. In it should be something like the following:\npick b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n\n# Rebase 5d9b87c..31964b0 onto 5d9b87c (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\nYou’ll notice the instructions and different options (formatted as comments) are plentiful. I have yet to explore what all these operations can do (maybe a future post). But here, we are focused on editing a past commit message.\nThe next step in the process was a little confusing. With a bit of reading of the Stack Overflow post and a little experimentation, I found out we need to manually change any pick to edit for any commit intended to be edited in the currently open file. Our file will look something like this:\nedit b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n...\nWe save the file and close our editor. Once back in the terminal, we’ll be on the commit targeted for edits. To make our edits, submit the following to the terminal:\ngit commit --amend\nOnce ran, the text editor will be opened to the commit message we targeted for edits. We’ll then make our changes, save them, and exit the text editor.\nNow, we need to return to the previous HEAD commit. To do this, we run the following command in our terminal:\ngit rebase --continue\n\n\nRewriting history\nLet’s look at the log and view our changes. We can do that again by submitting the following to our terminal:\ngit log --pretty=format:\"%h %s %n%b\"\nBelow is what gets printed.\n709c173 fix-found_bug\n- #1\n\ne0ed7ba feat-you_get_the_point\n- #1\n\n179be4a feat-another_awesome_new_feat\n- #1\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nSuccess! All our commits are now associated with issue #1. However, take a moment to compare the SHA-1 values from our previous log with the current log. Notice anything different? The SHA-1 values for both our edited commit message and all its children have been modified. We have just re-written part of our commit history.\nImportant point: You can break repos doing this if you’re not careful. This re-writing of history should only be applied in cases with unpushed commit messages and when you’re not collaborating on a branch with other people. If you make edits to your history using this approach, you’ll want to make sure to avoid using commands like git push --force. See the original Stack Overflow post for more detail.\n\n\nWrap-up\nSo there you have it. A little Git Fu magic to help edit past, unpushed commit messages.\nIf you know a better approach or if my Git Fu is way off, let me know. I have far from mastered git.\nHappy rebasing!\n\n\nResources to learn more\n\nHow do I modify a specific commit? Stack Overflow post submitted by Sam Liao and top answer from ZelluX\nGit Rebase Interactive :: A Practical Example YouTube tutorial from EdgeCoders\n7.6 Git Tools - Rewriting History from the git documentation\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Edit an Older Unpushed Commit},\n  date = {2023-10-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Edit an Older Unpushed Commit.”\nOctober 14, 2023."
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Here you’ll find posts related to things I’ve learned recently.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse list2env() or glue::glue_data() to use a set of elements from a tibble in a string\n\n\n\n\n\n\ntil\n\n\nbase r\n\n\ndata wrangling\n\n\n\nNeed an easy way to access a set of elements from a tibble for string interpolation? Here’s two examples\n\n\n\n\n\nDec 30, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nSeparate character strings into rows and columns using tidyr functions\n\n\n\n\n\n\ntil\n\n\ntidyr\n\n\ndata wrangling\n\n\n\nNeed to separate strings? Use the separate_* family of functions\n\n\n\n\n\nDec 27, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nSummarize logical vectors to calculate numeric summaries\n\n\n\n\n\n\ntil\n\n\ndata wrangling\n\n\nlogical vectors\n\n\nsummary statistics\n\n\n\nNeed proportion and count summaries from a logical vector? Use mean() and sum()\n\n\n\n\n\nDec 8, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nUse base::dput() to easily create and save objects\n\n\n\n\n\n\nproductivity\n\n\nvim\n\n\ntesting\n\n\ndata wrangling\n\n\n\nNeed to create and store an object quickly, use this trick\n\n\n\n\n\nFeb 10, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nCombine plots using patchwork\n\n\n\n\n\n\ndata visualization\n\n\n\nNeed to add two or more plots together? Use the patchwork package\n\n\n\n\n\nDec 23, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing base::tempdir() for temporary data storage\n\n\n\n\n\n\ndata wrangling\n\n\nworkflow\n\n\nproductivity\n\n\n\nNeed to store data in a place that’s not persistent, use a temporary directory\n\n\n\n\n\nNov 3, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating correlations with corrr\n\n\n\n\n\n\ndata analysis\n\n\nexploratory analysis\n\n\ndata visualization\n\n\n\nUse the corrr package to calculate and visualize correlations\n\n\n\n\n\nOct 22, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nEdit an older unpushed commit\n\n\n\n\n\n\ngit\n\n\nGitHub\n\n\n\nUse git rebase to edit previous commit messages\n\n\n\n\n\nOct 14, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nFind and replace in Vim\n\n\n\n\n\n\nvim\n\n\nneovim\n\n\nproductivity\n\n\n\nImproving productivity by using Vim’s :substitute command\n\n\n\n\n\nFeb 24, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html",
    "title": "The Hex Update: October, 2024",
    "section": "",
    "text": "New here? Check out this post to learn more.\nI’m little behind on posting. Nonetheless, here’s an update of what I’ve found to be interesting in the media industry as of late. We’ve got cookies, AI in journalism, the sea of media choices, another pivot to video for news, the importance of culture for a media organization, the political information ecosystem, and the share of internet traffic for the Chrome browser.\nIf you’re short on time, here’s some quick takeaways:\nWanna stay a little longer? Check out my summary below."
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html#google-forgoes-eliminating-cookies-for-now.",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html#google-forgoes-eliminating-cookies-for-now.",
    "title": "The Hex Update: October, 2024",
    "section": "Google forgoes eliminating cookies, for now.",
    "text": "Google forgoes eliminating cookies, for now.\nStarting in 2020, Google announced it would seek to depreciate third-party cookies from the Chrome browser. Following this announcement, advertisers and publishers scrambled to figure out the impact such a change would have on their tracking setups. Given industry pushback, Google scrapped their plans and instead will now give users the ability to make an informed choice. Soon, users will be given a prompt to decide how tracking will be applied across Google’s search products, and it will make the setting available to users if they change their minds.\nTalk of third-party cookie depreciation hasn’t been all bad. It’s prompted further innovation of more privacy-minded tracking methods, though there is little industry consensus on the best path forward. It has, however, motivated a greater emphasis on the collection of first-party data, which has the potential to create significant value. The collection of this type of data is not without issues, though. Privacy, transparency, and security are still a concern when collecting this type of data, and some solutions to address these concerns–like data clean rooms–may not be feasible or economical for every organization.\n\nHere are some links to go deeper\n\nGoogle gives up trying to eliminate cookies (Axios)\nGoogle’s official announcement (The Privacy Sandbox)\nWTF is a data clean room? (Digiday’s WTF series)\n\n\n\nWhat’s the takeaway?\nLet’s be clear: we’re talking about third-party cookies here, which are different from first-party cookies. First-party cookies are the technology analytics services like Google Analytics rely on for tracking events on a publisher’s platforms. These are not going away, at least for the time being. Third-party cookies are different; they allow for cross-domain tracking (e.g., retargeting).\nDoes this really matter? The majority of the internet has already moved away from third-party cookies. Browsers other than Chrome (e.g., Safari, Firefox) have already depreciated third-party cookies, and the majority of Chrome users have already disabled third-party cookies. I think the value here is not the actual depreciation of third-party cookies, but rather the conversations it has motivated throughout the industry.\nThe bottom line: users expect privacy. This should be the standard. Advertisers and publishers, however, rely on data to create better experiences for users and audiences. Knowing this, then, how can advertisers, publishers, and platforms come to a consensus around a solution that works for both? It may be initiatives like Google’s Privacy Sandbox, Universal IDs, or a greater reliance on the collection and use of first-party data."
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html#peoples-perceptions-of-the-use-of-ai-in-journalism",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html#peoples-perceptions-of-the-use-of-ai-in-journalism",
    "title": "The Hex Update: October, 2024",
    "section": "People’s perceptions of the use of AI in journalism",
    "text": "People’s perceptions of the use of AI in journalism\nThe Reuters Institute recently released a report focused on the public’s attitudes towards the use of artificial intelligence (AI) in journalism. Its aim was to better understand how audiences’ perceive the use of AI–specifically generative AI–in the creation of news. Using both survey and qualitative responses, the report highlighted some key, nuanced findings.\nSurprisingly, when viewed globally, the report found just under half of respondents have read a large or moderate amount about AI (45%). Results for the US were slightly higher, where 53% reported reading a large or moderate amount about AI. Some differences were also present across demographic groups, especially when looking at younger and older age cohorts (Under 35 = 56%; Over 35 = 42%).\nWhen it comes to perceptions on the use of news produced using AI, only 23% of US respondents mentioned feeling comfortable consuming news mostly produced by AI (Neither/nor = 18%; Uncomfortable = 52%; Don’t know = 7%). Although this is a pretty broad response towards the use of AI in news, the report went further by stating participant’s level of comfort is nuanced when different AI use cases were considered in the creation of news.\nThree use cases of AI were presented to participants: behind the scenes work; content delivery; and content creation. According to the qualitative responses, many were comfortable with ‘behind the scenes’ uses of AI, but were less comfortable when it came to identifying ways to deliver news in new ways and formats, and they were least comfortable with the use of AI for content generation. In fact, many respondents believed a human should always be ‘in the loop’ during the content creation process. It’s also important to highlight that respondents’ reported having varying comfort levels for different types of content generation AI would be used for. In fact, the report states most respondents were ‘strongly opposed’ to the use of AI to create realistic-looking photographs or videos, even when disclosed. Finally, the article devotes a section to discussing people’s perceptions around the disclosure of the use of AI in news production.\n\nHere’s a link to go deeper\n\nPublic attitudes towards the use of AI in journalism | Reuters Institute for the Study of Journalism\n\n\n\nWhat’s the takeaway?\nAlthough AI affords many efficiencies and enhancements, news publishers must consider how audiences view the use of AI in news. Audiences are comfortable with the application of AI in cases of improved efficiency and the news consumption experience. However, this report makes it clear: audiences, at least for now, are not fully comfortable–even strongly opposed–with it being a part of specific types of news content production. This is especially relevant when AI is used to generate or edit realistic photographs and videos. There’s even negative attitudes towards using this type of content even when it’s clearly labelled by producers.\nIn sight of these results, it’s also important to consider consumers’ perceptions around disclosure: what is needed, even ethical, to effectively disclose the use of AI within the creation of news? Moreover, it’s important to consider how the type of disclosure relates to users’ trust in news. Certainly, there’s more open questions to explore than answers at this time."
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html#the-sea-of-media-choices-is-bundling-the-answer",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html#the-sea-of-media-choices-is-bundling-the-answer",
    "title": "The Hex Update: October, 2024",
    "section": "The sea of media choices, is bundling the answer?",
    "text": "The sea of media choices, is bundling the answer?\nIt’s obvious, audiences have a myriad of choices when it comes to consuming media. This isn’t a new or profound insight. But take a moment and count how many subscription services you have (really count). Now, consider the number of different sources that are accessible to consumers. On average, they have access to 13 different entertainment sources or subscriptions according to a survey performed by Hub Intel. It’s no surprise consumers have subscription fatigue, and media companies are seeking opportunities to address it. Bundling services is one such strategy, which at times has been dubbed “cable 2.0”.\nKnowing the extent of choice, what would the ideal bundle look like for audiences? A staggering fact,\n\nTV is no longer the center of the entertainment universe.\n\nIndeed, according to the survey’s results, only about 50% of those 13 sources were premium video. This was even less for younger audiences. When given the opportunity to create their own ‘preferred bundle’, respondents’ service bundles went beyond just video and entertainment sources. The most requested services to be included within a bundle were high-speed internet, Netflix, mobile phone service, streaming music subscription, and a MVPD/vMVPD network bundle with live TV.\n\nHere’s a link to go deeper\n\nBundles Are Back, Baby! (Hub Intel on Substack)\n\n\n\nWhat’s the takeaway?\nThere’s one main takeaway: the services audiences are willing to pay for and are prioritising in their media consumption diet are internet-based, rather than legacy type media services. We know this is the case because of how people responded to prompts about what they view as an ideal service bundle.\nIt’s also important to consider the power of bundling strategies. This includes non-traditional types of bundles. Think of one of the original successful bundles: TV, internet, and home phone. Not necessarily related services, but it was a bundle consumers found value in and were willing to pay for. It would be interesting to explore how bundling strategies could be implemented across various media organizations, even some non-traditional bundling might provide some interesting value to audiences."
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html#another-pivot-to-video",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html#another-pivot-to-video",
    "title": "The Hex Update: October, 2024",
    "section": "Another pivot to video?",
    "text": "Another pivot to video?\nAccording to a study published by the Reuters Institute for the Study of Journalism, video and video-led services are driving growth in news. Despite the lack of success of the first pivot to video for publishers, social media platforms are once again prioritizing video, especially short-form content. As suggested in an article from NiemanLab, many of the traditional social media platforms are modifying their platforms and products to prioritize short-form video content in a bid to compete with the massive growth of TikTok.\nNews consumption growth is not evenly spread across platforms, but rather it’s concentrated to a select few. Much of this growth, globally, is not coming from legacy social media networks, but it is coming from platforms like YouTube, TikTok, and Instagram. This growth is also being propelled by the younger audiences on these platforms. Zooming in on these groups, three motivations for using social video for news were identified:\n\nTrustworthiness and authenticity. The unfiltered nature of video makes the coverage more trustworthy and authentic.\nConvenience. They’re already on these platforms, and the algorithm acts as a filter to send content users find to be interesting.\nAccess to different perspectives. This includes perspectives from others that align with a user.\n\nDespite this growth, publishers creating content aimed to reach audiences on these platforms must confront specific issues. Capturing attention is first, much of which is going to influencers and celebrities and not necessarily journalists or news organizations. Even if news organizations capture user’s attention, they’re confronted with another big issue: the monetization of content on these platforms.\n\nHere are some links to go deeper\n\nIs the news industry ready for another pivot to video\nWhy Facebook And Mark Zuckerberg Went All In On Live Video\nDid Facebook’s faulty data push news publishers to make terrible decisions on video?\n\n\n\nWhat’s the takeaway?\nIt’s clear, video content is once again the priority for these platforms. Media organizations, especially news organizations, must confront this fact to remain relevant. Users are already on these platforms, and they receive certain gratifications from consuming video in these places. It’s a simple idea: be where your audience is and serve them with what they want. A simple thought in theory, but more challenging in terms of what’s needed to execute. Monetization strategies will also need to be considered if a third-party platform is relied on to reach and engage audiences."
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html#an-interview-with-netflixs-co-ceo-greg-peters",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html#an-interview-with-netflixs-co-ceo-greg-peters",
    "title": "The Hex Update: October, 2024",
    "section": "An interview with Netflix’s co-CEO Greg Peters",
    "text": "An interview with Netflix’s co-CEO Greg Peters\nThe Decoder Podcast held an interview with Netflix’s co-CEO Greg Peters. Nilay Patel, the host of Decoder, devoted some questions to Netflix’s culture and its view on where the company fits within the broader media and entertainment industry.\nWhile on the topic of culture, Reed Hastings’, co-founder of Netflix, 125-page powerpoint presentation was discussed. This document was the initial articulation of the culture Netflix strives to achieve. I highly suggest looking through it. Though it’s been refined since originally shared, many of the key tenets of the original document are present in the current version.\nA few sections of the document struck me. For one, the emphasis on Netflix’s culture being likened to a sports team rather than a family was attention grabbing. They strive to hire the best talent and expect their workforce to be high-performing based on several general values.\nOn a first read, some might view this type of culture as being overly competitive and ruthless. But upon a deeper read, you begin to notice this is not the case: it’s emphasis is about embracing an open, honest, growth mindset type of culture that focuses its talent on solving the right challenges that are meaningful to the business.\nThe weight culture takes within the organization was also an interesting listen. Greg Peters even mentioned that although culture, strategy, and execution are all needed for a successful organization, he would take a great culture over excellent strategy and execution. You can still have a successful company if you have a great culture and just mediocre strategy and execution.\nAlthough not directly addressed, the importance of understanding product market fit was an important theme that came up. Knowing where your organization and products fit within the market and the lives of your customers is critical. Additionally, product market fit was also connected back to the importance of culture. Although process is important, especially in some areas (e.g., finance and legal), it can create rigidity and brittleness. This limits media organization’s ability to move fast within the competitive entertainment industry and broader attention economy.\nProcess models were also addressed in the interview. It was posited that many of the models organizations use today were developed during the industrial revolution. These models certainly have their place in industries like manufacturing where you can iterate on a process to squeeze out as much efficiency as possible for a single product line. However, in the creative-inventive industry, things change quickly and pivots are required to stay relevant. These older models of process efficiency just don’t work in this environment. With this in mind, media organizations need to cultivate and embrace a culture that facilitates pivoting, so it can continue to provide products and experiences the market wants.\n\nHere’s a link to go deeper\n\nNetflix co-CEO Greg Peters on the future of streaming and where ads, AI, and games fit in -Netflix Culture–The Best Work of Our Lives\n\n\n\nWhat’s the takeaway\nBottom line: listen to the whole podcast episode. It covers too many interesting topics to give all the themes discussed fair coverage in this summary. If forced to choose a few topics to focus on while listening, I’d closely focus on the discussion around culture, understanding product market fit, and how these two relate for organizations operating in a creative-inventive industry.\nNetflix’s culture may not be the best fit for every organization. However, it’s still an interesting articulation of the importance of culture for the success of an organization. It’s also critical, when working in the ever-changing, fast-moving media industry to consider how culture influences a media’s organization’s ability to pivot quickly to meet the needs of the market."
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html#the-political-information-ecosystem",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html#the-political-information-ecosystem",
    "title": "The Hex Update: October, 2024",
    "section": "The political information ecosystem",
    "text": "The political information ecosystem\nThe Computational Social Science Lab at Penn (CSSLab) makes available some interesting interactive data visualizations mapping the political information ecosystem. Want to know more about the overall news consumption patterns for the US or your specific state? Check out this visualization here. Are you interested in exploring the level of partisan news echo chambers, the idea that media exposes people to overwhelmingly partisan and like-minded news content in your state? This data visualization is useful. Are you a news producer who is interested in seeing how news TV audiences’ diets are changing? This figure might be helpful.\nMore here.\n\nWhat’s the takeaway\nThe trends highlighting the shift in TV news audiences is striking. Reviewing the overall news consumption trend, visually it seems audiences are spending less and less time consuming news on television. Where are these audiences going? In short, they’re moving more to no or minimal news viewing. Is this due to a shift in viewing to more digital spaces? Or, are audiences simply not watching news as much? A question worth further exploration."
  },
  {
    "objectID": "blog/posts/2024-11-01-2024-10-hex-update/index.html#one-plot-to-ponder",
    "href": "blog/posts/2024-11-01-2024-10-hex-update/index.html#one-plot-to-ponder",
    "title": "The Hex Update: October, 2024",
    "section": "One plot to ponder 📈",
    "text": "One plot to ponder 📈\nWith all the talk about the end of the end of cookies in the Chrome browser, I wanted to know more about how much Chrome was being used in the United States. Specifically, I had the following questions: How much is the Chrome browser being used? What other browsers, if any, compete with Chrome? So, I sought out some data and created a plot to ponder.\nThis publicly available data comes from Statcounter Global Stats. In short, Statcounter is a web analytics service which aggregates browser usage based on page view measurements of sites who have installed the service on their website. You can read more about how they calculate these values here.\nHave a look at and ponder about this trend:\n\n\n\n\n\nData made available by Statcounter Global Stats under a Creative Commons Attribution-Share Alike 3.0 Unported License."
  },
  {
    "objectID": "til/posts/2024-12-08-til-data-wrangling-numeric-summaries-logical-vectors/index.html",
    "href": "til/posts/2024-12-08-til-data-wrangling-numeric-summaries-logical-vectors/index.html",
    "title": "Summarize logical vectors to calculate numeric summaries",
    "section": "",
    "text": "Photo by Ovidiu Creanga\n\n\n\nBackground\nToday I relearned you can easily calculate counts and proportions with a logical vector (e.g., c(TRUE, FALSE, FALSE, TRUE)) in R.\n\nlibrary(tidyverse)\nlibrary(ids)\n\nI’ve been re-reading the second edition of R for Data Science for a Data Science Learning Community bookclub (check us out). While reading Chapter 12: Logical vectors, I was reminded counts and proportions can be calculated from a logical vector.\nI wanted to share what I learned out loud, so others have another example. I also hope writing this post helps me remember it in the future.\n\n\nSummaries from logical vectors\nThe concept is simple:\n\nsum() gives the number of TRUEs and mean() gives the proportion of TRUEs (because mean() is just sum() divided by length())\n\nThis works because TRUE = 1 and FALSE = 0 in the R programming language.\nLet’s look at an example of this in action. We start by creating an example dataset, which builds on the example used in the book:\n\ndata_user_engagement &lt;- data.frame(\n  date = sort(rep(seq(as_date(\"2024-12-02\"), as_date(\"2024-12-08\"), by = 1), times = 100)),\n  user_id = rep(random_id(bytes = 4, n = 100), times = 7),\n  time_engaged_sec = sample(c(1:100), 700, replace = TRUE)\n) |&gt;\ntibble()\n\nThis dataset is loosely based on the domain I work in: digital analytics. It’s modeled after event-based timeseries data for a week of web site visits. The dataset contains the following columns:\n\ndate - The date the event occurred.\nuser_id - A 4-byte user ID.\ntime_engaged_sec - Time spent engaged during the event (e.g., time spent on a webpage).\n\nSome questions we might ask about this dataset are: How many daily events were considered low-engagement events for users? What proportion of events each day were users engaged? Here’s the code to answer these questions, leveraging the summarization of logical vectors:\n\ndata_user_engagement |&gt;\n  group_by(date) |&gt;\n  summarise(\n    count_low_engagement = sum(time_engaged_sec &lt;= 10, na.rm = TRUE),\n    proportion_engaged = mean(time_engaged_sec &gt;= 30, na.rm = TRUE)\n  )\n\n# A tibble: 7 × 3\n  date       count_low_engagement proportion_engaged\n  &lt;date&gt;                    &lt;int&gt;              &lt;dbl&gt;\n1 2024-12-02                   13               0.7 \n2 2024-12-03                    8               0.67\n3 2024-12-04                    7               0.78\n4 2024-12-05                   10               0.69\n5 2024-12-06                   10               0.72\n6 2024-12-07                   11               0.72\n7 2024-12-08                    7               0.79\n\n\nAt first glance, you might ask where are the logical vectors? They’re created in the sum() and mean() functions when we use the &lt;= and &gt;= operators. That is, the statement time_engaged_sec &lt;= 10 initially creates the logical vector in the background, and then the sum() or mean() is computed on that logical vector.\nPretty neat, huh ?!\n\n\nWrap up\nThere are many other uses for logical vectors, but this was the most useful one I recently relearned. Check out Chapter 12: Logical vectors from the R4DS book to learn more.\nOne more tool to add to the analysis tool box. Thanks for spending time learning with me.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Summarize Logical Vectors to Calculate Numeric Summaries},\n  date = {2024-12-08},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Summarize Logical Vectors to Calculate\nNumeric Summaries.” December 8, 2024."
  },
  {
    "objectID": "til/posts/2024-12-27-til-tidyr-separate-character-string/index.html",
    "href": "til/posts/2024-12-27-til-tidyr-separate-character-string/index.html",
    "title": "Separate character strings into rows and columns using tidyr functions",
    "section": "",
    "text": "Photo by Johnny Briggs"
  },
  {
    "objectID": "til/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_many",
    "href": "til/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_many",
    "title": "Separate character strings into rows and columns using tidyr functions",
    "section": "too_many",
    "text": "too_many\nSay we have a case where we don’t need the end of the path, specifically the -monthly_users portion. For example:\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\")\n)\n\nError in `separate_wider_delim()`:\n! Expected 3 pieces in each element of `path`.\n! 12 values were too long.\nℹ Use `too_many = \"debug\"` to diagnose the problem.\nℹ Use `too_many = \"drop\"/\"merge\"` to silence this message.\n\n\nWe get an error. The error is the result of having more data then there are columns to separate into. To address this, we need to pass different options to the too_many argument of the function. Let’s use too_many = \"debug\" to receive additional information on what needs to be fixed. Although the problem is pretty straightforward here, I wanted to show this option in case you’re confronted with a situation with a more complex separation.\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\"),\n  too_many = \"debug\"\n)\n\nWarning: Debug mode activated: adding variables `path_ok`, `path_pieces`, and `path_remainder`.\n\n\n# A tibble: 12 × 7\n   date       age_grp gender path                            path_ok path_pieces path_remainder\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;                           &lt;lgl&gt;         &lt;int&gt; &lt;chr&gt;         \n 1 2024_10_01 1925    f      2024_10_01-1925-f-monthly_users FALSE             4 -monthly_users\n 2 2024_11_01 1925    f      2024_11_01-1925-f-monthly_users FALSE             4 -monthly_users\n 3 2024_12_01 1925    f      2024_12_01-1925-f-monthly_users FALSE             4 -monthly_users\n 4 2024_10_01 1925    m      2024_10_01-1925-m-monthly_users FALSE             4 -monthly_users\n 5 2024_11_01 1925    m      2024_11_01-1925-m-monthly_users FALSE             4 -monthly_users\n 6 2024_12_01 1925    m      2024_12_01-1925-m-monthly_users FALSE             4 -monthly_users\n 7 2024_10_01 2635    f      2024_10_01-2635-f-monthly_users FALSE             4 -monthly_users\n 8 2024_11_01 2635    f      2024_11_01-2635-f-monthly_users FALSE             4 -monthly_users\n 9 2024_12_01 2635    f      2024_12_01-2635-f-monthly_users FALSE             4 -monthly_users\n10 2024_10_01 2635    m      2024_10_01-2635-m-monthly_users FALSE             4 -monthly_users\n11 2024_11_01 2635    m      2024_11_01-2635-m-monthly_users FALSE             4 -monthly_users\n12 2024_12_01 2635    m      2024_12_01-2635-m-monthly_users FALSE             4 -monthly_users\n\n\nThe too_many = \"debug\" outputs a tibble with some additional columns ( *_ok, *_pieces, *_remainder) of information. The * being the name of the column to be separated. These columns contain information to help us quickly diagnose the problem. Using our example data, we get the following:\nThis column is useful for identifying the presence of any variable length strings.\n\npath_ok provides a boolean to quickly identify cases where the separation failed.\npath_pieces represents the number of pieces resulting from separating the string.\npath_remainder shows what’s left after the separation is performed. This is useful for identifying if there’s any additional information you want to retain in additional columns.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough not applicable here, a neat trick to quickly identify the columns that didn’t separate as we expected is to use the following:\n\ndebug_path |&gt; filter(!x_ok)\n\n\n\nNow that we have additional information to help us figure out what’s going on, we can choose another option for the too_many argument to handle our specific case. We have two additional options beyond error and debug:\n\n\ndrop will drop the additional information that doesn’t fit into our newly specified columns.\n\nmerge will keep the additional information, but it will merge it with the data in the final column.\n\nLet’s observe both options:\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\"),\n  too_many = \"drop\"\n)\n\n# A tibble: 12 × 3\n   date       age_grp gender\n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; \n 1 2024_10_01 1925    f     \n 2 2024_11_01 1925    f     \n 3 2024_12_01 1925    f     \n 4 2024_10_01 1925    m     \n 5 2024_11_01 1925    m     \n 6 2024_12_01 1925    m     \n 7 2024_10_01 2635    f     \n 8 2024_11_01 2635    f     \n 9 2024_12_01 2635    f     \n10 2024_10_01 2635    m     \n11 2024_11_01 2635    m     \n12 2024_12_01 2635    m     \n\n\n\nseparate_wider_delim(\n  data = data_file_paths,\n  cols = path,\n  delim = \"-\",\n  names = c(\"date\", \"age_grp\", \"gender\"),\n  too_many = \"merge\"\n)\n\n# A tibble: 12 × 3\n   date       age_grp gender         \n   &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;          \n 1 2024_10_01 1925    f-monthly_users\n 2 2024_11_01 1925    f-monthly_users\n 3 2024_12_01 1925    f-monthly_users\n 4 2024_10_01 1925    m-monthly_users\n 5 2024_11_01 1925    m-monthly_users\n 6 2024_12_01 1925    m-monthly_users\n 7 2024_10_01 2635    f-monthly_users\n 8 2024_11_01 2635    f-monthly_users\n 9 2024_12_01 2635    f-monthly_users\n10 2024_10_01 2635    m-monthly_users\n11 2024_11_01 2635    m-monthly_users\n12 2024_12_01 2635    m-monthly_users\n\n\nEither operation is pretty straightforward: drop the additional information or merge what’s left in the newly created column. Nonetheless, it’s likely best to debug first. Knowing what’s going on with your separation before applying a fix can be useful, and it will help you avoid parsing mistakes."
  },
  {
    "objectID": "til/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_few",
    "href": "til/posts/2024-12-27-til-tidyr-separate-character-string/index.html#too_few",
    "title": "Separate character strings into rows and columns using tidyr functions",
    "section": "too_few",
    "text": "too_few\nNext, I want to highlight options for when you have too few data. Let’s go back to some college sports examples, specifically college basketball game log data. Such data might look like this. Take note of the W/L column. Not only are wins and losses denoted, but the variable may also contain info if the win occurred during an overtime period. Let’s mimic this structure in some example data.\n\ndata_bball_wl &lt;- tibble(\n  game = c(1:6),\n  team = c(\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\",\n    \"Nebraska\"\n  ),\n  w_l = c(\n    \"W\",\n    \"W (1 OT)\",\n    \"L\",\n    \"W (3 OT)\",\n    \"L (2 OT)\",\n    \"W\"\n  )\n)\n\ndata_bball_wl\n\n# A tibble: 6 × 3\n   game team     w_l     \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;   \n1     1 Nebraska W       \n2     2 Nebraska W (1 OT)\n3     3 Nebraska L       \n4     4 Nebraska W (3 OT)\n5     5 Nebraska L (2 OT)\n6     6 Nebraska W       \n\n\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ots\")\n)\n\nError in `separate_wider_delim()`:\n! Expected 2 pieces in each element of `w_l`.\n! 3 values were too short.\nℹ Use `too_few = \"debug\"` to diagnose the problem.\nℹ Use `too_few = \"align_start\"/\"align_end\"` to silence this message.\n\n\nError, so let’s debug what’s happening with our separation.\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ots\"),\n  too_few = \"debug\"\n)\n\nWarning: Debug mode activated: adding variables `w_l_ok`, `w_l_pieces`, and `w_l_remainder`.\n\n\n# A tibble: 6 × 7\n   game team     w_l      ots   w_l_ok w_l_pieces w_l_remainder\n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;lgl&gt;       &lt;int&gt; &lt;chr&gt;        \n1     1 Nebraska W        &lt;NA&gt;  FALSE           1 \"\"           \n2     2 Nebraska W (1 OT) 1 OT) TRUE            2 \"\"           \n3     3 Nebraska L        &lt;NA&gt;  FALSE           1 \"\"           \n4     4 Nebraska W (3 OT) 3 OT) TRUE            2 \"\"           \n5     5 Nebraska L (2 OT) 2 OT) TRUE            2 \"\"           \n6     6 Nebraska W        &lt;NA&gt;  FALSE           1 \"\"           \n\n\nJust what we thought, rows 1, 3, 5, and 6 don’t contain enough information to complete our operation of filling the ots variable. The separate_wider_delim() function has two options for the too_many argument to address this issue:\n\n\nalign_end adds NA at the start of short matches to pad to the correct length.\n\nalign_start adds NA at the end of the short matches to pad to the correct length.\n\nI’ll start with align_end first, just to demonstrate what it does, though this operation isn’t what we’re looking to do here. Then, I’ll show you align_start, the operation needed to complete our separation successfully.\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ot\"),\n  too_few = \"align_end\"\n)\n\n# A tibble: 6 × 4\n   game team     w_l   ot   \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n1     1 Nebraska &lt;NA&gt;  W    \n2     2 Nebraska W     1 OT)\n3     3 Nebraska &lt;NA&gt;  L    \n4     4 Nebraska W     3 OT)\n5     5 Nebraska L     2 OT)\n6     6 Nebraska &lt;NA&gt;  W    \n\n\n\nseparate_wider_delim(\n  data = data_bball_wl,\n  cols = \"w_l\",\n  delim = \" (\",\n  names = c(\"w_l\", \"ot\"),\n  too_few = \"align_start\"\n)\n\n# A tibble: 6 × 4\n   game team     w_l   ot   \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n1     1 Nebraska W     &lt;NA&gt; \n2     2 Nebraska W     1 OT)\n3     3 Nebraska L     &lt;NA&gt; \n4     4 Nebraska W     3 OT)\n5     5 Nebraska L     2 OT)\n6     6 Nebraska W     &lt;NA&gt; \n\n\nIn short, all the align selection does is modify where the NA will be placed, essentially modifying the padding to create a correct length for the separation to be valid.\nGreat, now that we’ve identified where to separate the columns, we just need to do some additional string manipulation to finish the wrangling of this data. Below are the wrangling steps I applied:\n\ndata_bball_wl |&gt;\n  separate_wider_delim(\n    cols = \"w_l\",\n    delim = \" (\",\n    names = c(\"w_l\", \"ot\"),\n    too_few = \"align_start\"\n  ) |&gt;\n  mutate(\n    n_ot = str_remove(ot, \" OT\\\\)\"),\n    ot = ifelse(is.na(ot), FALSE, TRUE),\n  )\n\n# A tibble: 6 × 5\n   game team     w_l   ot    n_ot \n  &lt;int&gt; &lt;chr&gt;    &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;\n1     1 Nebraska W     FALSE &lt;NA&gt; \n2     2 Nebraska W     TRUE  1    \n3     3 Nebraska L     FALSE &lt;NA&gt; \n4     4 Nebraska W     TRUE  3    \n5     5 Nebraska L     TRUE  2    \n6     6 Nebraska W     FALSE &lt;NA&gt;"
  },
  {
    "objectID": "til/posts/2024-12-30-til-base-r-variable-assignment-list2env/index.html",
    "href": "til/posts/2024-12-30-til-base-r-variable-assignment-list2env/index.html",
    "title": "Use list2env() or glue::glue_data() to use a set of elements from a tibble in a string",
    "section": "",
    "text": "Photo by Ben Neale\n\nMotivation\nToday I learned base R’s list2env() function can be used to assign a set of variables to the Global Environment.\nFor a personal project, I was creating a simple CRUD application using Shiny. The purpose of the application was pretty straightforward: to serve as a tool for entering data into a database. The app had the following requirements:\n\nProvide inputs as fields for users to enter data that will be stored in a database.\nDisplay entered data via the user interface for easy visual inspection before writing data to the database.\nInclude a ‘Submit’ button for the user to submit the data to the database.\n\nOn the back end of this simple app, data was stored in the Global Environment as a tibble, so it could be easily displayed via the application’s UI. The tibble only contained one row of data, where the values in the variable were to be written via a SQL INSERT statement upon the user hitting a ‘Submit’ button.\nWhile working on this part of the app, my initial approach confronted me with a code smell.\n\nlibrary(tidyverse)\nlibrary(glue)\n\nThis approach stinks\nThe additional nuances of the Shiny application are not important. Rather, let’s focus on the actual problem I was confronted with: how do you create Global Environment variables from an existing object, specifically a tibble in my case?\nHere’s some example data to work with:\n\ndata_employee &lt;- tibble(\n  first_name = \"John\",\n  last_name = \"Smith\",\n  start_date = \"2024-03-04\",\n  department = \"accounting\"\n)\n\nAt this point, there is one object in the Global Environment, data_employee. To prove this, let’s submit ls() to the console, which will print all the objects in our current Global Environment.\n\nls()\n\n[1] \"data_employee\"\n\n\nWhat if I also wanted the tibble’s variables to be their own objects? That is, I wanted code resulting in four objects being made available in the Global Environment, each containing a value from a variable in the tibble: first_name, last_name, start_date, and department.\nMy overall aim in doing this was to pass the values of these variables to a SQL INSERT statement using the glue() package:\n\nquery_insert &lt;- glue(\"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\")\n\nMy initial solution was to do this:\n\nfirst_name &lt;- data_employee[[\"first_name\"]]\nlast_name  &lt;- data_employee[[\"last_name\"]]\nstart_date &lt;- data_employee[[\"start_date\"]]\ndepartment &lt;- data_employee[[\"department\"]]\n\nIndeed, we can confirm this works by once again submitting ls() to the console.\n\nls()\n\n[1] \"data_employee\" \"department\"    \"first_name\"    \"last_name\"     \"start_date\"   \n\n\nFurther confirmation results from inspecting the SQL INSERT statement string outputted from our use of the glue() function.\n\nquery_insert &lt;- glue(\"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\")\n\nquery_insert\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n)\n\n\nThis approach, though it works and isn’t too cumbersome for this specific example, it stinks and feels off. This especially became apparent when writing this out for the 10 fields of data I wanted to store within a database. I even physically cringed when I implemented it within the context of my application. There had to be a better way.\nLet’s start fresh by clearing the Global Environment, but keep our data_employee tibble to try another approach:\n\n\n\n\n\n\nNote\n\n\n\nI wouldn’t do this in my actual code. But I’m doing it here to better highlight the example.\n\n\n\nrm(\n  first_name,\n  last_name,\n  start_date,\n  department,\n  query_insert\n)\n\nls()\n\n[1] \"data_employee\"\n\n\nBase R’s list2env()\n\nlist2env() was the solution I was looking for. Here’s the description from the function’s documentation:\n\nFrom a named list x, create an envrionment containing all list components as objects, or “multi-assign” from x into a pre-existing environment.\n\nA little esoteric, so I found the following resources to be quite helpful:\n\nIteratively create global environment objects from tibble (Stack Overflow post)\nDynamic variable assignment in R\n\nEnvironments are an advanced topic, though a little context is helpful. Environments are just like any other data structure in R, but they serve as fenced object containers that can hold objects (my shallow interpretation). The Global Environment is one such container that can hold objects for an R session, though additional named environments could be created. As such, list2env() provides functionality to write named objects stored from a list to any environment we specify. Review Chapter 7: Environments from the Advanced R book for additional detail.\nUsing these concepts and list2env(), here’s how I fixed my code smell:\n\nlist2env(data_employee, .GlobalEnv)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\nls()\n\n[1] \"data_employee\" \"department\"    \"first_name\"    \"last_name\"     \"start_date\"   \n\n\n\nquery_insert &lt;- glue(\"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\")\n\nquery_insert\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n)\n\n\n🤯.\nWhat was once ~10 lines of messy, smelly code is now a one-liner. I was shook upon learning this.\nWrap up\nThe take away from this TIL is a game changer: use list2env() if you need to convert existing list elements into objects in your Global Environment. I certainly was witness to its utility when trying to solve my own code smell. I hope you can find a use for it in your own work.\nUntil next time, cheers! 🎉\nA follow up: glue::glue_data()\n\nThanks to the power of community, Tan Ho shared an even less anti-pattern-ish / code-smell-ish solution to my problem in the Data Science Learning Community’s Slack channel. He suggested using glue::glue_data().\nHere’s how it works:\n\nglue_data(\n  data_employee,\n  \"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n\")\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n\n\n🤯🤯.\nThis works as expected and comes with a couple added benefits. For one, the code’s simpler. In addition, we’re no longer creating variables that only get used once. The Global Environment is now much cleaner.\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Use `List2env()` or `Glue::glue\\_data()` to Use a Set of\n    Elements from a Tibble in a String},\n  date = {2024-12-30},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Use `List2env()` or `Glue::glue_data()` to\nUse a Set of Elements from a Tibble in a String.” December 30,\n2024."
  },
  {
    "objectID": "til/posts/2024-12-30-til-base-r-list2env-glue-data-string-interpolation/index.html",
    "href": "til/posts/2024-12-30-til-base-r-list2env-glue-data-string-interpolation/index.html",
    "title": "Use list2env() or glue::glue_data() to use a set of elements from a tibble in a string",
    "section": "",
    "text": "Photo by Ben Neale\n\nMotivation\nToday I learned base R’s list2env() function can be used to assign a set of variables to the Global Environment.\nFor a personal project, I was creating a simple CRUD application using Shiny. The purpose of the application was pretty straightforward: to serve as a tool for entering data into a database. The app had the following requirements:\n\nProvide inputs as fields for users to enter data that will be stored in a database.\nDisplay entered data via the user interface for easy visual inspection before writing data to the database.\nInclude a ‘Submit’ button for the user to submit the data to the database.\n\nOn the back end of this simple app, data was stored in the Global Environment as a tibble, so it could be easily displayed via the application’s UI. The tibble only contained one row of data, where the values in the variable were to be written via a SQL INSERT statement upon the user hitting a ‘Submit’ button.\nWhile working on this part of the app, my initial approach confronted me with a code smell.\n\nlibrary(tidyverse)\nlibrary(glue)\n\nThis approach stinks\nThe additional nuances of the Shiny application are not important. Rather, let’s focus on the actual problem I was confronted with: how do you create Global Environment variables from an existing object, specifically a tibble in my case?\nHere’s some example data to work with:\n\ndata_employee &lt;- tibble(\n  first_name = \"John\",\n  last_name = \"Smith\",\n  start_date = \"2024-03-04\",\n  department = \"accounting\"\n)\n\nAt this point, there is one object in the Global Environment, data_employee. To prove this, let’s submit ls() to the console, which will print all the objects in our current Global Environment.\n\nls()\n\n[1] \"data_employee\"\n\n\nWhat if I also wanted the tibble’s variables to be their own objects? That is, I wanted code resulting in four objects being made available in the Global Environment, each containing a value from a variable in the tibble: first_name, last_name, start_date, and department.\nMy overall aim in doing this was to pass the values of these variables to a SQL INSERT statement using the glue() package:\n\nquery_insert &lt;- glue(\"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\")\n\nMy initial solution was to do this:\n\nfirst_name &lt;- data_employee[[\"first_name\"]]\nlast_name  &lt;- data_employee[[\"last_name\"]]\nstart_date &lt;- data_employee[[\"start_date\"]]\ndepartment &lt;- data_employee[[\"department\"]]\n\nIndeed, we can confirm this works by once again submitting ls() to the console.\n\nls()\n\n[1] \"data_employee\" \"department\"    \"first_name\"    \"last_name\"     \"start_date\"   \n\n\nFurther confirmation results from inspecting the SQL INSERT statement string outputted from our use of the glue() function.\n\nquery_insert &lt;- glue(\"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\")\n\nquery_insert\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n)\n\n\nThis approach, though it works and isn’t too cumbersome for this specific example, it stinks and feels off. This especially became apparent when writing this out for the 10 fields of data I wanted to store within a database. I even physically cringed when I implemented it within the context of my application. There had to be a better way.\nLet’s start fresh by clearing the Global Environment, but keep our data_employee tibble to try another approach:\n\n\n\n\n\n\nNote\n\n\n\nI wouldn’t do this in my actual code. But I’m doing it here to better highlight the example.\n\n\n\nrm(\n  first_name,\n  last_name,\n  start_date,\n  department,\n  query_insert\n)\n\nls()\n\n[1] \"data_employee\"\n\n\nBase R’s list2env()\n\nlist2env() was the solution I was looking for. Here’s the description from the function’s documentation:\n\nFrom a named list x, create an envrionment containing all list components as objects, or “multi-assign” from x into a pre-existing environment.\n\nA little esoteric, so I found the following resources to be quite helpful:\n\nIteratively create global environment objects from tibble (Stack Overflow post)\nDynamic variable assignment in R\n\nEnvironments are an advanced topic, though a little context is helpful. Environments are just like any other data structure in R, but they serve as fenced object containers that can hold objects (my shallow interpretation). The Global Environment is one such container that can hold objects for an R session, though additional named environments could be created. As such, list2env() provides functionality to write named objects stored from a list to any environment we specify. Review Chapter 7: Environments from the Advanced R book for additional detail.\nUsing these concepts and list2env(), here’s how I fixed my code smell:\n\nlist2env(data_employee, .GlobalEnv)\n\n&lt;environment: R_GlobalEnv&gt;\n\n\n\nls()\n\n[1] \"data_employee\" \"department\"    \"first_name\"    \"last_name\"     \"start_date\"   \n\n\n\nquery_insert &lt;- glue(\"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n  )\n\")\n\nquery_insert\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n)\n\n\n🤯.\nWhat was once ~10 lines of messy, smelly code is now a one-liner. I was shook upon learning this.\nWrap up\nThe take away from this TIL is a game changer: use list2env() if you need to convert existing list elements into objects in your Global Environment. I certainly was witness to its utility when trying to solve my own code smell. I hope you can find a use for it in your own work.\nUntil next time, cheers! 🎉\nA follow up: glue::glue_data()\n\nThanks to the power of community, Tan Ho shared an even less anti-pattern-ish / code-smell-ish solution to my problem in the Data Science Learning Community’s Slack channel. He suggested using glue::glue_data().\nHere’s how it works:\n\nglue_data(\n  data_employee,\n  \"\n  INSERT INTO employees (\n      first_name,\n      last_name,\n      start_date,\n      department\n  )\n  VALUES (\n      {first_name},\n      {last_name},\n      {start_date},\n      {department}\n\")\n\nINSERT INTO employees (\n    first_name,\n    last_name,\n    start_date,\n    department\n)\nVALUES (\n    John,\n    Smith,\n    2024-03-04,\n    accounting\n\n\n🤯🤯.\nThis works as expected and comes with a couple added benefits. For one, the code’s simpler. In addition, we’re no longer creating variables that only get used once. The Global Environment is now much cleaner.\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {Use `List2env()` or `Glue::glue\\_data()` to Use a Set of\n    Elements from a Tibble in a String},\n  date = {2024-12-30},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “Use `List2env()` or `Glue::glue_data()` to\nUse a Set of Elements from a Tibble in a String.” December 30,\n2024."
  }
]