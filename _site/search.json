[
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A now page, why?"
  },
  {
    "objectID": "now.html#projects-im-working-on",
    "href": "now.html#projects-im-working-on",
    "title": "Now",
    "section": "Projects I’m working on",
    "text": "Projects I’m working on\n\nIt’s a Python summer\nThis summer I’ve been focusing on developing my Python programming skills. I have a pretty good handle of R, so I felt it was time to learn Python. This started with learning Python’s basic data types, struggling through understanding Numpy arrays, getting a handle on the extensive use cases of the pandas library, and learning how to manage environments using conda. I’m aiming to write more blog posts focused in this space to document what I’m learning.\n\n\nContinuing to experiment with Neovim\nI’ve heavily leaned into using Neovim for most of my workflows. During this transition, I’ve come across some really useful tools, like telescope, harpoon, and vim-fugitive. I’m trying to learn as much as I can, as I still don’t know if my configuration is correct … lol\n\n\nAttended Posit::conf(2023) Chicago\nI was fortunate to attend this year’s Posit conference in Chicago, a five day conference with workshops and speaker sessions. I’m still sorting through all that I learned. Here are some of the highlights:\n\nAttending the Introduction to Tidymodels workshop\nAttending the Package Development Masterclass workshop\nAll the keynote speakers, especially J.D. Long’s It’s Abstractions All the Way Down talk.\nNetworking and meeting some folks from the R4DS Online Learning Community (in person)."
  },
  {
    "objectID": "now.html#books-im-reading",
    "href": "now.html#books-im-reading",
    "title": "Now",
    "section": "Books I’m reading",
    "text": "Books I’m reading\n\nPython for Data Analysis by Wes McKinney\nBeing that it is a Python summer, I picked up this book as a starter to learn and practice using Python for data analysis. Currently, I’m working through the data cleaning, preparation, and wrangling material. I’m hoping to get to the chapters on modelling towards the end of the summer.\n\n\nTidy Modeling with R by Max Kuhn and Julia Silge\nWhile attending the Posit::conf(2023), I participated in a tidymodels workshop. As a result, I became more interested in how this framework and collection of packages could be applied to my own modeling and machine learning work. I’m specifically excited to learn more about the framework’s feature engineering functionality and predictive modeling capabilities.\n\n\nThe Caves of Steel by Max Kuhn and Julia Silge\nI’m continuing to read more of Isaac Asimov’s work. It started with I, Robot. Now I’ve begun reading the first book in his Robot Series. I’ve recognized I really enjoy Science Fiction."
  },
  {
    "objectID": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "href": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "title": "Now",
    "section": "A list of books I’ve read (ever since I’ve started keeping track)",
    "text": "A list of books I’ve read (ever since I’ve started keeping track)\n\nProfessional development reads\n\nEngineering Production-Grade Shiny Apps by Colin Fay, Sébastien Rochette, Vincent Guyader, and Cervan Girard\n\n\nAdvanced R by Hadley Wickham. Check out past book club meeting recordings here.\n\n\nR Packages by Hadley Wickham and Jenny Bryan. Check out past book club meeting recordings here.\n\n\nVim help files maintained by Carlo Teubner\n\n\nMastering Ubuntu by Jay LaCroix\n\n\nGoogle BigQuery: The Definitive Guide by Valliappa Lakshmanan and Jordan Tigani\n\n\nMastering Shiny by Hadley Wickham. Check out the past book club meeting recordings here.\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. Check out the past book club meeting recordings here.\n\n\nDocker Deep Dive by Nigel Poulton\n\n\n\nPersonal reads\n\nI, Robot by Isaac Asimov\n\n\nLeviathan Falls (The Expanse book 9) by James S.A. Corey\n\n\nTiamat’s Wrath (The Expanse book 8) by James S.A. Corey\n\n\nPersepolis Rising (The Expanse book 7) by James S.A. Corey\n\n\nBabylon’s Ashes (The Expanse book 6) by James S.A. Corey\n\n\nNemesis Games (The Expanse book 5) by James S.A. Corey\n\n\nCibola Burn (The Expanse book 4) by James S.A. Corey\n\n\nAbaddon’s Gate (The Expanse book 3) by James S.A. Corey\n\n\nCaliban’s War (The Expanse book 2) by James S.A. Corey\n\n\nLeviathon Wakes (The Expanse book 1) by James S.A. Corey\n\n\nThe Galaxy, and the Ground Within: A Novel (Wayfarers 4) by Becky Chambers\n\n\nRecord of a Spaceborn Few (Wayfarers 3) by Becky Chambers\n\n\nA Closed and Common Orbit (Wayfarers 2) by Becky Chambers\n\n\nThe Long Way to a Small, Angry Planet (Wayfarers 1) by Becky Chambers\n\n\nLast of the Breed by Louis L’Amour\n\n\nProject Hail Mary by Andy Weir\n\n\nFirebreak by Nicole Kornher-Stace\n\n\nDune Messiah by Frank Herbert\n\n\nDune by Frank Herbert\n\n\nThe Martian: A Novel by Andy Weir"
  },
  {
    "objectID": "til/posts/2023-10-14-edit-old-commit/index.html",
    "href": "til/posts/2023-10-14-edit-old-commit/index.html",
    "title": "Edit an older unpushed commit",
    "section": "",
    "text": "Photo by Yancy Min\n\n\nToday I learned how to edit older unpushed commit messages using git rebase.\nI’ve been attempting to be better about linking git commits to specific GitHub issues. Although I try to be disciplined, I forget to reference the issue in the commit message from time-to-time. Luckily, I researched and came upon a solution. The purpose of this post is to briefly document what I’ve learned.\nA quick note: I am not a Git Fu master. The approach I share here (which I learned from a Stack Overflow post) worked for a small project not intended to be in production. In fact, there may be better approaches to solve this problem given your specific situation. I for sure want to avoid receiving angry messages where someone applied what is discussed, and it took down a critical, in production system. Thus, make sure you are aware of what these commands will do to your commit history before applying them.\n\nThe problem\nLet’s take a look at a log from a practice repo I created. I’m using git’s --pretty=format flag here to simplify the printed output for this post; a simple git log will also return the same information but in a more verbose way.\ngit log --pretty=format:\"%h %s %n%b\"\nThis returns the following log information. Printed to the console is a log containing the various commit’s abbreviated SHA-1 values, subjects, and message bodies.\n31964b0 fix-found_bug\n- #1\n\nf8256d6 feat-you_get_the_point\n- #1\n\nb1b99e9 feat-another_awesome_new_feat\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nShoot! I forgot to tag the b1b99e9 commit as being related to issue #1. How can I edit before I push?\n\n\nThe solution\ngit rebase can be used here to edit the past commit message. Again, keep in mind these commits have not been pushed to the remote repository.\nFirst, we need to target the commit we want to edit. git rebase, with the --interactive flag, and the abbreviated SHA-1 value of the commit to be edited is used to do this:\ngit rebase --interactive b1b99e9~\nThis command will open our system’s default text editor. In it should be something like the following:\npick b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n\n# Rebase 5d9b87c..31964b0 onto 5d9b87c (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\nYou’ll notice the instructions and different options (formatted as comments) are plentiful. I have yet to explore what all these operations can do (maybe a future post). But here, we are focused on editing a past commit message.\nThe next step in the process was a little confusing. With a bit of reading of the Stack Overflow post and a little experimentation, I found out we need to manually change any pick to edit for any commit intended to be edited in the currently open file. Our file will look something like this:\nedit b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n...\nWe save the file and close our editor. Once back in the terminal, we’ll be on the commit targeted for edits. To make our edits, submit the following to the terminal:\ngit commit --amend\nOnce ran, the text editor will be opened to the commit message we targeted for edits. We’ll then make our changes, save them, and exit the text editor.\nNow, we need to return to the previous HEAD commit. To do this, we run the following command in our terminal:\ngit rebase --continue\n\n\nRewriting history\nLet’s look at the log and view our changes. We can do that again by submitting the following to our terminal:\ngit log --pretty=format:\"%h %s %n%b\"\nBelow is what gets printed.\n709c173 fix-found_bug\n- #1\n\ne0ed7ba feat-you_get_the_point\n- #1\n\n179be4a feat-another_awesome_new_feat\n- #1\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nSuccess! All our commits are now associated with issue #1. However, take a moment to compare the SHA-1 values from our previous log with the current log. Notice anything different? The SHA-1 values for both our edited commit message and all its children have been modified. We have just re-written part of our commit history.\nImportant point: You can break repos doing this if you’re not careful. This re-writing of history should only be applied in cases with unpushed commit messages and when you’re not collaborating on a branch with other people. If you make edits to your history using this approach, you’ll want to make sure to avoid using commands like git push --force. See the original Stack Overflow post for more detail.\n\n\nWrap-up\nSo there you have it. A little Git Fu magic to help edit past, unpushed commit messages.\nIf you know a better approach or if my Git Fu is way off, let me know. I have far from mastered git.\nHappy rebasing!\n\n\nResources to learn more\n\nHow do I modify a specific commit? Stack Overflow post submitted by Sam Liao and top answer from ZelluX\nGit Rebase Interactive :: A Practical Example YouTube tutorial from EdgeCoders\n7.6 Git Tools - Rewriting History from the git documentation\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Edit an Older Unpushed Commit},\n  date = {2023-10-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Edit an Older Unpushed Commit.”\nOctober 14, 2023."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html",
    "href": "til/posts/2023-03-14-vim-substitution/index.html",
    "title": "Find and replace in Vim",
    "section": "",
    "text": "Today I learned how to find and replace in Vim. I’ve found knowing a few variations of the substitute (:s or su for short) command to be a powerful skill to quickly and efficiently edit code and text within a file. By knowing a few simple command variations, you can greatly improve your productivity. You just have to know the different patterns and when to apply them.\nThis TIL post aims to highlight some of the basics of using Vim’s :s command. My intention is to get you up and running quickly. As such, this post provides several simple examples applying the command to some practical use cases. Although most of the examples use the R programming language, these concepts can be applied to any programming language or text editing task.\nThis post focuses on the basics. Indeed, the substitute command provides a lot of utility and different options to perform various find and replace editing tasks. If you’re looking to learn more advanced features, I suggest reading the docs (:help substitute). I also provide some additional links to other resources throughout and at the end of the post if you’re interested in learning more."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "title": "Find and replace in Vim",
    "section": "The basics",
    "text": "The basics\n:s can be used to find each occurance of a text string, and replace it with another text string. Say I have a character vector basket, and it contains an assortment of fruit. However, what if I want to replace the first apple in my basket with an orange using :s? First, I need to move my cursor to the line I want to find the first string. Then, I can enter the following into the command prompt to find the first instance of the string orange and replace it with the string apple:\n:s/orange/apple\nHere is what this looks like in action.\n\nHowever, what if I don’t want any oranges, and instead I just want apples rather than orangesin my basket. I can append the previous command with g to replace all instances of orange with apple. The g flag indicates to Vim that I want to replace globally to the current line. In other words, replace all instances on the current line.\n:s/orange/apple/g\nBelow is what this will look like in your editor.\n\nWant to find and replace text globally to the line and including multiple lines, then add % to the beginning of the command.\n:%s/orange/apple/g\n\n% is really useful if you want to refactor code efficiently within a file. Check out these two examples, one more contrived, the other a more practical, common application.\n:%s/power/horsepower/g\n:%s/data/cars_data/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "title": "Find and replace in Vim",
    "section": "Confirming replacement",
    "text": "Confirming replacement\nNot sure what all will be replaced and would rather go through each replacement step-by-step? Add c to the end of your command. Adding this flag will make Vim prompt you to confirm each replacement.\n:%s/orange/apple/gc\n\nIn the prompt, you’ll see something like replace with apple (y/n/a/q/l/^E/^Y). You’ll select the option that fulfills the action you want to perform. Here is a list of what each selection does:\n\ny - substitute this one match and move to the next (if any).\nn - skip this match and move to the next (if any).\na - substitue all (and it’s all matches) remaining matches.\nq - quit out of the prompt.\nl - subsitute this one match and quit. l is synonymous with “last”.\n^E - or Ctrl-e will scroll up.\n^Y - or Ctrl-y will scroll down.\n\nThe example above only highlights the use of y, so I suggest experimenting with each selection to get a feel for what they do."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "title": "Find and replace in Vim",
    "section": "Replacing by range",
    "text": "Replacing by range\nTake a look at the command pattern again, specifically the first portion, [range]:\n:[range]s[ubstitute]/{pattern}/{string}/[flags] count\nThe s command provides functionality to scope the find and replace operation to a specific part of your file. Indeed, this functionality was highlighted earlier when we passed % in an earlier command. % just indicated to Vim that we wanted to find and replace all lines in the file. However, we can be more specific.\nSay we now have a much larger basket, one that can hold both fruits and veggies. In the R programming language, this can be modeled using a tribble from the tribble package.\nWhat if we wanted to find the first two instances of carrots in our basket and replace it with kale. This can be done by passing a range at the start of the :s command. In this specific instance, I want to find and replace the carrots on lines 5 and 7 with the string kale, but I don’t want to change the one on line 8. To do this, I can run the following command:\n:5,7s/carrot/kale/g\n\nAnother variation is to start on your current line . and specify to Vim how many additional lines I would like to find and replace in the range. Keep in mind . represents the current line your cursor is located currently within the file. Once you postion your cursor, your command will look something like this:"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "title": "Find and replace in Vim",
    "section": "Replacing from current location to n lines",
    "text": "Replacing from current location to n lines\n:.,+2s/carrot/kale/g\n\nWhat if I had a basket with even more fruits and veggies, and I just wanted to start at my current location and replace all instances that follow? We can use the $ in the range input. The use of the dollar sign indicates to Vim that we want to replace starting at line 8 and go to the end of the file."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "title": "Find and replace in Vim",
    "section": "Replacing to end of file",
    "text": "Replacing to end of file\n:8,$s/carrot/kale/g\n\nOr, if you want to start from the current line and replace to the end of the file, you can do the following:\n:.,$s/carrot/kale/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "title": "Find and replace in Vim",
    "section": "Replacing using visual mode",
    "text": "Replacing using visual mode\nWe can also use visual mode to set the range of the find and replace operation. Just enter visual mode v or visual line mode Shift-v, highlight the range you want your find and replace operation to be applied, enter into command mode with :, and then enter your find and replace statement. Doing this will start the command line off with '&lt;,'&gt;, and you’ll just need to enter the rest of the command, the {pattern} and {string} portions.\n:'&lt;,'&gt;s/carrot/kale`"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "title": "Find and replace in Vim",
    "section": "Use objects in your search buffer",
    "text": "Use objects in your search buffer\nYour previous search history can also be used to do find and replace. Let’s go back to our miles-per-gallon plot example again. First I’ll hover my cursor over the word I want to replace and hit *. Now we can use the search value in our subsititution command. All I need to do is leave the {pattern} blank in the command. The command will look like this:\n:%s//horsepower"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "title": "Find and replace in Vim",
    "section": "Replace with what’s under your cursor",
    "text": "Replace with what’s under your cursor\nTo keep things simple, let’s go back to our first basket example. Specifically, let’s say I want to modify the string strawberry with the string banana, but use my cursor position to do this. First I have to make sure the cursor is hovering over the word I want to use for my replacement. Then, I enter the below command. When you see &lt;c-r&gt;&lt;c-w&gt;, this means you actually hit Ctrl-R and Ctrl-W on your keyboard. You’ll notice the string banana is populated into the command for us.\n%s/strawberry/&lt;c-r&gt;&lt;c-w&gt;"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "2023 data science rig: Set up and configuration\n\n\n\n\n\n\n\ntutorial\n\n\n\n\nOverviewing and reflecting on my current data science setup.\n\n\n\n\n\n\nJan 29, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nFlattening Google Analytics 4 data\n\n\n\n\n\n\n\nBigQuery\n\n\nsql\n\n\ndata wrangling\n\n\n\n\nLet’s deep dive into working with Google Analytics data stored in BigQuery.\n\n\n\n\n\n\nSep 20, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nExploratory analysis of Google Analytics 4 data for forecasting models\n\n\n\n\n\n\n\nforecasting\n\n\n\n\nExploring Google Analytics 4 data for forecasting models.\n\n\n\n\n\n\nMar 3, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nShiny summary tiles\n\n\n\n\n\n\n\nshiny\n\n\n\n\nBuilding custom metric summary tiles for Shiny.\n\n\n\n\n\n\nDec 30, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n2021 PBS TechCon: Your Data is Disgusting!\n\n\n\n\n\n\n\ntalks\n\n\n\n\nI was fortunate to be invited to present about topics I’m passionate about: tidy data and data pipelines.\n\n\n\n\n\n\nOct 19, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nImplementing a next and back button in Shiny\n\n\n\n\n\n\n\nshiny\n\n\n\n\nTaking the time to understand a challenging question from Mastering Shiny.\n\n\n\n\n\n\nSep 12, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nIntro Post\n\n\n\n\n\n\n\npersonal\n\n\n\n\nHello World!, my name is Collin, and this is my blog.\n\n\n\n\n\n\nApr 2, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "",
    "text": "Photo by T K\nI had the great fortune of being a presenter at this year’s PBS TechCon conference. The focus of my talk was to introduce attendees to the principles of tidy data and discuss a data pipeline project my team has been working on at Nebraska Public Media. Here’s the session description:\nAs part of my talk, I mentioned having put together a curated list of resources others could use to learn more about the topics covered. This list can be found in the following section of this blog post. If you’re interested in discussing these topics further, please reach out."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nTidy Data paper published in the Journal of Statistical Software written by Hadley Wickham\nTidy Data Chapter published in the open source R for Data Science book written by Hadley Wickham and Garrett Grolemun\nData Organization: Organizing Data in Spreadsheets post by Karl Broman\nData Organization: Organizing Data in Spreadsheets paper published in The American Statistician written by Karl Broman and Kara Woo\nTidy data section in Data Management in Large-Scale Education Research training modules written by Crystal Lewis\nTidy Data presented by Hadley Wickham\nTowards Data Science post published on Medium summarizing tidy data written by Benedict Neo"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Join a Community",
    "text": "Join a Community\n\nR for Data Science Online Learning Community\n\nJoin the Slack workspace\n@Collin Berke to get a hold of me in the workspace"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Open source workflow management tool",
    "text": "Open source workflow management tool\n\nApache Airflow"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "",
    "text": "Photo by Markus Winkler"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The purpose of this blog series",
    "text": "The purpose of this blog series\nThis series of blogposts will focus on creating forecasts using Google Analytics 4 data. Specifically, this series overviews the steps and methods involved when developing forecasts of time series data. This blog series begins with a post overviewing the wrangling, visualization, and exploratory analysis involved when working with time series data. The primary focus of the exploratory analysis will be to identify interesting trends for further analysis and application within forecasting models. Then, subsequent posts will focus on developing different forecasting models. The primary goal of this series is to generate a forecast of online order completions on the Google Merchandise store."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "A quick disclaimer",
    "text": "A quick disclaimer\nAnother intention of this series is to document and organize my learning and practice of time series analysis. Although I try my best to perform and report valid and accurate analysis, I will most likely get something wrong at some point in this series. I’m not an expert in this area. However, it’s my hope that this series can be a supplement to others who may be learning and practicing time series analysis. In fact, seeing somebody (i.e., myself) do something wrong might be a valuable learning experience for someone else, even if that someone is my future self. If I got something wrong, I would greatly appreciate the feedback and will make the necessary changes.\nThroughout the series, I will document the resources I used to learn the process involved when generating forecasting models. I highly suggest using these as the primary source to learn this subject, especially if you intend to use this type of analysis in your own work. Specifically, the process and methods detailed in this series are mostly inspired by the Forecasting: Principles and Practice online textbook by Rob J Hyndman and George Athanasopoulos, and it utilizes several packages to wrangle, visualize, and forecast time series data (e.g., tsibble; fable; and feasts). I am very thankful to the authors and contributors for making these materials open-source."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Setup",
    "text": "Setup\nThe following is the setup steps needed to perform this exploratory analysis.\n\n# Libraries needed\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(fable)\nlibrary(feasts)\nlibrary(fuzzyjoin)\nlibrary(bigrquery)\nlibrary(glue)\nlibrary(GGally)\nlibrary(scales)\nbq_auth()\n\n## Replace with your Google Cloud `project ID`\nproject_id &lt;- 'your.google.project.id'\n\n\n## Configure the plot theme\ntheme_set(\n  theme_minimal() +\n    theme(\n       plot.title = element_text(size = 14, face = \"bold\"),\n       plot.subtitle = element_text(size = 12),\n       panel.grid.minor = element_blank()\n    )\n)"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The data",
    "text": "The data\nAs was used in previous posts, Google Analytics 4 data for the Google Merchandise Store are used for the examples below. Data represents website usage from 2020-11-01 to 2021-12-31. Google’s Public Datasets initiative makes this data open and available for anyone to use (as long as you have a Google account and have access to Google Cloud resources). Data are stored in Google BigQuery, a data analytics warehouse solution, and are exported using a SQL like syntax. Details on how this data were exported can be found in this GitHub repository. More about the data can be found here."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Export the data",
    "text": "Export the data\nThe first step in the process was to export all page_view events. To do this, the following SQL code was submitted to BigQuery using the bigrquery package. Keep in mind Google charges for data processing performed by BigQuery. Each Google account–at least since the writing of this post–had a free tier of usage. If you’re following along and you don’t have any current Google Cloud projects attached to your billing account, this query should be well within the free usage quota. However, terms of service may change at anytime, so this might not always be the case. Nevertheless, it is best to keep informed about the data processing pricing rates before submitting any query to BigQuery.\nselect \n    event_date,\n    user_pseudo_id,\n    event_name,\n    key,\n    value.string_value\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nCROSS JOIN UNNEST(event_params)\nwhere \n   event_name = 'page_view' and \n   _TABLE_SUFFIX between '20201101' and '20211231' and \n   key = 'page_location'\nThe query returns a data set with 1,350,428 rows and the following columns:\n\nevent_date - represents the date the website event took place.\nuser_pseudo_id - represents a unique User ID.\nevent_name - The name of the event specified by Google Analytics. In our case this will just be page_view given the filtering criteria.\nkey - represents the page_location dimension from the data. This column should only contain page_location.\nstring_value - represents the page to which the event took place. In other words, the page path a page_view event was counted.\n\nThis query returns a lot of data. Thus, the analysis’ scope needed to be narrowed to make the exploratory analysis more manageable. To do this, top-level pages were identified and data wrangling procedures were performed to reduce the data down to pages relevant to the exploratory analysis.\n\nNarrowing the analysis’ scope to relevant pages\nThe overall aim of the series is to forecast Order Completed page views. As part of this, relevant pages that could be used to improve forecasting models needed to be a part of the exploratory analysis. However, this is challenging given the sheer amount of pages being represented within the data. Some pages relevant to the analysis, others, not so much. Given the number of possible pages, a decision was made to only examine key, top-level pages. The question is, then, what pages should be considered relevant to the analysis?\n\n\nDetermining top-level pages\nThe navigation bar of the Google Merchandise Store was used to determine the top-level pages. Indeed, it’s reasonable to expect the navigation bar is designed to drive users to key areas of the site. Developers won’t waste valuable navbar real-estate for content users would consider useless and/or irrelevant (i.e., these are developers at Google, so they mostly likely have a good idea of how users use a website). With this in mind, the following pages were identified as potential candidates for further analysis.\n\nNew products\nApparel\nLifestyle\nStationery\nCollections\nShop by Brand\nSale (i.e., Clearance)\nCampus Collection\n\nThe checkout flow is another key component of any e-commerce website. Indeed, a main goal of the site is to convert visits into order completions. As such, pages related to the checkout flow might be another area of interest in the analysis. It’s challenging to piece together the checkout flow by just looking at the data. So, I purchased a few products to observe the checkout flow and track the different pages that came up. The checkout flow–at least when I made my purchase–went in this specific order:\n\nReview basket\nSign in (I wasn’t signed into my Google account)\nReview my information\nPayment info\nOrder review\nOrder completed\n\nAlthough these pages were identified as potential candidates for further analysis, it’s important to recognize the Google Merchandise store is not static, and thus the design and layout may have changed from the dates the data represents vs. when I went through the checkout flow. Regardless, these initial observations provided a starting point to help narrow the analysis’ focus."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Filtering out homepage events",
    "text": "Filtering out homepage events\nNow that the analysis’ scope had been narrowed to top-level pages, events associated with homepage views were filtered out to reduce the number of events within the data. To do this, the regex_filter variable was created using the glue() function from the glue package, which was then applied within a filter statement.\n\nregex_page_filter &lt;- glue(\n  \"(\",\n  \"^https://www.googlemerchandisestore.com/$|\",\n  \"^https://googlemerchandisestore.com/$|\",\n  \"^https://shop.googlemerchandisestore.com/$\",\n  \")\"\n  )\n\nThe variable contained multiple regex expressions, as several page paths in the data represented home page visits. Defining the variable in this way ensured the filter excluded all data associated with homepage visits.\nOnce the filter statement was set up, the str_to_lower() function from the stringr package was used to convert all the page paths to lower case. The following code chunk demonstrates how these operations were performed.\n\nga4_pagepaths &lt;- ga4_pageviews %&gt;% \n  filter(!str_detect(string_value, regex_page_filter)) %&gt;% \n  mutate(string_value = str_to_lower(string_value))\n\nThe filtering resulted in a reduced data set (i.e., ~1 million rows). Since the intent was to further narrow the analysis’ scope, additional filtering was performed. Specifically, the data were filtered to return a data set containing the top-level pages identified previously.\nAnother variable–similar to regex_filter–was created and used to filter the data further. Given the number of pages, though, a filtering join would be more appropriate (e.g., semi-join). The problem is the join operation needed to filter the data needed to be based on several regular expressions.\nA semi-join using a regular expression is not supported with dplyr’s joins, so the regex_semi_join() function from David Robinson’s {fuzzyjoin} package was used. This package provides a set of join operations based on inexact matching. A separate data set (tracked_data), containing the regular expressions was then created, imported into the session, and used within the join operation. A dplyr::left_join() was then used to include this data within a tidy dataset. The following chunk provides example code to perform these operations.\n\ntracked_pages &lt;- read_csv(\"tracked_pages.csv\")\n\ntop_pages_data &lt;- ga4_pagepaths %&gt;% \n  mutate(\n    string_value = str_remove(\n      string_value,'https://shop.googlemerchandisestore.com/')) %&gt;% \n  regex_semi_join(tracked_pages, by = c(\"string_value\" = \"page_path\")) %&gt;% \n  regex_left_join(tracked_pages, by = c(\"string_value\" = \"page_path\"))\n\nAt this point, the data is more manageable and easier to work with. At the start, the initial export contained around 1.6 million rows. By narrowing the focus of the analysis and performing several data wrangling steps to filter the data, the final tidy data set contained around 320,000 rows.\nGiven the limited amount of storage available and how this post is hosted makes authentication into BigQuery challenging, I opted to not integrate the extraction steps into the rendering steps and to exclude the full data with this post. However, I included the filtered data set in a .rds file to conserve space. I imported this file by running the following code chunk to continue the exploratory analysis. I would skip this step and just directly export the data from BigQuery if this analysis was performed outside the forum of a blog post.\n\ntop_pages_data &lt;- readRDS(\"top_pages_data.rds\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Data exploration",
    "text": "Data exploration\nWith data in a tidy format, the exploratory analysis and further identification of relevant series for forecasts of Order Completed page views can take place. One area of possible exploration is to identify which pages generate a significant amount of traffic. Indeed, it’s possible that pages with a lot of traffic might also result in more order completions: more traffic indicates more interest; more interest could mean more orders.\nA few questions to answer:\n\nWhich top-level pages have the most unique users?\nWhat pages get the most traffic (i.e., page views)?\n\nA simple bar plot is created to answer these questions. Here’s the code to create these plots, using the ggplot2 package.\n\npage_summary &lt;- top_pages_data %&gt;% \n  group_by(page) %&gt;% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n()\n  ) %&gt;% \n  arrange(desc(unique_users))\n\n\nggplot(page_summary, aes(x = unique_users, y = reorder(page, unique_users))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by users\",\n       subtitle = \"Apparel page viewed by a significant amount of users\",\n       y = \"\",\n       x = \"Users\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       )\n\n\n\n\n\nggplot(page_summary, aes(x = views, y = reorder(page, views))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by views\",\n       subtitle = \"Apparel and basket pages generate significant amount of views\",\n       y = \"\", \n       x = \"Views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\nApparel clearly seems to not only have received a significant amount of users, but a high number of page views as well. It’s also interesting to note that the basket had nearly half the amount of users compared to apparel, but the amount of page views was similar. It’s also apparent, at least with the data available, that more users browsed clearance then they did new items during this period. Just looking at the current summary for the period, apparel might be a potential time series to include within forecasting models of order completions.\nAlthough the apparel page is a likely candidate for the forecasting models, supplemental data should be examined to justify its inclusion. For instance, actual purchase/financial data could provide further justification of the business case and value of focusing on this specific area within future analyses. For instance, apparel may drive a lot of traffic, but it may not be an area where much revenue or profit is generated. Thus, the focus on more money generating/profitable products may be better candidates to improve the accuracy of our forecasting models. Despite this, actual purchase and financial data are not available. As a result, this is not explored further."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Create time series data visualizations",
    "text": "Create time series data visualizations\nVisualizing the time series is the next step in the exploratory analysis. This step will be further helpful in identifying potential time series that may be of value in creating a forecast of Order Completed page views.\n\nConvert the tibble into a tsibble\nThe top_pages_data tibble is now converted to an object that contains temporal structure. To do this, the as_tsibble() function from the {tsibble} package is used. This package provides a set of tools to create and wrangle tidy temporal data. Before the temporal structure could be mapped to the data set, a few wrangling steps were performed: 1). the event_date column was converted into a date variable; and 2). data were aggregated to count the number of unique_users and views. The following code chunk contains an example of these steps.\n\npages_of_interest &lt;- c(\"Apparel\", \n                       \"Campus Collection\", \n                       \"Clearance\", \n                       \"Lifestyle\", \n                       \"New\", \n                       \"Order Completed\", \n                       \"Shop by Brand\")\n\ntidy_trend_data &lt;- top_pages_data %&gt;% \n  mutate(event_date = parse_date(event_date, \"%Y%m%d\")) %&gt;% \n  group_by(event_date, page) %&gt;% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n(), \n    .groups = \"drop\"\n  ) %&gt;% \n  as_tsibble(index = event_date, key = c(\"page\")) %&gt;% \n  filter(page %in% pages_of_interest)\n\nAt this point, several trend plots could be created using the ggplot2 package. However, the feasts package provides a convenient wrapper function to quickly make trend visualizations of tsibble objects, autoplot(). The outputted plot was then formatted to improve readability.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of page views\", \n       subtitle = \"Apparel drove a significant amount of views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.title = element_blank())\n\n\n\n\nggplot2’s facet_wrap() function was used to create a plot for each series. Splitting the plots into separate entities allowed for a clearer view of the characteristics within each series.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(.~page, scales = \"free_y\")  +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plots of page views\", \n       subtitle = \"Various characteristics are present within each series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Identify notable features using time plots",
    "text": "Identify notable features using time plots\nApparel again emerges as a potential series to include within the forecasting models, as this page generates a significant amount of traffic. Despite the sheer amount of traffic to the apparel page, though, other time series peak interest. Specifically, the Campus Collection, Clearance, Lifestyle, and New pages all have some interesting characteristics that could be used to improve forecasting models. The following plots contain the isolated trends. A description of the characteristics within each trend is provided.\n\nApparel page’s characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Apparel\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Apparel page views\", \n       subtitle = \"Series exhibits positive trend; slight cyclic patterns; no seasonal patterns present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA clear positive trend.\nThe series contains some cyclic elements and very little indication of a seasonal pattern. However, with a greater amount of points, a seasonal pattern might be revealed (e.g., holiday season shopping).\n\n\n\nCampus Collection page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Campus Collection\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Campus Collection page views\", \n       subtitle = \"Cyclic behavior present within the series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA slight positive trend is present up until the middle of the series. Towards the middle of the series, no real trend is present.\nGiven the variation is not of a fixed frequency, this series exhibits some cyclical behavior.\n\n\n\nClearance page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Clearance\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Clearance page views\", \n       subtitle = \"Slight trend components are present; weekly seasonality is also present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA slight upward trend towards the middle of the series, followed by a steep downward trend, and then a slight upward trend towards the end of the series is present.\nThe series also has a clear seasonal pattern, which seems to be weekly in nature. Perhaps products are moved to clearance on a weekly basis.\n\n\n\nLifestyle page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Lifestyle\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Lifestyle page views\", \n       subtitle = \"Trend not clear in this series; some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nTrend is not clear in this series.\nThere is some strong cyclic behavior being exhibited with limited seasonality with the time frame available.\n\n\n\nNew page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"New\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of New page views\", \n       subtitle = \"No trend present; some some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nNo real trend is present.\nStrong cyclic behavior is present within the series. Some seasonality is present. Indeed, this is similar to the Clearance series, as the seasonality seems to be weekly. Perhaps new products are released each week.\n\n\n\nShop by brand characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Shop by Brand\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Shop by Brand page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nThe trend here seems to be positive from the start, then declines sharply, and then exhibits a slight positive trend towards the end of the series.\nThere also seems to be some slight cyclicity with very little seasonality.\n\n\n\nOrder completed characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Order Completed\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Order Completed page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nThe trend from the start seems to be positive, up until the middle of the series. From there, a steep decline is present. Towards the end of the series, there is a subtle positive trend.\nTowards the beginning of the series, there seems to be some strong cyclicity. Towards the end of the series, there seems to be more of a seasonal pattern within the data. This cyclicity may be due to the time of year which this data represents, the holiday season.\n\nSince this analysis is focused on creating a forecast for order completions, additional work needed to be done to identify potential series that may improve the forecasts. To do this, several scatter plots were created to help identify variables that relate to order completions.\nBefore additional exploratory plots can be created, though, additional data wrangling steps needed to be taken. Specifically, the data was transformed from a long format to a wide format, where the page variable is turned into several numeric columns within the transformed data set. The following code chunk was used to perform this task.\n\ntidy_trend_wide &lt;- tidy_trend_data %&gt;% \n  select(-unique_users) %&gt;% \n  mutate(page = str_replace(str_to_lower(page), \" \", \"_\")) %&gt;% \n  pivot_wider(names_from = page, \n              values_from = views, \n              names_glue = \"{page}_views\")\n\nWith data in a wide format, the ggpairs() function from the GGally package was used to create a matrix of scatterplots and correlation estimates for the various series within the dataset.\nHere is the code to perform this analysis and output the matrix of plots.\n\ntidy_trend_wide %&gt;% \n  ggpairs(columns = 2:8)\n\n\n\n\nThe scatterplots and correlations output revealed some interesting relationships. For one, although previous exploratory analysis revealed apparel generated high volumes of views, the correlation analysis revealed a slight negative relationship with order completions. However, five variables seem to be highly correlated with order completions: Clearance (.877), Campus Collection (.846), Lifestyle (.769), New (.753), and Shop by Brand (.659). Evidence points to these series as being potentially valuable components of a forecasting model of Order Completed page views."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Narrow the focus further",
    "text": "Narrow the focus further\nAt this point, this post transitions into examining just the Order Completed page views, as this is the time series intended to be forecasted within future analyses done in this series of blog posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Lag plots",
    "text": "Lag plots\nIt’s time to shift focus onto exploring the characteristics of the outcome variable of future forecasting models, order completions, in more depth. The next step, then, is to examine for any lagged relationships present within the Order Completed page views time series.\nThe gg_lag function from the feats package makes it easy to produce the lag plots. Here the tidy_trend_wide data will be used.\n\ntidy_trend_wide %&gt;% \n  gg_lag(order_completed_views, geom = \"point\")\n\n\n\n\nThe plots provide little evidence that any lagged relationships–positive or negative–are present within this time series. Thus, no further steps were taken to account for lagged relationships at this point in the analysis."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nExploring for autocorrelation is the next step. A correlogram is created to explore for this characteristic within the series. A correlogram “measures the linear relationship between lagged values of a time series” (Hyndman and Athanasopoulos 2021). The ACF is first calculated for each value within the series. This value is then plotted according to it’s corresponding lag values. The ACF() function from the feasts package was used to calculate these values. The resulting data object is then passed along to the autoplot() function, which creates the correlogram for the data. Here is what the code looks like, along with the outputted plot.\n\ntidy_trend_wide %&gt;% \n  ACF(order_completed_views, lag_max = 28) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Inspect the correlogram",
    "text": "Inspect the correlogram\nThe correlogram clearly shows the data is not a white noise series. Moreover, the plot reveals several structural characteristics within the time series.\n\nThe correlogram, given the smaller lags are large, positive, and seem to decrease with each subsequent lag, which suggests the series contains some type of trend.\nThe plot also reveals a slight scalloped shape (i.e., peaks and valleys at specific intervals), which suggests some seasonality occurring within the process. Indeed, it seems peaks occur every seven days (e.g., lags 7 and 14). Thus, a slight weekly seasonality may be present within the time series.\n\nGiven these structural characteristics of the series, future forecasting steps will need to account for these issues. This topic will be further discussed in future posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Trend Decomposition",
    "text": "Trend Decomposition\nThe final exploratory analysis step is to split the series into its several components. This includes the seasonal, trend, and remainder components. Here an additive decomposition is performed. Transformations were not applied to this data before decomposition was performed.\nAn argument could be made to transform this time series using some mathematical operation. Indeed, transforming the series may improve forecasts generated from the data (Hyndman and Athanasopoulos 2021). However, this analysis doesn’t have access to a complete series of data. Having more data could lead to more informed decisions on the appropriate application of transformations. A full year or multiple years worth of data would be preferred. Interpretability is also a concern, as transformations would need to be converted back to the original scale once the forecast was created. Thus, it was decided that transformations were not going to be applied to the data. More about transforming the series can be referenced here.\nThe series was broken down into its multiple components: seasonal, trend-cycle, and remainder (Hyndman and Athanasopoulos 2021). Decomposing the series allows for more to be learned about the underlying structure of the time series. As a result, allowing for structural components of the time series that could improve forecasting models to be identified. Several functions from the {feasts} and {fabletools} packages simplified the decomposition process.\nFirst, the trend components are calculated using the STL() and model() functions. STL() decomposes the trend. The model() function creates a mabel object of estimates. The components() function is then used to view the model object.\n\norder_views_dcmp &lt;- tidy_trend_wide %&gt;% \n  model(stl = STL(order_completed_views))\n\ncomponents(order_views_dcmp)\n\n# A dable: 92 x 7 [1D]\n# Key:     .model [1]\n# :        order_completed_views = trend + season_week + remainder\n   .model event_date order_completed_views trend season_week remainder\n   &lt;chr&gt;  &lt;date&gt;                     &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n 1 stl    2020-11-01                    14  47.8      -37.6      3.80 \n 2 stl    2020-11-02                    73  47.9       12.9     12.2  \n 3 stl    2020-11-03                    69  47.9       23.1     -1.99 \n 4 stl    2020-11-04                    47  48.2       11.8    -13.0  \n 5 stl    2020-11-05                    28  48.5       -4.93   -15.5  \n 6 stl    2020-11-06                    62  48.7       13.0      0.285\n 7 stl    2020-11-07                    34  48.9      -17.9      3.00 \n 8 stl    2020-11-08                    32  51.7      -37.9     18.2  \n 9 stl    2020-11-09                    58  54.5       12.4     -8.95 \n10 stl    2020-11-10                    70  56.6       22.7     -9.33 \n# ℹ 82 more rows\n# ℹ 1 more variable: season_adjust &lt;dbl&gt;\n\n\nPlotting the decomposition is done by piping the output from the components() function to autoplot(). The visualization will contain the original trend, the trend component, the seasonal component, and the remainder of the series after the trend and seasonal components are removed.\n\ncomponents(order_views_dcmp) %&gt;% \n  autoplot() +\n  labs(x = \"Event Date\")\n\n\n\n\nScanning the components outputted by the decomposition, a few conclusions were drawn. Looking at the trend component, it seems a steady upward trend takes place from the start to the middle of the series. Then, a sharp negative trend followed by a slight increase towards the tail end of the series is present. Indeed, this might be additional seasonality that might become more apparent if additional data were available.\nThe seasonal component is also interesting here, as some type of cyclic weekly pattern seems to be present. This includes less traffic around the beginning of the week and weekends, where the majority of this cyclic pattern occurs during the week. It’s also interesting to note a consistent drop occured on most Thursdays of the week."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html",
    "href": "blog/posts/2023-01-29-2023-rig/index.html",
    "title": "2023 data science rig: Set up and configuration",
    "section": "",
    "text": "Photo by Barn Images"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "title": "2023 data science rig: Set up and configuration",
    "section": "A few extra configs to the operating system",
    "text": "A few extra configs to the operating system\nI also like to customize the appearance, system keymappings, and terminal aliases (more on this in the section on setting up Zsh) of my operating system. For one, I’m a fan of dark mode, so I set the system settings accordingly. I’m also a minimalist when it comes to the menu dock. I prefer to only include shortcuts that are necessary to my workflow. I also like to change the settings to automatically hide the dock when it’s not being used. I do this to maximize my workspace area. Here is a link to some docs if you’re interested in modifying your macOS system settings.\nThe caps lock key is useless in my workflow. Instead, I remap the ctrl key to the caps lock key. This is mostly done out of convenience, as I’ll use my machine as a true laptop from time to time. This is also essential because my IDE, Neovim, requires extensive use of the ctrl keys (more on the use of Neovim later). Since the MacBook Pro does not include a right-hand side ctrl key, and the left-hand side ctrl key is not in a comfortable position, this remap affords me some additional comfort when I use my machine as a laptop."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#homebrew",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#homebrew",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Homebrew",
    "text": "Homebrew\nHomebrew coins itself as the missing package manager for macOS (or Linux). It makes downloading open-source software much easier. Downloading and installing Homebrew is straight forward. Run the following command in a terminal to download Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nIf you need more specific instructions on downloading and installing Homebrew, check out the docs I linked above. With the Homebrew package manager installed, it’s a cinch to download other tools."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#oh-my-zsh",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#oh-my-zsh",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Oh My Zsh",
    "text": "Oh My Zsh\nNow it’s time to unleash the terminal by downloading Oh My Zsh. Download Oh My Zsh by running the following in your terminal:\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nOh My Zsh’s docs contain the best description of what it does:\n\nOh My Zsh will not make you a 10x developer…but you may feel like one.\n– Zsh docs\n\nFor reals though, Oh My Zsh is a convenient, intuitive means to configure your terminal. For one, it allows plugin installation. Plugins enhance the terminal experience and extend its utility. The following is a list of Zsh plugins I find useful:\n\ngit\nzsh-syntax-highlighting for terminal syntax highlighting.\nzsh-autosuggestions for command suggestions based on previous history.\n\n\nCustomized Zsh prompt\nAnother great feature of Zsh is the ability to customize the command line prompt. Many options are available. For me, I like the prompt to contain four pieces of information:\n\nThe time (24-hours with seconds);\nThe file path of the current working directory;\ngit branch information;\nAn indicator if any uncommitted changes exist in the directory.\n\nHere is what my prompt looks like:\n\n\n\nCustomized Zsh prompt\n\n\nTo achieve this custom setup, I place the following into my .zshrc file:\n# Prompt formatting\nautoload -Uz add-zsh-hook vcs_info\nsetopt prompt_subst\nadd-zsh-hook precmd vcs_info\nPROMPT='%F{blue}%*%f %F{green}%~%f %F{white}${vcs_info_msg_0_}%f$ '\n\nzstyle ':vcs_info:*' check-for-changes true\nzstyle ':vcs_info:*' unstagedstr ' *'\nzstyle ':vcs_info:*' stagedstr ' +'\nzstyle ':vcs_info:git:*' formats       '(%b%u%c)'\nzstyle ':vcs_info:git:*' actionformats '(%b|%a%u%c)'\nIndeed, this might not be the custom prompt for everyone. So, the following are links to blog posts that do an excellent job describing how to customize the different prompt elements:\n\nCustomizing my Zsh Prompt by Cassidy Williams\nCustomize your ZSH prompt with vcs_info by Arjan van der Gaag\n\n\n\nTerminal aliases\nThis year, I focused on transitioning to a more terminal based workflow. As part of this transition, I began utilizing terminal aliases. Aliases can be used to automate common tasks, like opening specific programs, web pages, or project files from the terminal.\nWith Zsh, creating aliases is pretty straightforward. To do this, you’ll need to place a file into the ~/oh-my-zsh/custom directory. This file can be named anything, but it needs to end in the .zsh extension. In this file you can include aliases like the following:\n# aliases to improve productivity\nalias email=\"open https://path-to-email.com/mail/inbox\"\nalias calendar=\"open https://path-to-calendar.com/\"\nalias projects=\"open https://path-to-github-projects.com/\"\nNow if you run email in your terminal prompt, a browser window with your email inbox will open. The above is just an example to get you started. I have additional aliases beyond the ones in the example. To get an idea of all the aliases I use, check out the dotfile here. You can customize any of these to your specific needs.\nThe rest of my Zsh configuration is pretty standard. Here is a link to a repo containing additional files to configure Zsh. Check it out if you’re interested in seeing how I specifically do something."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#additional-terminal-utilities",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#additional-terminal-utilities",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Additional terminal utilities",
    "text": "Additional terminal utilities\n\nJump\nNavigating the file system from the terminal can be tiring. Jump is a terminal utility that solves this problem. Simply put, this utility learns your navigational habits and allows you to easily jump back and forth between directories with very little typing.\nInstall jump using Homebrew. Run the following code in your terminal to install Jump:\nbrew install jump\n\n\ntmux\ntmux is a terminal multiplexer. It lets you create multiple windows and terminals in a single session. I find it useful in situations where you want multiple files, projects, or terminal windows to be open while you’re working.\nInstall tmux using Homebrew:\nbrew install tmux\nAlthough tmux is useful out of the box, some configuration steps are needed to make it more useful. My configuration mostly changes tmux’s keymaps, which makes them easier to remember and use (i.e., some of the defaults require some keyboard gymnastics).\nMuch of my tmux configuration is a derivative of the one discussed in the Getting Started with: tmux YouTube series from Learn Linux TV. If you want some more specific detail, you can check out my .tmux.conf configuration file here.\n\n\ngit\nI use git for version control. Homebrew can be used to install git:\nbrew install git\nSome additional configuration is needed for the local setup of git. Run the following code in the terminal. Make sure to replace what is in quotations with your information.\ngit config --global user.name \"&lt;full-name&gt;\"\ngit config --global user.email \"&lt;email&gt;\"\ngit config --global core.editor \"nvim\"\nThe user.name and user.email variables are required. You can exclude the core.editor configuration if you want to use the default editor. However, I like to use Neovim (more on Neovim in a later section) as my text editor, so I make it my default when working with git.\nAlong with git, I use GitHub for remote repositories. Some additional steps are needed to authenticate with this service. The GitHub CLI tool simplifies these steps.\n\n\nGitHub’s CLI tool\nBring GitHub to your command line with the GitHub CLI. This tool provides commands to do many of the same things you do on GitHub, but with terminal commands. Need to create an issue in a repo, run the following in your terminal:\ngh create issue\nWant to see all the pull requests in a repo needing review, run the following in your terminal:\ngh pr list\nYou can also use these commands within aliases to streamline your workflows. I particularly like my custom aliases to list and create issues and PRs.\n# Custom alias to list GitHub issues\n.il\n\n# Custom alias to create an issue\n.ic\nHomebrew, again, is used for the installation.\nbrew install gh\n\nAuthenticate using the GitHub CLI\nOnce installed, run the gh auth login command to walk you through the authentication flow. During the flow, you’ll have to make a few decisions. Your first decision will be the protocol you want to use for git operations. I select HTTPS. Second, you’ll need to decide how you want to authenticate the GitHub CLI. I select the web browser setting out of convenience. If you’re interested in other forms of authentication, I suggest checking out GitHub’s docs.\nOne minor, additional configuration step is to set Neovim as the default editor for use with the GitHub CLI. If you want to use the default editor, then skip this step. To modify the default editor, run the following command in the terminal:\ngh config set editor nvim"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-rig",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-rig",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install rig",
    "text": "Install rig\nHomebrew handles the installation of rig. Run the following in your terminal:\nbrew tap r-lib/rig\nbrew install --cask rig"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-the-most-recent-version-of-r",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-the-most-recent-version-of-r",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install the most recent version of R",
    "text": "Install the most recent version of R\nOnce rig is installed, download the most recent version of R by running the following in the terminal:\nrig add\nOnce the most recent version is downloaded, you can verify the installation was successful by printing out a list of all the R versions installed on your machine. If this is a fresh start on a new machine or it’s your first time downloading R, you should only see one version listed.\nrig list"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#download-rstudio",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#download-rstudio",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Download RStudio",
    "text": "Download RStudio\nAlthough I have made the switch to using a different IDE (more on this in the next section), I still teach classes and present to groups who mainly use RStudio. So to keep everything up to date and in synch, I download the current version of RStudio using Homebrew:\nbrew install --cask rstudio\nrig also makes it easy to open up a new session of RStudio from the terminal. To do this, run the following in the terminal:\nrig rstudio"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-r-packages",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-r-packages",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install R packages",
    "text": "Install R packages\nThis section overviews the installation of R packages I use most often. Indeed, it would be excessive to download and overview all the packages in my workflow. In addition, the following sections contain a brief description of what each package does and how it is used when I work with R.\nThe following code downloads packages I rely on most. If you use R, many of these packages will be familiar.\n\ninstall.packages(c(\n  devtools,\n  usethis,\n  roxygen2,\n  tidyverse,\n  lubridate,\n  testthat,\n  googleAnalyticsR,\n  bigrquery\n))\n\nIf you’re unfamiliar with loading packages in R, you’ll need to run this code an R console. This can be done either in RStudio or via an iTerm2 system terminal. From the system terminal, type the letter R and hit Enter. Doing this should change your terminal prompt, as you are now running in a R session. You’ll then run the code from above. Information will be printed to the terminal during the installation of the packages.\nOnce all these packages have been installed, run the quit() function to return back to the system’s original prompt. When quitting this R session, you may be prompted to save the workspace. Enter no, as there is no need to save this session’s information. The next few sections provide a brief description of how each of the installed packages are used within my workflow.\n\ndevtools\n\nThe aim of devtools is to make your life as a package developer easier by providing R functions that simplify many common tasks.\n– devtools package docs\n\nSimply put, I rely on devtools for package development. This package provides many convenience functions to manage the mundane tasks involved in package development.\n\n\nusethis\nusethis is a workflow package. It automates many tasks involved when setting up a project. It also contains convenience functions to help with other R project workflow tasks. I’m still exploring all the package’s functions, but using the one’s I’ve learned have made me more productive.\n\n\nroxygen2\nPackages need documentation. The roxygen2 package helps with the documentation setup and development process. If you’re familair with comments in R, you’ll find writing package documentation with roxygen2 intuitive.\n\n\ntidyverse\ntidyverse is mainly used for common data wrangling and analysis tasks. Although I use base functions from time-to-time, I learned R by using tidyverse packages; they’re ingrained throughout my workflow.\nIndeed, the tidyverse is not just a single package, but a collection of packages. Some of the tidyverse packages I rely on most often include:\n\nggplot2 for data visualization\ndplyr for manipulating data\ntidyr for common data tidying tasks\npurrr for functional programming\nstringr for working with string data\n\n\n\nlubridate\nlubridate is magic when it comes to working with date-time data. I use this package mostly to handle data with a time dimension, which usually occurs in cases where I’m working with and analyzing time series data. If you work with date time data, look into using lubridate.\n\n\ntestthat\nThe testhat package is used for writing tests (e.g., unit tests) for code, especially when developing a package. To write more robust code, it’s best practice to write tests. testthat provides a framework and several convenience functions to make composing tests more enjoyable.\n\n\ngoogleAnalyticsR\nPart of my work involves the analysis of web analytics data. Much of this data is collected with and made available via Google Analytics. googleAnalyticsR is a package that allows you to authenticate and export web analytics data using the Google Analytics Reporting API.\n\n\nbigrquery\nGoogle BigQuery is a data warehouse and analytics solution. To access data via its API, I rely on the bigrquery package. This package provides multiple convenience functions to extract, transform, and load data from and into BigQuery. bigrquery also provides several functions to perform some BigQuery administrative tasks.\nThe packages highlighted above are ones I rely on most often in my day-to-day workflow. Indeed, others are used less frequently, especially when performing specific analysis tasks. However, the use of some packages is project dependent and describing all the packages I use would be outside the scope of this post."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Google Cloud Command Line Interface (CLI)",
    "text": "Google Cloud Command Line Interface (CLI)\nI mainly use the Google Cloud Platform (GCP) for cloud based project development. Although I’ll use GCP’s web portal occasionally, the command line interface provides some useful utilities to work from the terminal. The Google Cloud CLI is made available by installing the Google Cloud Software Development Kit (SDK).\nGoogle BigQuery, a data warehouse solution, is a GCP service I use quite often. The Google Cloud CLI has the bq command, which is an interface with BigQuery. I also manage some compute instances in the cloud, so I use the gcloud compute instances command as well.\n\nInstalling the Google Cloud SDK\nInstall the GCP SDK with Homebrew. To download, run the following code in your terminal:\nbrew install --cask google-cloud-sdk\n\n\nAuthorizing the Google Cloud CLI\nYou can review Google Cloud CLI’s authentication steps here. I provided the link to these docs because depending on your current setup and needs, you may need to use different steps to authenticate. Most likely, though, if you’re intending to authenticate with a user account, you can run the following command in your terminal to walk through the authentication steps:\ngcloud init\nAgain, it’s best to review the docs linked above, so you’re aware of the steps needed to authenticate with your specific setup."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#neovim",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#neovim",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Neovim",
    "text": "Neovim\nTo be honest, there was no real reason why I chose Neovim. I just saw others using and suggesting to give it a try. I did briefly read some of the arguments for why Neovim is a good choice, though. From my shallow reading of the topic, most of the arguments I came across pertained to Neovim’s use of the lua programming language, a better plugin management experience, and some additional points that made it appealing. In fact, Neovim is considered to be an extension of Vim, rather than its own stand-alone text editor. It aims to be extensible, usable, and retain the good parts of Vim. Now, I haven’t developed a sufficient understanding of these arguments to fully articulate the benefits of using one Vim like editor from another. I just know I’m enjoying it thus far. I suggest giving it a try.\n\nInstalling Neovim\nHomebrew is used to download Neovim.\nbrew install neovim\n\n\nConfiguring Neovim\nAs mentioned in the intro to this section, Neovim’s setup and configuration can be its own series of posts; there are so many options and plugins available. The focus of the following sections is to draw attention to some of the tools I find useful when working in Neovim. Keep in mind, the configuration of Neovim is a bit of a learning curve. It can be frustrating when you first start, but very rewarding at times. You can review my configuration files here.\nMy Neovim setup is based on chris@machine’s Neovim from Scratch YouTube tutorial series. This series does an excellent job overviewing a complete Neovim setup using the Lua programming language. While my setup is mostly based on the one described in this series, I have added some custom configuration for my specific workflow.\n\n\nNeovim package manager\nI use packer for plugin management. Packer simplifies plugin installation. For example, here is the Lua code to install some plugins I highlight in the following sections:\nreturn packer.startup(function(use)\n  use \"wbthomason/packer.nvim\"   -- Have packer manage itself\n  use \"jalvesaq/Nvim-R\"          -- Tools to work with R in nvim\n\n  -- Colorschemes\n  use \"lunarvim/colorschemes\"     -- A selection of various colorschemes\n  use \"tomasiser/vim-code-dark\"\n  use \"EdenEast/nightfox.nvim\"\n  use \"folke/tokyonight.nvim\"\n\n  -- LSP \n  use \"neovim/nvim-lspconfig\"         -- enable LSP\n  use \"williamboman/mason.nvim\"\n  use \"williamboman/mason-lspconfig.nvim\"\n\n\n  -- Telescope\n  use \"nvim-telescope/telescope.nvim\"\n\n  -- Treesitter\n  use {\n    \"nvim-treesitter/nvim-treesitter\",\n    run = \":TSUpdate\",\n  } \n\n  -- Git \n  use \"lewis6991/gitsigns.nvim\"\n  use \"tpope/vim-fugitive\"\n\n  if PACKER_BOOTSTRAP then\n    require(\"packer\").sync()\n  end\nend)\nThis code might not make much sense, as I only included a snippet of the code needed to install plugins I use most often. It’s mainly intended to show with a few lines of code, packer can manage all the plugin installation steps. This example code deviates slightly from the original packer docs on how to install plugins. Check out the previously linked docs if you would like an alternative setup while using Packer.\nHere is a link to a file with all the plugins I use in my setup. Admittedly, some plugins are carry overs from chris@machine’s YouTube series, and I will fully admit I’m still learning the reason why some of these plugins are present within my configuration. Thus, my setup is not as lean as I would like it to be. But hey, I’m still learning.\n\n\nNeovim plugins\n\nNvim-R\nSince I mostly work with R, I use Nvim-R to write code and interact with the R console directly in Neovim. Nvim-R provides utilities to have the Vim experience, while also affording interactive analysis right at your fingertips. Here is what a session using Nvim-R looks like:\n\n\n\nNvim-r running in Neovim\n\n\nThe power of Nvim-R comes from its predefined keybindings keybindings, which allow you to quickly and easily do interactive analysis tasks using just a few keystrokes. I’ve found it’s the best option to work with R in Neovim. A whole blog post could be written about the use of Nvim-R, and I only hit the highlights here. I highly suggest checking it out if you’re looking to write R code with Neovim.\n\n\nvim-devtools-plugin\nAs mentioned above, I use devtools for package development. To leverage its functionality in Neovim, I use the vim-devtools-plugin. This plugin provides several convenient commands to run different devtools functions. This is especially useful as you can configure keymaps to these commands for added convenience and speed.\n\n\nTelescope\nFind, filter, preview, and pick. Telescope is great at these actions. Specifically, Telescope is a fuzzy file finder. However, it provides additional features that go beyond just working with a project’s files. I’m attempting to use it more and more in my workflow, as I mostly use it to find and navigate to specific files. However, I’ve begun to explore more of its functionality and integration with git.\n\n\nvim-fugitive\nDo yourself a favor, use vim-fugitive. Fugitive is a plugin that helps you work with Git while working in Neovim. In the past, my git and GitHub workflow was mainly done from the command line. However, jumping in and out of Neovim back to run this workflow became old quickly. To solve this, Fugitive provides the :Git or :G command to call git commands directly from the editor. Also, since I use Neovim as my editor for commit messages, I’m able to directly compose them without having to leave my current Neovim session.\n\n\nLSP\nNeovim supports the Language Server Protocol (LSP). LSP provides many different features. This includes go-to-definition (a great feature that speeds up editing), find references, hover, completion, and many other types of functionality. Most IDEs have LSP set up out-of-the-box. This is done so you can get started quickly working with any language without too much configuration.\nNeovim does provide an LSP client, but you’ll have to set up the individual servers for each language you would like to work with. This sounds harder then it is, but it does take a few steps to complete. A good rundown can be found in this video here, which is from the Neovim series I linked above. I recently made the switch over to the Mason plugin, which makes LSP server management so much simpler. I would suggest checking it out if you’re intending to work with other languages in Neovim.\nI’m still learning about LSP and how to set it up. I highly suggest reading up on the docs and reviewing other’s setups rather than relying solely on mine. Mine is still a work in progress.\n\n\nnvim-treesitter\nSetting up syntax highlighting is another important step when setting up Neovim. I use nvim-treesitter to improve the syntax highlighting within Neovim. This is another advanced topic I’m still learning about, so I just use a basic setup. You can read more about it in the plugin docs linked earlier.\n\n\nCustom Neovim keymaps\nSince Neovim is all about customization of your development environment, one thing to modify is Neovim’s keymaps. To configure, you just have to define the configuration in your Neovim config files. For example, the following is some code I use to customize my keymap setup.\n-- BigQuery keymappings \nkeymap(\"n\", \"&lt;C-b&gt;\", \":w | :! bq query &lt; % --format pretty &lt;CR&gt;\", opts)\n\n-- R coding keymappings\nkeymap(\"n\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"i\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;ts\", \":RStop&lt;cr&gt;\", opts)\nkeymap(\"n\", \"tt\", \"&lt;Esc&gt;&lt;C-w&gt;&lt;C-w&gt;i\", opts)\n\n-- R devtools keymappings\nkeymap(\"n\", \"&lt;leader&gt;I\", \":RInstallPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;L\", \":RLoadPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;B\", \":RBuildPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;E\", \":RCheckPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;T\", \":RTestPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;D\", \":RDocumentPackage&lt;Esc&gt;\", opts)\nI have additional custom keymappings in my setup, but including the entire file would be too much for this post. Nevertheless, you can access my keymapping configuration files to get a sense of other keymaps I have within my setup."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "I’m a media research analyst, data enthusiast, and news, sports, and podcast aficianado.\nProfessionally, I use data, audience measurement, and marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. I am particularly interested in the use and development of open-source statistical software (i.e. R) to achieve this goal, and gaining a broader understanding of the role these tools play in media, digital, and marketing analytics. I also adjunct university courses on the side.\nListening to NPR, watching PBS (especially NOVA), and college football and baseball are my jam.\n\n\nWant to know more about what I’m currently working on, reading, or mastering? Check out the now page.\n\n\n\n\nPh.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota\n\n\n\n\n\nDigital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#now",
    "href": "index.html#now",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Want to know more about what I’m currently working on, reading, or mastering? Check out the now page."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Ph.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Digital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html",
    "title": "Implementing a next and back button in Shiny",
    "section": "",
    "text": "Photo by John Barkiple"
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "title": "Implementing a next and back button in Shiny",
    "section": "The initial runtime variables",
    "text": "The initial runtime variables\nAs part of my testing of the actionButton() UI function, I found out the initial value being sent to the server was zero. I also found out that zero can’t be used for subsetting (i.e, nothing is gets returned to the UI). To address this issue, a variable with a reactive value of one needed to be in the environment upon runtime of the application. This is so we can use the initial value of one to return the first element of our data to our output$series in the textOutput() function in the UI when the application starts. Let’s take a look at this in action.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nYou’ll notice a new function here in the server, reactiveVal(). According to the documentation, this function is used to create a “reactive value” object within the app’s environment. Basically, I understand this function is just creating a reactive expression where the initial value is one upon the runtime of the application, which is then used in the subsetting operation applied in the renderText() function. Great, we have partly solved the indexing issue with the use of reactiveVal(1). You’ll also notice the buttons don’t work here because there is no dependency on them as an input, but I’ll get to that here shortly by applying some observeEvents() functions.\n\nThe maximum index value\nI also needed a solution to help limit the range of values that could be used for indexing in our subsetting operation. I now had the lower value one available in the environment, however I did not have the maximum value. At this point, I needed a function to calculate the length of the data and to treat it as a reactive expression, as this number might be dynamic in the larger application, and the users’ inputs will determine what data gets displayed within the application (e.g., filtering by product code selection). We can easily calculate the length of our data using the length() function and making this a reactive expression by wrapping it with the reactive() function. Here is what this looks like with code.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Determine the upper part of the subset index range\n  max_no_values &lt;- reactive(length(series))\n  \n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIt’s challenging to show this value in the environment in writing, but now given the current code, I have the lower value of the range, one, and the maximum value three corresponding to the number of values in our data structure available in the environment. This is great, so now I have those two values available to help with subsetting. At this point, we also need to incorporate the two user inputs, the Back and Next buttons. However, since we know these two buttons increment by one every time they are pressed, I need to rely on some mathematical operations to control the range of values used to subset the data. Given the simplified application, I know 1, 2, or 3 is the values and range of values I need to properly apply within a subsetting operation."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "title": "Implementing a next and back button in Shiny",
    "section": "Enter the %%",
    "text": "Enter the %%\nPart of getting this functionality to work required the use of the modulus %% and modular arithmetic. Basically, modulus is an arithmetic operation that performs a division and returns the remainder from the operation. I learned a lot about this in this article here (Busbee and Braunschweig n.d.). The R for Data Science book (Wickham and Grolemund 2017) also introduces the use of %% as well. While researching the modulus, I found many useful applications for it within programming. It’s definitely worth some more time learning of its other uses. When applied in our case, though, we needed it to keep the subsetting index within the bounds of the size of our data structure.\nI am far from a mathematician, so the following explanation of the logic behind how a modulus is applied here is going to be a little fast and loose. However, I’m going to take a crack at it. Take for example our application. On runtime, we have a reactive value place() that starts at the value one. We also know that our maximum number of values that can be used as an index for our subsetting operation is three, our max_no_values reactive (i.e., c(\"a\", \"b\", \"c\")). We can now use the modulus with these two values to limit the number we are using in the index of our subsetting based on the number of clicks by the user. Here is a simplified example using code illustrating this point.\n\nmax_no_values &lt;- 3\n\n# User clicks the button to increment the index of the subset\n# Vector corresponds to the value outputted by the `actionButton()`\nuser_clicks &lt;- c(0:12)\n\nuser_clicks %% max_no_values\n\n [1] 0 1 2 0 1 2 0 1 2 0 1 2 0\n\n\nEarlier in the post, we found out that we can’t use zero to subset, as nothing gets returned. So to solve our issue, we need to shift these values by adding one to the vector. Notice how that with every ‘click’ the range of these values never goes below one or exceeds three, even when a user’s click count (keep in mind every click of the actionButton() increments by one) goes above three. This is the power of the %%, as this operation keeps our index range between 1 - 3, regardless of how many times the user clicks an action button.\n\nuser_clicks %% max_no_values + 1\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3 1\n\n\nThe math is a little different for the Back button, though. However, the same principles apply.\n\n((user_clicks - 2) %% 3) + 1\n\n [1] 2 3 1 2 3 1 2 3 1 2 3 1 2\n\n\nLet’s use some print debugging here to show how the of %% works in action. I’m going to use the glue package to help make the messages sent to the console more human readable.\n\nlibrary(shiny)\nlibrary(glue)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Determine the total number \n  max_no_values &lt;- reactive(length(series))\n  \n  position &lt;- reactiveVal(1)\n  \n  # These cause a side-effect by changing the place value\n  observeEvent(input$forward, {\n    position((position() %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  observeEvent(input$back, {\n    position(((position() - 2) %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  output$series &lt;- renderText({\n    series[position()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIf you click the Back and Next button and watch your console, you’ll see the position value for every click being printed. While clicking these values, you will observe a couple of things:\n\nYou’ll notice the value zero is never passed as a subsetting index value.\nThe arithmetic operations constrain our subsetting values within a range of 1 - 3, the length of our character vector.\nMultiple clicks remain in order, regardless if the user clicks the Next or Back buttons (e.g., 1, 2, 3 or 3, 2, 1).\n\nAt this point, we can get rid of our print debugging code, test our working example, and bask in our accomplishment of understanding how this solution works. The next step is to now integrate what we know into the larger application. We’ll do that here in the next section of this post."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "title": "Implementing a next and back button in Shiny",
    "section": "Product selection",
    "text": "Product selection\nAs part of the original functionality of the app, users were given a selectInput() in the UI to filter for injuries that were the result of different products. The requirements stated the outputted narratives also needed to reflect the users’ filter selection. This functionality needed to be added back in, and it also needed to be reactive. I do this by adding the selected &lt;- reactive(injuries %&gt;% filter(prod_code == input$code)) near the beginning portion of the server section of the code. You’ll also notice we are using the filter() function and %&gt;% operator here, so we need to also bring in the dplyr package (i.e., library(dplyr)).\nThere are now two areas in the server that have a dependency on the selected() reactive expression, the max_no_stories() reactive and our output$narrative object. Since our reprex was using a simplified vector of data (e.g., c(\"a\", \"b\", \"c\")), we need to modify the code to use these reactives. The biggest change is we are now passing a tibble of data rather than a character vector of data. As such, I need to use selected()$narrative to refer to the narrative vectors we want to use in our server function. Nothing else really changes, as the underlying process of determining the range of values and using a mathematical operation to limit the indexing stays the same. We are just now applying this process to a different set of data, although it is technically a reactive expression rather than an object in our environment."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "title": "Implementing a next and back button in Shiny",
    "section": "Cases where users select a new product code",
    "text": "Cases where users select a new product code\nGiven the functionality provided within our application, it’s reasonable to expect users would change the product code (i.e., the main purpose is to give users tools to explore the data). It’s also reasonable that the user would then expect the narrative values to change based on their product selection, and indeed we have built this functionality into the app. However, what we didn’t account for yet was what users expectations are for the order to which the new filter data will be presented. When users make a change to their filtering criteria, they would most likely expect that the updated narrative data would start at the beginning, not where their previous clicks would place them within their previously selected data. Given this expectation, I now need some code to ‘reset’ the subsetting index when a user changes their product code filter.\nWhy might this be important? Take for example if the aim of this functionality was to output the most recent injury reported for a specific product code. Our user would expect that any time they switch their product code filtering input, the displayed narrative would be the most recent reported injury, and that each subsequent click would result in a chronological walk through the narratives, either forwards or backwards. This would especially be important if the app was connected to a streaming data source that isn’t static. Moreover, you might even modify the output$narrtive object to include the date, so the user is informed on when a specific injury was treated. For the sake of keeping things simple though, we will only add the reset behavior to the app in this post.\nThis reset of the indexing value was provided in the solutions guide referenced above, and it adds another observeEvent() to make this work. Specifically, it directed me to add this code to the server section of the application:\n\nobserveEvent(input$code, {\n    place(1)\n  })\n\nHere you can see that the observeEvent() is waiting for any changes to the input$code input. When a change occurs to this input, the place(1) is run, and the subsetting index is set back to one. We now have included functionality to the app where when the user changes the product code filtering, the narrative increment index will display the first value in that subset of injuries as selected by the user."
  },
  {
    "objectID": "blog/posts/2021-04-02-intro-post/index.html",
    "href": "blog/posts/2021-04-02-intro-post/index.html",
    "title": "Intro Post",
    "section": "",
    "text": "Hello World!\nI have decided to start writing a blog. I have never attempted anything like this before, but I felt it was time to start organizing some of my projects and analyses into one central location. For some time, I really wanted to develop a space where I could discuss topics I find interesting, both professionally and personally. So, Hello World!, my name is Collin, and this is my blog.\nI’m also on other platforms, however, I really don’t engage on them too often. Nevertheless, you can find more information about me and my projects in the following locations:\n\nGitHub is a great place to see the projects I am working on. Most are professional at this time.\nTwitter, I haven’t posted anything in a while, though I retweet and like stuff often.\nEmail, the best channel to get a hold of me: collin.berke@gmail.com.\n\n\n\nThe purpose of this blog\nThe purpose of this blog is to serve as a location for me to express my thoughts on topics I find interesting. To be honest, I don’t expect this to be a really niche blog with one focused, clear purpose. Most likely it will be data analysis and/or visualization focused, which I will apply to develop posts in areas I find interesting: open-source software, media/marketing analytics, data analysis, sports, media, etc. My purpose may become more refined once I find my voice.\n\n\nThe inspiration and motivation to do this blog\nI have spent countless hours reading, re-reading, bookmarking, and Googling multiple topics regarding the use of the statistical computing programming language called R. Much of this time has been spent accessing useful, open-source, and free content that has aided me professionally, and it has contributed to my deeper understanding of the topics I find interesting. I couldn’t even begin to describe how grateful I am for those who have spent time organizing and drafting content others find useful. In fact, it’s the #Rstats community that has motivated me to put this blog together, as I have seen how helpful and open it is to aiding in the development of others.\nI now feel I am in a place of not only being a consumer but a producer of this information. I will never be an expert in this area, as there is too much to learn for just one person. As one of my favorite podcasts (i.e., Make Me Smart With Kai and Molly) states in every episode, “None of us is as smart as all of us.” Thus, I feel it is time to start organizing and drafting content others will hopefully find useful, at the very least amusing. Even if this blog helps one person, that will be enough motivation to keep me working on it.\n\n\nWhat’s up next?\nNot sure. I’m just excited I got this blog up and running. Most likely I’ll do something sports related. Who knows–stay tuned.\n\n\nReferences & Acknowledgements\n\nI make every attempt to properly cite information from other sources. If I have failed to properly attribute credit to a source, please kindly let me know.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2021,\n  author = {Berke, Collin K},\n  title = {Intro {Post}},\n  date = {2021-04-02},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2021. “Intro Post.” April 2, 2021."
  },
  {
    "objectID": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "href": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "title": "Flattening Google Analytics 4 data",
    "section": "",
    "text": "Photo by Alvaro Calvo\n\n\n\nIntroduction\nWith the introduction of the Google Analytics 4 (GA4) BigQuery integration, understanding how to work with the underlying analytics data has become increasingly important. When first diving into this data, some of the data types may seem hard to work with. Specifically, analysts might be unfamiliar with the array and struct data types. Even more unfamiliar may be the combination of these two data types into complex, nested and repeated data structures. As such, some may become frustrated writing queries against this data. I know I did.\nIf you’re mainly coming from working with flat data files, these more complex data types may not be intuitive to work with, as the SQL syntax is not as straight forward as a simple SELECT FROM statement. Much of this unfamiliarity may come from the required use of unfamiliar BigQuery functions and operators, many of which are used to transform data from nested, repeated, or nested repeated structures to a flattened, denormalized form.\nAs such, this post aims to do three things: 1. Overview the array, struct, and array of struct data types in BigQuery; 2. Overview some of the approaches to flatten these data types; and 3. Apply this knowledge in the denormalization of Google Analytics 4 data stored in BigQuery.\nThis post mostly serves as notes that I wish I had when I began working with these data structures.\n\n\nArrays, structs, and array of structs\nBefore discussing the use of these data types in GA4 data, let’s take a step back and simply define what array and struct data types are in BigQuery. A good starting point is BigQuery’s arrays and structs documentation. According to the docs,\n\nAn array is an ordered list of zero or more elements of non-Array values. Elements in an array must share the same type.\n\n\nA struct is a container of ordered fields each with a type (required) and field name (optional).\n\nBoth definitions contain technical jargon that don’t really define, in an intuitive, useful way, what these data types are and how to use them, especially in the analysis of GA4 data. So let’s break each down by bringing in additional perspectives and through the use of several simplified examples.\nWhile learning more about arrays and structs, I found several blog posts that helped me better understand these structures and how to use them. Here is a list of the ones I found to be very helpful:\n\nHow to work with Arrays and Structs in Google BigQuery by Deepti Garg\nExplore Arrays and Structs for Better Query Performance in Google BigQuery by Skye Tran\nTutorial: BigQuery arrays and structs from Sho’t left to data science\n\nI highly suggest reading all of these. In fact, much of what follows is adapted from these posts, with a few examples I created to help me better understand how these data types are structured, stored, and queried. Towards the end of the post, the techniques learned from these posts and overviewed here will be applied to GA4 data, specifically the publicly available bigquery-public-data.ga4_obfuscated_sample_ecommerce data.\n\n\nArrays\nArrays are a collection of elements of the same datatype. If you’re familiar with the R programming language, an array is similair to a vector.\nLet’s create a table containing an array of planets in our solar system as an example, and then use the INFORMATION_SCHEMA view to verify the data was entered correctly. The following code will create this table in BigQuery:\ncreate or replace table examples.array_planets_example as \nwith a as (\n   select [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"] as planets\n)\nselect planets from a;\nThe INFORMATION_SCHEMA.COLUMNS view for the array_planets_example table can be queried to verify the data was entered correctly. This table is available for every table created in BigQuery, and it contains metadata about the table and the fields within. Here is the query needed to return this information:\nselect table_name,\n   column_name,\n   is_nullable,\n   data_type\nfrom examples.INFRORMATION_SCHEMA.COLUMNS\nwhere table_name = \"array_planets_example\";\nThe returned table will contain a data_type field, where the value ARRAY&lt;STRING&gt; will be present. This value represents the field in the array_planets_example contains an array with a list of string values. Although this example array contains a series of string values, arrays can hold various other data types, as long as the values are the same type across the collection. Overviewing all of the different data types that can be stored in an array is beyond the scope of this post, but check out the BigQuery docs for more examples.\n\nQuerying an array\nMultiple approaches are available to query an array. The type of approach will depend on if the returned data needs to maintain its grouped, repeated structure, or if the returned data needs to be flattened. If maintaining the repeated structure is required, then a simple SELECT statement will work. Using the array_planets_example table as an example, the query applying this approach will look something like this:\nselect planets\nfrom examples.array_planets_example\nIf each element of the array is to be outputted onto its own row (i.e., denormalized), multiple approaches are available. The first approach is to use the unnest() function. Here is an example using the planets array we created earlier:\nselect planets\nfrom examples.array_planets_example,\nunnest(planets) as planets\nThe second approach is to apply a correlated join through the use of cross join unnest(). This approach looks like this:\nselect planets\nfrom examples.array_planets_example\ncross join unnest(planets) as planets\nYou’ll notice this is only slightly different than the query above, and in fact the , used in the FROM clause is short-hand for the cross join statement. The last and final approach is to use a comma-join. This is similair to our first query, but now we refer to the table name before the array name we want flattened.\nselect planets\nfrom examples.array_planets_example, array_planets_example.planets as planets;\nWhich one do you choose? It really comes down to a matter of preference. All three approaches will lead to the same result. It depends on how explicit you want the code to be.\nThere is one note to be aware of if you’re applying these conventions to other arrays outside of analyzing GA4 data. The cross join approach will exclude NULL arrays. So if you want to retain rows containing NULL arrays, you’ll need to apply a left join. More about this is described in the BigQuery docs.\nKeep these approaches top-of-mind. They will be applied to flatten some of the fields in the GA4 dataset. In other words, get comfortable with using them.\n\n\n\nStructs\nThe structs data type holds attributes in key-value pairs. Structs can hold many different data types, even structs. We will see the use of structs within structs in the GA4 data. Keeping with the solar system theme of the post, the following example code will create a table utilizing the struct data type to hold the dimensions and distances of the planets in our solar system. The data used for this table is reported here.\ncreate or replace table examples.struct_solar_system as\nwith a as (\n  select \"Mercury\" as planet,\n  struct(0.39 as au_sun, 57900000 as km_sun, 4879 as km_diameter) as dims_distance union all\n  select \"Venus\" as planet,\n  struct(0.72 as au_sun, 108200000 as km_sun, 12104 as km_diameter) as dims_distance union all\n  select \"Earth\" as planet,\n  struct(1 as au_sun, 149600000 as km_sun, 12756 as km_diameter) as dims_distance union all\n  select \"Mars\" as planet,\n  struct(1.52 as au_sun, 227900000 as km_sun, 6792 as km_diameter) as dims_distance union all\n  select \"Jupiter\" as planet,\n  struct(5.2 as au_sun, 778600000 as km_sun, 142984 as km_diameter) as dims_distance union all\n  select \"Saturn\" as planet,\n  struct(9.54 as au_sun, 1433500000 as km_sun, 120536 as km_diameter) as dims_distance union all\n  select \"Uranus\" as planet,\n  struct(19.2 as au_sun, 2872500000 as km_sun, 51118 as km_diameter) as dims_distance union all\n  select \"Neptune\" as planet,\n  struct(30.06 as au_sun, 4495100000 as km_sun, 49528 as km_diameter) as dims_distance \n)\nselect * from a;\nThis table contains two columns. A column that holds a string value for the name of the planet and a struct column that contains a list of key value pairs of distance and dimensions for each planet.\nThe INFRORMATION_SCHEMA.COLUMNS table can then be queried again to verify the datatypes for each column were inputted correctly. Here is the code to do this:\nselect \n  table_name, \n  column_name,\n  is_nullable,\n  data_type\nfrom examples.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"struct_solar_system\";\nThe returned table will contain a data_type column with two values: STRING and STRUCT&lt;au_sun FLOAT64, km_sun INT64, km_diameter INT64&gt;. Take notice that the STRUCT value contains information about the data types contained within.\n\nQuerying a struct\nQuerying a struct requires the use of the . operator (i.e., dot operator) in the FROM clause to flatten the table. Take for example the case where we want to return a table of only the distance of each planet from the sun in kilometers. The following query will be used:\nselect \n  planet,\n  dims_distance.km_sun\nfrom examples.struct_solar_system;\nSay a denormalized table that contains both the distance from the sun in kilometers and each planet’s diameter in kilometers is wanted. The following query would be used:\nselect \n  planet,\n  dims_distance.km_sun,\n  dims_distance.km_diameter\nfrom examples.struct_solar_system;\nWhen reviewing these two examples, observe how the dot notation is being used. In the first, our select statement contains dims_distance.km_sun, which unnests the values and gives each its own row for each planet. This is expanded in the second query, where an additional line is added to the select statement, dims_distance.km_diameter. To unnest all the values in the struct, use the following query:\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system;\nIn fact, let’s expand this query to answer the following question: which planets are the closest and farthest from our sun. Take notice how the ORDER BY portion of the query doesn’t require the dims_distance prefix for the field we want to arrange our data.\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system\norder by km_sun;\n\n\n\nArray and structs in GA4 data\nNow that we have learned a little bit about our solar system, let’s return to Earth and the task at hand, flattening GA4 data. We just discussed how these data types are created and queried, it is now time to combine them into more complex data structures, as both of these structures are combined to create nested repeated data structures in the GA4 data. It’s best to start with an example. Specifically, let’s look at how these structures are applied in the event_params field.\nWe can start off by querying the INFORMATION_SCHEMA.COLUMNS view for one event to get an idea of its structure. The query to do this can be seen here:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"events_20210131\" and column_name = \"event_params\";\nThe data type is described in the returned table’s data_type field. This field contains the following value ARRAY&lt;STRUCT&lt;key STRING, value STRUCT&lt;string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64&gt;&gt;&gt;. It should be immediately apparent that both the array and struct values are being used here to create a repeated nested structure. In fact, the event_params value uses a struct within a struct. Given this structure, all the above methods will need to be employed to flatten this data.\nTo simplify this, let’s look at one instance of one event in the GA4 data. Specifically, let’s look at one instance of a page_view event. With this simplified example, we’ll go step-by-step, adding additional elements to the query needed to flatten this data.\nselect \n  event_date,\n  event_timestamp,\n  event_name,\n  event_params\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = 'page_view'\nlimit 1;\nAfter running this query, you’ll notice the output to the console is quite verbose, especially if you’re using the bq command-line tool. The verbosity of the output is due to the event_params field holding much of the data.\nThe first layer of the structure is an array, so the initial step is to use the unnest() function. The following can be done to achieve this:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param;\nYou’ll notice a nested FROM statement is being used here. This is done to limit the result set to one row, representing one page_view event for this simplified example. Later iterations of the query will eliminate this nested query.\nNow say we’re only interested in viewing the page_location parameter. We can use a where statement to filter out this information. Here is what this will look like:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key = 'page_location';\nInterested in viewing both the page_location and page_title parameters? Use the IN operator in the WHERE clause.\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key in ('page_location', 'page_title');\nWanna turn the key field into columns so you only have one row for this specific event? Use BigQuery’s pivot() operator. Here is how to achieve this in a query:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    key,\n    value.string_value\n  from (\n    select \n      event_date,\n      event_timestamp,\n      event_name,\n      event_params\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nSince the string values are all we care about here, the value.string_value was the only one retained in the query. The other nested value elements were eliminated from the SELECT statement.\n\n\nCombine other nested fields in the GA4 data\nNow that the event_params field has been flattened, let’s supplement this information with additional data in the table. Moreover, this will provide another example of how to apply these steps to flatten other elements in the GA4 data. Knowing where users originate is some additional context that may add to our event analysis, so let’s add that data to our flattened data. But first, let’s get some more information on what data type is used for the geo field in the GA4 data.\nOnce again, querying the INFORMATION_SCHEMA.COLUMNS view can be used to explore the geo field’s data type. Here is what the query looks like:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS`\nwhere table_name = \"events_20210131\" and column_name = \"geo\";\nThe value STRUCT&lt;continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING&gt; is returned. Let’s write a query to return the table without first unnesting the data.\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo \nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = \"page_view\"\nlimit 1;\nYou’ll notice this field contains a struct, where the dot operator will need to be applied to flatten this data. Let’s start by flattening this data and then combine it with the events_param data. For the sake of keeping the returned table simple, let’s just return the region and city fields in a denormalized form. The following will return a flattened table with these fields:\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo.region,\n  geo.city\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nlimit 1;\nAs expected, the table will return a flattened table containing five fields: event_date, event_name, user_pseudo_id, geo.region, and geo.city. This table was also limited to return only the first instance of the page_view in the table.\nNow, the next step is to add this geo data to our flattened event_params query. This is as simple as adding the . operator with the needed geo elements into the FROM statement. The query will now look like this:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nThe resulting table will contain one row with several fields representing the specific event. This is great for one event, but the next step will be to expand this denormalization to all page_view events in the table.\n\n\nExpand the unnesting to multiple page view events\nNow that we have the flattened table for one page_view event, let’s expand it to additional events. This requires a simple modification to the initial nested query, remove the limit 1 line.\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\nWe can now refactor the query to be more concise. Here is what this will look like:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\n\n\nApply these approaches across multiple days\nGenerating results for one day may not be enough, so there’s a few modifications that can be made to expand the final query to return additional days. This involves modifying the FROM and WHERE statements in the initial query.\nThe first step is to modify the FROM statement to use the * wildcard operator at the end of the table name. Since the GA4 tables are partitioned by day, this will allow for a range of tables to be defined within the WHERE clause. The table name will now be bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*.\nTo define the range of dates for the events (i.e., to query multiple tables), the WHERE clause will be expanded to include the use of _table_suffix. The _table_suffix is a special column used within a separate wildcard table that is used to match the range of values. Explaining the use of the wildcard table is beyond the scope of this post, but more about how this works can be found here. The WHERE clause will now look like this:\nwhere event_name = 'page_view' and\nkey in ('page_location', 'page_title') and\n_table_suffix between \"20210126\" and \"20210131\"\nYou’ll notice this statement uses the between operator, where two string values representing the date range are passed. This statement is inclusive, so it will include partitioned tables from 20210126 and 20210131, and all tables in between. Here is the query in its final form:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and \n  key in ('page_location', 'page_title') and\n  _table_suffix between \"20210126\" and \"20210131\"\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\norder by event_date\n\n\nWrap up\nThis post started out simple by defining what arrays, structs, and array of structs data types are in BigQuery. Through the use of several examples, this post overviewed several approaches to query these different data types, specifically highlighting how to flatten each type. A second aim of this post was to show the application of these methods to the flattening of GA4 data stored in BigQuery. This included the flattening and combination of the complex, nested, repeated and nested repeated data types used in the event_params and geo fields. Finally, this post shared queries that expanded the result set across multiple days worth of data.\nIf you found this post helpful or just have interest in this type of content, I would appreciate the follow on GitHub and/or Twitter. If you have suggestions on how to improve these queries or found something that I missed, please file an issue in the repo found here.\n\n\nAdditional resources\nI spent a lot of time researching how to write, use, and query arrays and structs in BigQuery. In the process of preparing this post, I wrote a lot of example queries and followed along with BigQuery’s turtorial on working with arrays and structs. As a result, I created multiple files that I organized into the GitHub repo for this post. These might be useful as a review after reading this post, or they might be a helpful quickstart quide for your own analysis of GA4 data stored in BigQuery. These additional notes can be found here.\n\n\nAdditional references\n\nHow to work with Arrays and Structs in Google BigQuery\nExplore Arrays and Structs for Better Query Performance in Google BigQuery\nTutorial: BigQuery arrays and structs\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2022,\n  author = {Berke, Collin K},\n  title = {Flattening {Google} {Analytics} 4 Data},\n  date = {2022-09-20},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2022. “Flattening Google Analytics 4\nData.” September 20, 2022."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "title": "Shiny summary tiles",
    "section": "",
    "text": "Photo by Stephen Dawson\nEffective reporting tools include user interface (UI) elements to quickly and effectively communicate summary metrics. Shiny, a free software package written in the R statistical computing language, provides several tools to communicate analysis and insights. Combining several of these elements together, a developer can create user interface elements that clearly communicate important summary metrics (e.g., Key Performance Indicators) to an application’s users.\nThis post details the steps to create the following simple Shiny application. Specifically, this post overviews the use of Shiny’s built-in functions to create simple summary metric tiles. In addition, this post describes how to add styling to UI elements by applying custom css to a Shiny application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "title": "Shiny summary tiles",
    "section": "The reactive graph",
    "text": "The reactive graph\nAlthough this app is simple and most of the elements can be easily managed, it’s always good practice to see the big picture of the app by plotting out a reactive graph first. It’s also good to have the intended reactive graph available as a quick reference, just in case unexpected results and/or behaviors are displayed while developing the application, and as a method for identifying any situations where computing resources are not being used efficiently.\nBelow is the reactive graph for the application to be developed:\n\n\n\n\n\n\n\nReactive graph for summary metric tiles\n\n\n\n\nAgain, a really simple application–one input (date), a reactive expression (data()), and five outputs (users; page_view; session_start; purchase; and event_date). The graph also details the dependencies clearly, where the outputs are dependent on the reactive data() object–which in cohort with the outputs–depends on the date input."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "title": "Shiny summary tiles",
    "section": "The setup",
    "text": "The setup\nThe first step is to import the R packages used within the application. The following code chunk contains the packages used for the application. A brief description of each is included.\n\nlibrary(shiny) # The Shiny app library\nlibrary(readr) # Import data\nlibrary(dplyr) # Pipe and data manipulation\nlibrary(tidyr) # Tidying data function\nlibrary(purrr) # Used for functional programming\nlibrary(glue)  # Used for string interpolation\n\nMany of these packages are part of the tidyverse, and thus the import could be simplified to just running library(tidyverse). Be aware this may bring in unused, unneeded libraries. There is nothing wrong with this approach. However, I opted to be more verbose with this example, so as to be clear about what libraries are utilized within the example application and to have more control on what packages were imported by the application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "title": "Shiny summary tiles",
    "section": "Application layout",
    "text": "Application layout\nThe next step is to code the layout of the UI. To keep the design simple, a sidebar will contain the application’s inputs, while the outputs will be placed within the main panel of the application. The general skeleton of the layout looks like this:\n\nui &lt;- fluidPage(\n   # Inputs\n   sidebarLayout(\n      sidebarPanel()\n   ),\n   # Outputs\n   mainPanel(\n      # Summary tiles\n      fluidRow(),\n      br(),\n      # Data information output\n      fluidRow()\n   )\n)\n\nThere’s nothing too fancy about this code, outside of it establishing the general layout of the application, so not much else will be said about what each element does here. However, Chapter 6 of Mastering Shiny discusses application layout if a more detailed description is needed."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "title": "Shiny summary tiles",
    "section": "The date input",
    "text": "The date input\nThe app requirements state users need to have the ability to modify the dates to which the data represents, and the summary metric tiles will change based on this user input. However, the app will not have any user input upon startup, so it needs to default to the most recent date within the data. To meet these requirements, we use the following code:\n\n# Code excluded for brevity\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    )\n\nThe shiny:dateInput() function is used to create the HTML for the input, which resides in the application’s sidebar. The function’s id argument is given the value of date, which will establish a connection to elements within the server. More on this later. Then, a string value of Select a date for summary: is passed along to the label argument. This value will be displayed above the date input in the UI.\nSince the app won’t have an initial user input upon the startup of the application, max(ga4_date$event_date) is passed along to the value argument. This will default the input to the most recent date within the data. In addition, the functions max and min arguments are passed similar calls. However, in the case of the min argument the base R min() function is used on the ga4_data$event_date."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "title": "Shiny summary tiles",
    "section": "First iteration of the summary metric tiles",
    "text": "First iteration of the summary metric tiles\n\nThe server side\n\nThe reactive data() object\nBefore the summary metrics can be displayed in the UI, the application needs data to create the outputs. In addition, since this data will be dependent on users’ input (i.e., the user can select a new date which subsequently changes the summary metric tile), this object needs to be reactive. To do this, the following code is added to the server side of the application.\n\nserver &lt;- function(input, output, session) {\n   data &lt;- reactive({\n      ga4_data %&gt;% filter(.data[[\"event_data\"]] == input$date)\n   })\n}\n\nIn practical terms, this code just filters the data for the date being passed along as the input$date object.\nAgain, this object could be the most recent date within the data, the date set by the max argument in the dateInput() function, or it could be based on a user’s modification of the date input. Since this code was wrapped inside of the reactive({}) function, Shiny will be listening for any changes made to the to the input$date object. Any changes that occur will result in the data() reactive expression to be modified, followed by new output values being displayed via the UI.\nOne other key concept is being exhibited here, tidy evaluation, specifically data-masking. Since technically dplyr::filter() is being used inside of a function, an explicit reference to the data is required. Thus, .data[[\"event_data\"]] notation is used to make it explicit on what data will be filtered. The specifics on how to use data-masking in the context of a Shiny app is beyond the scope of this post. However, the previously linked materials provide a more detailed description of these concepts.\n\n\nThe outputs\nLooking back at the reactive graph, the application requires five outputs to be in the server. These outputs will just be simple text outputs, so the use of the shiny::renderText() function will be sufficient to meet our requirements. The format() function is also applied to comma format any outputs that contain numbers (e.g., 2,576 vs 2576). Here is what the server looks like currently:\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_date, .data[[\"event_date\"]] == input$date)\n  })\n  \n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nAs part of the functionality requirements, the app needed some UI element informing users what date is being represented in the summary tiles. The output$date object was included to meet this requirement. The output$date object, aside from using the renderText() function, includes the use of the glue::glue() function to make the outputted message more informative.\nThe {glue} package is used to manipulate string literals with the use of the curly braces (e.g., {}). When applied here, the {data()$event_date} is evaluated as an R call, its value becomes appended to the string, and the whole string is then outputted to the application’s UI.\n\n\n\nBack to the UI\nNow that there are five elements being outputted from the server, UI elements need to be included to display the rendered outputs.\nWhen making early design decisions about the application’s layout, it was decided these elements were going to reside within the main panel of the application. Another decision made was to keep the summary metric tile elements on the same row, so as to seem as though they are related to one another (i.e., related KPIs). As for the UI element informing the user on the date the summary metric tiles represent, it was decided that this element would be placed on its own row.\nTo achieve the intended design, additional Shiny layout functions were applied to the application’s code. This includes using the fluidRow() and column() functions to achieve the wanted UI organization. The following code was used to achieve the placement of the summary tiles within the application’s layout:\n\nmainPanel(\n   fluidRow(\n      column(),\n      column(),\n      column()\n   ),\n   br(),\n   fluidRow()\n)\n\nAs for the design of the summary metric tiles, each tile needed to include some type of title followed by the text representing the metric. To achieve this, the shiny::div() function was used. This function creates an individual HTML tag that outputs the text being passed along into the function. Directly below the title element, the textOutput() function is used to display the outputs coming from the application’s server. The code for one summary metric tile would look like the following:\n\ncolumn(3,\n       div(\"Unique Users\"),\n       textOutput(\"users\")\n       )\n\nBy combining these elements, the application code in its current state can be seen here:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n  )\n  \n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        column(3,\n               div(\"Users\"),\n               textOutput(\"users\")\n        ),\n        column(3,\n               div(\"Page Views\"),\n               textOutput(\"page_view\")\n        ),\n        column(3,\n               div(\"Session Starts\"),\n               textOutput(\"session_start\")\n        ),\n        column(3,\n               div(\"Purchases\"),\n               textOutput(\"purchase\")\n        )    \n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  # Text output\n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nIndeed, this code works and meets the functionality requirements. However, it’s quite verbose and contains a lot of redundant, repeated code. Different techniques could be applied to make the application more eloquent and efficient in its design. The goal of the next few sections, then, will be to simplify the application through the development of functions and applying functional programming principles.\n\nSimplifying the outputs\nReviewing the server, most of the outputs are created through the use of repeated patterns of the same code. This breaks the DRY principle (Don’t Repeat Yourself) of software development. Both functions and the application of functional programming principles will be applied to address this issue.\nAn obvious pattern used to create the outputs is output$foo &lt;- renderText(format(bar, big.mark = ',')). This pattern could be converted into a function, and then this function could be used to iterate over the several reactive objects (e.g., data()$users) with the use of a {purrr} function. Since the side-effects are intended to be used rather than outputting a list object from our iteration, purrr::walk() will do the trick.\nUtilizing this strategy simplifies our code to the following:\n\nc('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n\nIndeed, I can’t take full credit for this solution. Thanks goes to @Kent Johnson in the R4DS Slack channel for helping me out.\nThe output$date object was left out of this simplification of the code. Certainly, the function could be made to be more general and flexible to handle this repetition of the renderText() function. However, this would be over engineering a solution to the problem."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "title": "Shiny summary tiles",
    "section": "Back to the UI",
    "text": "Back to the UI\nFunctions and functional programming principles will now be used to address these same issues on the UI side of the application. Much of the repetition occurs with the use of the following pattern:\n\ncolumn(3,\n       div(\"Metric Title\"),\n       textOutput(\"metric_output\")\n       )\n\nIndeed, this pattern is applied four times. Since it was copied and pasted more than twice and breaks the DRY principle, it would be best to convert it into a function and iterate it using functional programming tools."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "title": "Shiny summary tiles",
    "section": "Simplifying the UI with functional programming",
    "text": "Simplifying the UI with functional programming\nA helper function, make_summary_tile(), is added to the setup section of the application. The function looks like this:\n\nmake_summary_tile &lt;- function(title, text_output){\n  column(2,\n         div(title),\n         textOutput(text_output)\n  )\n}\n\nThere’s nothing too fancy or complicated about this function. It simply generalizes the pattern applied within the UI side of the first iteration of our application. As for placement, this function could be defined at the top of the application file or in a separate .R file embedded in a R/ sub-folder. Both strategies would make the function available for the app. Deciding which to use comes down to the intended organizational structure of the application.\nThe next step is to apply functional programming to iterate the make_summary_tile() function over the text outputs. Since the function requires two inputs, title and text_output, they were placed inside of a tibble to improve organization of the inputs being passed to the function through pmap().\n\n# Defined in the Setup section\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Used within the UI\npmap(tiles, make_summary_tile)\n\nWhat once required sixteen line’s of code was cut in half to eight (including the explicit definition of the inputs). In addition, coding the tiles using functional programming also makes it more flexible, where summary tiles could be easily added or taken away.\nDoing this would require some slight modification to the make_summary_tile() helper function, though. That is, a width argument would need to be added to the function, so the column width could be set to accommodate the number of outputs for the UI. There are lots of different options that could be explored here. At this point, though, the solution meets the functionality requirements.\nIn its current state, the application code looks like this:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\nmake_summary_tile &lt;- function(header, text_output){\n  column(2,\n         div(header),\n         textOutput(text_output)\n  )\n}\n\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n)\n\n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        pmap(tiles, make_summary_tile)\n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  c('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n  \n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nThe application works, meets the functionality requirements, and now is written in a way that reduces repetition and redundant patterns within the code. However, the summary metric tiles just blend into the UI, and nothing about the styling communicates they contain important information.\nSince these elements are meant to highlight key, important summary metrics, they need to be styled in a way that creates contrast between themselves and the application’s background. The next section focuses on applying custom CSS to give some contrast between these elements and the application’s background."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "title": "Shiny summary tiles",
    "section": "Creating the www folder and CSS file",
    "text": "Creating the www folder and CSS file\nSince the design opted for a file-based CSS approach, a separate www sub-folder in the application’s main project directory needs to be created. Once created, the custom CSS file will be placed inside this folder. The placement of this file can be seen in this Github repo.\nThe purpose of this folder is to make the file available to the web browser when the application starts. Placement of this file is critical. If it is not placed in the www sub-folder, then the CSS file will not be available when the application starts, and any custom styling will not be applied.\nOnce the www sub-folder is created, you can create a CSS file for the application in Rstudio by clicking File, hovering over New File, and selecting CSS File. Save the file in the www sub-folder and give it an informative name. In the case of this example, the file is named app-styling.css.\nThe main goal of the styling will be to create some contrast between the summary metric tiles and the application’s background. Specifically, CSS will be used to create a container that is a different color from the application’s background and includes some shading to make it seem like the element is hovering above the application’s main page. To do this, the app-styling.css file includes the following:\n#summary-tile{\n  font-size: 25px;\n  color:White;\n  text-align: center;\n  margin: 10px;\n  padding: 5px;\n  background-color: #0A145A;\n  border-radius: 15px;\n  box-shadow: 0 5px 20px 0 rgba(0,0,0, .25);\n  transition: transform 300ms;\n}\nA detailed description on how to create CSS selectors is outside the scope of this post. However, in general terms, this selector sets several values for multiple CSS properties by defining the id, #summary-tile within the file. More about this process of creating different CSS selectors can be found here.\nNow it’s just a matter of modifying the code to call this file and pass these style values to the summary tiles within the application. The following code is added to the ui side of our application to include our app-styling.css file:\n\ntags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"app-styling.css\"))\n\nSince the styling is being applied to the summary metric tiles, the make_summary_tile() function is modified to bring in the CSS elements. A css_id argument is added to the function.\n\nmake_summary_tile &lt;- function(header, text_output, css_id){\n  column(2,\n         div(header),\n         textOutput(text_output),\n         id = css_id\n  )\n}\n\nNow that we made this modification to the make_summary_tile(), its application in the UI is also modified. Specifically, the #summary-tile CSS element is explicitly called in pmap(). To do this, the code is modified like this:\n\npmap(tiles, ~make_summary_tile(\n          header = ..1, text_output = ..2, css_id = \"summary-tile\"))\n\nThe header, text_output, and css_id arguments are now explicitly defined in the pmap() call. To refer to the first two elements in the tiles data object, the ..1 (i.e., header column) and the ..2 (i.e., text_output column) are used. Check out the pmap() docs on how to apply the ..1, ..2 (?pmap) for more information."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Collin K. Berke, Ph.D. is a media research analyst in public media. Professionally, he uses data and media/marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. He is especially interested in the use and development of open-source statistical software (i.e. Rstats) to achieve this goal, and developing a broader understanding the role these tools can play in media, digital, and marketing analytics.\nHe has experience using different software, third-party services, and programming languages from developing several analytics projects, both in industry and academia. In regards to programming languages, he has developed projects using R, SQL, $bash, and a little bit of Python. He also has extensive experience using different analytics solutions. For data warehousing, he mostly uses database tools like Postgres and those in the Google Cloud Platform ecosystem (e.g., Google BigQuery). When it comes to automating workflows and data pipelines, he has experience implementing and working with Apache Airflow. He also has extensive experience using third-party tools and software to analyze, wrangle data, and communicate his analyses (e.g., Google Analytics, Google Sheets, Excel, Google Data Studio, and R Shiny). Most of his current work is industry related.\nCollin also serves as an adjunct instructor for the University of Nebraska-Lincoln and Southeast Community College, where he teaches courses in sports data visualization and analysis and communication specific courses. He holds a M.A. in Communication Studies from the The University of South Dakota and a Ph.D. in Media and Communication from Texas Tech University. He has also published and contributed to the publication of several academic journal articles.\nCollin is a self-proclaimed news, sports, and podcast junkie. He really enjoys listening to NPR, watching PBS (especially NOVA), and indulging in college football and baseball. At times, he will write blog posts on topics he finds interesting in these areas.\nCheck out the Now page to see what Collin is currently reading and working on."
  },
  {
    "objectID": "about.html#note-about-this-site",
    "href": "about.html#note-about-this-site",
    "title": "About",
    "section": "Note about this site",
    "text": "Note about this site\nThe views expressed on this site are my own, and they do not reflect the views of my employer, professional and/or community groups I hold membership. Any analyses hosted on this site were done for professional development or were for fun. I make every attempt to perform valid and accurate data analysis and reporting. Unless otherwise noted, none of the content on this site has been peer-reviewed, and thus any conclusions drawn or uses stemming from this work need to take these limitations into account.\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "til/posts/2023-10-24-temp-directories/index.html",
    "href": "til/posts/2023-10-24-temp-directories/index.html",
    "title": "Using base::tempdir() for temporary data storage",
    "section": "",
    "text": "Photo by Jesse Orrico\n\n\nToday I learned how to store data in R’s per-session temporary directory.\nRecently, I’ve been working on an R package for a project. This package contains some internal data, which is intended to be updated from time-to-time. As part of the data update process, I’m required to download a set of .zip files from cloud storage, unzip, wrangle, and make the data available in the package via the data folder.\nGiven the data I’m working with, I wanted to avoid storing pre-wrangled data in the data-raw directory of the package. My main concern was an accidental check-in of pre-proccessed data into version control. So, I sought out a means to solve this problem.\nThis post aims to overview an approach using R’s per-session temporary directory to store data temporarily. Specifically, this post will discuss the use of base::tempdir() and other system file management functions made available in R to store data in this directory.\n\n\n\n\n\n\nWarning\n\n\n\nUsing R’s per-session temporary directory may not be the right solution for your specific situation. If you’re working with sensitive data, make sure you follow your organization’s guidelines on where to store, access, and properly use your data.\nI am not a security expert.\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat are temporary directories?\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, I drafted this post on a computer running a Mac operating system. Some of what gets discussed here may not apply to Windows or Linux systems. The ideas and application should be similar, though I haven’t fully explored the differences.\n\n\nThe temporary directory, simply, is a location on your system. You can store files in this location just like any other directory. The difference is data stored within a temporary directory are not meant to be persistent, and your system will delete them automatically. File deletion either occurs when the system is shut down or after a set amount of time.\nIf you’re working on a Mac operating system, you can get the path to the temporary directory by running the following in your terminal:\necho $TMPDIR\nWhen I last ran this command on my system, echo returned the following path (later we’ll use base::tempdir() to get and use this path in R).\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/\nThis directory is located at the system level. The cd command can be used to navigate to it from the terminal. You may have to back up a few directories if your root starts at the user level, though. This is pretty standard, especially if you’re working on a Mac.\n\n\n\n\n\n\nNote\n\n\n\nSince I’m drafting this post on my personal machine, I’m not aware if you need admin privileges to access this folder. As such, you may run into issues if you’re not an admin on your machine.\n\n\nWith my curiosity peaked, I sought more information about what this directory was used for on a MacOS. Oddly enough, there is very little about this directory online. From what I can deduce, the /var directory is mainly a per-user cache for temporary files, and it provides security benefits beyond other cache locations on a Mac system (again, I’m not a security expert, so my previous statement may be inaccurate). Being that this location is temporary, this cache gets cleared every time the system restarts or every three days.\nAlthough there’s a lack of information about this directory online, I did come across a few blog posts and a Stack Overflow answer that were helpful in understanding this temporary directory in more depth: post 1; post 2; post 3. You might find these useful if you want to learn more. However, for me, the above is as far as I wanted to go to understand its purpose.\n\n\nAccess the temporary directory using base::tempdir()\nAt the start of every session, R creates a temporary per-session directory, and it removes this temporary directory when the session ends.. This temporary directory is stored in the system’s temporary directory location (e.g., /var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/). R also provides several functions to work with the temporary directory, create, and interact with files within it.\nbase::tempdir() can be used to print the file path of the temporary directory on your system. Let’s run it and take a look at what happens.\n\ntempdir()\n\nOutputted to the terminal is the path to the R session’s temporary directory. When I ran it, the returned path looked like this:\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T//RtmpaYxspA\nThe temporary directory R uses for the current session is labeled using the RtmpXXXXXX pattern. The final six characters of the path (i.e., the Xs) are determined by the system. Note, tempdir() doesn’t create this directory, it just prints the temporary directory’s path to the console. This directory is created every time a R session begins.\nSince the temporary directory is just like any location on your computer, you can navigate to it from your terminal during an active R session. With your terminal pointing to the temporary directory, you can use the following code to find R’s per-session temporary directory:\nla | grep \"Rtmp\"\nLet’s take a peak at what’s in this directory. R’s list.files() function can be helpful in this case.\n\nlist.files(tempdir())\n\ncharacter(0)\n\n\nMost R setups should start with an empty per-session directory. So the above should return character(0). Despite being empty now, list.files() will become handy again once we start to write files to this location.\n\n\nWriting files to the temporary directory\nNow that we know a little more about this temp directory and where it is located on our system, let’s write some data to it. We can do this by doing something like the following.\n\nwrite_csv(mtcars, file = paste0(tempdir(), \"/mtcars.csv\"))\n\nNow when we list the files in the temporary directory (e.g., list.files(tempdir())), you should see the mtcars.csv file.\nIf you’re looking to create files with unique names, you can pass the tempfile() function to the file argument. This looks something like this:\n\nwrite_csv(\n  mtcars, \n  file = tempfile(pattern = \"mtcars\", fileext = \".csv\")\n)\n\ntempfile() creates unique file names, which concatenates together the file path, the character vector passed to the pattern argument, a random string in hex, and the character vector inputed to the fileext argument. When you list the files in the temporary directory now, you’ll see the initial mtcars.csv file along with a file that looks something like this: mtcars7eb3503ac74c.csv. The random hex string ensures files remain unique.\nIndeed, the above is just one way to write files to the temporary directory. You can use other methods to read and write files at this location. However, you now know what is needed to interact with this directory, read and write files to and from it. At this point you can do any data wrangling steps your project requires. After which, we can go about deleting our files from this directory.\n\n\nDeleting files with file.remove()\nAlthough these files will eventually be removed by the system, we should be proactive and clean up after ourselves.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using this approach within functions, especially if their intended to be used by other users, you’ll want to be clear they will write data to and remove data from the user’s system.\nIndeed, it’s considered poor practice to change the R landscape on a user’s computer without good reason. So the least we can do here is clean up after ourselves.\n\n\nTo delete our files we wrote to the temporary directory, run the following in the console:\n\nfile.remove(list.files(tempdir(), full.names = TRUE, pattern = \".csv\"))\n\n[1] TRUE TRUE\n\n\nThe arguments of the list.files() function should be pretty straightforward. We want file paths to be full length (i.e., full.names = TRUE) and to list only files with the .csv extension (i.e., pattern = \".csv\"). Then, we use these full file paths within the file.remove() function, which will remove the files from R’s temporary directory.\n\n\nWrap-up\nToday I learned more about R’s per-session temporary directory, and how it can be used to write files not intended for persistent storage. I also learned how to use several base R functions to create files within this temporary directory by using tempfile() and tempdir(). I also demonstrated how the list.files() function can be used to list files within any directory on your system, specifically using it to list files in R’s temporary directory. Finally, I highlighted how files in the temporary directory can be deleted using the file.remove() function.\nHave fun using R’s per-session temporary directory. Cheers 🎉!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Using `Base::tempdir()` for Temporary Data Storage},\n  date = {2023-11-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Using `Base::tempdir()` for Temporary Data\nStorage.” November 3, 2023."
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "title": "Calculating correlations with corrr",
    "section": "",
    "text": "Photo by Omar Flores\nToday I learned calculating, visualising, and exploring correlations is easy with the corrr package.\nIn the past, I would rely on Base R’s stats::cor() for exploring correlations. This function is a powerful tool if you’re looking to do additional analysis beyond investigating correlation coefficients. stats::cor() has its pain points, though. Sometimes, I just want a package to explore correlations quickly and easily.\nI recently stumbled across the corrr package. It met all the needs I listed above. The purpose of this post is to highlight what I’ve learned while using this package, and to demonstrate functionality I’ve found useful. To get started, let’s attach some libraries and import some example data.\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(here)"
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "title": "Calculating correlations with corrr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I was finishing this post, the Nebraska Women’s Volleyball team beat #1 Wisconsin in a five set thriller.↩︎\nThe Nebraska Women’s Volleyball team broke the World Record for a women’s sporting event on 2023-08-30. Official attendance was 92,003.↩︎"
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Here you’ll find posts related to things I’ve learned recently.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nUsing base::tempdir() for temporary data storage\n\n\n\n\n\n\n\ndata wrangling\n\n\nworkflow\n\n\nproductivity\n\n\n\n\nNeed to store data in a place that’s not persistent, use a temporary directory\n\n\n\n\n\n\nNov 3, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nCalculating correlations with corrr\n\n\n\n\n\n\n\ndata analysis\n\n\nexploratory analysis\n\n\ndata visualization\n\n\n\n\nUse the corrr package to calculate and visualize correlations\n\n\n\n\n\n\nOct 22, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nEdit an older unpushed commit\n\n\n\n\n\n\n\ngit\n\n\nGitHub\n\n\n\n\nUse git rebase to edit previous commit messages\n\n\n\n\n\n\nOct 14, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nFind and replace in Vim\n\n\n\n\n\n\n\nvim\n\n\nneovim\n\n\nproductivity\n\n\n\n\nImproving productivity by using Vim’s :substitute command\n\n\n\n\n\n\nFeb 24, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  }
]