[
  {
    "objectID": "til/posts/2025-01-26-til-notes-quotes-links-credibility-blogs/index.html",
    "href": "til/posts/2025-01-26-til-notes-quotes-links-credibility-blogs/index.html",
    "title": "Notes on link blogs and more frequent blog posting",
    "section": "",
    "text": "Background\nI’ve really been focusing on developing a daily writing habit. As a result, I’ve been actively generating ideas on how to more frequently publish blog content. My biggest mental hurdle for turning posts around is a deeply embedded feeling that every piece of writing I produce needs to be polished prose, and everything included needs to be absent of error.\n\n\nLinks and thoughts on link blogging\nI recently came across this quote from an interview with Simon Willison on the Real Python podcast with Christopher Bailey (the quote is mentioned around 10m08s):\n\nCredibility is accumulated over time, and it doesn’t take much, like a link blog about a subject run that for six months and you will become one of the top 0.1% people on earth with credibility on that subject just from publishing a few notes and linking to a bunch of things about it.\n\nI found Willison’s statement motivating, inspiring, and a little freeing. It made me reconsider portions of my own blog and what’s their aim. This is my little corner the internet, and I set the expectations and rules of this site.\nCertainly, I believe there is room for more formal, well thought-out forms of writing, like you can read in the blog section of my site. However, I also see the Today I Learned (TIL) section of my site being more like a link blog, a scratch-pad of sorts, and at times a research notebook on topics I’m learning about. Learning is a messy endeavour some times, so why does every piece of writing from it need to be tidy? It doesn’t.\n\n\nMini-blogging and open-source ecosystems\nKelli Bodwin addressed these same ideas during her posit::conf(2024) talk. The talk contained a call to action for attendees to produce more content, even if they weren’t fully formed topics or ideas. Just sharing what you’ve currently learned or are working on strengthens open-source ecosystems and makes them more sustainable. David Robinson’s Rstudio::conf(2019) keynote The unreasonable effectiveness of public work was also referenced, which I have queued up to watch later. Although I suggest listening to Bodwin’s entire talk, you can hear more about these specific ideas around 13m20s.\nWho knows where these thoughts may lead. I’m still learning, which is the most important.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes on Link Blogs and More Frequent Blog Posting},\n  date = {2025-01-26},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. “Notes on Link Blogs and More Frequent Blog\nPosting.” January 26, 2025."
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A now page, why?"
  },
  {
    "objectID": "now.html#projects-im-working-on",
    "href": "now.html#projects-im-working-on",
    "title": "Now",
    "section": "Projects I’m working on",
    "text": "Projects I’m working on\n\nTransitioning to LazyVim\nAs of my last update, I decided to burn down my Neovim setup and start over. I was in search of something more enjoyable and manageable to work with. I came across LazyVim and started to really enjoy it. Out of the box it had sane defaults and configurations, so the setup was minimal and I’ve found it more enjoyable to work with. LazyVim Extras also make it easy to install and configure different plugins.\n\n\nBlogging more\nI’m attempting to increase the publishing of my blog posts. I was following a cadance of roughly posting every month. I want to bump this up to two a month, with a stretch goal being every week. The hardest parts are being realistic with myself, identifying shorter post topics to write about, and being kinder to myself. Writing posts with a tighter turn-around means embracing progress over perfection.\n\n\nThinking about the use of data and organizational culture\nMore and more often I’m being tasked with stratigizing how to improve the role and impact data has within an organization. These are not easy questions to answer. I’ve been doing a lot of exploring and experimentation. If anyone has any suggestions, let’s chat.\nIf you’re interested in what I’ve focused on in the past, check out my past updates"
  },
  {
    "objectID": "now.html#books-im-reading",
    "href": "now.html#books-im-reading",
    "title": "Now",
    "section": "Books I’m reading",
    "text": "Books I’m reading"
  },
  {
    "objectID": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "href": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "title": "Now",
    "section": "A list of books I’ve read (ever since I’ve started keeping track)",
    "text": "A list of books I’ve read (ever since I’ve started keeping track)\n\nPractical SQL, 2nd Edition: A Beginner’s Guide to Storytelling with Data by Anthony DeBarros\nI’m currently working with another collegue to sharpen and practice our SQL skills. Although I’m up-to-date on most of the basics and am usually pretty good at completing straight-forward queries, it was a good time to focus and become even more proficient with the language. So, we picked up this book to learn more.\n\n\nR for Data Science (2e) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\nCurrently, I’m leading another Data Science Learning Community bookclub meeting for this book. I haven’t read the second edition yet, so I’m excited to see and learn what’s new.\n\n\nWith the Old Breed: At Peleliu and Okinawa by E.B. Sledge\nThis is a personal read I pick up from time to time. I finished watching the HBO miniseries Band of Brothers and The Pacific, which were based on first hand accounts in books like this. With the Old Breed is a first-hand account of one serviceman’s experience on the frontline during operations in the Pacific Theater, specifically in Peleliu and Okinawa, during World War II. It’s unfathomable to comprehend the experiences and hardships many endured. Reading it has made me more deeply appreciate the sacrafices men and women of the armed services make for the United States of America.\n\n\nProfessional development reads\n\nMachine Learning with R - Fourth Edition by Brett Lantz\n\n\nPractical Tableau by Ryan Sleeper\n\n\nHBR Guide to Making Every Meeting Matter from the Harvard Business Review\n\n\nThe Checklist Manifesto: How to Get Things Right by Atul Gawande\n\n\nTidy Modeling with R by Max Kuhn and Julia Silge\n\n\nPython for Data Analysis by Wes McKinney\n\n\nEngineering Production-Grade Shiny Apps by Colin Fay, Sébastien Rochette, Vincent Guyader, and Cervan Girard\n\n\nAdvanced R by Hadley Wickham. Check out past book club meeting recordings here.\n\n\nR Packages by Hadley Wickham and Jenny Bryan. Check out past book club meeting recordings here.\n\n\nVim help files maintained by Carlo Teubner\n\n\nMastering Ubuntu by Jay LaCroix\n\n\nGoogle BigQuery: The Definitive Guide by Valliappa Lakshmanan and Jordan Tigani\n\n\nMastering Shiny by Hadley Wickham. Check out the past book club meeting recordings here.\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. Check out the past book club meeting recordings here.\n\n\nDocker Deep Dive by Nigel Poulton\n\n\n\nPersonal reads\n\nThe Currents of Space (Galactic Empire 2) by Issac Asimov\n\n\nThe Stars, Like Dust (Galactic Empire 1) by Issac Asimov\n\n\nRobots and Empire (The Robot Series 4) by Issac Asimov\n\n\nThe Robots of Dawn (The Robot Series 3) by Isaac Asimov\n\n\nThe Naked Sun (The Robot Series Book 2) by Isaac Asimov\n\n\nThe Caves of Steel (The Robot Series Book 1) by Isaac Asimov\n\n\nI, Robot by Isaac Asimov\n\n\nLeviathan Falls (The Expanse book 9) by James S.A. Corey\n\n\nTiamat’s Wrath (The Expanse book 8) by James S.A. Corey\n\n\nPersepolis Rising (The Expanse book 7) by James S.A. Corey\n\n\nBabylon’s Ashes (The Expanse book 6) by James S.A. Corey\n\n\nNemesis Games (The Expanse book 5) by James S.A. Corey\n\n\nCibola Burn (The Expanse book 4) by James S.A. Corey\n\n\nAbaddon’s Gate (The Expanse book 3) by James S.A. Corey\n\n\nCaliban’s War (The Expanse book 2) by James S.A. Corey\n\n\nLeviathon Wakes (The Expanse book 1) by James S.A. Corey\n\n\nThe Galaxy, and the Ground Within: A Novel (Wayfarers 4) by Becky Chambers\n\n\nRecord of a Spaceborn Few (Wayfarers 3) by Becky Chambers\n\n\nA Closed and Common Orbit (Wayfarers 2) by Becky Chambers\n\n\nThe Long Way to a Small, Angry Planet (Wayfarers 1) by Becky Chambers\n\n\nLast of the Breed by Louis L’Amour\n\n\nProject Hail Mary by Andy Weir\n\n\nFirebreak by Nicole Kornher-Stace\n\n\nDune Messiah by Frank Herbert\n\n\nDune by Frank Herbert\n\n\nThe Martian: A Novel by Andy Weir"
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Here you’ll find posts related to things I’ve learned recently.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAug 23, 2025\n\n\nNotes: Using gganimate to animate plots\n\n\nLearning more about data visualization animation from the ‘Getting Started’ vignette\n\n\n\n\nAug 3, 2025\n\n\nNotes from Rich Hickey’s talk Simple Made Easy\n\n\nLearning more about software and system design\n\n\n\n\nFeb 23, 2025\n\n\nNotes on customizing Neovim’s key bindings\n\n\nLearning more about how to customize the R.nvim plugin while using LazyVim\n\n\n\n\nFeb 15, 2025\n\n\nNotes on date and time data types in PostgreSQL\n\n\nA scratchpad of concepts and code examples I collected while learning about the dates and times data types\n\n\n\n\nFeb 1, 2025\n\n\nNotes on identifying explicit and implicit missing values\n\n\nHighlights from the DSLC book club discussion of Chapter 18: Missing values from R4DS\n\n\n\n\nJan 27, 2025\n\n\nNotes on the use and management of GitHub projects\n\n\nLearning how to take project management to the next level\n\n\n\n\nJan 26, 2025\n\n\nNotes on link blogs and more frequent blog posting\n\n\nSome notes, quotes, and links about writing more and sharing your work\n\n\n\n\nDec 30, 2024\n\n\nUse list2env() or glue::glue_data() to use a set of elements from a tibble in a string\n\n\nNeed an easy way to access a set of elements from a tibble for string interpolation? Here’s two examples\n\n\n\n\nDec 27, 2024\n\n\nSeparate character strings into rows and columns using tidyr functions\n\n\nNeed to separate strings? Use the separate_* family of functions\n\n\n\n\nDec 8, 2024\n\n\nSummarize logical vectors to calculate numeric summaries\n\n\nNeed proportion and count summaries from a logical vector? Use mean() and sum()\n\n\n\n\nFeb 10, 2024\n\n\nUse base::dput() to easily create and save objects\n\n\nNeed to create and store an object quickly, use this trick\n\n\n\n\nDec 23, 2023\n\n\nCombine plots using patchwork\n\n\nNeed to add two or more plots together? Use the patchwork package\n\n\n\n\nNov 3, 2023\n\n\nUsing base::tempdir() for temporary data storage\n\n\nNeed to store data in a place that’s not persistent, use a temporary directory\n\n\n\n\nOct 22, 2023\n\n\nCalculating correlations with corrr\n\n\nUse the corrr package to calculate and visualize correlations\n\n\n\n\nOct 14, 2023\n\n\nEdit an older unpushed commit\n\n\nUse git rebase to edit previous commit messages\n\n\n\n\nFeb 24, 2023\n\n\nFind and replace in Vim\n\n\nImproving productivity by using Vim’s :substitute command\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "I’m a media research analyst, data enthusiast, and news, sports, and podcast aficianado.\nProfessionally, I use data, audience measurement, and marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. I am particularly interested in the use and development of open-source statistical software (i.e. R) to achieve this goal, and gaining a broader understanding of the role these tools play in media, digital, and marketing analytics. I also adjunct university courses on the side.\nListening to NPR, watching PBS (especially NOVA), and college football and baseball are my jam.\n\n\nWant to know more about what I’m currently working on, reading, or mastering? Check out the now page.\n\n\n\n\nPh.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota\n\n\n\n\n\nDigital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#now",
    "href": "index.html#now",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Want to know more about what I’m currently working on, reading, or mastering? Check out the now page."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Ph.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Digital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "TidyTueday contribution: 2028-08-05\n\n\n\n\n\n\ndata visualization\n\n\ntidytuesday\n\n\n\nIncome Inequality Before and After Taxes\n\n\n\n\n\nAug 10, 2025\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s Worth Your Time: Read, Watch, and Listen\n\n\n\n\n\n\nreflection\n\n\ndata science\n\n\ndata analysis\n\n\nresources\n\n\n\nA curated list of data science, analysis, coding, tech-related, and miscellaneous work I’ve found to be useful and impactful\n\n\n\n\n\nJan 11, 2025\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hex Update: October, 2024\n\n\n\n\n\n\nthe hex update\n\n\nmedia\n\n\n\nKey insights and what I learned about the media industry as of October 2024\n\n\n\n\n\nNov 2, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow walkthrough: Interacting with Google BigQuery in R\n\n\n\n\n\n\nworkflow\n\n\ntutorial\n\n\nproductivity\n\n\nbigquery\n\n\nsql\n\n\n\nA tutorial on how to use the bigrquery package\n\n\n\n\n\nOct 5, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: k-means clustering of Google Analytics 4 data\n\n\n\n\n\n\nmachine learning\n\n\nunsupervised learning\n\n\nk-means clustering\n\n\n\nA tutorial on how to perform k-means clustering using Google Analytics data\n\n\n\n\n\nSep 14, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hex Update: June, 2024\n\n\n\n\n\n\nthe hex update\n\n\nmedia\n\n\n\nKey insights and what I learned about the media industry in June 2024\n\n\n\n\n\nJul 10, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: Market basket analysis of online merchandise store data\n\n\n\n\n\n\nmachine learning\n\n\nunsupervised learning\n\n\nassociation rules\n\n\nmarket basket analysis\n\n\n\nA tutorial on how to perform a market basket analysis using Google Analytics data\n\n\n\n\n\nJun 11, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring objects launched into space and gross domestic product\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nregression\n\n\n\nA contribution to the 2024-04-23 #tidytuesday social data project\n\n\n\n\n\nMay 3, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring data from the Fiscal Sponsor Directory\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-03-12 #tidytuesday social data project\n\n\n\n\n\nMar 22, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the relationship between trash processed by Mr. Trash Wheel and precipitation\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-03-05 #tidytuesday social data project\n\n\n\n\n\nMar 12, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the lifespans of historical figures born on a Leap Day\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-02-27 #tidytuesday social data project\n\n\n\n\n\nMar 5, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploring R Consortium ISC Grants\n\n\n\n\n\n\ndata wrangling\n\n\ndata visualization\n\n\ntidytuesday\n\n\nplotly\n\n\nTableau\n\n\n\nA contribution to the 2024-02-20 #tidytuesday social data project\n\n\n\n\n\nFeb 26, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n30 day tidymodels recipes challenge\n\n\n\n\n\n\nmachine learning\n\n\nfeature engineering\n\n\ntidymodels\n\n\ndata wrangling\n\n\n\nLearning how to use the recipes package, one day at a time\n\n\n\n\n\nJan 1, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nMessing with models: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data\n\n\n\n\n\n\ntutorial\n\n\ntidymodels\n\n\nclassification\n\n\ndecision tree\n\n\nlogistic regression\n\n\n\nUsing tidymodels to predict wins and losses for volleyball matches\n\n\n\n\n\nDec 7, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n2023 data science rig: Set up and configuration\n\n\n\n\n\n\ntutorial\n\n\n\nOverviewing and reflecting on my current data science setup.\n\n\n\n\n\nJan 29, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nFlattening Google Analytics 4 data\n\n\n\n\n\n\nBigQuery\n\n\nsql\n\n\ndata wrangling\n\n\n\nLet’s deep dive into working with Google Analytics data stored in BigQuery.\n\n\n\n\n\nSep 20, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory analysis of Google Analytics 4 data for forecasting models\n\n\n\n\n\n\nforecasting\n\n\n\nExploring Google Analytics 4 data for forecasting models.\n\n\n\n\n\nMar 3, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nShiny summary tiles\n\n\n\n\n\n\nshiny\n\n\n\nBuilding custom metric summary tiles for Shiny.\n\n\n\n\n\nDec 30, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n2021 PBS TechCon: Your Data is Disgusting!\n\n\n\n\n\n\ntalks\n\n\n\nI was fortunate to be invited to present about topics I’m passionate about: tidy data and data pipelines.\n\n\n\n\n\nOct 19, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a next and back button in Shiny\n\n\n\n\n\n\nshiny\n\n\n\nTaking the time to understand a challenging question from Mastering Shiny.\n\n\n\n\n\nSep 12, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\nIntro Post\n\n\n\n\n\n\npersonal\n\n\n\nHello World!, my name is Collin, and this is my blog.\n\n\n\n\n\nApr 2, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2025-01-11-post-data-science-analysis-things-you-should-read-watch-listen/index.html",
    "href": "blog/posts/2025-01-11-post-data-science-analysis-things-you-should-read-watch-listen/index.html",
    "title": "What’s Worth Your Time: Read, Watch, and Listen",
    "section": "",
    "text": "Photo by Cristina Gottardi\n\n\n\nMotivation\nWe’re a week into 2025, and I thought this would be a perfect time for some reflection. Inspired by Tan Ho’s post, I felt compiling a non-exhaustive list of content I’ve enjoyed would be a valuable, useful exercise to kick off the new year. Rather than have these resources sit untouched in my bookmarks, I thought I’d share.\nBelow you will find sections containing curated lists of posts, videos, and other content I’ve found relevant to shaping how I view and approach my work. Much of the content is data science, analysis, technology, career development, and media industry related. Some links are just stuff I’ve found interesting.\nEnjoy!\n\n\nData science, analysis, coding\n\nJenny Bryan: Project-oriented workflow, blog post (~15min read)\n\nThis is one of those pieces that was a paradigm shift in my thinking: self-contained projects. A simple idea with a major impact. In the past, my workflow would be a hodgepodge mess of errant scripts and files. The idea of using some simple conventions to allow my code to work on ‘another computer’ or in an ‘another environment’ was absent. Employing some of the post’s conventions increased my impact, as I was better positioned to share my work with others.\n\nJenny Bryan: Object of type ‘closure’ is not subsettable, YouTube video (53 mins)\n\nIf you work with the R programming language, this video is worth every minute. Learning how to debug code helps you more quickly identify where something is going wrong, determine what is going wrong, and come to a useful solution.\n\nEmily Riederer: RMarkdown Driven Development (RmdDD), blog post (~20 min read)\n\nRmdDD (now QmdDD for some) just works for analysis project development. A straightforward idea: get your entire process into one file, then refactor from there. This is how I generally approach most analysis projects now.\n\nSharla Gelfand: Don’t repeat yourself, talk to yourself! Reporting with R | RStudio(2020), YouTube video (21 mins)\n\nThe key takeaway: think of ways to incorporate structure into your projects that will be helpful the next time you have to complete something. This talk motivated me to be more intentional about project structure and to think about how this structure relates to the impact of my work. There’s power in being nice to your future self and others.\n\nEmily Riederer: Building a team of internal R packages, blog post (~35 mins)\n\nThis post outlines a framework for the development of internal R packages. It frames internal package development as value adds when they act like work colleagues, as they can be helpful to pass along institutional knowledge to others. I aspire to align my internal package development with the suggestions in this post.\n\nDavid Robinson: TidyTuesday live screencasts, YouTube videos (~45mins to ~1hr 15mins)\n\nAlthough David Robinson hasn’t posted a screencast in some time, I still find great value in these, now older, videos. Just watching someone and hearing their thought process as they work to visualize TidyTuesday datasets has been super valuable. I’ve learned a lot from this great resource.\n\nColin Gillespie: Getting the Most Out of Git - posit::conf(2024), YouTube video (21 mins)\n\nDespite this talk’s focus being about Git, I think there’s an even more general lesson shared here. That is, use only what’s needed for the type of project you’re working on and don’t fall into the trap of over burdening yourself and/or team with unneeded complexity. Git and platforms like GitHub have lots of functionality. Critical software is built using these tools, where their use is necessary. But, does my personal project need a DevOps strategy with fully automated testing and app deployment to be successful? Probably not. Can my team get some value from running a few automated tests when they submit a pull request? Maybe, as long as it doesn’t overburden them to get their work done. I struggle with finding the right balance here from time to time. This talk is a freeing reminder to use only what’s needed for the project.\n\n\n\n\nProductivity and project development\n\nJenny Bryan: How to Name Files Like a Normie NormConf YouTube video (5 mins) and GitHub repo (10 mins)\n\nTake five minutes to change your life forever. Regardless if you code or not, everyone who uses a computer needs to develop the skills for naming files. Incorporating some simple naming conventions can be a big productivity boost.\n\nHank Green: The Secret to My Productivity, YouTube video (4 mins). Thanks for sharing, Tan.\n\nI came away with a simple takeaway from this video: get your projects to 80%, share it with the world, and move on to the next. Too often we fall into the trap of attempting to get things perfect. Although striving for perfection makes you feel productive, in reality it slows your ability to learn.\n\n\n\n\nManagement, leadership, and organizational culture\n\nEmilie Schario & Taylor A. Murphy, PhD: Run Your Data Team Like a Product Team, blog post (~20 mins)\n\nI like the framing this post uses to describe how to build and run a data organization. It posits two things data teams need to do to meet their full potential: focus on building a Data Product and view others within your organization as your customers. Indeed, I certainly don’t contend this will work for every data team, but I found it a useful vision for the work my team does.\n\nJD Long: It’s Abstractions All the Way Down… - posit::conf(2023) YouTube recording (1hr 2mins)\n\nThis talk provided a clear framework to better understand where my work fits within a larger organization / network. Framing these ideas in terms of abstractions and providing thoughts on how to understand and work these abstractions has impacted how I approach and view the work within my organization.\n\nHarvard Business Review: HBR Guide to Making Every Meeting Matter, book (~$22).\n\nMeetings are inevitable. In fact, you may be tasked with leading a meeting some time in your career. Some meetings move the needle, others could have been an email. Prepare yourself with some useful techniques to make meetings more impactful, useful.\n\n\n\n\nMedia industry specific (the industry I work in)\n\nDecoder podcast, The Verge (1hr episodes)\n\nIf you’re interested in media or tech, this podcast is for you. Nilay Patel, the host, does a phenomenal job interviewing various CEOs and thought leaders in these industries. Listening to each episode has kept me up to date with the big trends and news happening in these spaces.\n\n\n\n\nPotpourri\nThis section is a miscellaneous collection of stuff I’ve enjoyed but really doesn’t fit into the other categories.\n\nMarketplace weekday radio show and podcast (~30 min episodes)\n\nI like to keep up with business and economic news. Marketplace is an excellent, weekday collection of stories focused on these topics.\n\nPlanet Money, podcast (~30 min episodes) National Public Radio (NPR)\n\nUnderstanding the complexities of the economy can be challenging. Planet Money helps put these topics into perspective.\n\n\n\n\nFinal thoughts\nThe above list is certainly not comprehensive of all the work I’ve found useful and impactful. Some content that has shaped my work and views is not included here. It will expand over time.\nI hope you found at least one useful takeaway. If so, let’s connect and chat about it:\n\nLinkedIn\nGitHub\nBluesky\n\nHere’s to a wonderful start to 2025. Cheers 🎉!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {What’s {Worth} {Your} {Time:} {Read,} {Watch,} and {Listen}},\n  date = {2025-01-11},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. “What’s Worth Your Time: Read, Watch, and\nListen.” January 11, 2025."
  },
  {
    "objectID": "til/posts/2025-01-27-til-notes-github-projects/index.html",
    "href": "til/posts/2025-01-27-til-notes-github-projects/index.html",
    "title": "Notes on the use and management of GitHub projects",
    "section": "",
    "text": "I recently spent some time focusing on my approach to project management, in hopes of developing skills to be a better team lead. One way I try to manage work is using GitHub Projects.\nBelow are some links and notes about what I’ve recently learned about GitHub projects.\nIn the spirit of attempting to do more link- and micro-blogging, some of these notes may seem disjointed, incomplete, or incohorent. However, it’s what I’ve learned thus far about managing projects in GitHub."
  },
  {
    "objectID": "til/posts/2025-01-27-til-notes-github-projects/index.html#adding-issues-to-a-project",
    "href": "til/posts/2025-01-27-til-notes-github-projects/index.html#adding-issues-to-a-project",
    "title": "Notes on the use and management of GitHub projects",
    "section": "Adding issues to a project",
    "text": "Adding issues to a project\nUse the ‘+ Add item’ to add issues or pull requests to a project. You can then locate the issue via the UI prompts, or you can paste the issue or pull request URL (very helpful). The bulk add issues and pull requests and Adding multiple issues or pull requests from a repository features seem really useful."
  },
  {
    "objectID": "til/posts/2025-01-27-til-notes-github-projects/index.html#adding-fields-to-help-manage-projects",
    "href": "til/posts/2025-01-27-til-notes-github-projects/index.html#adding-fields-to-help-manage-projects",
    "title": "Notes on the use and management of GitHub projects",
    "section": "Adding fields to help manage projects",
    "text": "Adding fields to help manage projects\nAn iteration field seems useful for managing sprint intervals. More about how to setup an iteration field can be found here."
  },
  {
    "objectID": "til/posts/2025-01-27-til-notes-github-projects/index.html#useful-commands-for-managing-issues-and-projects",
    "href": "til/posts/2025-01-27-til-notes-github-projects/index.html#useful-commands-for-managing-issues-and-projects",
    "title": "Notes on the use and management of GitHub projects",
    "section": "Useful commands for managing issues and projects",
    "text": "Useful commands for managing issues and projects\nCreate an issue with an interactive prompt.\ngh issue create\nSometimes you fall into common patterns when creating issues, so command flags are often helpful.\ngh issue create\ngh issue create -a \"@me\" -t \"New project to work on\" -l \"project\"\ngh issue create -a \"@coworker\" -t \"Fix this\" -l \"bug\"\nMore about how to manage issues in a repo via the command line can be found here.\nYou can list all your projects using the following commands:\n# Your projects\ngh project list\n\n# Organizational owned projects\ngh project list --owner owner_of_project\nView the project either in the command line or open it in a web browser.\n# From command-line\ngh project view 5\n\n# Open in web browser\ngh project view 5 --web\nYou can edit the project README by running the following:\ngh project edit 5 --readme \"Here be some info about the project\"\nHowever, this command seems to only allow you to add a new README and not edit it."
  },
  {
    "objectID": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html",
    "href": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html",
    "title": "Notes on identifying explicit and implicit missing values",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\nlibrary(nycflights13)\nToday I learned more about identifying explicit and missing values in R. During our weekly Data Science Learning Community’s (DSLC) bookclub meeting for the R for Data Science (R4DS) book, I was re-introduced to several methods to identify explicit and implicit missing values. Much of what is covered here comes from Chapter 18: Missing values of the book. I wanted to share what I’ve learned, in hopes I can better remember this information in the future."
  },
  {
    "objectID": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#base-rs-sapply",
    "href": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#base-rs-sapply",
    "title": "Notes on identifying explicit and implicit missing values",
    "section": "Base R’s sapply()\n",
    "text": "Base R’s sapply()\n\nThe first suggestion was to use base R’s sapply() with an anonymous function. There’s two variations: one that identifys the presence of any NAs across the columns. The second provides a count of NAs for each column.\n\nsapply(starwars, function(x) any(is.na(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year        sex     gender \n     FALSE       TRUE       TRUE       TRUE      FALSE      FALSE       TRUE       TRUE       TRUE \n homeworld    species      films   vehicles  starships \n      TRUE       TRUE      FALSE      FALSE      FALSE \n\n\n\nsapply(starwars, function(x) sum(is.na(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year        sex     gender \n         0          6         28          5          0          0         44          4          4 \n homeworld    species      films   vehicles  starships \n        10          4          0          0          0"
  },
  {
    "objectID": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#purrrmap_df-with-any-and-is.na",
    "href": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#purrrmap_df-with-any-and-is.na",
    "title": "Notes on identifying explicit and implicit missing values",
    "section": "\npurrr::map_df with any() and is.na()\n",
    "text": "purrr::map_df with any() and is.na()\n\nSimilar to the base R approach is the use of purrr::map_df() with an anonymous function. I’m quite partial to this approach, as it’s even more succinct, though it requires purrr as a dependency. However, if you’re already importing the tidyverse into your session, then why not go ahead and use it?\n\nmap_df(starwars, \\(x) any(is.na(x)))\n\n# A tibble: 1 × 14\n  name  height mass  hair_color skin_color eye_color birth_year sex   gender homeworld species films\n  &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt; &lt;lgl&gt;      &lt;lgl&gt;      &lt;lgl&gt;     &lt;lgl&gt;      &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt;     &lt;lgl&gt;   &lt;lgl&gt;\n1 FALSE TRUE   TRUE  TRUE       FALSE      FALSE     TRUE       TRUE  TRUE   TRUE      TRUE    FALSE\n# ℹ 2 more variables: vehicles &lt;lgl&gt;, starships &lt;lgl&gt;\n\n\n\nmap_df(starwars, \\(x) sum(is.na(x)))\n\n# A tibble: 1 × 14\n   name height  mass hair_color skin_color eye_color birth_year   sex gender homeworld species films\n  &lt;int&gt;  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;int&gt;     &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;int&gt;     &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n1     0      6    28          5          0         0         44     4      4        10       4     0\n# ℹ 2 more variables: vehicles &lt;int&gt;, starships &lt;int&gt;"
  },
  {
    "objectID": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#dplyrsummarise",
    "href": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#dplyrsummarise",
    "title": "Notes on identifying explicit and implicit missing values",
    "section": "dplyr::summarise()",
    "text": "dplyr::summarise()\nAnother approach involved the use of dplyr’s summarise() along with across(), everything(), and an anonymous function. This approach was meant only to count the amount of missing values within each column.\n\nstarwars |&gt;\n  summarise(across(everything(), \\(x) sum(is.na(x))))\n\n# A tibble: 1 × 14\n   name height  mass hair_color skin_color eye_color birth_year   sex gender homeworld species films\n  &lt;int&gt;  &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;int&gt;     &lt;int&gt;      &lt;int&gt; &lt;int&gt;  &lt;int&gt;     &lt;int&gt;   &lt;int&gt; &lt;int&gt;\n1     0      6    28          5          0         0         44     4      4        10       4     0\n# ℹ 2 more variables: vehicles &lt;int&gt;, starships &lt;int&gt;"
  },
  {
    "objectID": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#skimrskim",
    "href": "til/posts/2025-02-01-til-notes-r-identify-missing-values/index.html#skimrskim",
    "title": "Notes on identifying explicit and implicit missing values",
    "section": "skimr::skim()",
    "text": "skimr::skim()\nskimr::skim() was also discussed, though the output is more verbose than the other options. The output contains a sum of the number of missing values within each column. This is certainly the most succinct way to obtain this information, and it provides additional summary information about your data. However, it may be more information then you need to answer your question about the presence of missing values in your data.\n\nskim(starwars)\n\n\nData summary\n\n\nName\nstarwars\n\n\nNumber of rows\n87\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nlist\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1.00\n3\n21\n0\n87\n0\n\n\nhair_color\n5\n0.94\n4\n13\n0\n11\n0\n\n\nskin_color\n0\n1.00\n3\n19\n0\n31\n0\n\n\neye_color\n0\n1.00\n3\n13\n0\n15\n0\n\n\nsex\n4\n0.95\n4\n14\n0\n4\n0\n\n\ngender\n4\n0.95\n8\n9\n0\n2\n0\n\n\nhomeworld\n10\n0.89\n4\n14\n0\n48\n0\n\n\nspecies\n4\n0.95\n3\n14\n0\n37\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\nfilms\n0\n1\n24\n1\n7\n\n\nvehicles\n0\n1\n11\n0\n2\n\n\nstarships\n0\n1\n16\n0\n5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nheight\n6\n0.93\n174.60\n34.77\n66\n167.0\n180\n191.0\n264\n▂▁▇▅▁\n\n\nmass\n28\n0.68\n97.31\n169.46\n15\n55.6\n79\n84.5\n1358\n▇▁▁▁▁\n\n\nbirth_year\n44\n0.49\n87.57\n154.69\n8\n35.0\n52\n72.0\n896\n▇▁▁▁▁"
  },
  {
    "objectID": "til/posts/2025-02-15-til-links-notes-sql-postgresql-date-time-types/index.html",
    "href": "til/posts/2025-02-15-til-links-notes-sql-postgresql-date-time-types/index.html",
    "title": "Notes on date and time data types in PostgreSQL",
    "section": "",
    "text": "Today I spent some time learning more about date and time data types in PostgreSQL. Much of this was learned reviewing PostgreSQL’s documentation and some other reading (check out the list at the end of the post if you want to go deeper). This documentation deep dive will cover what I’ve learned recently about working with these data types, which includes:\n\nDate and time data types available in PostgreSQL\nArithmetic with date and time data types\nUseful date and time functions\nExtracting specific elements from dates or times\nBeing aware of and managing the trickiness of time zones\n\nBelow are notes, links, and some examples overviewing what I’ve learned recently. This post is a bit of a scratchpad, and it will only be lightly edited. Be aware, there will likely be grammatical or syntactical errors. This post also does not seek to be a comprehensive overview, but rather it aims to be a collection of topics I thought would be useful to refer back when working with date and time data types in PostgreSQL. I prioritized resource and documentation links to aid in deeper review of these topics.\n\nDate and time data types\nFour data types are available to represent dates and times within a PostgreSQL database:\n\ntimestamp or timestamp with timezone is a data type representing a specific date and time.\ndate is a data type recording some exact date.\ntime is a data type recording some point in time.\ninterval is a value representing some unit of time.\n\nPostgreSQL’s documentation further details date and time types.\nPostgreSQL has some special date and time input strings, which include the following:\nSELECT\n    'epoch'::timestamp, -- 1970-01-01 00:00:00+00\n    'now'::timestamp, -- Current transaction start time\n    'today'::timestamp, -- Midnight today\n    'tomorrow'::timestamp, -- Midnight tomorrow\n    'yesterday'::timestamp; -- Midnight yesterday\nIf we need to return current date or time values, the following functions are useful within a simple SELECT statement:\nSELECT\n    CURRENT_DATE,\n    CURRENT_TIME,\n    CURRENT_TIMESTAMP,\n    LOCALTIME,\n    LOCALTIMESTAMP;\nWe can further observe some specific examples of these types by running the following SQL statement:\nSELECT\n    '2024-01-01'::date,\n    '01:32:02.732'::time without time zone,\n    '01:32:02.732 CST'::time with time zone,\n    '2 years 3 months 12 hours'::interval;\nIntervals are also an interesting data type, which represent some unit of time as a number. For instance, you can write a SQL statement like the following:\nSELECT\n    '1.8 weeks'::interval,\n    '15 seconds'::interval,\n    '2 years'::interval,\n    '2 decades'::interval,\n    '4 13:33:33'::interval,\n    '100-09'::interval,\n    'P0020-10-05T23:10:16'::interval;\n\n\nArithmetic with date and time data types\nArithmetic operations can also be performed on date and time data types. Here’s a few examples:\n-- date + integer &gt;&gt; date\n-- Add one day\nSELECT current_date + 1 as tomorrow;\n\n-- date + interval &gt;&gt; date\nSELECT current_date + '1 year'::interval as a_year_from_today;\n\n-- date - date &gt;&gt; integer\n-- Number of days elapsed\nSELECT current_date - '2025-01-01'::date days_in_year;\n\n-- interval * double precision &gt;&gt; interval\nSELECT interval '1 hour' * 24 as hours_in_day;\nPostgreSQL’s docs goes into more detail and provides additional examples of the use of these operations.\n\n\nUseful date and time functions\nPostgreSQL provides several useful date and time functions. Some functions get a current date, date time, or time. Some are date and time constructors. Others assist in the completion of some type of operation.\nSELECT\n    now(), -- Current date and time\n    timeofday(), -- Current date and time formatted\n    current_time(0) as hmstz, -- Current time of day\n    localtime(0), -- Current time of day, with less precision\n    localtimestamp(0), -- Current date and time\n    make_date(2025, 02, 09), -- Create a date from integer values\n    make_time(9, 50, 40.5), -- Create a time with integer values\n    to_timestamp(1739116840), -- Unix epoch to timestamp with time zone\n    statement_timestamp(), -- Timestamp at the start of the statement\n    age(\n        timestamp '2025-02-09',\n        timestamp '1985-11-12'\n    ) AS how_old, -- Symbolic representation of age\n    date_bin(\n        '7 minutes',\n        timestamp '2025-02-09 09:50:40', timestamp '2025-02-09 00:00:00'\n    ), -- Bin into specified intervals, given a specific origin\n    date_trunc(\n        'hour',\n        timestamp '2025-02-09 09:50:40'\n    ), -- Truncate to a specific date or time unit\n    date_part(\n        'day',\n        timestamp '2025-02-09 09:50:40'\n    ); -- Extract a specific unit from a timestamp\n\n\nExtracting specific elements from date or time values\nSay we just want one element from our date objects, we can use PostgreSQL’s date_part() function. For example:\nSELECT\n  date_part('year', '2024-01-01'::date) as year,\n  date_part('month', '2024-01-01'::date) as month,\n  date_part('day', '2024-01-01'::date) as day,\n  date_part('epoch', '2024-01-01'::date) as epoch; -- # of seconds elapsed since 1970-01-01\nAdditional elements can be extracted using date_part, especially if you have a timestamp with a timezone field.\nThe extract function is also useful to extract subfields from date, date time, or time values. Below are several examples I thought would be useful.\nSELECT\n    EXTRACT(\n        DAY FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        DOW FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        MONTH FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        QUARTER FROM TIMESTAMP '2025-02-09 09:50:40'\n    ),\n    EXTRACT(\n        EPOCH FROM TIMESTAMP '2025-02-09 09:50:40'\n    )\n;\n\n\nThe trickiness of time zones\nIt’s critical to be aware of time zones when using date and time values, so it’s always good to be aware of the current time zone setting used for the database system you’re working with. There’s two ways to check this setting while working with a PostgreSQL database:\nSHOW timezone;\n-- or\nSELECT current_setting('timezone');\nThe current_setting() function can be handy for when you need to create a timestamp using the system’s time zone (example via)\nSELECT make_timestamptz(2025, 2, 02, 10, 39, 22.5, current_setting('timezone'))\nI also like that you can find all the time zones and narrow it down to a specific region by doing the following (example via):\nSELECT * FROM pg_timezone_names\nWHERE name LIKE 'America%'\nORDER BY name;\nThe table returned from the previous example also contains an is_dst field. This column denotes whether the timezone is exhibiting day lights savings time or not. This is useful because day lights savings time is a function of geography and politics. Not all regions of the world exhibit day lights savings time uniformly. Take for example Lord Howe Island. How this part of the world observes day lights savings time and its time zones is some interesting reading.\n\n\nAdditional resources\nHere’s a collection of additional resources to go deeper:\n\nDate/Time Types from the PostgreSQL documentation\nData Type Formatting Functions from the PostgreSQL documentation\nDate/Time Functions and Operators from the PostgreSQL documentation\nChapter 13: Working with dates and times from Practical SQL, 2nd Edition: A Beginner’s Guide to Storytelling with Data\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes on Date and Time Data Types in {PostgreSQL}},\n  date = {2025-02-15},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. “Notes on Date and Time Data Types in\nPostgreSQL.” February 15, 2025."
  },
  {
    "objectID": "til/posts/2025-02-23-til-notes-links-r-nvim-key-bindings/index.html",
    "href": "til/posts/2025-02-23-til-notes-links-r-nvim-key-bindings/index.html",
    "title": "Notes on customizing Neovim’s key bindings",
    "section": "",
    "text": "Background\nThe following post provides some notes on setting custom key bindings for R.nvim, a plugin that adds R support to Neovim. Specifically, this post focuses on how to set custom key bindings while using LazyVim. This involved some initial trial and error, along with some review of the docs and several resources. I’ve linked all the resources or point to relevant docs that helped me figure out how to do this.\n\n\nMotivation\nWhile transitioning over to LazyVim, I inherited some of my old config files into this new setup. This resulted in some conflicts with the new setup. Some of my old config’s key bindings overlapped with some of LazyVim’s global key bindings. Specifically, I wanted to map &lt;localleader&gt;L to run devtools::load_all(), but it was already mapped to bring up the change log for LazyVim. So, I wanted to modify this to better fit my workflow.\n\n\nDisabling global key bindings\nLazyVim’s website is pretty clear that global key bindings are disabled with the vim.keymap.del function. Another good starting point was the docs, which are reviewable by running help vim.keymap.del via nvim’s command line prompt (opened by pressing :).\nThe vim.keymap.del function has three parameters:\n\n{modes} - a string or an array of strings you want the key binding to be made available (e.g., n = normal mode; i = insert mode; v = visual mode).\n{lhs} - a string of the key map you’re looking to disable.\n{opts} - a lua table with any additional options you want to pass along.\n\nIn my case, disabling the current &lt;leader&gt;L key binding was pretty straightforward, so I added the following line to my ~/.config/lazyvim/lua/config/keymaps.lua file.\nvim.keymap.del(\"n\", \"&lt;leader&gt;L\")\nThis disabled the key binding from my current configuration. Now it was time to modify R.nvim in my config.\n\n\nCustomizing R.nvim’s key bindings\nBeyond disabling the global key binding, I wasn’t too sure where to start when it came to setting up a custom key bindings for LazyVim.\nAndrew Courter’s (@ascourter) video broadly overviewed the steps for setting up a key bindings for a plugin, and I found it to be a good starting point. The specific steps are detailed at ~4m53s of the video.\nNext, I consulted R.nvim’s docs to better understand how the plugin expected key bindings to be defined. I use LazyVim Extras along with a plugin extension file (located at ~/.config/lazyvim/lua/plugins/extend-r-nvim.lua) to manage setup and configuration of this plugin. This extension file follows the conventions detailed in the R.nvim-key-bindings section of the docs for the custom key bindings definitions.\nThe custom key binding configuration is defined deeply within several nested levels of a lua table. Specifically, this custom key binding configuration is defined in the table like this, which is associated with the config key of the table.\nconfig = function()\n  local opts = {\n    hook = {\n      on_filetype = function()\n        -- Map local leader L to run `devtools::load_all()`\n          vim.api.nvim_buf_set_keymap(\n            0,\n            \"n\",\n            \"&lt;LocalLeader&gt;L\",\n            \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::load_all()')&lt;CR&gt;\",\n            { desc = \"R devtools::load_all()\" }\n          )\n      end\n    }\n  }\nend\nThis configuration code uses the vim.api.nvim_buf_set_keymap() function to set a local key binding for the current buffer, which will likely be open to a file type used within the R programming environment (e.g., .R, .Rmd, or .qmd file). You can run help nvim_buf_set_keymap from neovim’s command line to view more info about the function. Within the function, we pass values to several parameters:\n\n{buffer} - specifies the buffer the key binding will be made available.\n{mode} - specifies the modal mode the key binding will be made available (e.g., normal, visual, insert, etc.).\n{lhs} - a parameter expecting a string representing the key binding definition. In our case &lt;LocalLeader&gt;L.\n{rhs} - a parameter expecting a string representing the lua command to be run. In our case the &lt;Cmd&gt;lua require('r.send').cmd('devtools::load_all()')&lt;CR&gt; is run upon pressing the key binding, which will send the devtools::load_all() function to the R interpreter. It’s important to call out the lua function r.send.cmd() is being used here to run our R code. Shortly, we’ll see another lua function that can run functions that take objects in the environment as an input.\n{opts} - a parameter that expects a lua table with additional options. Because the which.key plugin is a default plugin for the LazyVim distribution, my config passes along a lua table containing a desc key value with a string describing what the keymap does. which.key will then include this description in the help popup window when hitting specific key bindings.\n\n\n\nOther useful key bindings for R.nvim\nYou’ll likely want to add more key bindings then the one I’ve shared above. For instance, you’ll likely want to add additional package development convenience functions provided by the devtools package to your configuration:\nvim.api.nvim_buf_set_keymap(\n  0,\n  \"n\",\n  \"&lt;LocalLeader&gt;D\",\n  \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::document()')&lt;CR&gt;\",\n  { desc = \"R devtools::test()\" }\n)\n\nvim.api.nvim_buf_set_keymap(\n  0,\n  \"n\",\n  \"&lt;LocalLeader&gt;T\",\n  \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::test()')&lt;CR&gt;\",\n  { desc = \"R devtools::test()\" }\n)\n\nvim.api.nvim_buf_set_keymap(\n  0,\n  \"n\",\n  \"&lt;LocalLeader&gt;U\",\n  \"&lt;Cmd&gt;lua require('r.send').cmd('devtools::install()')&lt;CR&gt;\",\n  { desc = \"R devtools::install()\" }\n)\nIf you do any Shiny development, it’s convenient to have a key binding that quickly kicks off an app:\nvim.api.nvim_buf_set_keymap(\n  0,\n  \"n\",\n  \"&lt;LocalLeader&gt;sa\",\n  \"&lt;Cmd&gt;lua require('r.send').cmd('shiny::runApp()')&lt;CR&gt;\",\n  { desc = \"R shiny::runApp()\" }\n)\nYou’ll also likely want a key binding that interrupts a busy R terminal:\nvim.keymap.set(\n  \"n\",\n  \"&lt;LocalLeader&gt;ts\",\n  \"&lt;Cmd&gt;RStop&lt;CR&gt;\",\n  { desc = \"R stop terminal\" }\n)\nAbove you’ll notice we’re using an alternative function to specify our key binding, vim.keymap.set(). This is an alternative function for specifying key bindings.\nFinally, one of my favorites is to configure key bindings for common operations I perform all the time, like running dplyr::glimpse() on objects. The configuration code looks like this:\nvim.api.nvim_buf_set_keymap(\n  0,\n  \"n\",\n  \"&lt;LocalLeader&gt;g\",\n  \"&lt;Cmd&gt;lua require('r.run').action('dplyr::glimpse')&lt;CR&gt;\",\n  { desc = \"R dplyr::glimpse()\" }\n)\nTake notice of what’s happening here in the {rhs} parameter of the vim.api.nvim_buf_set_keymap() function. require() is calling r.run.action(), a lua function that will run an R function on the object currently under the cursor. This is very convenient, as you can put your cursor over any object and pass that object to the R function.\nCertainly, endless possibilities exist for the types of key binds one can configure. Having the ability to set key bindings to specific R functions and operations opens up the possibilities even more to enhance specific workflows.\n\n\nWrap up\nTo wrap up, this post overviewed the process of defining custom key bindings for Neovim while using the LazyVim distribution. Specifically, this involved describing how to disable global key bindings that may overlap with the intended setup, how to set key bindings for the R.nvim plugin, and it provided some additional examples of key bindings that might be helpful. Customization is a core reasong for using Neovim, so being able to customize key bindings useful for your workflows is a powerful tool to learn and use.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes on Customizing {Neovim’s} Key Bindings},\n  date = {2025-02-23},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. “Notes on Customizing Neovim’s Key\nBindings.” February 23, 2025."
  },
  {
    "objectID": "til/posts/2025-08-01-til-notes-simple-made-easy/index.html",
    "href": "til/posts/2025-08-01-til-notes-simple-made-easy/index.html",
    "title": "Notes from Rich Hickey’s talk Simple Made Easy",
    "section": "",
    "text": "I was introduced to the talk Simple Made Easy presented by Rich Hickey during a 2025 Nebraska.Code() session. It’s a fairly well-known talk in software development. Though outside of my domain, the topics covered were somewhat related to my work. I thus decided to stretch myself and review the talk.\nSeveral of the talk’s points stood out to me. Some I’m still trying to fully grok. Below are my notes and takeaways.\n\n\n\n\n\n\nNote\n\n\n\nThis post is written in the spirit of publishing more frequent blog posts. It’s a bit of a scratchpad of ideas, concepts, and ways of working that I found to be useful or interesting. As such, what’s here is lightly edited. Be aware: there will likely be spelling, grammatical, or syntactical errors along with some disjointed, incomplete ideas.\n\n\n\nTakeaways\n\nWe need to build simple systems if we want to build good systems.\n\nThe talk began by differentiating between the definitions of simple and easy in the context of software and system design (01m25s). Simple is about being one fold, one braid, or one twist. Simple is objective. Easy is defined as being near at hand, near to our understanding or current skill set. Easy is relative. Designing towards easy can be a limitation, as it forces familiarity.\n\nIf you want everything to be familiar you will never learn anything new because it can’t be significantly different from what you already know.\n\nThe talk then uses these two definitions to further distinguish between constructs vs. artifacts (09m05s). Developers work with constructs to create artifacts. At times, too much focus is placed on the constructs of the developer’s experience. Rather, a focus on software quality, correctness, maintenance, and the ability for change will yield more benefits.\n\nWe have to start assessing our constructs based around the artifacts, not around the look and feel of the experience of typing it in or the cultural aspects of that.\n\nUsers don’t care about our experience writing the program. They only care about whether the program does what it says it’s going to do and if it works well or what the level of the complexity the program yields via its output.\nThe talk then addressed the limitations of complexity (12m14s). Our ability to hold onto lots of information is limited. Intertwining of constructs requires us to hold lots of information in our mind. As such:\n\nComplexity undermines understanding.\n\nRich Hickey then provided some criticism of guard rail programming (e.g., using tests)(15m32s). A few questions were posed along with this point:\n\nDo guard rails help you get where you’re going?\nWhat’s true about every bug? They were written, it passed a type checker, and it passed all the tests.\n\nSo, we’ll always need to reason about our program because these guard rails are just safety nets. Being able to reason about or programs is important to debugging.\nThis type of development has been criticized for it’s impact on speed and agility. Hickey addressed this concern:\n\nIgnoring complexity will slow you down over the long haul.\n\nIf you focus to much on ease, you’ll be able to move as fast as possible at the start. However, no matter what, complexity will always catch up to you. The net effect will be that you’re not moving forward in any significant way. To make this point more concrete, the talk introduced the knitted castle problem (19m36s). What will be easier to change, a castle knitted of yarn or one crafted using Legos? The results can still be complex. But the system may be hard to change in the future. Thus, the benefits of focusing on simplicity include:\n\nEase of understanding\nEase of change\nEasier debugging\nFlexibility\n\nThe talk then introduced and applied the term complecting: to interleave, entwine, braid (31m36s). Complecting is bad. You should avoid complecting things whenever possible.\n\nYou can write as sophisticated a system with dramatically simpler tools, which means you’re going to be focusing on the system, what it’s supposed to do instead of all the guck that falls out of the constructs you’re using. If you want a simpler life, just choose simpler stuff.\n\n\nProgramming is not about typing, it’s about thinking.\n\nHowever, at times we do need to make our own constructs. This can require abstraction: drawing things away. A good framework to keep abstraction aligned with simplicity is to use the who, what, when, where, why, and how framework. Another is to take the stance of I don’t know, I don’t want to know.\nAlongside the above points, I found the following collection of quotes from the talk to be interesting:\n\nSimplicity is a choice. It’s your fault if you don’t have a simple system.\n\n\nEasy is not simple.\n\n\nSimplicity often means making more things, not fewer.\n\n\nSimplicity is the ultimate sophistication.\n– Leonardo da Vinci\n\n\n\nThoughts from the takeaways\nThe distinction between simple and easy stuck with me. Being able to have more precise language to describe the constructs we use in system and software design improves the quality of our output–for both developers and users.\nThe discussion of our limitations as developers is a point that resonated with me. Indeed, you can develop towards complexity. However, we’re all limited in our ability to hold large amounts of information. More information, thus, can result in limited understanding. Complecting only compounds problems due to this limitation.\nComplecting is a term I’m walking away with from this talk. The idea of multiple braids or folds as being bad seems like great advice. How to avoid this in practice seems like the challenging task. The talk provides some tools to address this, though.\nOverall, a good talk with some thought-provoking points. I’m still attempting to understand some of what is shared, but I found many of the points were accessible and applicable to the work I do. Despite not being a computer scientist, systems designer, or full-on software developer, I still got great value from reviewing this talk.\nIf I misunderstood these points or you’re someone who is interested in discussing this topic further, let’s connect!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes from {Rich} {Hickey’s} Talk {Simple} {Made} {Easy}},\n  date = {2025-08-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. “Notes from Rich Hickey’s Talk Simple Made\nEasy.” August 3, 2025."
  },
  {
    "objectID": "til/posts/2025-08-03-til-notes-simple-made-easy/index.html",
    "href": "til/posts/2025-08-03-til-notes-simple-made-easy/index.html",
    "title": "Notes from Rich Hickey’s talk Simple Made Easy",
    "section": "",
    "text": "I was introduced to the talk Simple Made Easy presented by Rich Hickey during a 2025 Nebraska.Code() session. It’s a fairly well-known talk in software development. Though outside of my domain, the topics covered were somewhat related to my work. I thus decided to stretch myself and review the talk.\nSeveral of the talk’s points stood out to me. Some I’m still trying to fully grok. Below are my notes and takeaways.\n\n\n\n\n\n\nNote\n\n\n\nThis post is written in the spirit of publishing more frequent blog posts. It’s a bit of a scratchpad of ideas, concepts, and ways of working that I found to be useful or interesting. As such, what’s here is lightly edited. Be aware: there will likely be spelling, grammatical, or syntactical errors along with some disjointed, incomplete ideas.\n\n\n\nTakeaways\n\nWe need to build simple systems if we want to build good systems.\n\nThe talk began by differentiating between the definitions of simple and easy in the context of software and system design (01m25s). Simple is about being one fold, one braid, or one twist. Simple is objective. Easy is defined as being near at hand, near to our understanding or current skill set. Easy is relative. Designing towards easy can be a limitation, as it forces familiarity.\n\nIf you want everything to be familiar you will never learn anything new because it can’t be significantly different from what you already know.\n\nThe talk then uses these two definitions to further distinguish between constructs vs. artifacts (09m05s). Developers work with constructs to create artifacts. At times, too much focus is placed on the constructs of the developer’s experience. Rather, a focus on software quality, correctness, maintenance, and the ability for change will yield more benefits.\n\nWe have to start assessing our constructs based around the artifacts, not around the look and feel of the experience of typing it in or the cultural aspects of that.\n\nUsers don’t care about our experience writing the program. They only care about whether the program does what it says it’s going to do and if it works well or what the level of the complexity the program yields via its output.\nThe talk then addressed the limitations of complexity (12m14s). Our ability to hold onto lots of information is limited. Intertwining of constructs requires us to hold lots of information in our mind. As such:\n\nComplexity undermines understanding.\n\nRich Hickey then provided some criticism of guard rail programming (e.g., using tests)(15m32s). A few questions were posed along with this point:\n\nDo guard rails help you get where you’re going?\nWhat’s true about every bug? They were written, it passed a type checker, and it passed all the tests.\n\nSo, we’ll always need to reason about our program because these guard rails are just safety nets. Being able to reason about or programs is important to debugging.\nThis type of development has been criticized for it’s impact on speed and agility. Hickey addressed this concern:\n\nIgnoring complexity will slow you down over the long haul.\n\nIf you focus to much on ease, you’ll be able to move as fast as possible at the start. However, no matter what, complexity will always catch up to you. The net effect will be that you’re not moving forward in any significant way. To make this point more concrete, the talk introduced the knitted castle problem (19m36s). What will be easier to change, a castle knitted of yarn or one crafted using Legos? The results can still be complex. But the system may be hard to change in the future. Thus, the benefits of focusing on simplicity include:\n\nEase of understanding\nEase of change\nEasier debugging\nFlexibility\n\nThe talk then introduced and applied the term complecting: to interleave, entwine, braid (31m36s). Complecting is bad. You should avoid complecting things whenever possible.\n\nYou can write as sophisticated a system with dramatically simpler tools, which means you’re going to be focusing on the system, what it’s supposed to do instead of all the guck that falls out of the constructs you’re using. If you want a simpler life, just choose simpler stuff.\n\n\nProgramming is not about typing, it’s about thinking.\n\nHowever, at times we do need to make our own constructs. This can require abstraction: drawing things away. A good framework to keep abstraction aligned with simplicity is to use the who, what, when, where, why, and how framework. Another is to take the stance of I don’t know, I don’t want to know.\nAlongside the above points, I found the following collection of quotes from the talk to be interesting:\n\nSimplicity is a choice. It’s your fault if you don’t have a simple system.\n\n\nEasy is not simple.\n\n\nSimplicity often means making more things, not fewer.\n\n\nSimplicity is the ultimate sophistication.\n– Leonardo da Vinci\n\n\n\nFinal thoughts\nThe distinction between simple and easy stuck with me. Being able to have more precise language to describe the constructs we use in system and software design improves the quality of our output–for both developers and users.\nThe discussion of our limitations as developers is a point that resonated with me. Indeed, you can develop towards complexity. However, we’re all limited in our ability to hold large amounts of information. More information, thus, can result in limited understanding. Complecting only compounds problems due to this limitation.\nComplecting is a term I’m walking away with from this talk. The idea of multiple braids or folds as being bad seems like great advice. How to avoid this in practice seems like the challenging task. The talk provides some tools to address this, though.\nOverall, a good talk with some thought-provoking points. I’m still attempting to understand some of what is shared, but I found many of the points were accessible and applicable to the work I do. Despite not being a computer scientist, systems designer, or full-on software developer, I still got great value from reviewing this talk.\nIf I misunderstood these points or you’re someone who is interested in discussing this topic further, let’s connect!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {Notes from {Rich} {Hickey’s} Talk {Simple} {Made} {Easy}},\n  date = {2025-08-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. “Notes from Rich Hickey’s Talk Simple Made\nEasy.” August 3, 2025."
  },
  {
    "objectID": "blog/posts/2025-08-10-post-tidy-tuesday-2025-08-05-income-inequality-before-and-after-taxes/index.html",
    "href": "blog/posts/2025-08-10-post-tidy-tuesday-2025-08-05-income-inequality-before-and-after-taxes/index.html",
    "title": "TidyTueday contribution: 2028-08-05",
    "section": "",
    "text": "Photo by Martin Sanchez\n\nBackground\nThis week’s TidyTuesday included data exploring Income Inequality Before and After Taxes. The article “Income inequality before and after taxes: how much do countries redistribute income?” by Joe Hasell inspired this week’s data. The data were processed by Our World in Data, and they orignated from various sources (see the documentation for more info).\nThe data provides an estimate called the Gini coefficient for both before and after taxes. Here’s the definition:\n\nThe Gini coefficient measures inequality on a scale from 0 to 1. Higher values indicate higher inequality. Inequality is measured here in terms of income before and after taxes and benefits.\n\nMy contribution\nHere is my contribution for the week of 2025-08-05:\n\n\n\n\nThe code I used to produce this animated plot follows.\nYou can also access the code via my TidyTuesday GitHub repository here.\nHere is the code for this specific contribution.\nIf you like what you see or want to discuss this week’s contribution, follow and connect with me in these places:\n\nBlueSky: collinberke.bsky.social\n\nGitHub: collinberke\n\nLinkedIn: @collinberke\n\nThe code\n\n# Setup -----\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(berkeBrand)\nlibrary(here)\nlibrary(glue)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(ggalt)\nlibrary(ggtext)\nlibrary(gganimate)\n\n# Get data -----\n\n# `gini_mi_eq` = pre-tax inequality\n# `gini_dhi_eq` = post-tax inequality\n\n# data-import-income-inequality\ntuesdaydata &lt;- tt_load('2025-08-05')\n\nincome_inequality_processed &lt;-\n  tuesdaydata$income_inequality_processed |&gt;\n  clean_names()\n\n# Explore data -----\n\n# data-explore\nnames(income_inequality_processed)\nglimpse(income_inequality_processed)\nskim(income_inequality_processed)\n\n# Questions to explore:\n#   * How many countries are represented in the data?\n#   * How many data points (i.e., years) are available for each country?\n#   * Range of data points per country?\n#   * Over what years does this data represent?\n\n# data-questions\n\n# Countries?\nincome_inequality_processed |&gt;\n  distinct(entity) |&gt;\n  count()\n\n# Data points?\ndata_points &lt;- income_inequality_processed |&gt;\n  group_by(entity) |&gt;\n  count(sort = TRUE) |&gt;\n  print(n = 100)\n\n# Per country?\nrange(data_points$n)\n\n# What country(s) only has one?\n# The Dominican Republic\ndata_points |&gt; filter(n == 1)\n\n# Years represented?\n# 1963 to 2023 for some countries, not all\nrange(income_inequality_processed$year)\n\n# Transform data -----\n\n# trnsfm-inequality-data\ndata_income_inequality &lt;- income_inequality_processed |&gt;\n  arrange(entity, year) |&gt;\n  complete(entity, year = 1963:2023) |&gt;\n  group_by(entity) |&gt;\n  fill(gini_mi_eq, gini_dhi_eq) |&gt;\n  mutate(diff = gini_mi_eq - gini_dhi_eq)\n\n## An animated dumbell chart -----\n\n# plot-text\n\ntitle &lt;- \"&lt;strong&gt;Income inequality before and after taxes&lt;/strong&gt;\"\nsubtitle &lt;- \"{closest_state} Gini coefficient: a measure of income inequality\"\ncaption &lt;- glue(\n  \"&lt;br&gt;\",\n  add_tt(\n    source = \"Processed from multiple sources by Our World in Data\",\n    tt_date = \"2025-08-05\"\n  ),\n  \"&lt;br&gt;\",\n  \"&lt;strong&gt;Note:&lt;/strong&gt; Countries ordered by maxmimum Gini coefficient after taxes, values forward filled\",\n  \"&lt;br&gt;\",\n  add_socials()\n)\nx &lt;- glue(\n  \"Gini coefficient\",\n  \"(&lt;span style = 'color: orange;'&gt;&lt;strong&gt;before&lt;/strong&gt;&lt;/span&gt;\",\n  \"and &lt;span style = 'color: blue;'&gt;&lt;strong&gt;after taxes&lt;/strong&gt;&lt;/span&gt;)\",\n  .sep = \" \"\n)\ny &lt;- \"\"\n\n## Static chart -----\n\n# I found this useful because the rendering of the animation can take some time.\n# Having this static plot allowed for faster iteration.\n\n# vis-dumbell-chart-ggplot2\nvis_income_inequality &lt;- ggplot(\n  data = data_income_inequality\n) +\n  geom_segment(\n    aes(\n      x = gini_mi_eq,\n      xend = gini_dhi_eq,\n      y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE),\n      yend = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)\n    ),\n    color = \"darkgrey\",\n    linewidth = .5\n  ) +\n  geom_point(\n    aes(x = gini_mi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 2,\n    fill = \"black\",\n    shape = 24\n  ) +\n  geom_point(\n    aes(x = gini_mi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 1,\n    fill = \"orange\",\n    shape = 24\n  ) +\n  geom_point(\n    aes(x = gini_dhi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 2,\n    fill = \"black\",\n    shape = 24\n  ) +\n  geom_point(\n    aes(x = gini_dhi_eq, y = reorder(entity, gini_dhi_eq, max, na.rm = TRUE)),\n    size = 1,\n    fill = \"blue\",\n    shape = 24\n  ) +\n  theme_minimal() +\n  labs(\n    title = title,\n    subtitle = subtitle,\n    caption = caption,\n    y = \"\",\n    x = x\n  ) +\n  theme(\n    plot.title = element_textbox_simple(size = 20),\n    plot.subtitle = element_text(size = 16, hjust = 0),\n    plot.caption = element_textbox_simple(halign = 1, vjust = -5),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 7, color = \"black\"),\n    axis.text.x = element_text(color = \"black\"),\n    axis.title.x = element_textbox_simple(halign = .5)\n  )\n\nggsave(\n  here(\n    \"blog/posts/\",\n    \"2025-08-10-post-tidy-tuesday-2025-08-05-income-inequality-before-and-after-taxes\",\n    \"vis-income-inequality-dumbell.png\"\n  )\n)\n\n# func-get-dumbell-chart\nget_dumbell_chart &lt;- function() {\n  vis_income_inequality\n}\n\n# vis-animation-transitions\ntimelapse_income_inequality_dumbell &lt;- get_dumbell_chart() +\n  transition_states(year, transition_length = 3, state_length = 5) +\n  ease_aes(\"quadratic-in-out\", interval = 1)\n\n# vis-animate-visualization\nanimated_income_inequality_plot &lt;- animate(\n  timelapse_income_inequality_dumbell,\n  nframes = 125,\n  duration = 45,\n  start_pause = 5,\n  end_pause = 5,\n  height = 7,\n  width = 7,\n  res = 150,\n  units = \"in\",\n  fps = 10,\n  renderer = gifski_renderer(loop = TRUE)\n)\n\nanim_save(\n  here(\n    \"blog/posts/\",\n    \"2025-08-10-post-tidy-tuesday-2025-08-05-income-inequality-before-and-after-taxes\",\n    \"vis-income-inequality-dumbell.gif\"\n  ),\n  animated_income_inequality_plot\n)\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@misc{berke2025,\n  author = {Berke, Collin K},\n  title = {TidyTueday Contribution: 2028-08-05},\n  date = {2025-08-10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2025. “TidyTueday Contribution:\n2028-08-05.” August 10, 2025."
  },
  {
    "objectID": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html",
    "href": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html",
    "title": "Notes: Using gganimate to animate plots",
    "section": "",
    "text": "I recently created a TidyTuesday data visualization utilizing the gganimate RStats package. While using the package, my focus was to ‘get something up and running quickly’. I wanted to go deeper, though. Below are my notes from diving into the package’s ‘Getting Started’ vignette."
  },
  {
    "objectID": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#transitions",
    "href": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#transitions",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Transitions",
    "text": "Transitions\nTransitions is the first concept to know when using gganimate. The terms/key functions include:\n\nThe transition_states() function creates transitions based on some discrete variable within your data. That is, it’s main purpose is to split the data into various frames, which are then later complied into a .gif file. In the example here, we’re splitting by the species variable.\ntweening: a calculation performed to ensure the transitions between each state of the animation are smooth.\n\nHere’s how we create the transitions utilizing the transitions_states() function, outputted as a .gif file.\n\nvis_flip_mass_trans &lt;- vis_flip_mass +\n  transition_states(\n    species,\n    transition_length = 2,\n    state_length = 1\n  )\n\nvis_flip_mass_trans\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhere’s the .gif?\nI noticed gganimate opens Preview (I’m on a Mac) after rendering the .gif file. As such, all you see is a collection of static plots. This didn’t allow me to ‘see’ the animation.\ngganimate writes the .gif file to a temporary directory. The tempdir() function can be used to print the file path of the temporary directory used for the current R session. This file path can then be used to point a web browser to the location of the .gif file. This video does a pretty good job highlighting how to do this using Google Chrome.\nThe anim_save() function can be used to save the .gif file wherever is most convenient."
  },
  {
    "objectID": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#easing",
    "href": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#easing",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Easing",
    "text": "Easing\nEasing is the next key concept to know when using the gganimate package.\n\nEasing: a calculation that takes place to create intermediary data for the tweening to occur. In other words, it’s a calculation to specify the velocity of change taking place between the transitions. gganimate has various types of easing that can be applied to the transitions. gganimate’s ease_aes() function is used to specify the different types of easing.\n\nHere are a few examples applied to our penguins scatterplot:\n\nvis_flip_mass_trans +\n  ease_aes(\"quintic-in-out\")\n\n\n\n\n\n\nvis_flip_mass_trans +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\n\n\nvis_flip_mass_trans +\n  ease_aes(\"quadratic-in-out\")\n\n\n\n\n\n\nvis_flip_mass_trans +\n  ease_aes(\"exponential-in-out\")\n\n\n\n\n\nA key point here is you have to specify the type of easing function to use, along with a modifier. You can read more about what’s available by viewing the ?ease_aes function’s documentation. Many combinations are available."
  },
  {
    "objectID": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#labelling",
    "href": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#labelling",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Labelling",
    "text": "Labelling\nThe ability to add labels to our animations is the next key concept to know. gganimate makes it easy to show dynamic labels. This is due to the package providing glue like syntax for plot labelling. For instance, let’s say we wanted to include the type of penguin species within the title to match the current state of the data being shown. This can be done by doing the following:\n\nvis_flip_mass_trans +\n  labs(\n    title = \"Flipper length and body mass for {closest_state} penguins\"\n  )\n\n\n\n\n\ngganimate also makes other transition variables available for labelling. These include frame and nframes. Check out the docs for more information and additional examples."
  },
  {
    "objectID": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#object-permanence",
    "href": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#object-permanence",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Object Permanence",
    "text": "Object Permanence\nObject permanence is another important concept to consider, especially as it relates to the semantics and validity of your plot. I think of object permanence like this: data points may not be connected, so the animation applied should avoid implying a connection. This isn’t true in all cases (i.e., timeseries data). However, it’s important to consider that if you have distinct classes in your data, the animations should make this clear. In the context of the penguins example, the different penguin species are not related, but with how the animation morphs to different species implies that they are connected–this is not true for this data.\nAs the docs mention, we need to tell gganimate to not morph observations between different categories in our data to make it clear that observations are not connected. The docs provide two suggestions for fixing this: add an aesthetic to distinguish between the groups or set the group directly. So, let’s apply the doc’s preferred fix to our example: set the group directly.\n\nggplot(penguins, aes(flipper_len, body_mass)) +\n  geom_point(aes(colour = species, group = 1L)) +\n  transition_states(\n    species,\n    transition_length = 2,\n    state_length = 1\n  ) +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\n\nNow the transitions make it a little more clear that these data are separate and not connected. This is done utilizing color and the different transitions states. Despite our best efforts, though, a different type of transition might make these more clear. This is where entering and exiting can be applied."
  },
  {
    "objectID": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#entering-and-exiting",
    "href": "til/posts/2025-08-23-til-notes-r-stats-gganimate/index.html#entering-and-exiting",
    "title": "Notes: Using gganimate to animate plots",
    "section": "Entering and Exiting",
    "text": "Entering and Exiting\nAs the docs mention, another alternative to fix our problem above is to provide transitions to have data appear and disapear. This is where the enter_*() and exit_*() family of functions become useful.\nLet’s apply some of these functions to our penguins plot. Here’s a few examples:\n\nanim &lt;- ggplot(penguins, aes(flipper_len, body_mass)) +\n  geom_point(aes(colour = species), size = 2) +\n  transition_states(\n    species,\n    transition_length = 2,\n    state_length = 1\n  )\n\nanim +\n  enter_fade() +\n  exit_shrink()\n\n\n\n\n\n\nanim +\n  enter_grow() +\n  exit_recolour(colour = \"#000000\")"
  },
  {
    "objectID": "say-hi.html",
    "href": "say-hi.html",
    "title": "Say Hi!",
    "section": "",
    "text": "I want to meet you. Yeah, you!\nI’ve set aside time on my calendar for 30-60 minute conversations over coffee, lunch, or other suggestions. These conversations can be virtual or in person if you’re in my area (Lincoln or Omaha, NE).\n\nWhat can our time together be used for?\nWhatever you want. Are you working as a data scientist, analyst, or any other type of professional in media or marketing? Let’s talk shop. Have an idea or blocker you want to discuss? I’m open to sharing objective, external feedback. Wrestling with a tricky piece of code? I might be able to help, or at the very least could be an extra hand in Googling or AI’ing your problem. Have a presentation you want to practice? With my background in communication and public speaking, I’d be glad to listen and offer guidance. Want to spend 10 minutes screaming into the void of the internet and head out? I’m open to that to.\nTo be clear, I’m not selling anything. My intention with this time is connection and networking, and it’s not to push a sales pitch or demo.\nIf any of this sounds interesting, schedule a time and come say hi!\n\n 👋 Say Hi! \n\n\n\nMy interests\nIf you need specifics, the topics I’d like to talk more about include:\n\nMedia, digital, and marketing data science and analytics.\nOpen-source statistical software: mostly R, but I’m open to discussing Python.\nR package development.\nNeovim and LazyVim.\nCloud computing.\nThe use of data and its relationship to organizational culture.\nData visualization.\n\n\n\nOther ways to connect\nNot interested in saying hi in person? That’s cool. Here are some other ways to connect with me:\n\nBlueSky: @collinberke.bsky.social\nLinkedIn: collinberke\nGitHub: @collinberke"
  }
]