[
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A now page, why?"
  },
  {
    "objectID": "now.html#projects-im-working-on",
    "href": "now.html#projects-im-working-on",
    "title": "Now",
    "section": "Projects I’m working on",
    "text": "Projects I’m working on\n\nWorking on a 30 day tidymodels recipes package challenge\nI’ve been developing my machine learning and modeling skills. Specifically, I’ve been focusing on learning the various steps to preprocess data for modelling and feature engineering. This includes becoming more proficient with the tidymodels recipes package. Check out my post to keep up with the latest.\n\n\nContinuing to experiment with Neovim\nI’ve heavily leaned into using Neovim for most of my workflows. During this transition, I’ve come across some really useful tools, like telescope, harpoon, and vim-fugitive. I’m trying to learn as much as I can, as I still don’t know if my configuration is correct … lol\n\n\nAttended Posit::conf(2023) Chicago\nI was fortunate to attend this year’s Posit conference in Chicago, a five day conference with workshops and speaker sessions. I’m still sorting through all that I learned. Here are some of the highlights:\n\nAttending the Introduction to Tidymodels workshop\nAttending the Package Development Masterclass workshop\nAll the keynote speakers, especially J.D. Long’s It’s Abstractions All the Way Down talk.\nNetworking and meeting some folks from the R4DS Online Learning Community (in person).\n\nIf you’re interested in what I’ve focused on in the past, check out my past updates"
  },
  {
    "objectID": "now.html#books-im-reading",
    "href": "now.html#books-im-reading",
    "title": "Now",
    "section": "Books I’m reading",
    "text": "Books I’m reading\n\nHBR Guide to Making Every Meeting Matter from the Harvard Business Review\nFor this coming year, I’ll be leading more organizational meetings. So, my manager suggested I pick up this book. This book provides some practical, quick to read chapters on different topics surrounding effective meetings. This includes topics like determining if you need to have a meeting, planning and facilitating meetings, and a grab bag of other topics. I’ve found the information useful thus far.\n\n\nThe Caves of Steel by Isaac Asimov\nI’m continuing to read more of Isaac Asimov’s work. It started with I, Robot. Now I’ve begun reading the first book in his Robot Series. I’ve recognized I really enjoy Science Fiction."
  },
  {
    "objectID": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "href": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "title": "Now",
    "section": "A list of books I’ve read (ever since I’ve started keeping track)",
    "text": "A list of books I’ve read (ever since I’ve started keeping track)\n\nProfessional development reads\n\nThe Checklist Manifesto: How to Get Things Right by Atul Gawande\n\n\nTidy Modeling with R by Max Kuhn and Julia Silge\n\n\nPython for Data Analysis by Wes McKinney\n\n\nEngineering Production-Grade Shiny Apps by Colin Fay, Sébastien Rochette, Vincent Guyader, and Cervan Girard\n\n\nAdvanced R by Hadley Wickham. Check out past book club meeting recordings here.\n\n\nR Packages by Hadley Wickham and Jenny Bryan. Check out past book club meeting recordings here.\n\n\nVim help files maintained by Carlo Teubner\n\n\nMastering Ubuntu by Jay LaCroix\n\n\nGoogle BigQuery: The Definitive Guide by Valliappa Lakshmanan and Jordan Tigani\n\n\nMastering Shiny by Hadley Wickham. Check out the past book club meeting recordings here.\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. Check out the past book club meeting recordings here.\n\n\nDocker Deep Dive by Nigel Poulton\n\n\n\nPersonal reads\n\nI, Robot by Isaac Asimov\n\n\nLeviathan Falls (The Expanse book 9) by James S.A. Corey\n\n\nTiamat’s Wrath (The Expanse book 8) by James S.A. Corey\n\n\nPersepolis Rising (The Expanse book 7) by James S.A. Corey\n\n\nBabylon’s Ashes (The Expanse book 6) by James S.A. Corey\n\n\nNemesis Games (The Expanse book 5) by James S.A. Corey\n\n\nCibola Burn (The Expanse book 4) by James S.A. Corey\n\n\nAbaddon’s Gate (The Expanse book 3) by James S.A. Corey\n\n\nCaliban’s War (The Expanse book 2) by James S.A. Corey\n\n\nLeviathon Wakes (The Expanse book 1) by James S.A. Corey\n\n\nThe Galaxy, and the Ground Within: A Novel (Wayfarers 4) by Becky Chambers\n\n\nRecord of a Spaceborn Few (Wayfarers 3) by Becky Chambers\n\n\nA Closed and Common Orbit (Wayfarers 2) by Becky Chambers\n\n\nThe Long Way to a Small, Angry Planet (Wayfarers 1) by Becky Chambers\n\n\nLast of the Breed by Louis L’Amour\n\n\nProject Hail Mary by Andy Weir\n\n\nFirebreak by Nicole Kornher-Stace\n\n\nDune Messiah by Frank Herbert\n\n\nDune by Frank Herbert\n\n\nThe Martian: A Novel by Andy Weir"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html",
    "title": "Combine plots using patchwork",
    "section": "",
    "text": "Image generated using the prompt ‘robot stitching a quilt of data visualizations in pop art style’ with the Bing Image Creator\nToday I learned the patchwork package makes it easy to combine multiple plots into a single plot. In this post, I overview what I’ve recently learned from using patchwork’s functions to create plot compositions."
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#show-me-the-money",
    "title": "Combine plots using patchwork",
    "section": "Show me the money",
    "text": "Show me the money\nLet’s start with a pretty straight forward plot, total revenue over time. Here’s the code I used to wrangle the data and create the plot:\n\ndata_revenue_trend &lt;- data_google_merch |&gt;\n  group_by(event_date, transaction_id) |&gt;\n  summarise(revenue = max(purchase_revenue_in_usd)) |&gt;\n  summarise(revenue = sum(revenue))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_rev_trend &lt;- ggplot(\n  data_revenue_trend, \n  aes(x = event_date, y = revenue)\n) +\n  geom_line(linewidth = 2) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Total Revenue ($USD)\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#items-generating-the-most-revenue",
    "title": "Combine plots using patchwork",
    "section": "Items generating the most revenue",
    "text": "Items generating the most revenue\nThe data is from an online store, so we should explore items generating the most revenue. To keep things simple, I use dplyr’s slice_max(10) to create a plot of the Top 10 revenue generating items. Because we’re slicing the data, other items are excluded from the plot. The following code wrangles the data and creates the plot for us.\n\ndata_items_rev &lt;- \n  data_google_merch |&gt;\n  group_by(item_name) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_rev_items &lt;- ggplot(\n  data_items_rev, \n  aes(\n    x = fct_reorder(item_name, revenue),\n    y = revenue)) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-generating-most-revenue",
    "title": "Combine plots using patchwork",
    "section": "Product categories generating most revenue",
    "text": "Product categories generating most revenue\nThe data also categorizes items into more general groupings. As such, we can create a plot ranking product categories by amount of revenue generated. No question, it’s apparel. The code to create the plot is similar to what we did above with items.\n\ndata_cat_rev &lt;- \n  data_google_merch |&gt;\n  group_by(item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd)) |&gt;\n  arrange(desc(revenue)) |&gt;\n  slice_max(revenue, n = 10)\n\n\nvis_high_cat_items &lt;- ggplot(\n  data_cat_rev, \n  aes(\n    x = fct_reorder(item_category, revenue),\n    y = revenue)) +\n  geom_col(fill = \"#191970\", alpha = .9) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  coord_flip() +\n  theme_minimal() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.x = element_blank()\n  ) +\n  labs(y = \"Revenue ($USD)\", x = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#product-categories-trend",
    "title": "Combine plots using patchwork",
    "section": "Product categories trend",
    "text": "Product categories trend\nNow that we’ve created a plot ranking product categories, let’s create a plot that breaks out several categories of interest over time. To do this, we’ll use the following code.\n\nimportant_cats &lt;- \n  c(\"Accessories\", \"Bags\", \"Apparel\", \n    \"Drinkware\", \"Lifestyle\")\n\ndata_cat_rev_trend &lt;- \n  data_google_merch |&gt;\n  filter(item_category %in% important_cats) |&gt;\n  group_by(event_date, item_category) |&gt;\n  summarise(revenue = sum(item_revenue_in_usd))\n\n`summarise()` has grouped output by 'event_date'. You can override using the `.groups` argument.\n\n\n\nvis_high_cat_trend &lt;- ggplot(\n  data_cat_rev_trend, \n  aes(x = event_date, y = revenue, color = item_category)) +\n  geom_line(linewidth = 2, alpha = .7) +\n  scale_x_date(date_breaks = \"2 week\", date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(size = 6)\n  ) +\n  labs(x = \"\", y = \"Revenue ($USD)\", color = \"\")"
  },
  {
    "objectID": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "href": "til/posts/2023-12-23-til-patchwork-ggplot2-combine-plots/index.html#other-operators-to-know",
    "title": "Combine plots using patchwork",
    "section": "Other operators to know",
    "text": "Other operators to know\npatchwork has other operators to modify the layout of the plot composition. Each operator is listed below with a brief description of its use.\n\n+ and - - combines plots together on the same level.\n| - combines plots beside each other (i.e., packing).\n/ - places plots on top of each other (i.e., stacking).\n* - adds objects like themes and facets to all plots on the current nesting level.\n& - will add objects recursively into nested patches.\n\nThe following examples highlight the use of several of these operators."
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html",
    "title": "Calculating correlations with corrr",
    "section": "",
    "text": "Photo by Omar Flores\nToday I learned calculating, visualising, and exploring correlations is easy with the corrr package.\nIn the past, I would rely on Base R’s stats::cor() for exploring correlations. This function is a powerful tool if you’re looking to do additional analysis beyond investigating correlation coefficients. stats::cor() has its pain points, though. Sometimes, I just want a package to explore correlations quickly and easily.\nI recently stumbled across the corrr package. It met all the needs I listed above. The purpose of this post is to highlight what I’ve learned while using this package, and to demonstrate functionality I’ve found useful. To get started, let’s attach some libraries and import some example data.\nlibrary(tidyverse)\nlibrary(corrr)\nlibrary(here)"
  },
  {
    "objectID": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "href": "til/posts/2023-10-22-correlations-with-corrr/index.html#footnotes",
    "title": "Calculating correlations with corrr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs I was finishing this post, the Nebraska Women’s Volleyball team beat #1 Wisconsin in a five set thriller.↩︎\nThe Nebraska Women’s Volleyball team broke the World Record for a women’s sporting event on 2023-08-30. Official attendance was 92,003.↩︎"
  },
  {
    "objectID": "til/posts/2023-10-24-temp-directories/index.html",
    "href": "til/posts/2023-10-24-temp-directories/index.html",
    "title": "Using base::tempdir() for temporary data storage",
    "section": "",
    "text": "Photo by Jesse Orrico\n\n\nToday I learned how to store data in R’s per-session temporary directory.\nRecently, I’ve been working on an R package for a project. This package contains some internal data, which is intended to be updated from time-to-time. As part of the data update process, I’m required to download a set of .zip files from cloud storage, unzip, wrangle, and make the data available in the package via the data folder.\nGiven the data I’m working with, I wanted to avoid storing pre-wrangled data in the data-raw directory of the package. My main concern was an accidental check-in of pre-proccessed data into version control. So, I sought out a means to solve this problem.\nThis post aims to overview an approach using R’s per-session temporary directory to store data temporarily. Specifically, this post will discuss the use of base::tempdir() and other system file management functions made available in R to store data in this directory.\n\n\n\n\n\n\nWarning\n\n\n\nUsing R’s per-session temporary directory may not be the right solution for your specific situation. If you’re working with sensitive data, make sure you follow your organization’s guidelines on where to store, access, and properly use your data.\nI am not a security expert.\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nWhat are temporary directories?\n\n\n\n\n\n\nNote\n\n\n\nAs of this writing, I drafted this post on a computer running a Mac operating system. Some of what gets discussed here may not apply to Windows or Linux systems. The ideas and application should be similar, though I haven’t fully explored the differences.\n\n\nThe temporary directory, simply, is a location on your system. You can store files in this location just like any other directory. The difference is data stored within a temporary directory are not meant to be persistent, and your system will delete them automatically. File deletion either occurs when the system is shut down or after a set amount of time.\nIf you’re working on a Mac operating system, you can get the path to the temporary directory by running the following in your terminal:\necho $TMPDIR\nWhen I last ran this command on my system, echo returned the following path (later we’ll use base::tempdir() to get and use this path in R).\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/\nThis directory is located at the system level. The cd command can be used to navigate to it from the terminal. You may have to back up a few directories if your root starts at the user level, though. This is pretty standard, especially if you’re working on a Mac.\n\n\n\n\n\n\nNote\n\n\n\nSince I’m drafting this post on my personal machine, I’m not aware if you need admin privileges to access this folder. As such, you may run into issues if you’re not an admin on your machine.\n\n\nWith my curiosity peaked, I sought more information about what this directory was used for on a MacOS. Oddly enough, there is very little about this directory online. From what I can deduce, the /var directory is mainly a per-user cache for temporary files, and it provides security benefits beyond other cache locations on a Mac system (again, I’m not a security expert, so my previous statement may be inaccurate). Being that this location is temporary, this cache gets cleared every time the system restarts or every three days.\nAlthough there’s a lack of information about this directory online, I did come across a few blog posts and a Stack Overflow answer that were helpful in understanding this temporary directory in more depth: post 1; post 2; post 3. You might find these useful if you want to learn more. However, for me, the above is as far as I wanted to go to understand its purpose.\n\n\nAccess the temporary directory using base::tempdir()\nAt the start of every session, R creates a temporary per-session directory, and it removes this temporary directory when the session ends.. This temporary directory is stored in the system’s temporary directory location (e.g., /var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T/). R also provides several functions to work with the temporary directory, create, and interact with files within it.\nbase::tempdir() can be used to print the file path of the temporary directory on your system. Let’s run it and take a look at what happens.\n\ntempdir()\n\nOutputted to the terminal is the path to the R session’s temporary directory. When I ran it, the returned path looked like this:\n/var/folders/_4/t3mpnn5n5rzg4sdq2fr_n4y80000gn/T//RtmpaYxspA\nThe temporary directory R uses for the current session is labeled using the RtmpXXXXXX pattern. The final six characters of the path (i.e., the Xs) are determined by the system. Note, tempdir() doesn’t create this directory, it just prints the temporary directory’s path to the console. This directory is created every time a R session begins.\nSince the temporary directory is just like any location on your computer, you can navigate to it from your terminal during an active R session. With your terminal pointing to the temporary directory, you can use the following code to find R’s per-session temporary directory:\nla | grep \"Rtmp\"\nLet’s take a peak at what’s in this directory. R’s list.files() function can be helpful in this case.\n\nlist.files(tempdir())\n\ncharacter(0)\n\n\nMost R setups should start with an empty per-session directory. So the above should return character(0). Despite being empty now, list.files() will become handy again once we start to write files to this location.\n\n\nWriting files to the temporary directory\nNow that we know a little more about this temp directory and where it is located on our system, let’s write some data to it. We can do this by doing something like the following.\n\nwrite_csv(mtcars, file = paste0(tempdir(), \"/mtcars.csv\"))\n\nNow when we list the files in the temporary directory (e.g., list.files(tempdir())), you should see the mtcars.csv file.\nIf you’re looking to create files with unique names, you can pass the tempfile() function to the file argument. This looks something like this:\n\nwrite_csv(\n  mtcars, \n  file = tempfile(pattern = \"mtcars\", fileext = \".csv\")\n)\n\ntempfile() creates unique file names, which concatenates together the file path, the character vector passed to the pattern argument, a random string in hex, and the character vector inputed to the fileext argument. When you list the files in the temporary directory now, you’ll see the initial mtcars.csv file along with a file that looks something like this: mtcars7eb3503ac74c.csv. The random hex string ensures files remain unique.\nIndeed, the above is just one way to write files to the temporary directory. You can use other methods to read and write files at this location. However, you now know what is needed to interact with this directory, read and write files to and from it. At this point you can do any data wrangling steps your project requires. After which, we can go about deleting our files from this directory.\n\n\nDeleting files with file.remove()\nAlthough these files will eventually be removed by the system, we should be proactive and clean up after ourselves.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using this approach within functions, especially if their intended to be used by other users, you’ll want to be clear they will write data to and remove data from the user’s system.\nIndeed, it’s considered poor practice to change the R landscape on a user’s computer without good reason. So the least we can do here is clean up after ourselves.\n\n\nTo delete our files we wrote to the temporary directory, run the following in the console:\n\nfile.remove(list.files(tempdir(), full.names = TRUE, pattern = \".csv\"))\n\n[1] TRUE TRUE\n\n\nThe arguments of the list.files() function should be pretty straightforward. We want file paths to be full length (i.e., full.names = TRUE) and to list only files with the .csv extension (i.e., pattern = \".csv\"). Then, we use these full file paths within the file.remove() function, which will remove the files from R’s temporary directory.\n\n\nWrap-up\nToday I learned more about R’s per-session temporary directory, and how it can be used to write files not intended for persistent storage. I also learned how to use several base R functions to create files within this temporary directory by using tempfile() and tempdir(). I also demonstrated how the list.files() function can be used to list files within any directory on your system, specifically using it to list files in R’s temporary directory. Finally, I highlighted how files in the temporary directory can be deleted using the file.remove() function.\nHave fun using R’s per-session temporary directory. Cheers 🎉!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Using `Base::tempdir()` for Temporary Data Storage},\n  date = {2023-11-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Using `Base::tempdir()` for Temporary Data\nStorage.” November 3, 2023."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Collin K. Berke, Ph.D. is a media research analyst in public media. Professionally, he uses data and media/marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. He is especially interested in the use and development of open-source statistical software (i.e. Rstats) to achieve this goal, and developing a broader understanding the role these tools can play in media, digital, and marketing analytics.\nHe has experience using different software, third-party services, and programming languages from developing several analytics projects, both in industry and academia. In regards to programming languages, he has developed projects using R, SQL, $bash, and a little bit of Python. He also has extensive experience using different analytics solutions. For data warehousing, he mostly uses database tools like Postgres and those in the Google Cloud Platform ecosystem (e.g., Google BigQuery). When it comes to automating workflows and data pipelines, he has experience implementing and working with Apache Airflow. He also has extensive experience using third-party tools and software to analyze, wrangle data, and communicate his analyses (e.g., Google Analytics, Google Sheets, Excel, Google Data Studio, and R Shiny). Most of his current work is industry related.\nCollin also serves as an adjunct instructor for the University of Nebraska-Lincoln and Southeast Community College, where he teaches courses in sports data visualization and analysis and communication specific courses. He holds a M.A. in Communication Studies from the The University of South Dakota and a Ph.D. in Media and Communication from Texas Tech University. He has also published and contributed to the publication of several academic journal articles.\nCollin is a self-proclaimed news, sports, and podcast junkie. He really enjoys listening to NPR, watching PBS (especially NOVA), and indulging in college football and baseball. At times, he will write blog posts on topics he finds interesting in these areas.\nCheck out the Now page to see what Collin is currently reading and working on."
  },
  {
    "objectID": "about.html#note-about-this-site",
    "href": "about.html#note-about-this-site",
    "title": "About",
    "section": "Note about this site",
    "text": "Note about this site\nThe views expressed on this site are my own, and they do not reflect the views of my employer, professional and/or community groups I hold membership. Any analyses hosted on this site were done for professional development or were for fun. I make every attempt to perform valid and accurate data analysis and reporting. Unless otherwise noted, none of the content on this site has been peer-reviewed, and thus any conclusions drawn or uses stemming from this work need to take these limitations into account.\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html",
    "title": "Shiny summary tiles",
    "section": "",
    "text": "Photo by Stephen Dawson\nEffective reporting tools include user interface (UI) elements to quickly and effectively communicate summary metrics. Shiny, a free software package written in the R statistical computing language, provides several tools to communicate analysis and insights. Combining several of these elements together, a developer can create user interface elements that clearly communicate important summary metrics (e.g., Key Performance Indicators) to an application’s users.\nThis post details the steps to create the following simple Shiny application. Specifically, this post overviews the use of Shiny’s built-in functions to create simple summary metric tiles. In addition, this post describes how to add styling to UI elements by applying custom css to a Shiny application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-reactive-graph",
    "title": "Shiny summary tiles",
    "section": "The reactive graph",
    "text": "The reactive graph\nAlthough this app is simple and most of the elements can be easily managed, it’s always good practice to see the big picture of the app by plotting out a reactive graph first. It’s also good to have the intended reactive graph available as a quick reference, just in case unexpected results and/or behaviors are displayed while developing the application, and as a method for identifying any situations where computing resources are not being used efficiently.\nBelow is the reactive graph for the application to be developed:\n\n\n\n\n\n\n\nReactive graph for summary metric tiles\n\n\n\n\nAgain, a really simple application–one input (date), a reactive expression (data()), and five outputs (users; page_view; session_start; purchase; and event_date). The graph also details the dependencies clearly, where the outputs are dependent on the reactive data() object–which in cohort with the outputs–depends on the date input."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-setup",
    "title": "Shiny summary tiles",
    "section": "The setup",
    "text": "The setup\nThe first step is to import the R packages used within the application. The following code chunk contains the packages used for the application. A brief description of each is included.\n\nlibrary(shiny) # The Shiny app library\nlibrary(readr) # Import data\nlibrary(dplyr) # Pipe and data manipulation\nlibrary(tidyr) # Tidying data function\nlibrary(purrr) # Used for functional programming\nlibrary(glue)  # Used for string interpolation\n\nMany of these packages are part of the tidyverse, and thus the import could be simplified to just running library(tidyverse). Be aware this may bring in unused, unneeded libraries. There is nothing wrong with this approach. However, I opted to be more verbose with this example, so as to be clear about what libraries are utilized within the example application and to have more control on what packages were imported by the application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#application-layout",
    "title": "Shiny summary tiles",
    "section": "Application layout",
    "text": "Application layout\nThe next step is to code the layout of the UI. To keep the design simple, a sidebar will contain the application’s inputs, while the outputs will be placed within the main panel of the application. The general skeleton of the layout looks like this:\n\nui &lt;- fluidPage(\n   # Inputs\n   sidebarLayout(\n      sidebarPanel()\n   ),\n   # Outputs\n   mainPanel(\n      # Summary tiles\n      fluidRow(),\n      br(),\n      # Data information output\n      fluidRow()\n   )\n)\n\nThere’s nothing too fancy about this code, outside of it establishing the general layout of the application, so not much else will be said about what each element does here. However, Chapter 6 of Mastering Shiny discusses application layout if a more detailed description is needed."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#the-date-input",
    "title": "Shiny summary tiles",
    "section": "The date input",
    "text": "The date input\nThe app requirements state users need to have the ability to modify the dates to which the data represents, and the summary metric tiles will change based on this user input. However, the app will not have any user input upon startup, so it needs to default to the most recent date within the data. To meet these requirements, we use the following code:\n\n# Code excluded for brevity\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    )\n\nThe shiny:dateInput() function is used to create the HTML for the input, which resides in the application’s sidebar. The function’s id argument is given the value of date, which will establish a connection to elements within the server. More on this later. Then, a string value of Select a date for summary: is passed along to the label argument. This value will be displayed above the date input in the UI.\nSince the app won’t have an initial user input upon the startup of the application, max(ga4_date$event_date) is passed along to the value argument. This will default the input to the most recent date within the data. In addition, the functions max and min arguments are passed similar calls. However, in the case of the min argument the base R min() function is used on the ga4_data$event_date."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "title": "Shiny summary tiles",
    "section": "First iteration of the summary metric tiles",
    "text": "First iteration of the summary metric tiles\n\nThe server side\n\nThe reactive data() object\nBefore the summary metrics can be displayed in the UI, the application needs data to create the outputs. In addition, since this data will be dependent on users’ input (i.e., the user can select a new date which subsequently changes the summary metric tile), this object needs to be reactive. To do this, the following code is added to the server side of the application.\n\nserver &lt;- function(input, output, session) {\n   data &lt;- reactive({\n      ga4_data %&gt;% filter(.data[[\"event_data\"]] == input$date)\n   })\n}\n\nIn practical terms, this code just filters the data for the date being passed along as the input$date object.\nAgain, this object could be the most recent date within the data, the date set by the max argument in the dateInput() function, or it could be based on a user’s modification of the date input. Since this code was wrapped inside of the reactive({}) function, Shiny will be listening for any changes made to the to the input$date object. Any changes that occur will result in the data() reactive expression to be modified, followed by new output values being displayed via the UI.\nOne other key concept is being exhibited here, tidy evaluation, specifically data-masking. Since technically dplyr::filter() is being used inside of a function, an explicit reference to the data is required. Thus, .data[[\"event_data\"]] notation is used to make it explicit on what data will be filtered. The specifics on how to use data-masking in the context of a Shiny app is beyond the scope of this post. However, the previously linked materials provide a more detailed description of these concepts.\n\n\nThe outputs\nLooking back at the reactive graph, the application requires five outputs to be in the server. These outputs will just be simple text outputs, so the use of the shiny::renderText() function will be sufficient to meet our requirements. The format() function is also applied to comma format any outputs that contain numbers (e.g., 2,576 vs 2576). Here is what the server looks like currently:\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_date, .data[[\"event_date\"]] == input$date)\n  })\n  \n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nAs part of the functionality requirements, the app needed some UI element informing users what date is being represented in the summary tiles. The output$date object was included to meet this requirement. The output$date object, aside from using the renderText() function, includes the use of the glue::glue() function to make the outputted message more informative.\nThe {glue} package is used to manipulate string literals with the use of the curly braces (e.g., {}). When applied here, the {data()$event_date} is evaluated as an R call, its value becomes appended to the string, and the whole string is then outputted to the application’s UI.\n\n\n\nBack to the UI\nNow that there are five elements being outputted from the server, UI elements need to be included to display the rendered outputs.\nWhen making early design decisions about the application’s layout, it was decided these elements were going to reside within the main panel of the application. Another decision made was to keep the summary metric tile elements on the same row, so as to seem as though they are related to one another (i.e., related KPIs). As for the UI element informing the user on the date the summary metric tiles represent, it was decided that this element would be placed on its own row.\nTo achieve the intended design, additional Shiny layout functions were applied to the application’s code. This includes using the fluidRow() and column() functions to achieve the wanted UI organization. The following code was used to achieve the placement of the summary tiles within the application’s layout:\n\nmainPanel(\n   fluidRow(\n      column(),\n      column(),\n      column()\n   ),\n   br(),\n   fluidRow()\n)\n\nAs for the design of the summary metric tiles, each tile needed to include some type of title followed by the text representing the metric. To achieve this, the shiny::div() function was used. This function creates an individual HTML tag that outputs the text being passed along into the function. Directly below the title element, the textOutput() function is used to display the outputs coming from the application’s server. The code for one summary metric tile would look like the following:\n\ncolumn(3,\n       div(\"Unique Users\"),\n       textOutput(\"users\")\n       )\n\nBy combining these elements, the application code in its current state can be seen here:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n  )\n  \n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        column(3,\n               div(\"Users\"),\n               textOutput(\"users\")\n        ),\n        column(3,\n               div(\"Page Views\"),\n               textOutput(\"page_view\")\n        ),\n        column(3,\n               div(\"Session Starts\"),\n               textOutput(\"session_start\")\n        ),\n        column(3,\n               div(\"Purchases\"),\n               textOutput(\"purchase\")\n        )    \n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  # Text output\n  output$users &lt;- renderText(format(data()$users, big.mark = ','))\n  output$page_view &lt;- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start &lt;- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase &lt;- renderText(format(data()$purchase, big.mark = ','))\n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nIndeed, this code works and meets the functionality requirements. However, it’s quite verbose and contains a lot of redundant, repeated code. Different techniques could be applied to make the application more eloquent and efficient in its design. The goal of the next few sections, then, will be to simplify the application through the development of functions and applying functional programming principles.\n\nSimplifying the outputs\nReviewing the server, most of the outputs are created through the use of repeated patterns of the same code. This breaks the DRY principle (Don’t Repeat Yourself) of software development. Both functions and the application of functional programming principles will be applied to address this issue.\nAn obvious pattern used to create the outputs is output$foo &lt;- renderText(format(bar, big.mark = ',')). This pattern could be converted into a function, and then this function could be used to iterate over the several reactive objects (e.g., data()$users) with the use of a {purrr} function. Since the side-effects are intended to be used rather than outputting a list object from our iteration, purrr::walk() will do the trick.\nUtilizing this strategy simplifies our code to the following:\n\nc('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n\nIndeed, I can’t take full credit for this solution. Thanks goes to @Kent Johnson in the R4DS Slack channel for helping me out.\nThe output$date object was left out of this simplification of the code. Certainly, the function could be made to be more general and flexible to handle this repetition of the renderText() function. However, this would be over engineering a solution to the problem."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#back-to-the-ui-1",
    "title": "Shiny summary tiles",
    "section": "Back to the UI",
    "text": "Back to the UI\nFunctions and functional programming principles will now be used to address these same issues on the UI side of the application. Much of the repetition occurs with the use of the following pattern:\n\ncolumn(3,\n       div(\"Metric Title\"),\n       textOutput(\"metric_output\")\n       )\n\nIndeed, this pattern is applied four times. Since it was copied and pasted more than twice and breaks the DRY principle, it would be best to convert it into a function and iterate it using functional programming tools."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "title": "Shiny summary tiles",
    "section": "Simplifying the UI with functional programming",
    "text": "Simplifying the UI with functional programming\nA helper function, make_summary_tile(), is added to the setup section of the application. The function looks like this:\n\nmake_summary_tile &lt;- function(title, text_output){\n  column(2,\n         div(title),\n         textOutput(text_output)\n  )\n}\n\nThere’s nothing too fancy or complicated about this function. It simply generalizes the pattern applied within the UI side of the first iteration of our application. As for placement, this function could be defined at the top of the application file or in a separate .R file embedded in a R/ sub-folder. Both strategies would make the function available for the app. Deciding which to use comes down to the intended organizational structure of the application.\nThe next step is to apply functional programming to iterate the make_summary_tile() function over the text outputs. Since the function requires two inputs, title and text_output, they were placed inside of a tibble to improve organization of the inputs being passed to the function through pmap().\n\n# Defined in the Setup section\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Used within the UI\npmap(tiles, make_summary_tile)\n\nWhat once required sixteen line’s of code was cut in half to eight (including the explicit definition of the inputs). In addition, coding the tiles using functional programming also makes it more flexible, where summary tiles could be easily added or taken away.\nDoing this would require some slight modification to the make_summary_tile() helper function, though. That is, a width argument would need to be added to the function, so the column width could be set to accommodate the number of outputs for the UI. There are lots of different options that could be explored here. At this point, though, the solution meets the functionality requirements.\nIn its current state, the application code looks like this:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\nmake_summary_tile &lt;- function(header, text_output){\n  column(2,\n         div(header),\n         textOutput(text_output)\n  )\n}\n\ntiles &lt;- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Import data -------------------------------------------------------------\n\nga4_data &lt;- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n)\n\n# UI ----------------------------------------------------------------------\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        pmap(tiles, make_summary_tile)\n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  c('users', 'page_view', 'session_start', 'purchase') %&gt;% \n    walk(~{output[[.x]] &lt;- renderText(format(data()[[.x]], big.mark = ','))})\n  \n  output$date &lt;- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nThe application works, meets the functionality requirements, and now is written in a way that reduces repetition and redundant patterns within the code. However, the summary metric tiles just blend into the UI, and nothing about the styling communicates they contain important information.\nSince these elements are meant to highlight key, important summary metrics, they need to be styled in a way that creates contrast between themselves and the application’s background. The next section focuses on applying custom CSS to give some contrast between these elements and the application’s background."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "href": "blog/posts/2021-12-30-shiny-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "title": "Shiny summary tiles",
    "section": "Creating the www folder and CSS file",
    "text": "Creating the www folder and CSS file\nSince the design opted for a file-based CSS approach, a separate www sub-folder in the application’s main project directory needs to be created. Once created, the custom CSS file will be placed inside this folder. The placement of this file can be seen in this Github repo.\nThe purpose of this folder is to make the file available to the web browser when the application starts. Placement of this file is critical. If it is not placed in the www sub-folder, then the CSS file will not be available when the application starts, and any custom styling will not be applied.\nOnce the www sub-folder is created, you can create a CSS file for the application in Rstudio by clicking File, hovering over New File, and selecting CSS File. Save the file in the www sub-folder and give it an informative name. In the case of this example, the file is named app-styling.css.\nThe main goal of the styling will be to create some contrast between the summary metric tiles and the application’s background. Specifically, CSS will be used to create a container that is a different color from the application’s background and includes some shading to make it seem like the element is hovering above the application’s main page. To do this, the app-styling.css file includes the following:\n#summary-tile{\n  font-size: 25px;\n  color:White;\n  text-align: center;\n  margin: 10px;\n  padding: 5px;\n  background-color: #0A145A;\n  border-radius: 15px;\n  box-shadow: 0 5px 20px 0 rgba(0,0,0, .25);\n  transition: transform 300ms;\n}\nA detailed description on how to create CSS selectors is outside the scope of this post. However, in general terms, this selector sets several values for multiple CSS properties by defining the id, #summary-tile within the file. More about this process of creating different CSS selectors can be found here.\nNow it’s just a matter of modifying the code to call this file and pass these style values to the summary tiles within the application. The following code is added to the ui side of our application to include our app-styling.css file:\n\ntags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"app-styling.css\"))\n\nSince the styling is being applied to the summary metric tiles, the make_summary_tile() function is modified to bring in the CSS elements. A css_id argument is added to the function.\n\nmake_summary_tile &lt;- function(header, text_output, css_id){\n  column(2,\n         div(header),\n         textOutput(text_output),\n         id = css_id\n  )\n}\n\nNow that we made this modification to the make_summary_tile(), its application in the UI is also modified. Specifically, the #summary-tile CSS element is explicitly called in pmap(). To do this, the code is modified like this:\n\npmap(tiles, ~make_summary_tile(\n          header = ..1, text_output = ..2, css_id = \"summary-tile\"))\n\nThe header, text_output, and css_id arguments are now explicitly defined in the pmap() call. To refer to the first two elements in the tiles data object, the ..1 (i.e., header column) and the ..2 (i.e., text_output column) are used. Check out the pmap() docs on how to apply the ..1, ..2 (?pmap) for more information."
  },
  {
    "objectID": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "href": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "title": "Flattening Google Analytics 4 data",
    "section": "",
    "text": "Photo by Alvaro Calvo\n\n\n\nIntroduction\nWith the introduction of the Google Analytics 4 (GA4) BigQuery integration, understanding how to work with the underlying analytics data has become increasingly important. When first diving into this data, some of the data types may seem hard to work with. Specifically, analysts might be unfamiliar with the array and struct data types. Even more unfamiliar may be the combination of these two data types into complex, nested and repeated data structures. As such, some may become frustrated writing queries against this data. I know I did.\nIf you’re mainly coming from working with flat data files, these more complex data types may not be intuitive to work with, as the SQL syntax is not as straight forward as a simple SELECT FROM statement. Much of this unfamiliarity may come from the required use of unfamiliar BigQuery functions and operators, many of which are used to transform data from nested, repeated, or nested repeated structures to a flattened, denormalized form.\nAs such, this post aims to do three things: 1. Overview the array, struct, and array of struct data types in BigQuery; 2. Overview some of the approaches to flatten these data types; and 3. Apply this knowledge in the denormalization of Google Analytics 4 data stored in BigQuery.\nThis post mostly serves as notes that I wish I had when I began working with these data structures.\n\n\nArrays, structs, and array of structs\nBefore discussing the use of these data types in GA4 data, let’s take a step back and simply define what array and struct data types are in BigQuery. A good starting point is BigQuery’s arrays and structs documentation. According to the docs,\n\nAn array is an ordered list of zero or more elements of non-Array values. Elements in an array must share the same type.\n\n\nA struct is a container of ordered fields each with a type (required) and field name (optional).\n\nBoth definitions contain technical jargon that don’t really define, in an intuitive, useful way, what these data types are and how to use them, especially in the analysis of GA4 data. So let’s break each down by bringing in additional perspectives and through the use of several simplified examples.\nWhile learning more about arrays and structs, I found several blog posts that helped me better understand these structures and how to use them. Here is a list of the ones I found to be very helpful:\n\nHow to work with Arrays and Structs in Google BigQuery by Deepti Garg\nExplore Arrays and Structs for Better Query Performance in Google BigQuery by Skye Tran\nTutorial: BigQuery arrays and structs from Sho’t left to data science\n\nI highly suggest reading all of these. In fact, much of what follows is adapted from these posts, with a few examples I created to help me better understand how these data types are structured, stored, and queried. Towards the end of the post, the techniques learned from these posts and overviewed here will be applied to GA4 data, specifically the publicly available bigquery-public-data.ga4_obfuscated_sample_ecommerce data.\n\n\nArrays\nArrays are a collection of elements of the same datatype. If you’re familiar with the R programming language, an array is similair to a vector.\nLet’s create a table containing an array of planets in our solar system as an example, and then use the INFORMATION_SCHEMA view to verify the data was entered correctly. The following code will create this table in BigQuery:\ncreate or replace table examples.array_planets_example as \nwith a as (\n   select [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"] as planets\n)\nselect planets from a;\nThe INFORMATION_SCHEMA.COLUMNS view for the array_planets_example table can be queried to verify the data was entered correctly. This table is available for every table created in BigQuery, and it contains metadata about the table and the fields within. Here is the query needed to return this information:\nselect table_name,\n   column_name,\n   is_nullable,\n   data_type\nfrom examples.INFRORMATION_SCHEMA.COLUMNS\nwhere table_name = \"array_planets_example\";\nThe returned table will contain a data_type field, where the value ARRAY&lt;STRING&gt; will be present. This value represents the field in the array_planets_example contains an array with a list of string values. Although this example array contains a series of string values, arrays can hold various other data types, as long as the values are the same type across the collection. Overviewing all of the different data types that can be stored in an array is beyond the scope of this post, but check out the BigQuery docs for more examples.\n\nQuerying an array\nMultiple approaches are available to query an array. The type of approach will depend on if the returned data needs to maintain its grouped, repeated structure, or if the returned data needs to be flattened. If maintaining the repeated structure is required, then a simple SELECT statement will work. Using the array_planets_example table as an example, the query applying this approach will look something like this:\nselect planets\nfrom examples.array_planets_example\nIf each element of the array is to be outputted onto its own row (i.e., denormalized), multiple approaches are available. The first approach is to use the unnest() function. Here is an example using the planets array we created earlier:\nselect planets\nfrom examples.array_planets_example,\nunnest(planets) as planets\nThe second approach is to apply a correlated join through the use of cross join unnest(). This approach looks like this:\nselect planets\nfrom examples.array_planets_example\ncross join unnest(planets) as planets\nYou’ll notice this is only slightly different than the query above, and in fact the , used in the FROM clause is short-hand for the cross join statement. The last and final approach is to use a comma-join. This is similair to our first query, but now we refer to the table name before the array name we want flattened.\nselect planets\nfrom examples.array_planets_example, array_planets_example.planets as planets;\nWhich one do you choose? It really comes down to a matter of preference. All three approaches will lead to the same result. It depends on how explicit you want the code to be.\nThere is one note to be aware of if you’re applying these conventions to other arrays outside of analyzing GA4 data. The cross join approach will exclude NULL arrays. So if you want to retain rows containing NULL arrays, you’ll need to apply a left join. More about this is described in the BigQuery docs.\nKeep these approaches top-of-mind. They will be applied to flatten some of the fields in the GA4 dataset. In other words, get comfortable with using them.\n\n\n\nStructs\nThe structs data type holds attributes in key-value pairs. Structs can hold many different data types, even structs. We will see the use of structs within structs in the GA4 data. Keeping with the solar system theme of the post, the following example code will create a table utilizing the struct data type to hold the dimensions and distances of the planets in our solar system. The data used for this table is reported here.\ncreate or replace table examples.struct_solar_system as\nwith a as (\n  select \"Mercury\" as planet,\n  struct(0.39 as au_sun, 57900000 as km_sun, 4879 as km_diameter) as dims_distance union all\n  select \"Venus\" as planet,\n  struct(0.72 as au_sun, 108200000 as km_sun, 12104 as km_diameter) as dims_distance union all\n  select \"Earth\" as planet,\n  struct(1 as au_sun, 149600000 as km_sun, 12756 as km_diameter) as dims_distance union all\n  select \"Mars\" as planet,\n  struct(1.52 as au_sun, 227900000 as km_sun, 6792 as km_diameter) as dims_distance union all\n  select \"Jupiter\" as planet,\n  struct(5.2 as au_sun, 778600000 as km_sun, 142984 as km_diameter) as dims_distance union all\n  select \"Saturn\" as planet,\n  struct(9.54 as au_sun, 1433500000 as km_sun, 120536 as km_diameter) as dims_distance union all\n  select \"Uranus\" as planet,\n  struct(19.2 as au_sun, 2872500000 as km_sun, 51118 as km_diameter) as dims_distance union all\n  select \"Neptune\" as planet,\n  struct(30.06 as au_sun, 4495100000 as km_sun, 49528 as km_diameter) as dims_distance \n)\nselect * from a;\nThis table contains two columns. A column that holds a string value for the name of the planet and a struct column that contains a list of key value pairs of distance and dimensions for each planet.\nThe INFRORMATION_SCHEMA.COLUMNS table can then be queried again to verify the datatypes for each column were inputted correctly. Here is the code to do this:\nselect \n  table_name, \n  column_name,\n  is_nullable,\n  data_type\nfrom examples.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"struct_solar_system\";\nThe returned table will contain a data_type column with two values: STRING and STRUCT&lt;au_sun FLOAT64, km_sun INT64, km_diameter INT64&gt;. Take notice that the STRUCT value contains information about the data types contained within.\n\nQuerying a struct\nQuerying a struct requires the use of the . operator (i.e., dot operator) in the FROM clause to flatten the table. Take for example the case where we want to return a table of only the distance of each planet from the sun in kilometers. The following query will be used:\nselect \n  planet,\n  dims_distance.km_sun\nfrom examples.struct_solar_system;\nSay a denormalized table that contains both the distance from the sun in kilometers and each planet’s diameter in kilometers is wanted. The following query would be used:\nselect \n  planet,\n  dims_distance.km_sun,\n  dims_distance.km_diameter\nfrom examples.struct_solar_system;\nWhen reviewing these two examples, observe how the dot notation is being used. In the first, our select statement contains dims_distance.km_sun, which unnests the values and gives each its own row for each planet. This is expanded in the second query, where an additional line is added to the select statement, dims_distance.km_diameter. To unnest all the values in the struct, use the following query:\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system;\nIn fact, let’s expand this query to answer the following question: which planets are the closest and farthest from our sun. Take notice how the ORDER BY portion of the query doesn’t require the dims_distance prefix for the field we want to arrange our data.\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system\norder by km_sun;\n\n\n\nArray and structs in GA4 data\nNow that we have learned a little bit about our solar system, let’s return to Earth and the task at hand, flattening GA4 data. We just discussed how these data types are created and queried, it is now time to combine them into more complex data structures, as both of these structures are combined to create nested repeated data structures in the GA4 data. It’s best to start with an example. Specifically, let’s look at how these structures are applied in the event_params field.\nWe can start off by querying the INFORMATION_SCHEMA.COLUMNS view for one event to get an idea of its structure. The query to do this can be seen here:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"events_20210131\" and column_name = \"event_params\";\nThe data type is described in the returned table’s data_type field. This field contains the following value ARRAY&lt;STRUCT&lt;key STRING, value STRUCT&lt;string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64&gt;&gt;&gt;. It should be immediately apparent that both the array and struct values are being used here to create a repeated nested structure. In fact, the event_params value uses a struct within a struct. Given this structure, all the above methods will need to be employed to flatten this data.\nTo simplify this, let’s look at one instance of one event in the GA4 data. Specifically, let’s look at one instance of a page_view event. With this simplified example, we’ll go step-by-step, adding additional elements to the query needed to flatten this data.\nselect \n  event_date,\n  event_timestamp,\n  event_name,\n  event_params\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = 'page_view'\nlimit 1;\nAfter running this query, you’ll notice the output to the console is quite verbose, especially if you’re using the bq command-line tool. The verbosity of the output is due to the event_params field holding much of the data.\nThe first layer of the structure is an array, so the initial step is to use the unnest() function. The following can be done to achieve this:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param;\nYou’ll notice a nested FROM statement is being used here. This is done to limit the result set to one row, representing one page_view event for this simplified example. Later iterations of the query will eliminate this nested query.\nNow say we’re only interested in viewing the page_location parameter. We can use a where statement to filter out this information. Here is what this will look like:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key = 'page_location';\nInterested in viewing both the page_location and page_title parameters? Use the IN operator in the WHERE clause.\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key in ('page_location', 'page_title');\nWanna turn the key field into columns so you only have one row for this specific event? Use BigQuery’s pivot() operator. Here is how to achieve this in a query:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    key,\n    value.string_value\n  from (\n    select \n      event_date,\n      event_timestamp,\n      event_name,\n      event_params\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nSince the string values are all we care about here, the value.string_value was the only one retained in the query. The other nested value elements were eliminated from the SELECT statement.\n\n\nCombine other nested fields in the GA4 data\nNow that the event_params field has been flattened, let’s supplement this information with additional data in the table. Moreover, this will provide another example of how to apply these steps to flatten other elements in the GA4 data. Knowing where users originate is some additional context that may add to our event analysis, so let’s add that data to our flattened data. But first, let’s get some more information on what data type is used for the geo field in the GA4 data.\nOnce again, querying the INFORMATION_SCHEMA.COLUMNS view can be used to explore the geo field’s data type. Here is what the query looks like:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS`\nwhere table_name = \"events_20210131\" and column_name = \"geo\";\nThe value STRUCT&lt;continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING&gt; is returned. Let’s write a query to return the table without first unnesting the data.\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo \nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = \"page_view\"\nlimit 1;\nYou’ll notice this field contains a struct, where the dot operator will need to be applied to flatten this data. Let’s start by flattening this data and then combine it with the events_param data. For the sake of keeping the returned table simple, let’s just return the region and city fields in a denormalized form. The following will return a flattened table with these fields:\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo.region,\n  geo.city\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nlimit 1;\nAs expected, the table will return a flattened table containing five fields: event_date, event_name, user_pseudo_id, geo.region, and geo.city. This table was also limited to return only the first instance of the page_view in the table.\nNow, the next step is to add this geo data to our flattened event_params query. This is as simple as adding the . operator with the needed geo elements into the FROM statement. The query will now look like this:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nThe resulting table will contain one row with several fields representing the specific event. This is great for one event, but the next step will be to expand this denormalization to all page_view events in the table.\n\n\nExpand the unnesting to multiple page view events\nNow that we have the flattened table for one page_view event, let’s expand it to additional events. This requires a simple modification to the initial nested query, remove the limit 1 line.\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\nWe can now refactor the query to be more concise. Here is what this will look like:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\n\n\nApply these approaches across multiple days\nGenerating results for one day may not be enough, so there’s a few modifications that can be made to expand the final query to return additional days. This involves modifying the FROM and WHERE statements in the initial query.\nThe first step is to modify the FROM statement to use the * wildcard operator at the end of the table name. Since the GA4 tables are partitioned by day, this will allow for a range of tables to be defined within the WHERE clause. The table name will now be bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*.\nTo define the range of dates for the events (i.e., to query multiple tables), the WHERE clause will be expanded to include the use of _table_suffix. The _table_suffix is a special column used within a separate wildcard table that is used to match the range of values. Explaining the use of the wildcard table is beyond the scope of this post, but more about how this works can be found here. The WHERE clause will now look like this:\nwhere event_name = 'page_view' and\nkey in ('page_location', 'page_title') and\n_table_suffix between \"20210126\" and \"20210131\"\nYou’ll notice this statement uses the between operator, where two string values representing the date range are passed. This statement is inclusive, so it will include partitioned tables from 20210126 and 20210131, and all tables in between. Here is the query in its final form:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and \n  key in ('page_location', 'page_title') and\n  _table_suffix between \"20210126\" and \"20210131\"\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\norder by event_date\n\n\nWrap up\nThis post started out simple by defining what arrays, structs, and array of structs data types are in BigQuery. Through the use of several examples, this post overviewed several approaches to query these different data types, specifically highlighting how to flatten each type. A second aim of this post was to show the application of these methods to the flattening of GA4 data stored in BigQuery. This included the flattening and combination of the complex, nested, repeated and nested repeated data types used in the event_params and geo fields. Finally, this post shared queries that expanded the result set across multiple days worth of data.\nIf you found this post helpful or just have interest in this type of content, I would appreciate the follow on GitHub and/or Twitter. If you have suggestions on how to improve these queries or found something that I missed, please file an issue in the repo found here.\n\n\nAdditional resources\nI spent a lot of time researching how to write, use, and query arrays and structs in BigQuery. In the process of preparing this post, I wrote a lot of example queries and followed along with BigQuery’s turtorial on working with arrays and structs. As a result, I created multiple files that I organized into the GitHub repo for this post. These might be useful as a review after reading this post, or they might be a helpful quickstart quide for your own analysis of GA4 data stored in BigQuery. These additional notes can be found here.\n\n\nAdditional references\n\nHow to work with Arrays and Structs in Google BigQuery\nExplore Arrays and Structs for Better Query Performance in Google BigQuery\nTutorial: BigQuery arrays and structs\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2022,\n  author = {Berke, Collin K},\n  title = {Flattening {Google} {Analytics} 4 Data},\n  date = {2022-09-20},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2022. “Flattening Google Analytics 4\nData.” September 20, 2022."
  },
  {
    "objectID": "blog/posts/2021-04-02-intro-post/index.html",
    "href": "blog/posts/2021-04-02-intro-post/index.html",
    "title": "Intro Post",
    "section": "",
    "text": "Hello World!\nI have decided to start writing a blog. I have never attempted anything like this before, but I felt it was time to start organizing some of my projects and analyses into one central location. For some time, I really wanted to develop a space where I could discuss topics I find interesting, both professionally and personally. So, Hello World!, my name is Collin, and this is my blog.\nI’m also on other platforms, however, I really don’t engage on them too often. Nevertheless, you can find more information about me and my projects in the following locations:\n\nGitHub is a great place to see the projects I am working on. Most are professional at this time.\nTwitter, I haven’t posted anything in a while, though I retweet and like stuff often.\nEmail, the best channel to get a hold of me: collin.berke@gmail.com.\n\n\n\nThe purpose of this blog\nThe purpose of this blog is to serve as a location for me to express my thoughts on topics I find interesting. To be honest, I don’t expect this to be a really niche blog with one focused, clear purpose. Most likely it will be data analysis and/or visualization focused, which I will apply to develop posts in areas I find interesting: open-source software, media/marketing analytics, data analysis, sports, media, etc. My purpose may become more refined once I find my voice.\n\n\nThe inspiration and motivation to do this blog\nI have spent countless hours reading, re-reading, bookmarking, and Googling multiple topics regarding the use of the statistical computing programming language called R. Much of this time has been spent accessing useful, open-source, and free content that has aided me professionally, and it has contributed to my deeper understanding of the topics I find interesting. I couldn’t even begin to describe how grateful I am for those who have spent time organizing and drafting content others find useful. In fact, it’s the #Rstats community that has motivated me to put this blog together, as I have seen how helpful and open it is to aiding in the development of others.\nI now feel I am in a place of not only being a consumer but a producer of this information. I will never be an expert in this area, as there is too much to learn for just one person. As one of my favorite podcasts (i.e., Make Me Smart With Kai and Molly) states in every episode, “None of us is as smart as all of us.” Thus, I feel it is time to start organizing and drafting content others will hopefully find useful, at the very least amusing. Even if this blog helps one person, that will be enough motivation to keep me working on it.\n\n\nWhat’s up next?\nNot sure. I’m just excited I got this blog up and running. Most likely I’ll do something sports related. Who knows–stay tuned.\n\n\nReferences & Acknowledgements\n\nI make every attempt to properly cite information from other sources. If I have failed to properly attribute credit to a source, please kindly let me know.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2021,\n  author = {Berke, Collin K},\n  title = {Intro {Post}},\n  date = {2021-04-02},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2021. “Intro Post.” April 2, 2021."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html",
    "href": "blog/posts/2023-01-29-2023-rig/index.html",
    "title": "2023 data science rig: Set up and configuration",
    "section": "",
    "text": "Photo by Barn Images"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#a-few-extra-configs-to-the-operating-system",
    "title": "2023 data science rig: Set up and configuration",
    "section": "A few extra configs to the operating system",
    "text": "A few extra configs to the operating system\nI also like to customize the appearance, system keymappings, and terminal aliases (more on this in the section on setting up Zsh) of my operating system. For one, I’m a fan of dark mode, so I set the system settings accordingly. I’m also a minimalist when it comes to the menu dock. I prefer to only include shortcuts that are necessary to my workflow. I also like to change the settings to automatically hide the dock when it’s not being used. I do this to maximize my workspace area. Here is a link to some docs if you’re interested in modifying your macOS system settings.\nThe caps lock key is useless in my workflow. Instead, I remap the ctrl key to the caps lock key. This is mostly done out of convenience, as I’ll use my machine as a true laptop from time to time. This is also essential because my IDE, Neovim, requires extensive use of the ctrl keys (more on the use of Neovim later). Since the MacBook Pro does not include a right-hand side ctrl key, and the left-hand side ctrl key is not in a comfortable position, this remap affords me some additional comfort when I use my machine as a laptop."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#homebrew",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#homebrew",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Homebrew",
    "text": "Homebrew\nHomebrew coins itself as the missing package manager for macOS (or Linux). It makes downloading open-source software much easier. Downloading and installing Homebrew is straight forward. Run the following command in a terminal to download Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nIf you need more specific instructions on downloading and installing Homebrew, check out the docs I linked above. With the Homebrew package manager installed, it’s a cinch to download other tools."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#oh-my-zsh",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#oh-my-zsh",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Oh My Zsh",
    "text": "Oh My Zsh\nNow it’s time to unleash the terminal by downloading Oh My Zsh. Download Oh My Zsh by running the following in your terminal:\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\nOh My Zsh’s docs contain the best description of what it does:\n\nOh My Zsh will not make you a 10x developer…but you may feel like one.\n– Zsh docs\n\nFor reals though, Oh My Zsh is a convenient, intuitive means to configure your terminal. For one, it allows plugin installation. Plugins enhance the terminal experience and extend its utility. The following is a list of Zsh plugins I find useful:\n\ngit\nzsh-syntax-highlighting for terminal syntax highlighting.\nzsh-autosuggestions for command suggestions based on previous history.\n\n\nCustomized Zsh prompt\nAnother great feature of Zsh is the ability to customize the command line prompt. Many options are available. For me, I like the prompt to contain four pieces of information:\n\nThe time (24-hours with seconds);\nThe file path of the current working directory;\ngit branch information;\nAn indicator if any uncommitted changes exist in the directory.\n\nHere is what my prompt looks like:\n\n\n\nCustomized Zsh prompt\n\n\nTo achieve this custom setup, I place the following into my .zshrc file:\n# Prompt formatting\nautoload -Uz add-zsh-hook vcs_info\nsetopt prompt_subst\nadd-zsh-hook precmd vcs_info\nPROMPT='%F{blue}%*%f %F{green}%~%f %F{white}${vcs_info_msg_0_}%f$ '\n\nzstyle ':vcs_info:*' check-for-changes true\nzstyle ':vcs_info:*' unstagedstr ' *'\nzstyle ':vcs_info:*' stagedstr ' +'\nzstyle ':vcs_info:git:*' formats       '(%b%u%c)'\nzstyle ':vcs_info:git:*' actionformats '(%b|%a%u%c)'\nIndeed, this might not be the custom prompt for everyone. So, the following are links to blog posts that do an excellent job describing how to customize the different prompt elements:\n\nCustomizing my Zsh Prompt by Cassidy Williams\nCustomize your ZSH prompt with vcs_info by Arjan van der Gaag\n\n\n\nTerminal aliases\nThis year, I focused on transitioning to a more terminal based workflow. As part of this transition, I began utilizing terminal aliases. Aliases can be used to automate common tasks, like opening specific programs, web pages, or project files from the terminal.\nWith Zsh, creating aliases is pretty straightforward. To do this, you’ll need to place a file into the ~/oh-my-zsh/custom directory. This file can be named anything, but it needs to end in the .zsh extension. In this file you can include aliases like the following:\n# aliases to improve productivity\nalias email=\"open https://path-to-email.com/mail/inbox\"\nalias calendar=\"open https://path-to-calendar.com/\"\nalias projects=\"open https://path-to-github-projects.com/\"\nNow if you run email in your terminal prompt, a browser window with your email inbox will open. The above is just an example to get you started. I have additional aliases beyond the ones in the example. To get an idea of all the aliases I use, check out the dotfile here. You can customize any of these to your specific needs.\nThe rest of my Zsh configuration is pretty standard. Here is a link to a repo containing additional files to configure Zsh. Check it out if you’re interested in seeing how I specifically do something."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#additional-terminal-utilities",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#additional-terminal-utilities",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Additional terminal utilities",
    "text": "Additional terminal utilities\n\nJump\nNavigating the file system from the terminal can be tiring. Jump is a terminal utility that solves this problem. Simply put, this utility learns your navigational habits and allows you to easily jump back and forth between directories with very little typing.\nInstall jump using Homebrew. Run the following code in your terminal to install Jump:\nbrew install jump\n\n\ntmux\ntmux is a terminal multiplexer. It lets you create multiple windows and terminals in a single session. I find it useful in situations where you want multiple files, projects, or terminal windows to be open while you’re working.\nInstall tmux using Homebrew:\nbrew install tmux\nAlthough tmux is useful out of the box, some configuration steps are needed to make it more useful. My configuration mostly changes tmux’s keymaps, which makes them easier to remember and use (i.e., some of the defaults require some keyboard gymnastics).\nMuch of my tmux configuration is a derivative of the one discussed in the Getting Started with: tmux YouTube series from Learn Linux TV. If you want some more specific detail, you can check out my .tmux.conf configuration file here.\n\n\ngit\nI use git for version control. Homebrew can be used to install git:\nbrew install git\nSome additional configuration is needed for the local setup of git. Run the following code in the terminal. Make sure to replace what is in quotations with your information.\ngit config --global user.name \"&lt;full-name&gt;\"\ngit config --global user.email \"&lt;email&gt;\"\ngit config --global core.editor \"nvim\"\nThe user.name and user.email variables are required. You can exclude the core.editor configuration if you want to use the default editor. However, I like to use Neovim (more on Neovim in a later section) as my text editor, so I make it my default when working with git.\nAlong with git, I use GitHub for remote repositories. Some additional steps are needed to authenticate with this service. The GitHub CLI tool simplifies these steps.\n\n\nGitHub’s CLI tool\nBring GitHub to your command line with the GitHub CLI. This tool provides commands to do many of the same things you do on GitHub, but with terminal commands. Need to create an issue in a repo, run the following in your terminal:\ngh create issue\nWant to see all the pull requests in a repo needing review, run the following in your terminal:\ngh pr list\nYou can also use these commands within aliases to streamline your workflows. I particularly like my custom aliases to list and create issues and PRs.\n# Custom alias to list GitHub issues\n.il\n\n# Custom alias to create an issue\n.ic\nHomebrew, again, is used for the installation.\nbrew install gh\n\nAuthenticate using the GitHub CLI\nOnce installed, run the gh auth login command to walk you through the authentication flow. During the flow, you’ll have to make a few decisions. Your first decision will be the protocol you want to use for git operations. I select HTTPS. Second, you’ll need to decide how you want to authenticate the GitHub CLI. I select the web browser setting out of convenience. If you’re interested in other forms of authentication, I suggest checking out GitHub’s docs.\nOne minor, additional configuration step is to set Neovim as the default editor for use with the GitHub CLI. If you want to use the default editor, then skip this step. To modify the default editor, run the following command in the terminal:\ngh config set editor nvim"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-rig",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-rig",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install rig",
    "text": "Install rig\nHomebrew handles the installation of rig. Run the following in your terminal:\nbrew tap r-lib/rig\nbrew install --cask rig"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-the-most-recent-version-of-r",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-the-most-recent-version-of-r",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install the most recent version of R",
    "text": "Install the most recent version of R\nOnce rig is installed, download the most recent version of R by running the following in the terminal:\nrig add\nOnce the most recent version is downloaded, you can verify the installation was successful by printing out a list of all the R versions installed on your machine. If this is a fresh start on a new machine or it’s your first time downloading R, you should only see one version listed.\nrig list"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#download-rstudio",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#download-rstudio",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Download RStudio",
    "text": "Download RStudio\nAlthough I have made the switch to using a different IDE (more on this in the next section), I still teach classes and present to groups who mainly use RStudio. So to keep everything up to date and in synch, I download the current version of RStudio using Homebrew:\nbrew install --cask rstudio\nrig also makes it easy to open up a new session of RStudio from the terminal. To do this, run the following in the terminal:\nrig rstudio"
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#install-r-packages",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#install-r-packages",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Install R packages",
    "text": "Install R packages\nThis section overviews the installation of R packages I use most often. Indeed, it would be excessive to download and overview all the packages in my workflow. In addition, the following sections contain a brief description of what each package does and how it is used when I work with R.\nThe following code downloads packages I rely on most. If you use R, many of these packages will be familiar.\n\ninstall.packages(c(\n  devtools,\n  usethis,\n  roxygen2,\n  tidyverse,\n  lubridate,\n  testthat,\n  googleAnalyticsR,\n  bigrquery\n))\n\nIf you’re unfamiliar with loading packages in R, you’ll need to run this code an R console. This can be done either in RStudio or via an iTerm2 system terminal. From the system terminal, type the letter R and hit Enter. Doing this should change your terminal prompt, as you are now running in a R session. You’ll then run the code from above. Information will be printed to the terminal during the installation of the packages.\nOnce all these packages have been installed, run the quit() function to return back to the system’s original prompt. When quitting this R session, you may be prompted to save the workspace. Enter no, as there is no need to save this session’s information. The next few sections provide a brief description of how each of the installed packages are used within my workflow.\n\ndevtools\n\nThe aim of devtools is to make your life as a package developer easier by providing R functions that simplify many common tasks.\n– devtools package docs\n\nSimply put, I rely on devtools for package development. This package provides many convenience functions to manage the mundane tasks involved in package development.\n\n\nusethis\nusethis is a workflow package. It automates many tasks involved when setting up a project. It also contains convenience functions to help with other R project workflow tasks. I’m still exploring all the package’s functions, but using the one’s I’ve learned have made me more productive.\n\n\nroxygen2\nPackages need documentation. The roxygen2 package helps with the documentation setup and development process. If you’re familair with comments in R, you’ll find writing package documentation with roxygen2 intuitive.\n\n\ntidyverse\ntidyverse is mainly used for common data wrangling and analysis tasks. Although I use base functions from time-to-time, I learned R by using tidyverse packages; they’re ingrained throughout my workflow.\nIndeed, the tidyverse is not just a single package, but a collection of packages. Some of the tidyverse packages I rely on most often include:\n\nggplot2 for data visualization\ndplyr for manipulating data\ntidyr for common data tidying tasks\npurrr for functional programming\nstringr for working with string data\n\n\n\nlubridate\nlubridate is magic when it comes to working with date-time data. I use this package mostly to handle data with a time dimension, which usually occurs in cases where I’m working with and analyzing time series data. If you work with date time data, look into using lubridate.\n\n\ntestthat\nThe testhat package is used for writing tests (e.g., unit tests) for code, especially when developing a package. To write more robust code, it’s best practice to write tests. testthat provides a framework and several convenience functions to make composing tests more enjoyable.\n\n\ngoogleAnalyticsR\nPart of my work involves the analysis of web analytics data. Much of this data is collected with and made available via Google Analytics. googleAnalyticsR is a package that allows you to authenticate and export web analytics data using the Google Analytics Reporting API.\n\n\nbigrquery\nGoogle BigQuery is a data warehouse and analytics solution. To access data via its API, I rely on the bigrquery package. This package provides multiple convenience functions to extract, transform, and load data from and into BigQuery. bigrquery also provides several functions to perform some BigQuery administrative tasks.\nThe packages highlighted above are ones I rely on most often in my day-to-day workflow. Indeed, others are used less frequently, especially when performing specific analysis tasks. However, the use of some packages is project dependent and describing all the packages I use would be outside the scope of this post."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#google-cloud-command-line-interface-cli",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Google Cloud Command Line Interface (CLI)",
    "text": "Google Cloud Command Line Interface (CLI)\nI mainly use the Google Cloud Platform (GCP) for cloud based project development. Although I’ll use GCP’s web portal occasionally, the command line interface provides some useful utilities to work from the terminal. The Google Cloud CLI is made available by installing the Google Cloud Software Development Kit (SDK).\nGoogle BigQuery, a data warehouse solution, is a GCP service I use quite often. The Google Cloud CLI has the bq command, which is an interface with BigQuery. I also manage some compute instances in the cloud, so I use the gcloud compute instances command as well.\n\nInstalling the Google Cloud SDK\nInstall the GCP SDK with Homebrew. To download, run the following code in your terminal:\nbrew install --cask google-cloud-sdk\n\n\nAuthorizing the Google Cloud CLI\nYou can review Google Cloud CLI’s authentication steps here. I provided the link to these docs because depending on your current setup and needs, you may need to use different steps to authenticate. Most likely, though, if you’re intending to authenticate with a user account, you can run the following command in your terminal to walk through the authentication steps:\ngcloud init\nAgain, it’s best to review the docs linked above, so you’re aware of the steps needed to authenticate with your specific setup."
  },
  {
    "objectID": "blog/posts/2023-01-29-2023-rig/index.html#neovim",
    "href": "blog/posts/2023-01-29-2023-rig/index.html#neovim",
    "title": "2023 data science rig: Set up and configuration",
    "section": "Neovim",
    "text": "Neovim\nTo be honest, there was no real reason why I chose Neovim. I just saw others using and suggesting to give it a try. I did briefly read some of the arguments for why Neovim is a good choice, though. From my shallow reading of the topic, most of the arguments I came across pertained to Neovim’s use of the lua programming language, a better plugin management experience, and some additional points that made it appealing. In fact, Neovim is considered to be an extension of Vim, rather than its own stand-alone text editor. It aims to be extensible, usable, and retain the good parts of Vim. Now, I haven’t developed a sufficient understanding of these arguments to fully articulate the benefits of using one Vim like editor from another. I just know I’m enjoying it thus far. I suggest giving it a try.\n\nInstalling Neovim\nHomebrew is used to download Neovim.\nbrew install neovim\n\n\nConfiguring Neovim\nAs mentioned in the intro to this section, Neovim’s setup and configuration can be its own series of posts; there are so many options and plugins available. The focus of the following sections is to draw attention to some of the tools I find useful when working in Neovim. Keep in mind, the configuration of Neovim is a bit of a learning curve. It can be frustrating when you first start, but very rewarding at times. You can review my configuration files here.\nMy Neovim setup is based on chris@machine’s Neovim from Scratch YouTube tutorial series. This series does an excellent job overviewing a complete Neovim setup using the Lua programming language. While my setup is mostly based on the one described in this series, I have added some custom configuration for my specific workflow.\n\n\nNeovim package manager\nI use packer for plugin management. Packer simplifies plugin installation. For example, here is the Lua code to install some plugins I highlight in the following sections:\nreturn packer.startup(function(use)\n  use \"wbthomason/packer.nvim\"   -- Have packer manage itself\n  use \"jalvesaq/Nvim-R\"          -- Tools to work with R in nvim\n\n  -- Colorschemes\n  use \"lunarvim/colorschemes\"     -- A selection of various colorschemes\n  use \"tomasiser/vim-code-dark\"\n  use \"EdenEast/nightfox.nvim\"\n  use \"folke/tokyonight.nvim\"\n\n  -- LSP \n  use \"neovim/nvim-lspconfig\"         -- enable LSP\n  use \"williamboman/mason.nvim\"\n  use \"williamboman/mason-lspconfig.nvim\"\n\n\n  -- Telescope\n  use \"nvim-telescope/telescope.nvim\"\n\n  -- Treesitter\n  use {\n    \"nvim-treesitter/nvim-treesitter\",\n    run = \":TSUpdate\",\n  } \n\n  -- Git \n  use \"lewis6991/gitsigns.nvim\"\n  use \"tpope/vim-fugitive\"\n\n  if PACKER_BOOTSTRAP then\n    require(\"packer\").sync()\n  end\nend)\nThis code might not make much sense, as I only included a snippet of the code needed to install plugins I use most often. It’s mainly intended to show with a few lines of code, packer can manage all the plugin installation steps. This example code deviates slightly from the original packer docs on how to install plugins. Check out the previously linked docs if you would like an alternative setup while using Packer.\nHere is a link to a file with all the plugins I use in my setup. Admittedly, some plugins are carry overs from chris@machine’s YouTube series, and I will fully admit I’m still learning the reason why some of these plugins are present within my configuration. Thus, my setup is not as lean as I would like it to be. But hey, I’m still learning.\n\n\nNeovim plugins\n\nNvim-R\nSince I mostly work with R, I use Nvim-R to write code and interact with the R console directly in Neovim. Nvim-R provides utilities to have the Vim experience, while also affording interactive analysis right at your fingertips. Here is what a session using Nvim-R looks like:\n\n\n\nNvim-r running in Neovim\n\n\nThe power of Nvim-R comes from its predefined keybindings keybindings, which allow you to quickly and easily do interactive analysis tasks using just a few keystrokes. I’ve found it’s the best option to work with R in Neovim. A whole blog post could be written about the use of Nvim-R, and I only hit the highlights here. I highly suggest checking it out if you’re looking to write R code with Neovim.\n\n\nvim-devtools-plugin\nAs mentioned above, I use devtools for package development. To leverage its functionality in Neovim, I use the vim-devtools-plugin. This plugin provides several convenient commands to run different devtools functions. This is especially useful as you can configure keymaps to these commands for added convenience and speed.\n\n\nTelescope\nFind, filter, preview, and pick. Telescope is great at these actions. Specifically, Telescope is a fuzzy file finder. However, it provides additional features that go beyond just working with a project’s files. I’m attempting to use it more and more in my workflow, as I mostly use it to find and navigate to specific files. However, I’ve begun to explore more of its functionality and integration with git.\n\n\nvim-fugitive\nDo yourself a favor, use vim-fugitive. Fugitive is a plugin that helps you work with Git while working in Neovim. In the past, my git and GitHub workflow was mainly done from the command line. However, jumping in and out of Neovim back to run this workflow became old quickly. To solve this, Fugitive provides the :Git or :G command to call git commands directly from the editor. Also, since I use Neovim as my editor for commit messages, I’m able to directly compose them without having to leave my current Neovim session.\n\n\nLSP\nNeovim supports the Language Server Protocol (LSP). LSP provides many different features. This includes go-to-definition (a great feature that speeds up editing), find references, hover, completion, and many other types of functionality. Most IDEs have LSP set up out-of-the-box. This is done so you can get started quickly working with any language without too much configuration.\nNeovim does provide an LSP client, but you’ll have to set up the individual servers for each language you would like to work with. This sounds harder then it is, but it does take a few steps to complete. A good rundown can be found in this video here, which is from the Neovim series I linked above. I recently made the switch over to the Mason plugin, which makes LSP server management so much simpler. I would suggest checking it out if you’re intending to work with other languages in Neovim.\nI’m still learning about LSP and how to set it up. I highly suggest reading up on the docs and reviewing other’s setups rather than relying solely on mine. Mine is still a work in progress.\n\n\nnvim-treesitter\nSetting up syntax highlighting is another important step when setting up Neovim. I use nvim-treesitter to improve the syntax highlighting within Neovim. This is another advanced topic I’m still learning about, so I just use a basic setup. You can read more about it in the plugin docs linked earlier.\n\n\nCustom Neovim keymaps\nSince Neovim is all about customization of your development environment, one thing to modify is Neovim’s keymaps. To configure, you just have to define the configuration in your Neovim config files. For example, the following is some code I use to customize my keymap setup.\n-- BigQuery keymappings \nkeymap(\"n\", \"&lt;C-b&gt;\", \":w | :! bq query &lt; % --format pretty &lt;CR&gt;\", opts)\n\n-- R coding keymappings\nkeymap(\"n\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"i\", \"\\\\M\", \"|&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;ts\", \":RStop&lt;cr&gt;\", opts)\nkeymap(\"n\", \"tt\", \"&lt;Esc&gt;&lt;C-w&gt;&lt;C-w&gt;i\", opts)\n\n-- R devtools keymappings\nkeymap(\"n\", \"&lt;leader&gt;I\", \":RInstallPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;L\", \":RLoadPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;B\", \":RBuildPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;E\", \":RCheckPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;T\", \":RTestPackage&lt;Esc&gt;\", opts)\nkeymap(\"n\", \"&lt;leader&gt;D\", \":RDocumentPackage&lt;Esc&gt;\", opts)\nI have additional custom keymappings in my setup, but including the entire file would be too much for this post. Nevertheless, you can access my keymapping configuration files to get a sense of other keymaps I have within my setup."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html",
    "title": "Implementing a next and back button in Shiny",
    "section": "",
    "text": "Photo by John Barkiple"
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "title": "Implementing a next and back button in Shiny",
    "section": "The initial runtime variables",
    "text": "The initial runtime variables\nAs part of my testing of the actionButton() UI function, I found out the initial value being sent to the server was zero. I also found out that zero can’t be used for subsetting (i.e, nothing is gets returned to the UI). To address this issue, a variable with a reactive value of one needed to be in the environment upon runtime of the application. This is so we can use the initial value of one to return the first element of our data to our output$series in the textOutput() function in the UI when the application starts. Let’s take a look at this in action.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nYou’ll notice a new function here in the server, reactiveVal(). According to the documentation, this function is used to create a “reactive value” object within the app’s environment. Basically, I understand this function is just creating a reactive expression where the initial value is one upon the runtime of the application, which is then used in the subsetting operation applied in the renderText() function. Great, we have partly solved the indexing issue with the use of reactiveVal(1). You’ll also notice the buttons don’t work here because there is no dependency on them as an input, but I’ll get to that here shortly by applying some observeEvents() functions.\n\nThe maximum index value\nI also needed a solution to help limit the range of values that could be used for indexing in our subsetting operation. I now had the lower value one available in the environment, however I did not have the maximum value. At this point, I needed a function to calculate the length of the data and to treat it as a reactive expression, as this number might be dynamic in the larger application, and the users’ inputs will determine what data gets displayed within the application (e.g., filtering by product code selection). We can easily calculate the length of our data using the length() function and making this a reactive expression by wrapping it with the reactive() function. Here is what this looks like with code.\n\nlibrary(shiny)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Determine the upper part of the subset index range\n  max_no_values &lt;- reactive(length(series))\n  \n  # Create a reactive value of 1 in the environment\n  place &lt;- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series &lt;- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIt’s challenging to show this value in the environment in writing, but now given the current code, I have the lower value of the range, one, and the maximum value three corresponding to the number of values in our data structure available in the environment. This is great, so now I have those two values available to help with subsetting. At this point, we also need to incorporate the two user inputs, the Back and Next buttons. However, since we know these two buttons increment by one every time they are pressed, I need to rely on some mathematical operations to control the range of values used to subset the data. Given the simplified application, I know 1, 2, or 3 is the values and range of values I need to properly apply within a subsetting operation."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#enter-the",
    "title": "Implementing a next and back button in Shiny",
    "section": "Enter the %%",
    "text": "Enter the %%\nPart of getting this functionality to work required the use of the modulus %% and modular arithmetic. Basically, modulus is an arithmetic operation that performs a division and returns the remainder from the operation. I learned a lot about this in this article here (Busbee and Braunschweig n.d.). The R for Data Science book (Wickham and Grolemund 2017) also introduces the use of %% as well. While researching the modulus, I found many useful applications for it within programming. It’s definitely worth some more time learning of its other uses. When applied in our case, though, we needed it to keep the subsetting index within the bounds of the size of our data structure.\nI am far from a mathematician, so the following explanation of the logic behind how a modulus is applied here is going to be a little fast and loose. However, I’m going to take a crack at it. Take for example our application. On runtime, we have a reactive value place() that starts at the value one. We also know that our maximum number of values that can be used as an index for our subsetting operation is three, our max_no_values reactive (i.e., c(\"a\", \"b\", \"c\")). We can now use the modulus with these two values to limit the number we are using in the index of our subsetting based on the number of clicks by the user. Here is a simplified example using code illustrating this point.\n\nmax_no_values &lt;- 3\n\n# User clicks the button to increment the index of the subset\n# Vector corresponds to the value outputted by the `actionButton()`\nuser_clicks &lt;- c(0:12)\n\nuser_clicks %% max_no_values\n\n [1] 0 1 2 0 1 2 0 1 2 0 1 2 0\n\n\nEarlier in the post, we found out that we can’t use zero to subset, as nothing gets returned. So to solve our issue, we need to shift these values by adding one to the vector. Notice how that with every ‘click’ the range of these values never goes below one or exceeds three, even when a user’s click count (keep in mind every click of the actionButton() increments by one) goes above three. This is the power of the %%, as this operation keeps our index range between 1 - 3, regardless of how many times the user clicks an action button.\n\nuser_clicks %% max_no_values + 1\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3 1\n\n\nThe math is a little different for the Back button, though. However, the same principles apply.\n\n((user_clicks - 2) %% 3) + 1\n\n [1] 2 3 1 2 3 1 2 3 1 2 3 1 2\n\n\nLet’s use some print debugging here to show how the of %% works in action. I’m going to use the glue package to help make the messages sent to the console more human readable.\n\nlibrary(shiny)\nlibrary(glue)\n\nseries &lt;- c(\"a\", \"b\", \"c\")\n\nui &lt;- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver &lt;- function(input, output, session) {\n \n  # Determine the total number \n  max_no_values &lt;- reactive(length(series))\n  \n  position &lt;- reactiveVal(1)\n  \n  # These cause a side-effect by changing the place value\n  observeEvent(input$forward, {\n    position((position() %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  observeEvent(input$back, {\n    position(((position() - 2) %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  output$series &lt;- renderText({\n    series[position()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIf you click the Back and Next button and watch your console, you’ll see the position value for every click being printed. While clicking these values, you will observe a couple of things:\n\nYou’ll notice the value zero is never passed as a subsetting index value.\nThe arithmetic operations constrain our subsetting values within a range of 1 - 3, the length of our character vector.\nMultiple clicks remain in order, regardless if the user clicks the Next or Back buttons (e.g., 1, 2, 3 or 3, 2, 1).\n\nAt this point, we can get rid of our print debugging code, test our working example, and bask in our accomplishment of understanding how this solution works. The next step is to now integrate what we know into the larger application. We’ll do that here in the next section of this post."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#product-selection",
    "title": "Implementing a next and back button in Shiny",
    "section": "Product selection",
    "text": "Product selection\nAs part of the original functionality of the app, users were given a selectInput() in the UI to filter for injuries that were the result of different products. The requirements stated the outputted narratives also needed to reflect the users’ filter selection. This functionality needed to be added back in, and it also needed to be reactive. I do this by adding the selected &lt;- reactive(injuries %&gt;% filter(prod_code == input$code)) near the beginning portion of the server section of the code. You’ll also notice we are using the filter() function and %&gt;% operator here, so we need to also bring in the dplyr package (i.e., library(dplyr)).\nThere are now two areas in the server that have a dependency on the selected() reactive expression, the max_no_stories() reactive and our output$narrative object. Since our reprex was using a simplified vector of data (e.g., c(\"a\", \"b\", \"c\")), we need to modify the code to use these reactives. The biggest change is we are now passing a tibble of data rather than a character vector of data. As such, I need to use selected()$narrative to refer to the narrative vectors we want to use in our server function. Nothing else really changes, as the underlying process of determining the range of values and using a mathematical operation to limit the indexing stays the same. We are just now applying this process to a different set of data, although it is technically a reactive expression rather than an object in our environment."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "href": "blog/posts/2021-09-12-shiny-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "title": "Implementing a next and back button in Shiny",
    "section": "Cases where users select a new product code",
    "text": "Cases where users select a new product code\nGiven the functionality provided within our application, it’s reasonable to expect users would change the product code (i.e., the main purpose is to give users tools to explore the data). It’s also reasonable that the user would then expect the narrative values to change based on their product selection, and indeed we have built this functionality into the app. However, what we didn’t account for yet was what users expectations are for the order to which the new filter data will be presented. When users make a change to their filtering criteria, they would most likely expect that the updated narrative data would start at the beginning, not where their previous clicks would place them within their previously selected data. Given this expectation, I now need some code to ‘reset’ the subsetting index when a user changes their product code filter.\nWhy might this be important? Take for example if the aim of this functionality was to output the most recent injury reported for a specific product code. Our user would expect that any time they switch their product code filtering input, the displayed narrative would be the most recent reported injury, and that each subsequent click would result in a chronological walk through the narratives, either forwards or backwards. This would especially be important if the app was connected to a streaming data source that isn’t static. Moreover, you might even modify the output$narrtive object to include the date, so the user is informed on when a specific injury was treated. For the sake of keeping things simple though, we will only add the reset behavior to the app in this post.\nThis reset of the indexing value was provided in the solutions guide referenced above, and it adds another observeEvent() to make this work. Specifically, it directed me to add this code to the server section of the application:\n\nobserveEvent(input$code, {\n    place(1)\n  })\n\nHere you can see that the observeEvent() is waiting for any changes to the input$code input. When a change occurs to this input, the place(1) is run, and the subsetting index is set back to one. We now have included functionality to the app where when the user changes the product code filtering, the narrative increment index will display the first value in that subset of injuries as selected by the user."
  },
  {
    "objectID": "now-old.html",
    "href": "now-old.html",
    "title": "Past Now Page Updates",
    "section": "",
    "text": "2024-01-15 update\n\nIt’s a Python summer\nThis summer I’ve been focusing on developing my Python programming skills. I have a pretty good handle of R, so I felt it was time to learn Python. This started with learning Python’s basic data types, struggling through understanding Numpy arrays, getting a handle on the extensive use cases of the pandas library, and learning how to manage environments using conda. I’m aiming to write more blog posts focused in this space to document what I’m learning.\n\n\n\nPre 2024-01-15 updates\n\nProject Conduit\nCurrently developing and maintaining a data pipeline project built using R and Python. Technology utilized includes Google Cloud resources, Docker, Apache Airflow, Google BigQuery, Google Analytics, Google Data Studio, and Shiny. The goal is to centralize and automate data processing, storage, and reporting.\n\n\nR for Data Science Online Learning Community book club facilitator\nRecently started facilitating an R for Data Science Online Learning Community online book club (check it out by joining the Slack workspace). This group is currently reading through Hadley Wickham’s Advanced R book. The group meets weekly online over Zoom. Meetings are open to anyone who is a part of the Slack group (Join the #book_club-advr channel to keep up with the book club). Check out the playlist of past meeting recordings here. I would love for more to join and be a part of this group.\n\n\nExperimenting with Neovim\nI’ve been experimenting more and more with Neovim for my development work. I’m becoming more comfortable with the different modes, movements, actions, and various tools for editing text and code. Still struggling through the configuration and plugin ecosystem to set up workflows that are the most productive. Going through this process has been a challenge, but has me really reflecting on how I approach my work, evaluating what is needed, not needed, and focusing on the bad habits I need to break."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "I’m a media research analyst, data enthusiast, and news, sports, and podcast aficianado.\nProfessionally, I use data, audience measurement, and marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. I am particularly interested in the use and development of open-source statistical software (i.e. R) to achieve this goal, and gaining a broader understanding of the role these tools play in media, digital, and marketing analytics. I also adjunct university courses on the side.\nListening to NPR, watching PBS (especially NOVA), and college football and baseball are my jam.\n\n\nWant to know more about what I’m currently working on, reading, or mastering? Check out the now page.\n\n\n\n\nPh.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota\n\n\n\n\n\nDigital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "index.html#now",
    "href": "index.html#now",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Want to know more about what I’m currently working on, reading, or mastering? Check out the now page."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Ph.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "Digital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "",
    "text": "Image generated using the prompt “volleyball analytics in a pop art style” with the Bing Image Creator\nThe initial rounds of the NCAA women’s volleyball tournament have just begun. As such, I felt it was a good opportunity to understand more about the game while learning to specify models using Big Ten women’s volleyball match data and the tidymodels framework. This post sought to specify a predictive model of wins and loses. It then used this model to explore and predict match outcomes of the #1 team going into the tournament, the Nebraska Cornhuskers.\nThis post overviews the use of the tidymodels framework to fit and train predictive models. Specifically, it aims to be an introductory tutorial on the use of tidymodels to split data into test and training sets, specify a model, and assess model fit using both the training and testing data. To do this, I explored the fit of two binary classification models to NCAA Big Ten women’s volleyball match data, with the goal to predict wins and loses.\nBeing a high-level overview, this post will not cover topics like feature engineering, resampling techniques, hyperparameter tuning, or ensemble methods. Most assuredly, additional modeling procedures would lead to improved model predictions. As such, I plan to write future posts overviewing these topics.\nLet’s attach the libraries we’ll need for the session.\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(here)\nlibrary(glue)\nlibrary(rpart.plot)\nlibrary(patchwork)\ntidymodels_prefer()"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#feature-exploration",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#feature-exploration",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Feature exploration",
    "text": "Feature exploration\nGiven the number of features in the data, we can easily obtain summary information using skimr::skim().\n\nskim(data_vball_train)\n\n\nData summary\n\n\nName\ndata_vball_train\n\n\nNumber of rows\n1243\n\n\nNumber of columns\n27\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nfactor\n1\n\n\nnumeric\n23\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nteam_name\n0\n1\n13\n25\n0\n14\n0\n\n\nopponent\n0\n1\n3\n40\n0\n329\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2021-01-22\n2023-11-25\n2022-09-09\n228\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nw_l\n0\n1\nFALSE\n2\nwin: 701, los: 542\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nset_wins\n0\n1\n2.03\n1.22\n0.00\n1.00\n3.00\n3.00\n3.00\n▃▂▁▂▇\n\n\nset_loss\n0\n1\n1.65\n1.31\n0.00\n0.00\n2.00\n3.00\n3.00\n▆▃▁▂▇\n\n\ns\n0\n1\n3.68\n0.76\n3.00\n3.00\n4.00\n4.00\n5.00\n▇▁▅▁▃\n\n\nkills\n0\n1\n47.02\n11.49\n15.00\n39.00\n46.00\n55.00\n84.00\n▁▆▇▅▁\n\n\nerrors\n0\n1\n18.86\n6.57\n3.00\n14.00\n19.00\n23.00\n44.00\n▃▇▇▂▁\n\n\ntotal_attacks\n0\n1\n125.31\n30.19\n63.00\n101.00\n121.00\n148.00\n237.00\n▃▇▆▂▁\n\n\nhit_pct\n0\n1\n0.23\n0.09\n-0.10\n0.17\n0.23\n0.30\n0.54\n▁▃▇▅▁\n\n\nassists\n0\n1\n43.23\n10.84\n15.00\n36.00\n43.00\n50.50\n75.00\n▂▆▇▃▁\n\n\naces\n0\n1\n5.18\n2.86\n0.00\n3.00\n5.00\n7.00\n18.00\n▅▇▃▁▁\n\n\nserr\n0\n1\n7.97\n3.41\n1.00\n5.00\n8.00\n10.00\n23.00\n▅▇▅▁▁\n\n\ndigs\n0\n1\n51.55\n15.14\n16.00\n40.00\n49.00\n61.00\n108.00\n▂▇▅▂▁\n\n\nblock_solos\n0\n1\n1.64\n1.62\n0.00\n1.00\n1.00\n2.00\n12.00\n▇▂▁▁▁\n\n\nblock_assists\n0\n1\n14.25\n6.78\n0.00\n10.00\n14.00\n18.00\n38.00\n▂▇▆▂▁\n\n\nopp_kills\n0\n1\n43.64\n15.01\n0.00\n34.00\n44.00\n54.00\n84.00\n▁▃▇▆▁\n\n\nopp_errors\n0\n1\n19.34\n7.05\n0.00\n15.00\n20.00\n24.00\n46.00\n▁▆▇▂▁\n\n\nopp_total_attacks\n0\n1\n121.76\n36.90\n0.00\n100.00\n119.00\n148.00\n237.00\n▁▂▇▅▁\n\n\nopp_hit_pct\n0\n1\n0.19\n0.10\n-0.14\n0.13\n0.20\n0.25\n0.50\n▁▃▇▅▁\n\n\nopp_assists\n0\n1\n40.20\n13.99\n0.00\n31.00\n41.00\n50.00\n75.00\n▁▃▇▆▁\n\n\nopp_aces\n0\n1\n4.56\n2.91\n0.00\n2.00\n4.00\n6.00\n14.00\n▆▇▆▂▁\n\n\nopp_serr\n0\n1\n7.58\n3.66\n0.00\n5.00\n7.00\n10.00\n23.00\n▃▇▃▁▁\n\n\nopp_digs\n0\n1\n48.99\n17.90\n0.00\n38.00\n48.00\n60.00\n108.00\n▁▆▇▃▁\n\n\nopp_block_solos\n0\n1\n1.43\n1.44\n0.00\n0.00\n1.00\n2.00\n12.00\n▇▂▁▁▁\n\n\nopp_block_assists\n0\n1\n12.71\n7.30\n0.00\n8.00\n12.00\n18.00\n40.00\n▆▇▃▁▁\n\n\n\n\n\nA few things to note from the initial exploratory data analysis:\n\nTeam errors, attacks, and digs distribution exhibits a slight right skew.\nAces, service errors, block solos, opponent aces, opponent errors, opponent block solos, and opponent block assists exhibit a greater degree of skewness to the right.\n\nAn argument could be made for further exploratory analysis of these variables, followed by some feature engineering. Although this additional work may improve our final predictive model, this post is a general overview of specifying, fitting, and assessing models using the tidymodels framework. I will thus not address these topics further. However, I intend to write a future post focusing on feature engineering using tidymodels’ recipes package."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#examine-correlations-among-features",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#examine-correlations-among-features",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Examine correlations among features",
    "text": "Examine correlations among features\nThe next step in the exploratory analysis is to identify the presence of any correlations among features. This can easily be done using functions from the corrr package. Specifically, the correlate() function calculates correlations among the various numeric features within our data. The output from the correlate() function is then passed to the autoplot() method, which outputs a visualization of the correlations values.\n\ndata_vball_train |&gt; \n  correlate() |&gt;\n  corrr::focus(-set_wins, -set_loss, -s, mirror = TRUE) |&gt;\n  autoplot(triangular = \"lower\")\n\nNon-numeric variables removed from input: `date`, `team_name`, `opponent`, and `w_l`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\nThe plot indicates correlations of varying degrees among features. Feature engineering and feature reduction approaches could be used to address these correlations. However, these approaches will not be explored in this post."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#specify-our-models",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#specify-our-models",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Specify our models",
    "text": "Specify our models\nTo keep things simple, I’ll explore the fit of two models to the training data. However, tidymodels has interfaces to fit a wide-range of models, many of which are implemented via the parsnip package.\nThe models I intend to fit to our data include:\n\nA logistic regression using glm.\nA decision tree using rpart.\n\nWhen specifying a model with tidymodels, we do three things:\n\nUse parsnip functions to specify the mathematical structure of the model we intend to use (e.g., logistic_reg(); decision_tree()).\nSpecify the engine we want to use to fit our model. This is done using the set_engine() function.\nWhen required, we declare the mode of the model (i.e., is it regression or classification). Some models can perform both, so we need to explicitly set the mode with the set_mode() function.\n\nSpecifying the two models in this post looks like this:\n\n# Logistic regression specification\nlog_reg_spec &lt;- \n  logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n# Decision tree specification\ndt_spec &lt;- \n  decision_tree() |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n\nLet’s take a moment to breakdown what’s going on here. The calls to logistic_regression() and decision_tree() establishes the mathematical structure we want to use to fit our model to the data. set_engine(\"glm\") and set_engine(\"rpart\") specifies the model’s engine, i.e., the software we want to use to fit our model. For our decision tree, since it can perform both regression and classification, we specify it’s mode using set_mode(\"classification\"). You’ll notice our logistic regression specification excludes this function. This is because logistic regression is only used to perform classification, thus we don’t need to set its mode.\nIf you’re curious or want more information on what parsnip is doing in the background, you can pipe the model specification object to the translate() function. Here’s what the output looks like for our decision tree specification:\n\ndt_spec |&gt; translate()\n\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\nModel fit template:\nrpart::rpart(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\n\nIf you’re interested in viewing the types of engines available for your model, you can use parsnip’s show_engines() function. Here you’ll need to pass a string character of the model function you want to explore as an argument. This is what this looks like for logistic_reg():\n\nshow_engines(\"logistic_reg\")\n\n# A tibble: 7 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 glm       classification\n2 glmnet    classification\n3 LiblineaR classification\n4 spark     classification\n5 keras     classification\n6 stan      classification\n7 brulee    classification"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#create-workflows",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#create-workflows",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Create workflows",
    "text": "Create workflows\nFrom here, we’ll create workflow objects using tidymodel’s workflow package. Workflow objects make it easier to work with different modeling objects by combining objects into one object. Although this isn’t too important for our current modeling task, the use of workflows will be beneficial later when we attempt to improve upon our models, like I’ll do in future posts.\nIn this case, our model specification and model formula are combined into a workflow object. Here I just choose a few features to include within the model. For this post, I mainly focused on using team oriented features within our model to predict wins and losses. Indeed, others could have been included, as the data also contained opponent oriented statistics. To keep things simple, however, I chose to only include the following features within our model:\n\nHitting percentage\nErrors\nBlock solos\nBlock assists\nDigs\n\nThe workflow() function sets up the beginning of our workflow object. We’ll add the model object with add_model(), followed by the formula object using add_formula().\n\nlog_reg_wflow &lt;-\n  workflow() |&gt;\n  add_model(log_reg_spec) |&gt;\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n\ndt_wflow &lt;- \n  workflow() |&gt;\n  add_model(dt_spec) |&gt;\n  add_formula(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs\n  )\n\nThis syntax can be a bit long, so there’s a shortcut. We can pass both the model formula and the model specification as arguments to the workflow() function instead of using a piped chain of functions.\n\nlog_reg_wflow &lt;- \n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    log_reg_spec\n  )\n\ndt_wflow &lt;-\n  workflow(\n    w_l ~ hit_pct + errors + block_solos + block_assists + digs,\n    dt_spec\n  )"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#fit-our-models",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#fit-our-models",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Fit our models",
    "text": "Fit our models\nNow with our models specified, we can go about fitting our model to the training data using the fit() method. We do the following to fit both models to the training data:\n\nlog_reg_fit &lt;- log_reg_wflow |&gt; fit(data = data_vball_train)\n\ndt_fit &lt;- dt_wflow |&gt; fit(data = data_vball_train)\n\nLet’s take a look at the log_reg_fit and dt_fit fit objects.\n\nlog_reg_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────────────────────────\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n── Model ───────────────────────────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:      1703 \nResidual Deviance: 874.4    AIC: 886.4\n\n\n\ndt_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────────────────────────\nw_l ~ hit_pct + errors + block_solos + block_assists + digs\n\n── Model ───────────────────────────────────────────────────────────────────────────────────────────\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct&lt; 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists&lt; 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists&gt;=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct&lt; 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct&gt;=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs&lt; 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs&gt;=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct&gt;=0.2255 642  81 win (0.1261682 0.8738318) *\n\n\nWhen the fit objects are called, tidymodels prints information about our fitted models to the console. First, we get notified this object is a trained workflow. Second, preprocessing information is included. Since we only set a model function during preprocessing, we only see the model formula printed in this section. Lastly, tidymodels outputs model specific information and summary information about the model fit.\n\nExplore the fit\nNow that we have the fit object, we can obtain more information about the fit using the extract_fit_engine() function.\n\nlog_reg_fit |&gt; extract_fit_engine()\n\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n  (Intercept)        hit_pct         errors    block_solos  block_assists           digs  \n     -8.54857       30.05361       -0.01277        0.20006        0.08764        0.01375  \n\nDegrees of Freedom: 1242 Total (i.e. Null);  1237 Residual\nNull Deviance:      1703 \nResidual Deviance: 874.4    AIC: 886.4\n\n\n\ndt_fit |&gt; extract_fit_engine() \n\nn= 1243 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 1243 542 win (0.4360418 0.5639582)  \n   2) hit_pct&lt; 0.2255 601 140 loss (0.7670549 0.2329451)  \n     4) block_assists&lt; 17.5 429  56 loss (0.8694639 0.1305361) *\n     5) block_assists&gt;=17.5 172  84 loss (0.5116279 0.4883721)  \n      10) hit_pct&lt; 0.1355 32   4 loss (0.8750000 0.1250000) *\n      11) hit_pct&gt;=0.1355 140  60 win (0.4285714 0.5714286)  \n        22) digs&lt; 45.5 14   4 loss (0.7142857 0.2857143) *\n        23) digs&gt;=45.5 126  50 win (0.3968254 0.6031746) *\n   3) hit_pct&gt;=0.2255 642  81 win (0.1261682 0.8738318) *\n\n\nThe output when passing the fit object to the extract_fit_engine() is similar to what was printed when we called the fit object alone. However, the extract_* family of workflow functions are great for extracting elements of a workflow. According to the docs (?extract_fit_engine), this family of functions are helpful when accessing elements within the fit object. This is especially helpful when needing to pass along elements of the fit object to generics like print(), summary(), and plot().\n\n# Not evaluated to conserve space, but I encourage\n# you to run it on your own\nlog_reg_fit |&gt; extract_fit_engine() |&gt; plot()\n\nAlthough extract_* functions afford convenience, the docs warn to avoid situations where you invoke a predict() method on the extracted object. Specifically, the docs state:\n\nThere may be preprocessing operations that workflows has executed on the data prior to giving it to the model. Bypassing these can lead to errors or silently generating incorrect predictions.\n\nIn other words,\n\n# BAD, NO NO\nlog_reg_fit |&gt; extract_fit_engine() |&gt; predict(new_data)\n\n# Good\nlog_reg_fit |&gt; predict(new_data)\n\nThe fit object can also be passed to other generics, like broom::tidy(). The general tidy() method, when passed a fit object, is useful to view and use the coefficients table from the logistic regression model.\n\ntidy(log_reg_fit)\n\n# A tibble: 6 × 5\n  term          estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -8.55     0.769     -11.1   1.10e-28\n2 hit_pct        30.1      2.09       14.4   5.50e-47\n3 errors         -0.0128   0.0212     -0.603 5.46e- 1\n4 block_solos     0.200    0.0515      3.89  1.02e- 4\n5 block_assists   0.0876   0.0137      6.39  1.64e-10\n6 digs            0.0137   0.00703     1.96  5.05e- 2\n\n\nBeyond summarizing the model with the coefficients table, we can also create some plots from the model’s predictions from the training data. Here we need to use the augment() function. Later, we’ll explore this function in more depth when we calculate assessment metrics. For now, I’m using it to obtain the prediction estimates for winning.\n\ndata_vball_aug &lt;- augment(log_reg_fit, data_vball_train)\n\nWith this data, we can visualize these prediction estimates with the various features used within the model. Since we’re creating several visualizations using similar code, I created a plot_log_mdl() function to simplify the plotting. Lastly, I used the patchwork package to combine the plots into one visualization. Below is the code to create these visualizations.\n\nplot_log_mdl &lt;- function(data, x_var, y_var, color) {\n  ggplot() +\n    geom_point(\n      data = data_vball_aug, \n      aes(x = {{ x_var }}, y = {{ y_var }}, color = {{ color }}),\n      alpha = .4\n    ) +\n    geom_smooth(\n      data = data_vball_aug,\n      aes(x = {{ x_var }}, y = {{ y_var }}),\n      method = \"glm\", \n      method.args = list(family = \"binomial\"),\n      se = FALSE\n    ) +\n    labs(color = \"\") +\n    theme_minimal()\n}\n\n\nplot_hit_pct &lt;- \n  plot_log_mdl(data_vball_aug, hit_pct, .pred_win, w_l)\n\nplot_errors &lt;- \n  plot_log_mdl(data_vball_aug, errors, .pred_win, w_l)\n\nplot_block_solos &lt;- \n  plot_log_mdl(data_vball_aug, block_solos, .pred_win, w_l) +\n  scale_x_continuous(labels = label_number(accuracy = 1))   \n\nplot_block_assists &lt;- \n  plot_log_mdl(data_vball_aug, block_assists, .pred_win, w_l)\n\nplot_digs &lt;-\n  plot_log_mdl(data_vball_aug, digs, .pred_win, w_l)\n\nwrap_plots(\n  plot_hit_pct, \n  plot_errors,\n  plot_block_solos, \n  plot_block_assists,\n  plot_digs, \n  guides = \"collect\"\n) & theme(legend.position = \"bottom\")\n\n\n\n\nTo summarise our decision tree, we need to use the rpart.plot package to create a plot of the tree. The code to do this looks like this:\n\ndt_fit |&gt;\n  extract_fit_engine() |&gt;\n  rpart.plot(roundint = FALSE)\n\n\n\n\nBefore transitioning to model assessment, let’s explore the predictions for both models using the augment() function again. According to the docs,\n\nAugment accepts a model object and a dataset and adds information about each observation in the dataset.\n\naugment() produces new columns from the original data set to which makes it easy to examine model predictions. For instance, we can create a data set with the .pred_class, .pred_win, and .pred_loss columns. augment() also makes a guarantee that a tibble with the same number of rows as the passed data set will be returned, and all new column names will be prefixed with a ..\nHere we’ll pipe the tibble returned from augment() to the relocate() function. This will make it easier to view the variables we are interested in further examining by moving these columns to the left of the tibble.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n\n# A tibble: 1,243 × 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 loss  loss          0.0616       0.938 2021-01-29 Illinois Figh… Wiscons…        0        3     3\n 2 loss  loss          0.297        0.703 2021-01-30 Illinois Figh… Wiscons…        1        3     4\n 3 loss  loss          0.138        0.862 2021-02-06 Illinois Figh… @ Penn …        2        3     5\n 4 loss  loss          0.0572       0.943 2021-02-19 Illinois Figh… Ohio St.        1        3     4\n 5 loss  loss          0.199        0.801 2021-02-20 Illinois Figh… Ohio St.        2        3     5\n 6 loss  loss          0.0787       0.921 2021-03-05 Illinois Figh… Nebraska        0        3     3\n 7 loss  loss          0.0281       0.972 2021-03-06 Illinois Figh… Nebraska        0        3     3\n 8 loss  win           0.636        0.364 2021-03-12 Illinois Figh… @ Minne…        2        3     5\n 9 loss  loss          0.00129      0.999 2021-03-13 Illinois Figh… @ Minne…        0        3     3\n10 loss  loss          0.0114       0.989 2021-04-02 Illinois Figh… @ Purdue        0        3     3\n# ℹ 1,233 more rows\n# ℹ 20 more variables: kills &lt;dbl&gt;, errors &lt;dbl&gt;, total_attacks &lt;dbl&gt;, hit_pct &lt;dbl&gt;,\n#   assists &lt;dbl&gt;, aces &lt;dbl&gt;, serr &lt;dbl&gt;, digs &lt;dbl&gt;, block_solos &lt;dbl&gt;, block_assists &lt;dbl&gt;,\n#   opp_kills &lt;dbl&gt;, opp_errors &lt;dbl&gt;, opp_total_attacks &lt;dbl&gt;, opp_hit_pct &lt;dbl&gt;,\n#   opp_assists &lt;dbl&gt;, opp_aces &lt;dbl&gt;, opp_serr &lt;dbl&gt;, opp_digs &lt;dbl&gt;, opp_block_solos &lt;dbl&gt;,\n#   opp_block_assists &lt;dbl&gt;\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  relocate(w_l, .pred_class, .pred_win, .pred_loss)\n\n# A tibble: 1,243 × 30\n   w_l   .pred_class .pred_win .pred_loss date       team_name      opponent set_wins set_loss     s\n   &lt;fct&gt; &lt;fct&gt;           &lt;dbl&gt;      &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1 loss  loss            0.131      0.869 2021-01-29 Illinois Figh… Wiscons…        0        3     3\n 2 loss  loss            0.131      0.869 2021-01-30 Illinois Figh… Wiscons…        1        3     4\n 3 loss  loss            0.131      0.869 2021-02-06 Illinois Figh… @ Penn …        2        3     5\n 4 loss  loss            0.131      0.869 2021-02-19 Illinois Figh… Ohio St.        1        3     4\n 5 loss  loss            0.131      0.869 2021-02-20 Illinois Figh… Ohio St.        2        3     5\n 6 loss  loss            0.131      0.869 2021-03-05 Illinois Figh… Nebraska        0        3     3\n 7 loss  loss            0.131      0.869 2021-03-06 Illinois Figh… Nebraska        0        3     3\n 8 loss  win             0.603      0.397 2021-03-12 Illinois Figh… @ Minne…        2        3     5\n 9 loss  loss            0.131      0.869 2021-03-13 Illinois Figh… @ Minne…        0        3     3\n10 loss  loss            0.131      0.869 2021-04-02 Illinois Figh… @ Purdue        0        3     3\n# ℹ 1,233 more rows\n# ℹ 20 more variables: kills &lt;dbl&gt;, errors &lt;dbl&gt;, total_attacks &lt;dbl&gt;, hit_pct &lt;dbl&gt;,\n#   assists &lt;dbl&gt;, aces &lt;dbl&gt;, serr &lt;dbl&gt;, digs &lt;dbl&gt;, block_solos &lt;dbl&gt;, block_assists &lt;dbl&gt;,\n#   opp_kills &lt;dbl&gt;, opp_errors &lt;dbl&gt;, opp_total_attacks &lt;dbl&gt;, opp_hit_pct &lt;dbl&gt;,\n#   opp_assists &lt;dbl&gt;, opp_aces &lt;dbl&gt;, opp_serr &lt;dbl&gt;, opp_digs &lt;dbl&gt;, opp_block_solos &lt;dbl&gt;,\n#   opp_block_assists &lt;dbl&gt;"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#model-assessment",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#model-assessment",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Model assessment",
    "text": "Model assessment\nSince we’re fitting a binary classification model, we will use several measurements to assess model performance. Many of these measurements can be calculated using functions from the yardstick package. To start, we can calculate several measurements using the hard class predictions: a confusion matrix; accuracy; specificity; ROC curves; etc.\n\nCreate a confusion matrix\nFirst, let’s start by creating a confusion matrix. A confusion matrix is simply a cross-tabulation of the observed and predicted classes, and it summarizes how many times the model predicted a class correctly vs. how many times it predicted it incorrectly. The calculation of the table is pretty straight forward for a binary-classification model. The yardstick package makes it easy to calculate this table with the conf_mat() function.\nconf_mat()’s two main arguments are truth and estimate. truth pertains to the column containing the true class predictions (i.e., what was actually recorded). The estimate is the name of the column containing the discrete class prediction (i.e., the prediction made by the model).\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class)\n\n          Truth\nPrediction loss win\n      loss  438  89\n      win   104 612\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class)\n\n          Truth\nPrediction loss win\n      loss  411  64\n      win   131 637\n\n\nThe conf_mat() also has an autoplot() method. This makes it easier to visualize the confusion matrix, either as a mosaic plot or a heatmap.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class) |&gt;\n  autoplot(type = \"mosaic\") \n\n\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  conf_mat(truth = w_l, estimate = .pred_class) |&gt;\n  autoplot(type = \"heatmap\")\n\n\n\n\nA few things to note from the confusion matrices created from our two models:\n\nThe logistic regression does well predicting wins and losses, though it slightly over predicts wins in cases of losses and losses in cases of wins. However, prediction accuracy is pretty balanced.\nThe decision tree does better reducing cases where it predicts a loss when a win occurred, but it predicted more wins when a loss took place. Thus, the decision tree model seems fairly optimistic when it comes to predicting wins when a loss occurred.\n\nAfter examining the confusion matrix, we can move forward with calculating some quantitative summary metrics from the results of the confusion matrix, which we can use to better compare the fit between the two models.\n\n\nMeasure model accuracy\nOne way to summarize the confusion matrix is to calculate the proportion of data that is predicted correctly, also known as accuracy. yardstick’s accuracy() function simplifies this calculation for us. Again, we just pipe our augment() function to the accuracy() function, and we specify which column is the truth and which is the estimate class prediction from the model.\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  accuracy(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.845\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  accuracy(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.843\n\n\nWhen it comes to accuracy, both models are fairly similar in their ability to predict cases correctly. The logistic regression’s accuracy is slightly better, though.\n\n\nMeasure model sensitivity and specificity\nSensitivity and specificity are additional assessment metrics we can calculate. Sensitivity in this case is the percentage of matches that were wins that were correctly identified by the model. Specificity is the percentage of matches that were losses that were correctly identified by the model. The @StatQuest YouTube channel has a good video breaking down how these metrics are calculated.\nyardstick makes it easy to calculate these metrics with the sensitivity() and specificity() functions. As we did with calculating accuracy, we pipe the output of the augment() function to the sensitivity() function. We also specify the column that represents the true values to the truth argument, then pass the class predictions made by the model to the estimate argument. This looks like the following for both our logistic regression and decision tree models:\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  sensitivity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.808\n\n\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  specificity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.873\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  sensitivity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.758\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  specificity(truth = w_l, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.909\n\n\nA few things to note:\n\nThe logistic regression (sensitivity = 80.8%) was much better at predicting matches that were wins than the decision tree model (sensitivity = 75.8%).\nThe decision tree was much better at identifying losses, though (90.9% vs. 87.3%).\n\n\nSimplify metric calculations with metric_set()\nAlthough the above code provided the output we were looking for, we can simplify our code by using yardstick’s metric_set() function. Inside metric_set() we specify the different metrics we want to calculate for each model.\n\nvball_mdl_metrics &lt;- \n  metric_set(accuracy, sensitivity, specificity)\n\nThen we do as before, pipe the output from augment() to our metric set object vball_mdl_metrics, and specify the column that represents the truth and the column that represents the model’s class prediction. Here’s what this looks like for both our models:\n\naugment(log_reg_fit, data_vball_train) |&gt;\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.845\n2 sensitivity binary         0.808\n3 specificity binary         0.873\n\n\n\naugment(dt_fit, data_vball_train) |&gt;\n  vball_mdl_metrics(truth = w_l, estimate = .pred_class)\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.843\n2 sensitivity binary         0.758\n3 specificity binary         0.909\n\n\nNow it’s much easier to make comparisons, and we write less code for the same amount of information. A big win!\n\n\n\nROC curves and AUC estimates\nReceiver operating characteristic (ROC) curves visually summarise classification model specificity and sensitivity using different threshold values. From this curve, an area under the curve (AUC) metric can be calculated. The AUC is a useful summary metric and can be used to compare the fit of two or more models. Again, @StatQuest has a pretty good video explaining the fundamentals of ROC curves and AUC estimates.\nBeing a useful way to summarise model performance, the yardstick package makes several functions available to calculate both the ROC curve and AUC metric. An autoplot() method is also available to easily plot the ROC curve for us.\nLet’s take a look at how this is done with our logistic regression model. Here’s the code:\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss)\n\n# A tibble: 1,245 × 3\n     .threshold specificity sensitivity\n          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf             0                 1\n 2    0.0000835     0                 1\n 3    0.000181      0.00143           1\n 4    0.000405      0.00285           1\n 5    0.000415      0.00428           1\n 6    0.000496      0.00571           1\n 7    0.000824      0.00713           1\n 8    0.000839      0.00856           1\n 9    0.000922      0.00999           1\n10    0.000979      0.0114            1\n# ℹ 1,235 more rows\n\n\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_auc(truth = w_l, .pred_loss)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.920\n\n\n\naugment(log_reg_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss) |&gt;\n  autoplot()\n\n\n\n\nYou’ll likely notice the syntax is pretty intuitive. You’ll also notice the code is similar to our other model performance metric calculations. First we use augment() to create the data we need. Second, we pipe the output of the augment() function to either the roc_curve() or roc_auc() function. The roc_curve() function calculates the ROC curve values and returns a tibble, which we will later pipe to the autoplot() method. The roc_auc() function calculates the area under the curve metric.\nSince we’re comparing two models, we perform these steps again for the decision tree model.\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss)\n\n# A tibble: 7 × 3\n  .threshold specificity sensitivity\n       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1   -Inf           0          1     \n2      0.126       0          1     \n3      0.397       0.800      0.851 \n4      0.714       0.909      0.758 \n5      0.869       0.914      0.740 \n6      0.875       0.994      0.0517\n7    Inf           1          0     \n\n\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_auc(truth = w_l, .pred_loss)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.864\n\n\n\naugment(dt_fit, new_data = data_vball_train) |&gt;\n  roc_curve(truth = w_l, .pred_loss) |&gt;\n  autoplot()\n\n\n\n\nA few notes from comparing the ROC curve and AUC metrics:\n\nThe AUC indicates a better model fit across different thresholds for the logistic regression model (AUC = .920) vs. the decision tree (AUC = .864).\nWhen visually examining the ROC curves for both models, it seems the logistic regression model is a better fitting model for the data."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#extract-the-final-workflow",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#extract-the-final-workflow",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Extract the final workflow",
    "text": "Extract the final workflow\nOnce the final candidate model is identified, we can extract the final workflow using the hardhat package’s extract_workflow() function. Here we’ll use this workflow object to make predictions, but this workflow object is also useful if you intend to deploy this model.\n\nfinal_fit_wflow &lt;- extract_workflow(final_log_reg_fit)"
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#make-predictions",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#make-predictions",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "Make predictions",
    "text": "Make predictions\nAt this point in the season, let’s see how the Nebraska women’s volleyball team stacked up in several of their matches using our model. First, let’s examine Nebraska’s win against Wisconsin, a five set thriller.\n\nwisc_mtch_one &lt;- data_vball |&gt; \n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-10-21\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_one)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 loss       \n\n\nAccording to our model, Nebraska should have lost this match. This makes Nebraska’s win even more impressive. The grittiness to pull out a win, even when evidence suggests they shouldn’t have, speaks volumes of this team. Indeed, wins and losses for volleyball matches are a function of many different factors. Factors that may not be fully captured by the data or this specific model.\nWhat about Nebraska’s 0-3, second match loss against Wisconsin?\n\nwisc_mtch_two &lt;- data_vball |&gt; \n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-24\"))\n\npredict(final_fit_wflow, new_data = wisc_mtch_two)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 loss       \n\n\nNo surprise, the model predicted Nebraska would lose this match. It’s a pretty steep hill to climb when you hit a .243 and only have 5 total blocks.\nAnother nail-biter was Nebraska’s second match against Penn State. Let’s take a look at what the model would predict.\n\npenn_state &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", date == as_date(\"2023-11-03\"))\n\npredict(final_fit_wflow, new_data = penn_state)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nEven though the match was close, the model predicted Nebraska would win this match. It may have been a nail-biter to watch, but Nebraska played well enough to win the match, according to our model."
  },
  {
    "objectID": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#the-ncaa-tournament-and-our-model",
    "href": "blog/posts/2023-12-07-tidymodels-bigten-volleyball/index.html#the-ncaa-tournament-and-our-model",
    "title": "Messing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data",
    "section": "The NCAA tournament and our model",
    "text": "The NCAA tournament and our model\nWe’re through the initial rounds of the 2023 NCAA women’s volleyball tournament. Let’s look at a couple of scenarios for Nebraska using our final model.\n\n\n\n\n\n\nNote\n\n\n\nI’m extrapolating a bit here, since the data I’m using only includes Big Ten volleyball team matches. The NCAA tournament will include teams from many other conferences, so the predictions don’t fully generalize to tournament matches.\nWe could avert the extrapolation here by obtaining match data for all NCAA volleyball matches for the 2021, 2022, and 2023 seasons. For the sake of keeping this post manageable, I did not obtain this data.\n\n\nFirst, let’s just say Nebraska plays to up to their regular season average for hit percentage, errors, block solos, block assists, and digs in NCAA tournament matches. What does our model predict in regards to Nebraska winning or losing a match?\n\nseason_avg &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |&gt;\n  summarise(across(where(is.numeric), mean)) |&gt;\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = season_avg)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nIf Nebraska can hit at least a .290, commit less than 17 errors, have one solo block, have 16 block assists, and dig the ball roughly 48 times, then according to the model, they should win matches. Put another way, if Nebraska performs close to their regular season average for these statistics, then the model suggests they will win matches.\nThis is very encouraging, since the Huskers should be playing their best volleyball here at the end of the season. One would hope this means they perform near or better than their average in tournament matches.\nOne last scenario, let’s look at the low end of Nebraska’s performance this season. Specifically, let’s see what the model predicts if Nebraska will win or lose a match at the 25% quartile for these statistics.\n\nquantile_25 &lt;- data_vball |&gt;\n  filter(team_name == \"Nebraska Huskers\", year(date) == 2023) |&gt;\n  summarise(across(where(is.numeric), ~quantile(.x, .25))) |&gt;\n  select(hit_pct, errors, block_solos, block_assists, digs)\n\npredict(final_fit_wflow, new_data = quantile_25)\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 win        \n\n\nAccording to the model, if Nebraska can perform up to their 25% quartile of their regular season statistics, the model suggests they should win matches. Matches like those in the NCAA tournament. So even if Nebraska doesn’t perform to their potential or just has an off match, they should win if they can at least achieve the 25% quartile of their regular season statistics.\n\n“All models are wrong, but some are useful.”\n- George Box\n\nAgain, many factors determine if a team wins or loses a match in volleyball (see the model’s prediction for Nebraska’s first match against Wisconsin). This is just one, simple model aimed at predicting wins and losses based on hit percentage, errors, block solos, block assists, and digs. A model that certainly could be improved."
  },
  {
    "objectID": "blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/index.html",
    "href": "blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/index.html",
    "title": "30 day tidymodels recipes challenge",
    "section": "",
    "text": "Photo by Nicolas Gras\n\n\n\nBackground\nBefore the holidays, I came across Emil Hvitfeldt’s #adventofsteps LinkedIn posts. Following a model popularized by advent of code–an annual tradition of online programming puzzles based on the theme of an advent calendar–these posts provided daily examples on the use of various step_* functions from the tidymodels’ recipes package. This post, with a slight spin, is inspired by these posts.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(corrr)\nlibrary(skimr)\nlibrary(janitor)\ntidymodels_prefer()\n\n\n\nMy spin on this\nOne of my personal goals this coming year is to learn and practice using the different tidymodels’ packages. To complete this goal, I thought a 30 day recipes challenge would be a good start. Each day during this 30 day personal challenge, I will focus on learning and creating some daily notes about one functionality of the recipes package. First, I start with the basics (e.g., how to create a recipe object). Then, I’ll focus on describing the various step_* functions.\nTo keep me on track, while also avoiding making this a chore, I’m going to place a 1-hour a day stopgap on studying, practicing, and documenting what I’ve learned. Depending on my schedule and motivation, I may work ahead on some material, but I will strive to update this post once a day.\nGiven the time constraint I’m imposing on myself, some of my daily notes or examples may result in an incomplete description of functionality. In cases like this, I’ll try to link to relevant documentation for you to follow up and learn more. Please be flexible with any grammar and spelling errors during this challenge, as I’ll likely edit very little until the end of the 30 days, if at all.\nSince the aim of this post is to document what I’m learning, all errors are completely mine. I highly suggest following up with the recipes package’s documentation and the Tidy Modeling with R book following a review of these notes. Both do a more thorough job overviewing the package’s functionality.\n\n\nWhat I intend to get out of this challenge\nBy the end of this challenge, I hope to have pushed myself to learn more about how to use tidymodels’s recipe package, and to create several example use cases of different functionality.\n\n\nDay 01 - Create a recipe\nFirst off, what is a recipe? According to the docs:\n\nA recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.\n\nSo, I start this personal challenge by overviewing how to create a recipe object with the recipes package. The recipe() function is used to create a recipe object.\nWhen creating a recipe, we need to consider what roles variables take. In simple modeling tasks, you’ll just have outcomes and predictors. However, variables may take on other roles (i.e., IDs). As such, the recipe() function provides multiple means for specifying the role of a variable:\n\nThe formula\nManually updating roles using the update_role() function.\n\nLet’s use the credit_data from tidymodels’ modeldata package. You can get more information about this data by running ?credit_data in your console.\n\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\n\n# Create splits for examples\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n\n# No outcome variable, `.` is a shortcut for **all** variables\ncredit_rec &lt;- recipe(~., data = credit_train)\n\n# Outcome with specific variables to be included within model\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)\n\n# Recipe uses `data` only as a template, all the data is not needed\n# Useful in cases when you're working with large data\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = head(credit_train)\n)\n\n\n# Use `update_role()` to specify variable roles\ncredit_rec_update &lt;- recipe(credit_train) |&gt;\n  update_role(Status, new_role = \"outcome\") |&gt;\n  update_role(\n    Seniority, Home, Time, Age, Marital, Records, \n    Job, Expenses, Income, Assets, Debt, Amount, \n    Price, new_role = \"predictor\"\n  )\n\ncredit_rec_update\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\n\n\nThe update_role() function is useful in cases where you might have an ID variable you don’t want to include within your model.\n\ncredit_data_id &lt;- credit_data |&gt;\n  mutate(id = 1:n(), .before = 1)\n\nset.seed(2)\ncredit_id_split &lt;- \n  initial_split(credit_data_id, prop = 0.8, strata = Status)\ncredit_id_train &lt;- training(credit_id_split)\ncredit_id_test &lt;- testing(credit_id_split)\n\n\n# Manually add an 'id' role to a variable\ncredit_id_rec &lt;- recipe(credit_id_train) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  update_role(Status, new_role = \"outcome\") |&gt;\n  update_role(\n    Seniority, Home, Time, Age, Marital, Records, \n    Job, Expenses, Income, Assets, Debt, Amount, \n    Price, new_role = \"predictor\"\n  ) \n\ncredit_id_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\nid:         1\n\n\nIn case you ever need to remove a role, you can use remove_role().\n\ncredit_no_id_rec &lt;- credit_id_rec |&gt;\n  remove_role(id, old_role = \"id\")\n\n# id will be assigned and 'undeclared' role\ncredit_no_id_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:          1\npredictor:       13\nundeclared role:  1\n\n\nEach recipe has its own summary method. We can wrap the recipe object within summary() to output more information about each variable and its assigned role.\n\n# Formula specified recipe\nsummary(credit_rec)\n\n# A tibble: 4 × 4\n  variable type      role      source  \n  &lt;chr&gt;    &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 Debt     &lt;chr [2]&gt; predictor original\n2 Income   &lt;chr [2]&gt; predictor original\n3 Assets   &lt;chr [2]&gt; predictor original\n4 Status   &lt;chr [3]&gt; outcome   original\n\n# Manually specified using `update_role()`\nsummary(credit_rec_update)\n\n# A tibble: 14 × 4\n   variable  type      role      source  \n   &lt;chr&gt;     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Status    &lt;chr [3]&gt; outcome   original\n 2 Seniority &lt;chr [2]&gt; predictor original\n 3 Home      &lt;chr [3]&gt; predictor original\n 4 Time      &lt;chr [2]&gt; predictor original\n 5 Age       &lt;chr [2]&gt; predictor original\n 6 Marital   &lt;chr [3]&gt; predictor original\n 7 Records   &lt;chr [3]&gt; predictor original\n 8 Job       &lt;chr [3]&gt; predictor original\n 9 Expenses  &lt;chr [2]&gt; predictor original\n10 Income    &lt;chr [2]&gt; predictor original\n11 Assets    &lt;chr [2]&gt; predictor original\n12 Debt      &lt;chr [2]&gt; predictor original\n13 Amount    &lt;chr [2]&gt; predictor original\n14 Price     &lt;chr [2]&gt; predictor original\n\n# Recipe with a variable holding the 'id' role\nsummary(credit_id_rec)\n\n# A tibble: 15 × 4\n   variable  type      role      source  \n   &lt;chr&gt;     &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 id        &lt;chr [2]&gt; id        original\n 2 Status    &lt;chr [3]&gt; outcome   original\n 3 Seniority &lt;chr [2]&gt; predictor original\n 4 Home      &lt;chr [3]&gt; predictor original\n 5 Time      &lt;chr [2]&gt; predictor original\n 6 Age       &lt;chr [2]&gt; predictor original\n 7 Marital   &lt;chr [3]&gt; predictor original\n 8 Records   &lt;chr [3]&gt; predictor original\n 9 Job       &lt;chr [3]&gt; predictor original\n10 Expenses  &lt;chr [2]&gt; predictor original\n11 Income    &lt;chr [2]&gt; predictor original\n12 Assets    &lt;chr [2]&gt; predictor original\n13 Debt      &lt;chr [2]&gt; predictor original\n14 Amount    &lt;chr [2]&gt; predictor original\n15 Price     &lt;chr [2]&gt; predictor original\n\n\n\n\nDay 02 - How to use prep() and bake()\nLet’s stick with the credit data for today’s examples.\n\n# Same code from day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\n\n# Create splits for our day 2 examples\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\nWe’re going to continue to use the previously specified limited model from day 01 for our examples.\n\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)\n\nNow that we know how to specify a recipe, we need to learn how to use recipes’ prep() and bake() functions. prep() calculates any intermediate values required for preprocessing. bake() applies the preprocessing steps–using any intermediate values–to our testing and training data.\nprep() and bake() can be confusing at first. However, I like the following analogy from the R4DS learning community’s Q&A with the authors of the Tidy Modeling with R book:\n\nThey’re analogous to fit() and predict() … prep() is like fitting where you’re estimating stuff and bake() is like you’re applying it.\n- Max Kuhn\n\nFor a more formal treatment, the prep() docs state:\n\nFor a recipe with at least one preprocessing operation, estimate the required parameters from a training set that can be later applied to other data sets.\n\nThe bake() docs state:\n\nFor a recipe with at least one preprocessing operation that has been trained by prep(), apply the computations to new data.\n\nWhy two separate functions? Some preprocessing steps need an intermediate calculation step to be performed before applying the recipe to the data (e.g., step_normalize() and step_center(); more on this later). To better articulate this point, I’m going to fast-forward a bit in our challenge and apply the step_center() function to our recipe. step_center() is used to center variables.\nWhen centering a variable, we need to make an intermediate calculation (i.e., prep()) before applying the calculation to perform the centering to our data (i.e., bake()).\nFor our example, say we want to center the Debt variable. To do this, we can simply add step_center(Debt) to our recipe. When we pipe the recipe object to prep(), the mean is calculated in the background to perform the preprocessing step.\n\ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n) |&gt;\n  step_center(Debt) |&gt;\n  prep() \n\nWe can see this calculated value by using the number argument in the tidy.recipe() method.\n\n# Print a summary of the recipe steps to be performed\ntidy(credit_rec)\n\n# A tibble: 1 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_lw98c\n\n# Print additional information about the first recipe step\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms value id          \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 Debt   337. center_lw98c\n\n\nTake note, though, the Debt variable has not been centered yet, and we are still working with a recipe object.\nWe then apply the centering transformation to the data by piping the prepped recipe to bake(). We can apply the preprocessing to the training data by passing the NULL to the new_data argument. bake() returns a tibble with our transformed variable using our training data.\n\ncredit_baked &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n) |&gt;\n  step_center(Debt) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_baked\n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt; &lt;fct&gt; \n 1 -337.     80      0 bad   \n 2 -337.     50      0 bad   \n 3 -337.    107      0 bad   \n 4  163.    112   2000 bad   \n 5 -337.     85   5000 bad   \n 6   NA      NA     NA bad   \n 7 -337.     90      0 bad   \n 8 -337.     71   3000 bad   \n 9 -337.    128      0 bad   \n10 -337.    100      0 bad   \n# ℹ 3,553 more rows\n\n\nMost likely, you won’t use prep() and bake() for other modeling tasks. However, they’ll be important as we continue exploring the recipes package in the coming days.\n\n\nDay 03 - Selector functions\nRemaining consistent, let’s continue using the credit_data data for some of today’s examples. We’ll also use the Chicago data set for a couple additional examples. You can read more about this data by running ?Chicago in your console.\nHere we’ll get our data and split it into training and testing for both data sets.\n\n# Same code from day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\n# For reproducibility\nset.seed(1)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8, strata = Status)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\n\ndata(Chicago, package = \"modeldata\")\nglimpse(Chicago)\n\nRows: 5,698\nColumns: 50\n$ ridership        &lt;dbl&gt; 15.732, 15.762, 15.872, 15.874, 15.423, 2.425, 1.467, 15.511, 15.927, 15.…\n$ Austin           &lt;dbl&gt; 1.463, 1.505, 1.519, 1.490, 1.496, 0.693, 0.408, 0.987, 1.551, 1.588, 1.5…\n$ Quincy_Wells     &lt;dbl&gt; 8.371, 8.351, 8.359, 7.852, 7.621, 0.911, 0.414, 4.807, 8.227, 8.246, 8.0…\n$ Belmont          &lt;dbl&gt; 4.599, 4.725, 4.684, 4.769, 4.720, 2.274, 1.631, 3.517, 4.707, 4.774, 4.8…\n$ Archer_35th      &lt;dbl&gt; 2.009, 2.088, 2.108, 2.166, 2.058, 0.624, 0.378, 1.339, 2.221, 2.227, 2.1…\n$ Oak_Park         &lt;dbl&gt; 1.421, 1.429, 1.488, 1.445, 1.415, 0.426, 0.225, 0.879, 1.457, 1.475, 1.4…\n$ Western          &lt;dbl&gt; 3.319, 3.344, 3.363, 3.359, 3.271, 1.111, 0.567, 1.937, 3.457, 3.511, 3.4…\n$ Clark_Lake       &lt;dbl&gt; 15.561, 15.720, 15.558, 15.745, 15.602, 2.413, 1.374, 9.017, 16.003, 15.8…\n$ Clinton          &lt;dbl&gt; 2.403, 2.402, 2.367, 2.415, 2.416, 0.814, 0.583, 1.501, 2.437, 2.457, 2.4…\n$ Merchandise_Mart &lt;dbl&gt; 6.481, 6.477, 6.405, 6.489, 5.798, 0.858, 0.268, 4.193, 6.378, 6.458, 6.2…\n$ Irving_Park      &lt;dbl&gt; 3.744, 3.853, 3.861, 3.843, 3.878, 1.735, 1.164, 2.903, 3.828, 3.869, 3.8…\n$ Washington_Wells &lt;dbl&gt; 7.560, 7.576, 7.620, 7.364, 7.089, 0.786, 0.298, 4.731, 7.479, 7.547, 7.2…\n$ Harlem           &lt;dbl&gt; 2.655, 2.760, 2.789, 2.812, 2.732, 1.034, 0.642, 1.958, 2.742, 2.753, 2.7…\n$ Monroe           &lt;dbl&gt; 5.672, 6.013, 5.786, 5.959, 5.769, 1.044, 0.530, 3.165, 5.935, 5.829, 5.9…\n$ Polk             &lt;dbl&gt; 2.481, 2.436, 2.526, 2.450, 2.573, 0.006, 0.000, 1.065, 2.533, 2.566, 2.4…\n$ Ashland          &lt;dbl&gt; 1.319, 1.314, 1.324, 1.350, 1.355, 0.566, 0.347, 0.852, 1.400, 1.358, 1.4…\n$ Kedzie           &lt;dbl&gt; 3.013, 3.020, 2.982, 3.013, 3.085, 1.130, 0.635, 1.969, 3.149, 3.099, 3.1…\n$ Addison          &lt;dbl&gt; 2.500, 2.570, 2.587, 2.528, 2.557, 0.800, 0.487, 1.560, 2.574, 2.618, 2.5…\n$ Jefferson_Park   &lt;dbl&gt; 6.595, 6.750, 6.967, 7.013, 6.922, 2.765, 1.856, 4.928, 6.817, 6.853, 6.8…\n$ Montrose         &lt;dbl&gt; 1.836, 1.915, 1.977, 1.979, 1.953, 0.772, 0.475, 1.325, 2.040, 2.038, 2.0…\n$ California       &lt;dbl&gt; 0.756, 0.781, 0.812, 0.776, 0.789, 0.370, 0.274, 0.473, 0.844, 0.835, 0.8…\n$ temp_min         &lt;dbl&gt; 15.1, 25.0, 19.0, 15.1, 21.0, 19.0, 15.1, 26.6, 34.0, 33.1, 23.0, 0.0, 10…\n$ temp             &lt;dbl&gt; 19.45, 30.45, 25.00, 22.45, 27.00, 24.80, 18.00, 32.00, 37.40, 34.00, 28.…\n$ temp_max         &lt;dbl&gt; 30.0, 36.0, 28.9, 27.0, 32.0, 30.0, 28.9, 41.0, 43.0, 36.0, 33.1, 21.2, 3…\n$ temp_change      &lt;dbl&gt; 14.9, 11.0, 9.9, 11.9, 11.0, 11.0, 13.8, 14.4, 9.0, 2.9, 10.1, 21.2, 20.0…\n$ dew              &lt;dbl&gt; 13.45, 25.00, 18.00, 10.90, 21.90, 15.10, 10.90, 30.20, 35.60, 30.90, 21.…\n$ humidity         &lt;dbl&gt; 78.0, 79.0, 81.0, 66.5, 84.0, 71.0, 74.0, 93.0, 93.0, 89.0, 80.0, 66.5, 7…\n$ pressure         &lt;dbl&gt; 30.430, 30.190, 30.160, 30.440, 29.910, 30.280, 30.330, 30.040, 29.400, 2…\n$ pressure_change  &lt;dbl&gt; 0.12, 0.18, 0.23, 0.16, 0.65, 0.49, 0.10, 0.78, 0.16, 0.48, 0.23, 0.28, 0…\n$ wind             &lt;dbl&gt; 5.20, 8.10, 10.40, 9.80, 12.70, 12.70, 8.10, 8.10, 9.20, 11.50, 11.50, 12…\n$ wind_max         &lt;dbl&gt; 10.4, 11.5, 19.6, 16.1, 19.6, 17.3, 13.8, 17.3, 23.0, 16.1, 16.1, 19.6, 1…\n$ gust             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ gust_max         &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 25.3, 26.5, 0.0, 26.5, 31.1, 0.0, 0.0, 23.0, 26.5, 0.…\n$ percip           &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ percip_max       &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.07, 0.11, 0.01, 0.00, 0.00, 0…\n$ weather_rain     &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ weather_snow     &lt;dbl&gt; 0.00000000, 0.00000000, 0.21428571, 0.00000000, 0.51612903, 0.04000000, 0…\n$ weather_cloud    &lt;dbl&gt; 0.7083333, 1.0000000, 0.3571429, 0.2916667, 0.4516129, 0.6400000, 0.52000…\n$ weather_storm    &lt;dbl&gt; 0.00000000, 0.20833333, 0.07142857, 0.04166667, 0.45161290, 0.24000000, 0…\n$ Blackhawks_Away  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Blackhawks_Home  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Bulls_Away       &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Bulls_Home       &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ Bears_Away       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Bears_Home       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ WhiteSox_Away    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ WhiteSox_Home    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Cubs_Away        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Cubs_Home        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ date             &lt;date&gt; 2001-01-22, 2001-01-23, 2001-01-24, 2001-01-25, 2001-01-26, 2001-01-27, …\n\n# For reproducibility\nset.seed(2)\nchicago_split &lt;- initial_split(Chicago, prop = 0.8)\nchicago_train &lt;- training(chicago_split)\nchicago_test &lt;- testing(chicago_split)\n\nWhen using recipes, we often need to select a group of variables (e.g., all predictors, all numeric variables, all categorical variables, etc.) to apply preprocessing steps. Indeed, we certainly could just explicitly specify each variable by name within our recipe. There’s a better way, though. Use selector functions.\nSelector functions can be used to choose variables based on:\n\nVariable names\nCurrent role\nData type\nAny combination of the above three\n\nThe first set of selectors comes from the tidyselect package, which allows you to make selections based on variable names. Some common ones include:\n\ntidyselect::starts_with()\ntidyselect::ends_with()\ntidyselect::contains()\ntidyselect::everything()\n\nCheck out recipes’ ?selections and the tidyselect docs for a more exhaustive list of available selection functions. Included above are the ones I commonly use. Here are a few examples of how to use these selector functions to center variables.\n\n# Apply the centering to variables that start with the *weather* prefix\nchicago_rec &lt;- \n  recipe(ridership ~ ., data = chicago_train) |&gt;\n  step_center(starts_with(\"weather\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nchicago_rec |&gt; select(starts_with(\"weather\"))\n\n# A tibble: 4,558 × 4\n   weather_rain weather_snow weather_cloud weather_storm\n          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1      -0.0803      -0.0529       0.0973        -0.0518\n 2      -0.0386       0.572       -0.403          0.0315\n 3      -0.0803       0.147       -0.00272        0.0398\n 4      -0.0803      -0.0529       0.264          0.198 \n 5      -0.0803      -0.0529       0.193          0.0612\n 6      -0.0803      -0.0529       0.264          0.323 \n 7      -0.0803      -0.0529       0.264          0.201 \n 8      -0.0803       0.614       -0.403         -0.236 \n 9      -0.0803      -0.0529       0.144         -0.100 \n10       0.0678      -0.0529      -0.0694        -0.112 \n# ℹ 4,548 more rows\n\n\nSelections also allows us to use the - to exclude specific variables or groupings of variables while using selector functions.\n\nchicago_rec &lt;- \n  recipe(ridership ~ ., data = chicago_train) |&gt;\n  step_center(-date, -starts_with(\"weather\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nchicago_rec\n\n# A tibble: 4,558 × 50\n     Austin Quincy_Wells Belmont Archer_35th Oak_Park Western Clark_Lake Clinton Merchandise_Mart\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1  0.756          2.49    1.44        0.914   0.645    0.903       6.57   0.972            1.29 \n 2  0.279          0.222   1.54        0.577   0.316    1.35        3.80   1.34             0.379\n 3  0.579          2.96    1.64        0.949   0.537    1.20        6.12   1.45             3.52 \n 4  0.584          1.91    0.686       0.768   0.470    0.849       5.65   0.504            1.21 \n 5  0.616          2.41    1.47        0.964   0.569    0.831       6.15   0.995            1.92 \n 6  0.660          2.68    1.42        0.982   0.532    0.981       6.02   1.20             2.32 \n 7 -1.04          -5.56   -2.33       -1.61   -0.852   -2.25      -10.7   -1.55            -3.60 \n 8 -0.00927        0.699   0.483       0.217   0.0441   0.209       1.63   0.999            0.262\n 9 -1.06          -4.55   -2.45       -1.50   -1.05    -2.00      -10.9   -1.58            -4.26 \n10  0.536          2.03    0.500       0.372   0.560    0.345       5.71   0.557            1.36 \n# ℹ 4,548 more rows\n# ℹ 41 more variables: Irving_Park &lt;dbl&gt;, Washington_Wells &lt;dbl&gt;, Harlem &lt;dbl&gt;, Monroe &lt;dbl&gt;,\n#   Polk &lt;dbl&gt;, Ashland &lt;dbl&gt;, Kedzie &lt;dbl&gt;, Addison &lt;dbl&gt;, Jefferson_Park &lt;dbl&gt;, Montrose &lt;dbl&gt;,\n#   California &lt;dbl&gt;, temp_min &lt;dbl&gt;, temp &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_change &lt;dbl&gt;, dew &lt;dbl&gt;,\n#   humidity &lt;dbl&gt;, pressure &lt;dbl&gt;, pressure_change &lt;dbl&gt;, wind &lt;dbl&gt;, wind_max &lt;dbl&gt;, gust &lt;dbl&gt;,\n#   gust_max &lt;dbl&gt;, percip &lt;dbl&gt;, percip_max &lt;dbl&gt;, weather_rain &lt;dbl&gt;, weather_snow &lt;dbl&gt;,\n#   weather_cloud &lt;dbl&gt;, weather_storm &lt;dbl&gt;, Blackhawks_Away &lt;dbl&gt;, Blackhawks_Home &lt;dbl&gt;, …\n\n# To show centering was not applied to variables with the *weather* prefix\nchicago_rec |&gt; select(starts_with(\"weather\"))\n\n# A tibble: 4,558 × 4\n   weather_rain weather_snow weather_cloud weather_storm\n          &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1       0             0             0.833        0.208 \n 2       0.0417        0.625         0.333        0.292 \n 3       0             0.2           0.733        0.3   \n 4       0             0             1            0.458 \n 5       0             0             0.929        0.321 \n 6       0             0             1            0.583 \n 7       0             0             1            0.462 \n 8       0             0.667         0.333        0.0238\n 9       0             0             0.88         0.16  \n10       0.148         0             0.667        0.148 \n# ℹ 4,548 more rows\n\n\nrecipes provides functions to select variables based on role and type. This includes the has_role() and has_type() functions.\n\n# Simplified recipe, applying centering to variables with predictor role \ncredit_rec &lt;- recipe(\n  Status ~ Debt + Income + Assets, \n  data = credit_train\n)  |&gt;\n  step_center(has_role(\"predictor\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec\n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ℹ 3,553 more rows\n\n\n\n# Applying centering to variables with type numeric\ncredit_rec_type &lt;- recipe(Status ~ ., data = credit_train) |&gt;\n  step_center(has_type(match = \"numeric\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_type\n\n# A tibble: 3,563 × 14\n   Seniority Home    Time      Age Marital Records Job   Expenses Income Assets  Debt Amount   Price\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     -7.91 pare…   1.54   3.94   married no      part…    34.6   -60.7 -5233. -337.  159.    -2.20\n 2     -7.91 other -28.5  -16.1    single  yes     part…   -20.4   -90.7 -5233. -337. -641.  -970.  \n 3     -5.91 rent   13.5  -12.1    single  no      fixed    -9.42  -33.7 -5233. -337.  459.   719.  \n 4     -6.91 owner  13.5    7.94   married no      part…    49.6   -28.7 -3233.  163. -441.  -138.  \n 5     -4.91 owner -22.5  -14.1    married no      fixed    19.6   -55.7  -233. -337. -441.   130.  \n 6     -7.91 &lt;NA&gt;    1.54  -0.0589 single  no      &lt;NA&gt;    -20.4    NA      NA    NA   459.   380.  \n 7     -2.91 rent    1.54  -6.06   single  no      fixed   -11.4   -50.7 -5233. -337.  259.   230.  \n 8     -5.91 owner  13.5    5.94   married no      part…    19.6   -69.7 -2233. -337.  459.    81.8 \n 9     -5.91 rent  -10.5  -10.1    separa… no      fixed    -7.42  -12.7 -5233. -337. -591.  -925.  \n10     -6.91 rent    1.54  -8.06   married yes     free…    29.6   -40.7 -5233. -337.  -41.1 -125.  \n# ℹ 3,553 more rows\n# ℹ 1 more variable: Status &lt;fct&gt;\n\n\nAlthough has_role() and has_type() are available, you’ll most likely rely on functions that are more specific. The docs state (?has_role):\n\nIn most cases, the right approach for users will be to use the predictor-specific selectors such as all_numeric_predictors() and all_nominal_predictors().\n\nThese include functions to select variables based on type:\n\nall_numeric() - includes all numeric variables.\nall_nominal() - includes both character and factor variables.\n\n\n# Center **all** numeric variables\ncredit_rec_type &lt;- recipe(\n  Status ~ Debt + Income + Assets,\n  data = credit_train\n) |&gt;\n  step_center(all_numeric()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_type \n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ℹ 3,553 more rows\n\n\nFunctions to select by role:\n\nall_predictors()\nall_outcomes()\n\n\n# Center all predictors\ncredit_rec_role &lt;- \n  recipe(\n    Status ~ Debt + Income + Assets, \n    data = credit_train\n  ) |&gt;\n  step_center(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_role\n\n# A tibble: 3,563 × 4\n    Debt Income Assets Status\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; \n 1 -337.  -60.7 -5233. bad   \n 2 -337.  -90.7 -5233. bad   \n 3 -337.  -33.7 -5233. bad   \n 4  163.  -28.7 -3233. bad   \n 5 -337.  -55.7  -233. bad   \n 6   NA    NA      NA  bad   \n 7 -337.  -50.7 -5233. bad   \n 8 -337.  -69.7 -2233. bad   \n 9 -337.  -12.7 -5233. bad   \n10 -337.  -40.7 -5233. bad   \n# ℹ 3,553 more rows\n\n\nFunctions to select variables that intersect by role and type:\n\nall_numeric_predictors()\nall_nominal_predictors()\n\n\ncredit_rec_num_pred &lt;- \n  recipe(Status ~ ., data = credit_train) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\ncredit_rec_num_pred\n\n# A tibble: 3,563 × 14\n   Seniority Home    Time      Age Marital Records Job   Expenses Income Assets  Debt Amount   Price\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     -7.91 pare…   1.54   3.94   married no      part…    34.6   -60.7 -5233. -337.  159.    -2.20\n 2     -7.91 other -28.5  -16.1    single  yes     part…   -20.4   -90.7 -5233. -337. -641.  -970.  \n 3     -5.91 rent   13.5  -12.1    single  no      fixed    -9.42  -33.7 -5233. -337.  459.   719.  \n 4     -6.91 owner  13.5    7.94   married no      part…    49.6   -28.7 -3233.  163. -441.  -138.  \n 5     -4.91 owner -22.5  -14.1    married no      fixed    19.6   -55.7  -233. -337. -441.   130.  \n 6     -7.91 &lt;NA&gt;    1.54  -0.0589 single  no      &lt;NA&gt;    -20.4    NA      NA    NA   459.   380.  \n 7     -2.91 rent    1.54  -6.06   single  no      fixed   -11.4   -50.7 -5233. -337.  259.   230.  \n 8     -5.91 owner  13.5    5.94   married no      part…    19.6   -69.7 -2233. -337.  459.    81.8 \n 9     -5.91 rent  -10.5  -10.1    separa… no      fixed    -7.42  -12.7 -5233. -337. -591.  -925.  \n10     -6.91 rent    1.54  -8.06   married yes     free…    29.6   -40.7 -5233. -337.  -41.1 -125.  \n# ℹ 3,553 more rows\n# ℹ 1 more variable: Status &lt;fct&gt;\n\n\nSelector functions will become useful as we continue to explore the step_* functions within the recipes package.\n\n\nDay 04 - Create dummy variables using step_dummy()\nBefore starting our overview of recipes’ step_* functions, we need a bit of direction on what preprocessing steps might be required or beneficial to apply. The type of data preprocessing is determined by the model being fit. As a starting point, the Tidy Modeling with R book provides an appendix with a table of preprocessing recommendations based on the types of models being used. This table is separate from the types of feature engineering that may be applied, but it’s a good baseline for determining the initial step_* functions to be included within a recipe.\nDummy variables is the first preprocessing method highlighted in this appendix. That is, the encoding of qualitative predictors into numeric predictors. Closely related is one-hot encoding. When dummy variables are created, most commonly, nominal variable columns are converted into separate columns of 1’s and 0’s. recipes’ step_dummy() function performs these preprocessing operations.\nLet’s continue using the credit_data for today’s examples. Take note, this data contains some NA‘s. To address this issue, I’m just going to drop any cases with a missing value using dplyr’s drop_na() function. Indeed, this issue could be addressed with imputation through the use of recipes’ step_impute_* functions (more on this in the coming days).\n\n# Same code as day 01\ndata(credit_data, package = \"modeldata\")\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 14\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\ncredit_data &lt;- credit_data |&gt;\n  drop_na()\n\n\n# Create the split, training and testing data\nset.seed(20230104)\ncredit_split &lt;- initial_split(credit_data, prop = 0.8)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\nHere’s the recipe we’ll use. I’m gonna keep it simple, so it’s easier to observe the results of adding step_dummy() to our recipe.\n\ncredit_rec &lt;- \n  recipe(\n    Status ~ Job + Home + Marital, \n    data = credit_train\n  ) \n\nLet’s create dummy variables from the Job column. But first, let’s take a look at how many different variable levels there are.\n\nunique(credit_data$Job)\n\n[1] freelance fixed     partime   others   \nLevels: fixed freelance others partime\n\n\nSince we have four levels (freelance, fixed, partime, others), the step_dummy() function will create three columns. The fixed Job level will be the reference group, since it’s the first level specified for the factor.\n\ncredit_rec |&gt;\n  step_dummy(Job) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 6\n   Home    Marital Status Job_freelance Job_others Job_partime\n   &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 owner   married good               0          0           0\n 2 other   married bad                0          1           0\n 3 owner   married good               0          0           0\n 4 owner   married good               1          0           0\n 5 parents single  good               0          0           0\n 6 rent    single  good               0          0           0\n 7 parents single  good               0          0           0\n 8 other   widow   good               0          0           0\n 9 priv    single  good               1          0           0\n10 owner   married bad                0          0           0\n# ℹ 3,221 more rows\n\n\nTake note of the naming conventions applied to the new dummy columns. step_dummy() uses the following naming convention variable-name_variable-level. This makes it easier to know what variable the dummy variables originated.\nSay you don’t want to drop the original column when the dummy variables are created. We can pass TRUE to the keep_original_cols argument. This will retain the original column, while also creating the dummy variables.\n\ncredit_rec |&gt;\n  step_dummy(Job, keep_original_cols = TRUE) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 7\n   Job       Home    Marital Status Job_freelance Job_others Job_partime\n   &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 fixed     owner   married good               0          0           0\n 2 others    other   married bad                0          1           0\n 3 fixed     owner   married good               0          0           0\n 4 freelance owner   married good               1          0           0\n 5 fixed     parents single  good               0          0           0\n 6 fixed     rent    single  good               0          0           0\n 7 fixed     parents single  good               0          0           0\n 8 fixed     other   widow   good               0          0           0\n 9 freelance priv    single  good               1          0           0\n10 fixed     owner   married bad                0          0           0\n# ℹ 3,221 more rows\n\n\nWhat about one-hot encoding? To apply one-hot encoding we specify FALSE to the one_hot argument within the function. The preprocessed, baked data will now contain four columns. One column for each level of the source column.\n\ncredit_rec |&gt;\n  step_dummy(Job, one_hot = TRUE) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 7\n   Home    Marital Status Job_fixed Job_freelance Job_others Job_partime\n   &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1 owner   married good           1             0          0           0\n 2 other   married bad            0             0          1           0\n 3 owner   married good           1             0          0           0\n 4 owner   married good           0             1          0           0\n 5 parents single  good           1             0          0           0\n 6 rent    single  good           1             0          0           0\n 7 parents single  good           1             0          0           0\n 8 other   widow   good           1             0          0           0\n 9 priv    single  good           0             1          0           0\n10 owner   married bad            1             0          0           0\n# ℹ 3,221 more rows\n\n\nWe can scale this preprocessing to all nominal predictors by using, you guessed it, selector functions.\n\ncredit_rec |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 3,231 × 13\n   Status Job_freelance Job_others Job_partime Home_other Home_owner Home_parents Home_priv\n   &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 good               0          0           0          0          1            0         0\n 2 bad                0          1           0          1          0            0         0\n 3 good               0          0           0          0          1            0         0\n 4 good               1          0           0          0          1            0         0\n 5 good               0          0           0          0          0            1         0\n 6 good               0          0           0          0          0            0         0\n 7 good               0          0           0          0          0            1         0\n 8 good               0          0           0          1          0            0         0\n 9 good               1          0           0          0          0            0         1\n10 bad                0          0           0          0          1            0         0\n# ℹ 3,221 more rows\n# ℹ 5 more variables: Home_rent &lt;dbl&gt;, Marital_married &lt;dbl&gt;, Marital_separated &lt;dbl&gt;,\n#   Marital_single &lt;dbl&gt;, Marital_widow &lt;dbl&gt;\n\n\nThat’s a lot of additional columns. How can we keep track of all these additional columns and how they were preprocessed? We can summary and tidy our prepped recipe. Summarizing the prepped recipe is useful because of the source column that gets outputted. In our example, the source column of the returned tibble contains two values: original (i.e., the column was an original column in the data set) and derived (i.e., a column created from the preprocessing step). When we tidy() the recipe object returned from step_dummy(), a tibble with two columns is returned: terms and columns. terms represents the original variable the dummy variables were created from. columns represents the newly preprocessed dummy variable.\n\n# Prep our dummy variables\ncredit_rec &lt;- \n  credit_rec |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep()\n\nsummary(credit_rec)\n\n# A tibble: 13 × 4\n   variable          type      role      source  \n   &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 Status            &lt;chr [3]&gt; outcome   original\n 2 Job_freelance     &lt;chr [2]&gt; predictor derived \n 3 Job_others        &lt;chr [2]&gt; predictor derived \n 4 Job_partime       &lt;chr [2]&gt; predictor derived \n 5 Home_other        &lt;chr [2]&gt; predictor derived \n 6 Home_owner        &lt;chr [2]&gt; predictor derived \n 7 Home_parents      &lt;chr [2]&gt; predictor derived \n 8 Home_priv         &lt;chr [2]&gt; predictor derived \n 9 Home_rent         &lt;chr [2]&gt; predictor derived \n10 Marital_married   &lt;chr [2]&gt; predictor derived \n11 Marital_separated &lt;chr [2]&gt; predictor derived \n12 Marital_single    &lt;chr [2]&gt; predictor derived \n13 Marital_widow     &lt;chr [2]&gt; predictor derived \n\n# View what preprocessing steps are applied\ntidy(credit_rec)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id         \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;      \n1      1 step      dummy TRUE    FALSE dummy_9a72e\n\n# Drill down and view what was done in during this specific step \ntidy(credit_rec, number = 1)\n\n# A tibble: 12 × 3\n   terms   columns   id         \n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;      \n 1 Job     freelance dummy_9a72e\n 2 Job     others    dummy_9a72e\n 3 Job     partime   dummy_9a72e\n 4 Home    other     dummy_9a72e\n 5 Home    owner     dummy_9a72e\n 6 Home    parents   dummy_9a72e\n 7 Home    priv      dummy_9a72e\n 8 Home    rent      dummy_9a72e\n 9 Marital married   dummy_9a72e\n10 Marital separated dummy_9a72e\n11 Marital single    dummy_9a72e\n12 Marital widow     dummy_9a72e\n\n\nWhen it comes to specifying interactions within a model, there are some special considerations when using dummy variables. I don’t have much time to discuss this today, but I hope to address it on a future day of this challenge. I suggest reviewing the ‘Interactions with Dummy Variables’ section from the ‘Dummies’ vignette (vignettes(\"Dummies\", package = \"recipes\")) for more information.\nOne more thing, step_dummy() is useful for straight forward dummy variable creation. However, recipes also has some other closely related step_* functions. Here is a list of a few from the ‘Dummies’ vignette:\n\nstep_other() - collapses infrequently occurring levels into an ‘other’ category.\nstep_holiday() - creates dummy variables from dates to capture holidays. Useful when working with time series data.\nstep_zv() - removes dummy variables that are zero-variance.\n\nI look to highlight the use of some of these step_* functions in the coming days.\n\n\nDay 05 - Create a binary indicator variable for holidays using step_holiday()\nStaying on the topic of dummy variables, I wanted to take a day to focus on the use of recipes’ step_holiday() function. It seems to be pretty useful when working with time series data.\nFor today’s example, I’m going to use some obfuscated, simulated Google Analytics ecommerce data. This emulates data closely related to what would be collected for the Google Merchandise Store. You can learn more about this data by clicking on the previously linked docs. Let’s do some data wrangling.\nSome notes about what wrangling was done:\n\nParse the event_date column into a date variable.\nCalculate the revenue generated from the purchase of items based on quantity.\nRetain only relevant columns.\n\nFor simplicity, I’m not going to create a testing training split for this data.\n\ndata_ga &lt;- \n  read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  ) |&gt;\n  mutate(\n    event_date = ymd(event_date),\n    revenue = price_in_usd * quantity\n  ) |&gt;\n  select(event_date, transaction_id, item_category, revenue)\n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s start on our recipe. Since we have an id variable, transaction_id, let’s update the recipe to change it’s role to id. Once we do that, we can pass the event_date to the step_holiday() function. Before we bake our recipe, I wanna prep() and summarise the preprocessing to see what columns will get added.\n\nga_rec &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  update_role(transaction_id, new_role = \"id\") |&gt;\n  step_holiday(event_date) |&gt;\n  prep() \n\nsummary(ga_rec)\n\n# A tibble: 7 × 4\n  variable                type      role      source  \n  &lt;chr&gt;                   &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 event_date              &lt;chr [1]&gt; predictor original\n2 transaction_id          &lt;chr [2]&gt; id        original\n3 item_category           &lt;chr [3]&gt; predictor original\n4 revenue                 &lt;chr [2]&gt; outcome   original\n5 event_date_LaborDay     &lt;chr [2]&gt; predictor derived \n6 event_date_NewYearsDay  &lt;chr [2]&gt; predictor derived \n7 event_date_ChristmasDay &lt;chr [2]&gt; predictor derived \n\n\nNote, three new columns will be added once the recipe is baked. This includes:\n\nevent_date_LaborDay - a dummy variable to represent an item purchases on Labor Day.\nevent_date_NewYearsDay - a dummy variable to represent item purchases on New Years Day.\nevent_date_ChristmasDay - a dummy variable to represent item purchases made on Christmas Day.\n\nYou can see the variables that get added by baking the recipe.\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 9,365 × 7\n   event_date transaction_id item_category       revenue event_date_LaborDay event_date_NewYearsDay\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt;               &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance                12                   0                      0\n 2 2020-12-01          10648 Accessories               2                   0                      0\n 3 2020-12-01          10648 Drinkware                 4                   0                      0\n 4 2020-12-01          10648 Small Goods               2                   0                      0\n 5 2020-12-01          10648 Office                    3                   0                      0\n 6 2020-12-01          10648 Accessories               3                   0                      0\n 7 2020-12-01          10648 Apparel                  14                   0                      0\n 8 2020-12-01         171491 Apparel                  48                   0                      0\n 9 2020-12-01         171491 Drinkware                14                   0                      0\n10 2020-12-01         174748 Uncategorized Items      44                   0                      0\n# ℹ 9,355 more rows\n# ℹ 1 more variable: event_date_ChristmasDay &lt;int&gt;\n\n\nLabor Day, New Years Day, and Christmas Day are the default holidays preprocessed by the function. You can modify this by passing a character vector of holidays to step_holiday()’s holidays argument. For instance, say we wanted to create dummy variables for Boxing Day and the United State’s Thanksgiving Day holiday, while excluding Labor Day. The following code will specify this preprocessing step for us:\n\nga_rec_holidays &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  update_role(transaction_id, new_role = \"transaction_id\") |&gt;\n  step_holiday(\n    event_date, \n    holidays = c(\"USThanksgivingDay\", \"ChristmasDay\", \"BoxingDay\", \"NewYearsDay\")\n  ) |&gt;\n  prep()\n\nsummary(ga_rec_holidays)\n\n# A tibble: 8 × 4\n  variable                     type      role           source  \n  &lt;chr&gt;                        &lt;list&gt;    &lt;chr&gt;          &lt;chr&gt;   \n1 event_date                   &lt;chr [1]&gt; predictor      original\n2 transaction_id               &lt;chr [2]&gt; transaction_id original\n3 item_category                &lt;chr [3]&gt; predictor      original\n4 revenue                      &lt;chr [2]&gt; outcome        original\n5 event_date_USThanksgivingDay &lt;chr [2]&gt; predictor      derived \n6 event_date_ChristmasDay      &lt;chr [2]&gt; predictor      derived \n7 event_date_BoxingDay         &lt;chr [2]&gt; predictor      derived \n8 event_date_NewYearsDay       &lt;chr [2]&gt; predictor      derived \n\n\nNow we have a dummy variable for all four of these holidays. Let’s bake our recipe and see the final result.\n\nbake(ga_rec_holidays, new_data = NULL)\n\n# A tibble: 9,365 × 8\n   event_date transaction_id item_category     revenue event_date_USThanksg…¹ event_date_Christmas…²\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;                  &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance              12                      0                      0\n 2 2020-12-01          10648 Accessories             2                      0                      0\n 3 2020-12-01          10648 Drinkware               4                      0                      0\n 4 2020-12-01          10648 Small Goods             2                      0                      0\n 5 2020-12-01          10648 Office                  3                      0                      0\n 6 2020-12-01          10648 Accessories             3                      0                      0\n 7 2020-12-01          10648 Apparel                14                      0                      0\n 8 2020-12-01         171491 Apparel                48                      0                      0\n 9 2020-12-01         171491 Drinkware              14                      0                      0\n10 2020-12-01         174748 Uncategorized It…      44                      0                      0\n# ℹ 9,355 more rows\n# ℹ abbreviated names: ¹​event_date_USThanksgivingDay, ²​event_date_ChristmasDay\n# ℹ 2 more variables: event_date_BoxingDay &lt;int&gt;, event_date_NewYearsDay &lt;int&gt;\n\n\nIndeed, there are many holidays that could be specified for dummy variable creation. All the available holidays can be seen by running timeDate::listHolidays() in your console. Last time I checked, there were 118 available holidays.\n\n\nDay 06 - Use step_zv() to drop variables with one value\nFor today, I’m focusing on recipes’ step_zv() function. This function is a filter function, which drops variables that only contain one value.\nAt first, I didn’t really understand why step_zv() was made available. Why would you want a step to drop variables within a recipe? Then it clicked working on yesterday’s example using the obfuscated Google Analytics data for the Google Merchandise store.\nBut first, let’s get our data again and specify our recipe. I’m going to keep things simple here. First, I’m just going to use data_ga, which was previously wrangled in yesterday’s post (check it out if you want more info). Second, I’m going to skip creating a testing and training split. Lastly, I’m going to create dummy variables using step_holiday(), just to show how step_zv() can be useful.\n\nga_rec &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  step_holiday(event_date)\n\nsummary(ga_rec)\n\n# A tibble: 4 × 4\n  variable       type      role      source  \n  &lt;chr&gt;          &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 event_date     &lt;chr [1]&gt; predictor original\n2 transaction_id &lt;chr [2]&gt; predictor original\n3 item_category  &lt;chr [3]&gt; predictor original\n4 revenue        &lt;chr [2]&gt; outcome   original\n\n\nLet’s take a closer look at our data. You’ll notice the range of the event_date is a subset of data. data_ga’s event_date ranges between the US holiday season. It starts right before Christmas and moves into the first month of the new year.\n\nc(\n  min_date = min(data_ga$event_date), \n  max_date = max(data_ga$event_date)\n)\n\n    min_date     max_date \n\"2020-12-01\" \"2021-01-30\" \n\n\nIf you remember from yesterday’s post, one of the default holidays for step_holiday() is Labor Day. As such, a dummy variable with all 0’s will be created for the Labor Day holiday. Purchases made on these dates were not included within this data.\n\nga_prep &lt;- prep(ga_rec)\n\ntidy(ga_prep, number = 1)\n\n# A tibble: 3 × 3\n  terms      holiday      id           \n  &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;        \n1 event_date LaborDay     holiday_cClNx\n2 event_date NewYearsDay  holiday_cClNx\n3 event_date ChristmasDay holiday_cClNx\n\n# Check the unique values\nbake(ga_prep, new_data = NULL) |&gt; \n  select(event_date_LaborDay) |&gt;\n  distinct(event_date_LaborDay)\n\n# A tibble: 1 × 1\n  event_date_LaborDay\n                &lt;int&gt;\n1                   0\n\n\nAs such, this variable is not very useful and should be dropped before being applied within our model. This is why step_zv() can be handy, especially in situations where you have a lot of variables that could only have one value. step_zv() makes it easy to drop all unnecessary variables in one step, while allowing you to continue working with a recipe object.\nIndeed, keen observers might note this step could be mitigated by modifying the holiday argument in step_holiday(). However, the function’s utility extends beyond just step_holiday(). You might even consider useful as a final step you apply to every recipe.\n\nga_rec_drop &lt;- recipe(revenue ~ ., data = data_ga) |&gt;\n  step_holiday(event_date) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep()\n\nga_rec_drop\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 9365 data points and 164 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Holiday features from: event_date | Trained\n\n\n• Zero variance filter removed: event_date_LaborDay | Trained\n\ntidy(ga_rec_drop)\n\n# A tibble: 2 × 6\n  number operation type    trained skip  id           \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;        \n1      1 step      holiday TRUE    FALSE holiday_FpWeG\n2      2 step      zv      TRUE    FALSE zv_3cDmc     \n\n\nTake note, the prep() output informs us of the variables that were dropped when the step was applied. This is something to keep an eye on, just in case you need to explore situations where many variables are dropped, and you need to explore what your recipe is actually doing.\nFor completeness, lets bake() our final recipe.\n\nga_rec &lt;- recipe(revenue ~., data = data_ga) |&gt;\n  step_holiday(event_date) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_rec\n\n# A tibble: 9,365 × 6\n   event_date transaction_id item_category     revenue event_date_NewYearsDay event_date_Christmas…¹\n   &lt;date&gt;              &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;                  &lt;int&gt;                  &lt;int&gt;\n 1 2020-12-01          10648 Clearance              12                      0                      0\n 2 2020-12-01          10648 Accessories             2                      0                      0\n 3 2020-12-01          10648 Drinkware               4                      0                      0\n 4 2020-12-01          10648 Small Goods             2                      0                      0\n 5 2020-12-01          10648 Office                  3                      0                      0\n 6 2020-12-01          10648 Accessories             3                      0                      0\n 7 2020-12-01          10648 Apparel                14                      0                      0\n 8 2020-12-01         171491 Apparel                48                      0                      0\n 9 2020-12-01         171491 Drinkware              14                      0                      0\n10 2020-12-01         174748 Uncategorized It…      44                      0                      0\n# ℹ 9,355 more rows\n# ℹ abbreviated name: ¹​event_date_ChristmasDay\n\nglimpse(ga_rec)\n\nRows: 9,365\nColumns: 6\n$ event_date              &lt;date&gt; 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-…\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1…\n$ item_category           &lt;fct&gt; Clearance, Accessories, Drinkware, Small Goods, Office, Accessorie…\n$ revenue                 &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, …\n$ event_date_NewYearsDay  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ event_date_ChristmasDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\nDay 07 - Use step_impute_*() functions for imputation\nThe recipes package makes it easy to perform imputation tasks. As of this writing, recipes had the following functions to perform different methods of imputation:\n\nstep_impute_bag()\nstep_impute_knn()\nstep_impute_linear()\nstep_impute_lower()\nstep_impute_mean()\nstep_impute_median()\nstep_impute_mode()\nstep_impute_roll()\n\nFor today’s examples, I’m going to highlight the use of step_impute_mean(), step_input_median(), and step_input_mode(). First, though, we need some data with missing values. Let’s switch it up a bit and use the Palmer Station penguin data (run ?penguins in your console to get more information about the data). In brief, these data represent different measurements of various penguin species in Antarctica.\n\ndata(penguins, package = \"modeldata\") \n\n# Add an id column\npenguins &lt;- \n  penguins |&gt; mutate(id = 1:n(), .before = everything())\n\nThis data set contains some missing values that could be addressed using imputation methods. Let’s take a moment and explore the data a little further.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\n# What columns have missing data?\nmap_df(penguins, \\(x) any(is.na(x)))\n\n# A tibble: 1 × 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  TRUE           TRUE          TRUE              TRUE        TRUE \n\n# What percentage of data is missing in each column?\nmap(penguins, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$species\n[1] 0\n\n$island\n[1] 0\n\n$bill_length_mm\n[1] 0.005813953\n\n$bill_depth_mm\n[1] 0.005813953\n\n$flipper_length_mm\n[1] 0.005813953\n\n$body_mass_g\n[1] 0.005813953\n\n$sex\n[1] 0.03197674\n\n# Missing data examples\nmissing_examples &lt;- c(4, 12, 69, 272)\npenguins |&gt; slice(missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nThe following columns contain missing data (included are the variable types):\n\nbill_length_mm - double\nbill_depth_mm - double\nflipper_length_mm - integer\nbody_mass_g - integer\nsex - factor\n\nYou’ll also notice some of these variables are of various types (i.e. factor, double, or integer). Indeed, the variable type will determine the method of imputation applied.\n\nset.seed(20240107)\npenguins_split &lt;- initial_split(penguins, prop = .8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nLet’s start by highlighting how to apply mean substitution as our imputation method. Specifically, let’s apply this step to our first numeric variable with missing values, bill_length_mm.\n\n\n\n\n\n\nNote\n\n\n\nTake note of the importance of the use of prep() here. Remember, some recipe steps need to calculate an intermediate value before applying it to the final baked data. This is highlighted with the tidy(penquin_rec, number = 1) in the code below.\n\n\n\npenguins_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_mean(bill_length_mm) |&gt;\n  prep()\n\nsummary(penguins_rec)\n\n# A tibble: 8 × 4\n  variable          type      role      source  \n  &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 id                &lt;chr [2]&gt; id        original\n2 species           &lt;chr [3]&gt; predictor original\n3 island            &lt;chr [3]&gt; predictor original\n4 bill_length_mm    &lt;chr [2]&gt; predictor original\n5 bill_depth_mm     &lt;chr [2]&gt; predictor original\n6 flipper_length_mm &lt;chr [2]&gt; predictor original\n7 body_mass_g       &lt;chr [2]&gt; predictor original\n8 sex               &lt;chr [3]&gt; predictor original\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm  43.7 impute_mean_yJPI2\n\n\n\npenguins_baked &lt;- bake(penguins_rec, new_data = NULL)\n\npenguins_baked\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\n# Imputation should result in a complete column of data\nany(is.na(penguins_baked$bill_length_mm))\n\n[1] FALSE\n\n# The missing values have now been substituted\npenguins_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n1   272 Gentoo  Biscoe              43.7          NA                  NA          NA &lt;NA&gt; \n2     4 Adelie  Torgersen           43.7          NA                  NA          NA &lt;NA&gt; \n3    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt; \n\n\nstep_impute_mean() also includes a trim argument, which trims observations from the end of the variable before the mean is computed. This is also a tuning parameter, which can be used in any hyperparameter tuning applied within your modeling. I would like to explore this more, but it’s outside the scope of this post. Just to highlight the use of the trim argument, here’s some example code:\n\npenguin_mean_trim_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  step_impute_mean(bill_length_mm, trim = .5) |&gt;\n  prep()\n\n# Notice how the intermediate calculation changed because\n# we trimmed the observations used to make the mean calculation\ntidy(penguin_mean_trim_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm    44 impute_mean_sGprM\n\n\nLet’s bake this recipe for completeness.\n\npenguins_mean_trim_baked &lt;- \n  bake(penguin_mean_trim_rec, new_data = NULL)\n\npenguins_mean_trim_baked\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\n# The missing values have now been imputed\npenguins_mean_trim_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;\n1   272 Gentoo  Biscoe              44            NA                  NA          NA &lt;NA&gt; \n2     4 Adelie  Torgersen           44            NA                  NA          NA &lt;NA&gt; \n3    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt; \n\n\nMean substitution is just one imputation step. The recipes package also includes the step_impute_median() and step_impute_mode(). These step functions have similar syntax, just a different calculated metric is applied in the background. Let’s apply step_impute_median() to bill_depth_mm, flipper_length_mm, and body_mass_g.\nIn addition, we’ll apply step_impute_mode() to impute values for the missing data within the sex variable. Take note, the docs for this function state:\n\nImpute nominal data using the most common value.\n\nSo, it only seems step_impute_mode() can only be used to impute missing values for nominal variables.\n\npenguin_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  step_impute_mean(bill_length_mm) |&gt;\n  step_impute_median(\n    bill_depth_mm, \n    flipper_length_mm, \n    body_mass_g\n  ) |&gt;\n  step_impute_mode(sex) |&gt;\n  prep()\n\npenguin_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 8\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 11 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: bill_length_mm | Trained\n\n\n• Median imputation for: bill_depth_mm, flipper_length_mm, body_mass_g | Trained\n\n\n• Mode imputation for: sex | Trained\n\ntidy(penguin_rec)\n\n# A tibble: 3 × 6\n  number operation type          trained skip  id                 \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;              \n1      1 step      impute_mean   TRUE    FALSE impute_mean_fOycb  \n2      2 step      impute_median TRUE    FALSE impute_median_zocWX\n3      3 step      impute_mode   TRUE    FALSE impute_mode_PGohX  \n\n\nLet’s take a look at the calculated values for all these steps.\n\nmap(1:3, \\(x) tidy(penguin_rec, number = x))\n\n[[1]]\n# A tibble: 1 × 3\n  terms          value id               \n  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;            \n1 bill_length_mm  43.7 impute_mean_fOycb\n\n[[2]]\n# A tibble: 3 × 3\n  terms              value id                 \n  &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;              \n1 bill_depth_mm       17.3 impute_median_zocWX\n2 flipper_length_mm  196   impute_median_zocWX\n3 body_mass_g       4050   impute_median_zocWX\n\n[[3]]\n# A tibble: 1 × 3\n  terms value  id               \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;            \n1 sex   female impute_mode_PGohX\n\n\nAs always, let’s bake this recipe and look at the final data, which should now contain no missing data.\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\nmap_df(baked_penguin, \\(x) any(is.na(x)))\n\n# A tibble: 1 × 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  FALSE          FALSE         FALSE             FALSE       FALSE\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              43.7          17.3               196        4050 female\n2     4 Adelie  Torgersen           43.7          17.3               196        4050 female\n3    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n\n\nThat’s all the time I have for today. Tomorrow I’ll pick up exploring some more of the other step_impute_* functions.\n\n\nDay 08 - Use bagged tree models to impute missing data with step_impute_bag()\nTo start, I wanted to highlight a really good, simplified definition of imputation from the Feature Engineering and Selection: A Practical Approach for Predictive Models book by Max Kuhn and Kjell Johnson.\n\nImputation uses information and relationships among the non-missing predictors to provide an estimate to fill in the missing values.\n\nYesterday we used the step_impute_mean(), step_impute_median(), and step_impute_mode() functions to calculate missing values. However, we can also use tree-based methods, which uses information from different variables rather than just values in rows, to perform our imputation step.\nTo be honest, this imputation method was beyond my current knowledge set. Thus, my explanation of what is happening on the backend may be quite general. However, check out the ‘Trees’ section from the Feature Engineering and Selection: A Practical Approach for Predictive Models book for a good starting point to learn more. The book does suggest using bagged models can produce reasonable outputs, which results in values to be produced within the range of the training data. Such methods also retains all predictors, unlike when case-wise deletion is used to manage missing data.\nrecipes’ step_impute_bag() function is used to impute missing data using bagged tree models. To highlight the use of this step, let’s go back to using the penguins data from yesterday.\n\ndata(penguins, package = \"modeldata\")\nmissing_examples &lt;- c(4, 12, 69, 272)\n\n# Create an id variable\npenguins &lt;- \n  penguins |&gt; \n  mutate(id = 1:n(), .before = everything())\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\n# Print the missing examples\npenguins |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nWe’ll now create our training and testing split.\n\nset.seed(20240108)\npenguins_split &lt;- initial_split(penguins, prop = 0.8)\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nTo start small, let’s use a bagged tree model to impute values for the missing data in the bill_length_mm variable. The syntax is pretty straightforward:\n\npenguins_rec &lt;- recipe (~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(bill_length_mm) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 9 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Bagged tree imputation for: bill_length_mm | Trained\n\n\nBefore we bake() our recipe, let’s tidy() our prepped recipe a bit to see what’s happening under the hood.\n\ntidy(penguins_rec)\n\n# A tibble: 1 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_bag TRUE    FALSE impute_bag_OQalP\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms          model     id              \n  &lt;chr&gt;          &lt;list&gt;    &lt;chr&gt;           \n1 bill_length_mm &lt;regbagg&gt; impute_bag_OQalP\n\n\nTidying down to the bagging step, you’ll see this step outputs a tibble with three columns:\n\nterms - the selectors or variables selected.\nmodel - the bagged tree model object.\nid - a unique id for the step being applied in the recipe.\n\nLet’s bake the recipe and see the result of our imputation step.\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1    35 Adelie    Dream               36.4          17                 195        3325 female\n 2   111 Adelie    Biscoe              38.1          16.5               198        3825 female\n 3   245 Gentoo    Biscoe              45.5          14.5               212        4750 female\n 4    75 Adelie    Torgersen           35.5          17.5               190        3700 female\n 5     1 Adelie    Torgersen           39.1          18.7               181        3750 male  \n 6    96 Adelie    Dream               40.8          18.9               208        4300 male  \n 7   338 Chinstrap Dream               46.8          16.5               189        3650 female\n 8    24 Adelie    Biscoe              38.2          18.1               185        3950 male  \n 9    20 Adelie    Torgersen           46            21.5               194        4200 male  \n10    40 Adelie    Dream               39.8          19.1               184        4650 male  \n# ℹ 265 more rows\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              43.7          17.3               196        4050 female\n2     4 Adelie  Torgersen           43.7          17.3               196        4050 female\n3    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n\n\nstep_impute_bagged() also has several options to modify the imputation method. First, it has an impute_with argument that allows you to be selective about what variables are used as predictors in the bagged tree model. We’ll specify these variables by passing them into the imp_vars() function to the argument.\nThis argument accepts the various selector functions as well. For instance, the default for the argument is the all_predictors() function. The following code uses this argument to limit the imputation to the bill_depth_mm and sex variables (I’m not a biologist, so I have no idea if this is actually a good approach).\nI did come across a cryptic warning when first doing this, though. This warning also resulted in the imputation step to not be applied.\n\npenguin_rec &lt;- recipe(~ ., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    impute_with = imp_vars(bill_depth_mm, sex)\n  ) |&gt;\n  prep()\n\nWarning: All predictors are missing; cannot impute\n\n\nI assumed this was because all the predictors used to create the model for imputation had missing values. So, I applied some imputation to these first before applying the step_impute_bag() and the warning went away. However, I’m unsure if this was the initial problem. I might submit an issue to the recipes GitHub repo to which I’ll link later. Nevertheless, I got the example to work. Here’s the code:\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_mean(bill_depth_mm) |&gt;\n  step_impute_mode(sex) |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    impute_with = imp_vars(bill_depth_mm)\n  ) |&gt;\n  prep() \n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           41.0          17.2                NA          NA male  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 male  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              41.0          17.2                NA          NA male  \n\n\nGiven we’re using a bagged tree model to perform imputation, we can modify the number of bagged trees used in each model in the step_impute_bag() function. To do this, we just pass a value to the trees argument. Indeed, its suggested to keep this value between 25 - 50 trees.\n\n# The default is 25 trees\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(bill_length_mm, trees = 50) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           38.3          NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              47.8          NA                  NA          NA &lt;NA&gt;  \n\n\nThe last step_impute_bag() argument I’ll highlight is options. ipred::ipredbagg() implements the bagged model used for this imputation step. Thus, the options argument is used to pass arguments to this function. For example, if we want to speed up execution, we can lower the nbagg argument, the number of bootstrap replications applied, and indicate we don’t want to return a data frame of predictors by setting keepX = FALSE.\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(\n    bill_length_mm,\n    options = list(nbagg = 2, keepX = FALSE)\n  ) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   280 Chinstrap Dream               45.4          18.7               188        3525 female\n 2   160 Gentoo    Biscoe              46.7          15.3               219        5200 male  \n 3    27 Adelie    Biscoe              40.6          18.6               183        3550 male  \n 4   274 Gentoo    Biscoe              50.4          15.7               222        5750 male  \n 5   288 Chinstrap Dream               51.7          20.3               194        3775 male  \n 6    46 Adelie    Dream               39.6          18.8               190        4600 male  \n 7   316 Chinstrap Dream               53.5          19.9               205        4500 male  \n 8   286 Chinstrap Dream               51.3          19.9               198        3700 male  \n 9   164 Gentoo    Biscoe              49            16.1               216        5550 male  \n10    79 Adelie    Torgersen           36.2          16.1               187        3550 female\n# ℹ 265 more rows\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           38.1          NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              48.0          NA                  NA          NA &lt;NA&gt;  \n\n\nJust for the heck of it, let’s apply step_impute_bag() to all predictor variables in our recipe.\n\npenguin_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_bag(all_predictors()) |&gt;\n  prep()\n\nbaked_penguin &lt;- bake(penguin_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   280 Chinstrap Dream               45.4          18.7               188        3525 female\n 2   160 Gentoo    Biscoe              46.7          15.3               219        5200 male  \n 3    27 Adelie    Biscoe              40.6          18.6               183        3550 male  \n 4   274 Gentoo    Biscoe              50.4          15.7               222        5750 male  \n 5   288 Chinstrap Dream               51.7          20.3               194        3775 male  \n 6    46 Adelie    Dream               39.6          18.8               190        4600 male  \n 7   316 Chinstrap Dream               53.5          19.9               205        4500 male  \n 8   286 Chinstrap Dream               51.3          19.9               198        3700 male  \n 9   164 Gentoo    Biscoe              49            16.1               216        5550 male  \n10    79 Adelie    Torgersen           36.2          16.1               187        3550 female\n# ℹ 265 more rows\n\n# There should now be no missing data\nmap_df(baked_penguin, \\(x) any(is.na(x)))\n\n# A tibble: 1 × 8\n  id    species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;          &lt;lgl&gt;         &lt;lgl&gt;             &lt;lgl&gt;       &lt;lgl&gt;\n1 FALSE FALSE   FALSE  FALSE          FALSE         FALSE             FALSE       FALSE\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           37.9          17.8               188        3546 male  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 female\n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              47.9          14.8               214        5028 female\n\n\nThat’s it for today. Tomorrow I’ll focus on the use of step_impute_knn().\n\n\nDay 09 - Impute missing values using step_impute_knn()\nToday, we’re focusing on imputing missing data using recipes’ step_impute_knn() function. In short, this function uses a k-nearest neighbors approach to impute missing values.\nFor today’s examples, I’m going to stick with the penguins data we’ve been using the past few days. Given this data is relatively small (n = 344), it’s a good candidate for using a k-nearest neighbor approach to imputation.\n\ndata(penguins, package = \"modeldata\")\n\n# Create a row id\npenguins &lt;- penguins |&gt;\n  mutate(id = 1:n(), .before = everything())\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ id                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 2…\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\n# Percent missing for each column\nmap(penguins, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$species\n[1] 0\n\n$island\n[1] 0\n\n$bill_length_mm\n[1] 0.005813953\n\n$bill_depth_mm\n[1] 0.005813953\n\n$flipper_length_mm\n[1] 0.005813953\n\n$body_mass_g\n[1] 0.005813953\n\n$sex\n[1] 0.03197674\n\n\nJust for a refresher, let’s peek at a few of the missing values.\n\nmissing_examples &lt;- c(4, 12, 69, 272)\n\npenguins |&gt; filter(id %in% missing_examples)\n\n# A tibble: 4 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1     4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n2    12 Adelie  Torgersen           37.8          17.3               180        3700 &lt;NA&gt;  \n3    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n4   272 Gentoo  Biscoe              NA            NA                  NA          NA &lt;NA&gt;  \n\n\nLet’s create our training and testing split.\n\nset.seed(20240109)\npenguins_split &lt;- initial_split(penguins, prop = 0.8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nSince imputation can be applied to either numeric or nominal data, step_impute_knn() uses Gower’s distance for calculating nearest neighbors (you can learn more by running ?step_impute_knn in your console). Once the neighbors are calculated, nominal variables are predicted using the mean, and numeric data is predicted using the mode. The number of neighbors can be set by specifying the neighbors argument of the function, which can also be used for hyperparameter tuning.\nLet’s start by imputing values for our missing data in the sex column. Here’s the code for this initial recipe.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(sex) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• K-nearest neighbor imputation for: sex | Trained\n\n\nBefore we bake and examine what the imputation step does, let’s drill down and see what’s occurring at the prep stage. tidy() will be used to do this. Similar to step_impute_bag(), a tibble of terms, predictors, neighbors (specific to k-nearest neighbors), and an id is returned.\n\ntidy(penguins_rec)\n\n# A tibble: 1 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_knn TRUE    FALSE impute_knn_dK6OX\n\n# Drill down into the specific impute_knn step\ntidy(penguins_rec, number = 1)\n\n# A tibble: 6 × 4\n  terms predictors        neighbors id              \n  &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;           \n1 sex   species                   5 impute_knn_dK6OX\n2 sex   island                    5 impute_knn_dK6OX\n3 sex   bill_length_mm            5 impute_knn_dK6OX\n4 sex   bill_depth_mm             5 impute_knn_dK6OX\n5 sex   flipper_length_mm         5 impute_knn_dK6OX\n6 sex   body_mass_g               5 impute_knn_dK6OX\n\n\nLet’s bake our recipe and examine the result of our imputation step.\n\nbaked_penguins &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguins\n\n# A tibble: 275 × 8\n      id species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1   168 Gentoo    Biscoe              49.3          15.7               217        5850 male  \n 2   196 Gentoo    Biscoe              49.6          15                 216        4750 male  \n 3   117 Adelie    Torgersen           38.6          17                 188        2900 female\n 4    39 Adelie    Dream               37.6          19.3               181        3300 female\n 5   299 Chinstrap Dream               43.2          16.6               187        2900 female\n 6   207 Gentoo    Biscoe              46.5          14.4               217        4900 female\n 7   167 Gentoo    Biscoe              45.8          14.6               210        4200 female\n 8   101 Adelie    Biscoe              35            17.9               192        3725 female\n 9   339 Chinstrap Dream               45.7          17                 195        3650 female\n10   267 Gentoo    Biscoe              46.2          14.1               217        4375 female\n# ℹ 265 more rows\n\nbaked_penguins |&gt; \n  filter(id %in% missing_examples) |&gt; \n  relocate(sex, .after = 1) \n\n# A tibble: 3 × 8\n     id sex    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1   272 male   Gentoo  Biscoe              NA            NA                  NA          NA\n2    69 female Adelie  Torgersen           35.9          16.6               190        3050\n3     4 male   Adelie  Torgersen           NA            NA                  NA          NA\n\n\nAs mentioned before, neighbors is an argument to set the number of neighbors to use in our estimation. The function defaults to five, but we can modify this to any integer value. It is suggested that 5 - 10 neighbors is a sensible default. However, this is dependent on the data you are working with. For our next example, let’s constrain this parameter to 3, while also applying our imputation step to all numeric predictors.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(all_numeric_predictors(), neighbors = 3) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• K-nearest neighbor imputation for: bill_length_mm and bill_depth_mm, ... | Trained\n\n\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 8\n     id species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n  &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n1   272 Gentoo  Biscoe              47.2          15.4               214        5217 &lt;NA&gt;  \n2    69 Adelie  Torgersen           35.9          16.6               190        3050 female\n3     4 Adelie  Torgersen           36.6          18.3               184        3658 &lt;NA&gt;  \n\n\nJust like step_impute_bag(), step_impute_knn() provides both an impute_with and options argument. We can be explicit about the variables to use with our knn calculations by passing a comma-separated list of names to the imp_vars() function to the impute_with arugment. options accepts a list of arguments. These get passed along to the underlying gower::gower_topn() function running under the hood, which performs the k-nearest neighbors calculation using Gower’s distance. According to the docs, the only two options accepted are:\n\nnthread - specify the number of threads to use for parallelization.\neps - optional option for variable ranges (I’m not quite sure what this does).\n\nMy assumption is these options can be used to optimize the run-time for our calculations. However, I would consult the documentation for the gower::gower_topn() function to verify. The key takeaway here is that step_impute_knn() provides an interface to configure options for the function it wraps.\nHere’s an example constraining our sex imputation to the bill_length and bill_depth_mm variables.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_knn(\n    sex, \n    impute_with = imp_vars(bill_depth_mm, bill_length_mm)\n  ) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 10 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• K-nearest neighbor imputation for: sex | Trained\n\n\nLet’s bake our final example and examine what happened.\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin |&gt; \n  filter(id %in% missing_examples) |&gt;\n  relocate(sex, .after = 1)\n\n# A tibble: 3 × 8\n     id sex    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1   272 female Gentoo  Biscoe              NA            NA                  NA          NA\n2    69 female Adelie  Torgersen           35.9          16.6               190        3050\n3     4 female Adelie  Torgersen           NA            NA                  NA          NA\n\n\nSo there you have it, another example of a step_impute_* function. Tomorrow I’ll continue exploring imputation steps by highlighting the use of the step_impute_linear() function.\n\n\nDay 10 - Impute missing values using a linear model with step_impute_linear()\nSo here we are, day 10. We continue our overview of recipes’ imputation steps. Specifically, I’m going to highlight the use of step_impute_linear() for today. step_impute_linear() uses linear regression models to impute missing data. Indeed, when there is a strong, linear relationship between a complete predictor variable and one that requires imputation (i.e., contains missing data), linear methods for imputation may be a good approach. Such a method is also really quick and requires few computational resources to calculate.\nFor today’s examples, we’re going back to our credit_data data. You can read more about this data by running ?credit_data in your console.\n\ndata(credit_data, package = \"modeldata\")\n\ncredit_data &lt;- credit_data |&gt;\n  mutate(id = seq_len(nrow(credit_data)), .before = everything()) |&gt;\n  as_tibble()\n\nglimpse(credit_data)\n\nRows: 4,454\nColumns: 15\n$ id        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 2…\n$ Status    &lt;fct&gt; good, good, bad, good, good, good, good, good, good, bad, good, good, good, good…\n$ Seniority &lt;int&gt; 9, 17, 10, 0, 0, 1, 29, 9, 0, 0, 6, 7, 8, 19, 0, 0, 15, 33, 0, 1, 2, 5, 1, 27, 2…\n$ Home      &lt;fct&gt; rent, rent, owner, rent, rent, owner, owner, parents, owner, parents, owner, own…\n$ Time      &lt;int&gt; 60, 60, 36, 60, 36, 60, 60, 12, 60, 48, 48, 36, 60, 36, 18, 24, 24, 24, 48, 60, …\n$ Age       &lt;int&gt; 30, 58, 46, 24, 26, 36, 44, 27, 32, 41, 34, 29, 30, 37, 21, 68, 52, 68, 36, 31, …\n$ Marital   &lt;fct&gt; married, widow, married, single, single, married, married, single, married, marr…\n$ Records   &lt;fct&gt; no, no, yes, no, no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no, no…\n$ Job       &lt;fct&gt; freelance, fixed, freelance, fixed, fixed, fixed, fixed, fixed, freelance, parti…\n$ Expenses  &lt;int&gt; 73, 48, 90, 63, 46, 75, 75, 35, 90, 90, 60, 60, 75, 75, 35, 75, 35, 65, 45, 35, …\n$ Income    &lt;int&gt; 129, 131, 200, 182, 107, 214, 125, 80, 107, 80, 125, 121, 199, 170, 50, 131, 330…\n$ Assets    &lt;int&gt; 0, 0, 3000, 2500, 0, 3500, 10000, 0, 15000, 0, 4000, 3000, 5000, 3500, 0, 4162, …\n$ Debt      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2500, 260, 0, 0, 0, 2000, 0, 0, 0, 0, 500, 0…\n$ Amount    &lt;int&gt; 800, 1000, 2000, 900, 310, 650, 1600, 200, 1200, 1200, 1150, 650, 1500, 600, 400…\n$ Price     &lt;int&gt; 846, 1658, 2985, 1325, 910, 1645, 1800, 1093, 1957, 1468, 1577, 915, 1650, 940, …\n\nmap(credit_data, \\(x) mean(is.na(x)))\n\n$id\n[1] 0\n\n$Status\n[1] 0\n\n$Seniority\n[1] 0\n\n$Home\n[1] 0.001347104\n\n$Time\n[1] 0\n\n$Age\n[1] 0\n\n$Marital\n[1] 0.0002245173\n\n$Records\n[1] 0\n\n$Job\n[1] 0.0004490346\n\n$Expenses\n[1] 0\n\n$Income\n[1] 0.08554109\n\n$Assets\n[1] 0.01055231\n\n$Debt\n[1] 0.004041311\n\n$Amount\n[1] 0\n\n$Price\n[1] 0\n\n\nLet’s peek at some missing data examples.\n\nmissing_examples &lt;- c(114, 195, 206, 242, 496)\n\ncredit_data |&gt; filter(id %in% missing_examples) \n\n# A tibble: 5 × 15\n     id Status Seniority Home   Time   Age Marital Records Job   Expenses Income Assets  Debt Amount\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1   114 bad            0 owner    36    39 single  no      free…       35     NA   4000     0   1000\n2   195 bad            0 other    36    48 married yes     free…       45     NA      0     0   1600\n3   206 good          10 owner    36    45 married yes     free…       60     NA   9500   250    750\n4   242 bad           10 rent     60    43 married no      free…       90     NA      0     0   1350\n5   496 bad            3 owner    60    33 separa… no      free…       35     NA   6000     0    950\n# ℹ 1 more variable: Price &lt;int&gt;\n\n\nAs mentioned before, linear imputation methods are useful in cases where you have a strong, linear relationship between a complete variable (i.e., contains no missing data) and one where imputation is to be applied. For our example, let’s use linear methods to impute values for missing data in the Income variable. The Senority variable is complete, so we’ll use it for our imputation step. Although the relationship between these two variables isn’t a strong, linear one, let’s use it for example sake.\n\n# Use corrr::correlate() to examine correlation among numeric variables\ncorrelate(credit_data)\n\nNon-numeric variables removed from input: `Status`, `Home`, `Marital`, `Records`, and `Job`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 10 × 11\n   term            id Seniority     Time     Age Expenses  Income   Assets     Debt   Amount   Price\n   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 id        NA        -0.00362  8.19e-3 -0.0165 -2.71e-1 -0.116  -0.00533 -0.00578  0.0264   0.0266\n 2 Seniority -0.00362  NA       -2.14e-2  0.506   1.26e-1  0.122   0.127   -0.0191  -0.00791  0.0409\n 3 Time       0.00819  -0.0214  NA       -0.0517 -8.40e-4 -0.0430 -0.0848   0.0577   0.431    0.130 \n 4 Age       -0.0165    0.506   -5.17e-2 NA       2.48e-1  0.147   0.185   -0.0459   0.0292   0.0489\n 5 Expenses  -0.271     0.126   -8.40e-4  0.248  NA        0.258   0.0184   0.0148   0.0492   0.0403\n 6 Income    -0.116     0.122   -4.30e-2  0.147   2.58e-1 NA       0.237    0.151    0.192    0.227 \n 7 Assets    -0.00533   0.127   -8.48e-2  0.185   1.84e-2  0.237  NA        0.191    0.147    0.200 \n 8 Debt      -0.00578  -0.0191   5.77e-2 -0.0459  1.48e-2  0.151   0.191   NA        0.0525   0.0456\n 9 Amount     0.0264   -0.00791  4.31e-1  0.0292  4.92e-2  0.192   0.147    0.0525  NA        0.725 \n10 Price      0.0266    0.0409   1.30e-1  0.0489  4.03e-2  0.227   0.200    0.0456   0.725   NA     \n\n\nWe start off by creating our testing and training split.\n\nset.seed(20240110)\ncredit_split &lt;- initial_split(credit_data, prop = .80)\n\ncredit_tr &lt;- training(credit_split)\ncredit_te &lt;- testing(credit_split)\n\n\n\n\n\n\n\nNote\n\n\n\nThe docs mention imputed variables must be of type numeric. Also, this method requires predictors to be complete cases. As such, the imputation model will only use training set predictors that don’t have any missing values.\n\n\nJust like we did with other imputation methods that use a modeling approach, we can be specific about what variables to use as predictors. We do this by passing them to the impute_with argument, which are wrapped in the imp_vars() function. Here’s what this looks like:\n\ncredit_rec &lt;- recipe(~., data = credit_tr) |&gt;\n  update_role(id, new_role = \"id\") |&gt;\n  step_impute_linear(Income, impute_with = imp_vars(Seniority)) |&gt;\n  prep(credit_tr)\n\ncredit_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 14\nid:         1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 3563 data points and 319 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Linear regression imputation for: Income | Trained\n\n\nBefore we bake our recipe, let’s take a look at what’s happening under the hood with tidy().\n\ntidy(credit_rec)\n\n# A tibble: 1 × 6\n  number operation type          trained skip  id                 \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;         &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;              \n1      1 step      impute_linear TRUE    FALSE impute_linear_5l9Ot\n\ntidy(credit_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms  model  id                 \n  &lt;chr&gt;  &lt;list&gt; &lt;chr&gt;              \n1 Income &lt;lm&gt;   impute_linear_5l9Ot\n\n\nA tibble is outputted with three columns:\n\nterms - the variable we’re seeking to replace missing values with imputed values.\nmodel - the model object used to calculate the imputed value. Note, the model object is lm.\nid - a unique id for the step being performed.\n\nReady, get set, bake.\n\ncredit_baked &lt;- bake(credit_rec, new_data = NULL)\n\n# View some of the imputed values\ncredit_baked |&gt; filter(id %in% missing_examples)\n\n# A tibble: 3 × 15\n     id Status Seniority Home   Time   Age Marital Records Job   Expenses Income Assets  Debt Amount\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n1   496 bad            3 owner    60    33 separa… no      free…       35    135   6000     0    950\n2   242 bad           10 rent     60    43 married no      free…       90    143      0     0   1350\n3   206 good          10 owner    36    45 married yes     free…       60    143   9500   250    750\n# ℹ 1 more variable: Price &lt;int&gt;\n\n\nThe recipes’ step_impute_linear() documentation also has a really good example of how to visualize the imputed data, using a dataset with complete values. It’s purpose is to show a comparison between the original values and the newly imputed values.\nThe following code is adapted from this example. Here I show the regression line used for the imputed Income values based on Seniority. It’s just a linear regression. If you’re interested in seeing the full example, run ?step_impute_linear() in your console, and scroll down to the examples section.\n\nggplot(credit_baked, aes(x = Seniority, y = Income)) +\n  geom_abline(col = \"green\") +\n  geom_point(alpha = .3) +\n  labs(title = \"Imputed Values\")\n\n\n\n\nAgain, this relationship is not a strong, linear one. Therefore, a linear imputation method of estimation may not be the best approach for this data. Nevertheless, it serves as an example of how to do it using the recipes package.\nDay 10, check. Another 20 to go. Tomorrow I’ll continue my exploration of step_impute_*() functions by highlighting the use of the step_impute_lower() function.\n\n\nDay 11 - Impute values using step_impute_lower()\nstep_impute_lower() is our focus for today. According to the docs, step_impute_lower() calculates a variable’s minimum, simulates a value between zero and the minimum, and imputes this value for any cases at the minimum value. This imputation method is useful when we have non-negative numeric data, where values cannot be measured below a known value.\nFor today’s examples, I’m going to use the modeldata package’s crickets data. This data comes from a study examining the relationship between chirp rates and temperature for two different cricket species (run ?crickets in your console to learn more).\n\ndata(crickets, package = \"modeldata\")\n\nglimpse(crickets)\n\nRows: 31\nColumns: 3\n$ species &lt;fct&gt; O. exclamationis, O. exclamationis, O. exclamationis, O. exclamationis, O. exclama…\n$ temp    &lt;dbl&gt; 20.8, 20.8, 24.0, 24.0, 24.0, 24.0, 26.2, 26.2, 26.2, 26.2, 28.4, 29.0, 30.4, 30.4…\n$ rate    &lt;dbl&gt; 67.9, 65.1, 77.3, 78.7, 79.4, 80.4, 85.8, 86.6, 87.5, 89.1, 98.6, 100.8, 99.3, 101…\n\nskim(crickets)\n\n\nData summary\n\n\nName\ncrickets\n\n\nNumber of rows\n31\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1\nFALSE\n2\nO. : 17, O. : 14\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntemp\n0\n1\n23.76\n3.82\n17.2\n20.80\n24.0\n26.35\n30.4\n▆▆▆▇▅\n\n\nrate\n0\n1\n72.89\n16.91\n44.3\n59.45\n76.2\n85.25\n101.7\n▅▅▇▆▃\n\n\n\n\n\nWe’re also going to modify the data a bit to better highlight what step_impute_lower() is doing. Here I’m just truncating all temp values less than or equal to 21 to 21.\n\n# Create a floor at temp = 21\ncrickets &lt;- crickets |&gt; \n  mutate(temp = case_when(\n    temp &lt;= 21 ~ 21,\n    TRUE ~ as.double(temp)\n  ))\n\nprint(crickets, n = 50)\n\n# A tibble: 31 × 3\n   species           temp  rate\n   &lt;fct&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 O. exclamationis  21    67.9\n 2 O. exclamationis  21    65.1\n 3 O. exclamationis  24    77.3\n 4 O. exclamationis  24    78.7\n 5 O. exclamationis  24    79.4\n 6 O. exclamationis  24    80.4\n 7 O. exclamationis  26.2  85.8\n 8 O. exclamationis  26.2  86.6\n 9 O. exclamationis  26.2  87.5\n10 O. exclamationis  26.2  89.1\n11 O. exclamationis  28.4  98.6\n12 O. exclamationis  29   101. \n13 O. exclamationis  30.4  99.3\n14 O. exclamationis  30.4 102. \n15 O. niveus         21    44.3\n16 O. niveus         21    47.2\n17 O. niveus         21    47.6\n18 O. niveus         21    49.6\n19 O. niveus         21    50.3\n20 O. niveus         21    51.8\n21 O. niveus         21    60  \n22 O. niveus         21    58.5\n23 O. niveus         21    58.9\n24 O. niveus         22.1  60.7\n25 O. niveus         23.5  69.8\n26 O. niveus         24.2  70.9\n27 O. niveus         25.9  76.2\n28 O. niveus         26.5  76.1\n29 O. niveus         26.5  77  \n30 O. niveus         26.5  77.7\n31 O. niveus         28.6  84.7\n\n\nFor simplicity, I’m not going to create a testing and training split and will use the full data for our example. Let’s start with our recipe.\n\ncrickets_rec &lt;- recipe(~., data = crickets) |&gt;\n  step_impute_lower(temp) |&gt;\n  prep()\n\ncrickets_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 31 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Lower bound imputation for: temp | Trained\n\n\nLet’s take a quick peek at what’s happening under the hood by tidying our recipe object.\n\ntidy(crickets_rec)\n\n# A tibble: 1 × 6\n  number operation type         trained skip  id                \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;             \n1      1 step      impute_lower TRUE    FALSE impute_lower_h7llz\n\ntidy(crickets_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms value id                \n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;             \n1 temp     21 impute_lower_h7llz\n\n\nYou’ll notice the value column in the tibble represents the minimum value within the column. Using this value, step_impute_lower() will replace these values with any value between 0 and 21. Let’s bake this recipe and verify this is the case.\n\nbaked_crickets &lt;- bake(crickets_rec, new_data = NULL)\n\nprint(baked_crickets, n = 50)\n\n# A tibble: 31 × 3\n   species             temp  rate\n   &lt;fct&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 O. exclamationis  7.84    67.9\n 2 O. exclamationis 20.1     65.1\n 3 O. exclamationis 24       77.3\n 4 O. exclamationis 24       78.7\n 5 O. exclamationis 24       79.4\n 6 O. exclamationis 24       80.4\n 7 O. exclamationis 26.2     85.8\n 8 O. exclamationis 26.2     86.6\n 9 O. exclamationis 26.2     87.5\n10 O. exclamationis 26.2     89.1\n11 O. exclamationis 28.4     98.6\n12 O. exclamationis 29      101. \n13 O. exclamationis 30.4     99.3\n14 O. exclamationis 30.4    102. \n15 O. niveus         5.51    44.3\n16 O. niveus         7.75    47.2\n17 O. niveus         6.99    47.6\n18 O. niveus         8.92    49.6\n19 O. niveus        15.2     50.3\n20 O. niveus         0.0919  51.8\n21 O. niveus        13.6     60  \n22 O. niveus        12.0     58.5\n23 O. niveus        10.0     58.9\n24 O. niveus        22.1     60.7\n25 O. niveus        23.5     69.8\n26 O. niveus        24.2     70.9\n27 O. niveus        25.9     76.2\n28 O. niveus        26.5     76.1\n29 O. niveus        26.5     77  \n30 O. niveus        26.5     77.7\n31 O. niveus        28.6     84.7\n\n\nGreat! All values of 21 have now been imputed with a value between 0 and the minimum value for the column, 21.\nWe can also create a plot to highlight what step_impute_lower() is doing. This approach is adapted from the example in the docs (?step_impute_lower()).\n\nplot(baked_crickets$temp, crickets$temp,\n  ylab = \"pre-imputation\", xlab = \"imputed\"\n)\n\n\n\n\nstep_impute_lower() is pretty straightforward in my opinion. Give it a try. That’s it for today. A short one. Tomorrow is our final step_impute_*() function, step_impute_roll().\n\n\nDay 12 - Impute values using a rolling window via step_impute_roll()\nWe’re going to round out our discussion of step_impute_*() functions by highlighting the use of step_impute_roll(). step_impute_roll() utilizes window functions to calculate missing values. At first, I had trouble understanding how window functions are utilized. However, seeing a simplified example helped me better understand what was occurring.\nLet’s get some data for today’s examples. I’m going back to our obfuscated Google Analytics data from the Google Merchandise store for today’s examples.\n\ndata_ga &lt;- \n  read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_google_merch.csv\")\n  ) |&gt;\n  mutate(\n    event_date = ymd(event_date),\n    revenue = price_in_usd * quantity\n  ) \n\nRows: 9365 Columns: 14\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): item_name, item_category, shipping_tier, payment_type, category, country, region, city\ndbl (6): event_date, purchase_revenue_in_usd, transaction_id, price_in_usd, quantity, item_reven...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data_ga)\n\nRows: 9,365\nColumns: 15\n$ event_date              &lt;date&gt; 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-12-01, 2020-…\n$ purchase_revenue_in_usd &lt;dbl&gt; 40, 40, 40, 40, 40, 40, 40, 62, 62, 44, 28, 28, 36, 36, 36, 36, 92…\n$ transaction_id          &lt;dbl&gt; 10648, 10648, 10648, 10648, 10648, 10648, 10648, 171491, 171491, 1…\n$ item_name               &lt;chr&gt; \"Google Hemp Tote\", \"Android SM S/F18 Sticker Sheet\", \"Android Buo…\n$ item_category           &lt;chr&gt; \"Clearance\", \"Accessories\", \"Drinkware\", \"Small Goods\", \"Office\", …\n$ price_in_usd            &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 7, 92, 7, 14,…\n$ quantity                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 53, 1, 1, 1, 1,…\n$ item_revenue_in_usd     &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, …\n$ shipping_tier           &lt;chr&gt; \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"FedEx Ground\", \"F…\n$ payment_type            &lt;chr&gt; \"Pay with credit card\", \"Pay with credit card\", \"Pay with credit c…\n$ category                &lt;chr&gt; \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobile\", \"mobil…\n$ country                 &lt;chr&gt; \"United States\", \"United States\", \"United States\", \"United States\"…\n$ region                  &lt;chr&gt; \"California\", \"California\", \"California\", \"California\", \"Californi…\n$ city                    &lt;chr&gt; \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San Jose\", \"San J…\n$ revenue                 &lt;dbl&gt; 12, 2, 4, 2, 3, 3, 14, 48, 14, 44, 14, 14, 1, 4, 16, 14, 92, 371, …\n\nskim(data_ga)\n\n\nData summary\n\n\nName\ndata_ga\n\n\nNumber of rows\n9365\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nDate\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nitem_name\n0\n1.00\n8\n41\n0\n385\n0\n\n\nitem_category\n164\n0.98\n3\n23\n0\n20\n0\n\n\nshipping_tier\n109\n0.99\n10\n22\n0\n13\n0\n\n\npayment_type\n0\n1.00\n20\n20\n0\n1\n0\n\n\ncategory\n0\n1.00\n6\n7\n0\n3\n0\n\n\ncountry\n0\n1.00\n4\n20\n0\n96\n0\n\n\nregion\n0\n1.00\n4\n52\n0\n289\n0\n\n\ncity\n0\n1.00\n4\n24\n0\n434\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nevent_date\n0\n1\n2020-12-01\n2021-01-30\n2020-12-16\n61\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npurchase_revenue_in_usd\n0\n1\n101.84\n118.06\n2\n39\n72\n125\n1530\n▇▁▁▁▁\n\n\ntransaction_id\n0\n1\n487977.44\n282873.71\n546\n249662\n479506\n724658\n999850\n▇▇▇▇▇\n\n\nprice_in_usd\n0\n1\n19.52\n18.74\n1\n7\n14\n24\n120\n▇▂▁▁▁\n\n\nquantity\n0\n1\n1.45\n2.77\n1\n1\n1\n1\n160\n▇▁▁▁▁\n\n\nitem_revenue_in_usd\n0\n1\n23.26\n28.61\n1\n8\n15\n30\n704\n▇▁▁▁▁\n\n\nrevenue\n0\n1\n23.26\n28.58\n1\n8\n15\n30\n704\n▇▁▁▁▁\n\n\n\n\n\nTo make it clear what step_impute_roll() is doing, I’m going to wrangle the data to only look at total revenue for Clearance item purchases for a specific date range (2 weeks). Since this data is complete, I will introduce some missing values into the data.\n\ndata_ga &lt;- data_ga |&gt;\n  select(event_date, transaction_id, item_category, revenue) |&gt;\n  filter(item_category == \"Clearance\") |&gt;\n  group_by(event_date) |&gt;\n  summarise(total_rev = sum(revenue))  |&gt;\n  filter(\n    event_date &gt;= as_date('2020-12-14') & \n    event_date &lt;= as_date('2020-12-27')\n  )\n\n# Introduce some NAs for the example\ndata_ga$total_rev[c(1, 6, 7, 8, 14)] &lt;- NA\n\nLet’s get our recipe set up to the point of prepping it.\n\nga_rec &lt;- recipe(~., data = data_ga) |&gt;\n  update_role(event_date, new_role = \"data_ref\") |&gt;\n  step_impute_roll(total_rev, window = 3) |&gt;\n  prep()\n\nga_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\ndata_ref:  1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 14 data points and 5 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Rolling imputation for: total_rev | Trained\n\ntidy(ga_rec)\n\n# A tibble: 1 × 6\n  number operation type        trained skip  id               \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;            \n1      1 step      impute_roll TRUE    FALSE impute_roll_6HJg3\n\ntidy(ga_rec, number = 1)\n\n# A tibble: 1 × 3\n  terms     window id               \n  &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;            \n1 total_rev      3 impute_roll_6HJg3\n\n\nNot too much useful information here. Let’s move forward with bake and see the imputed values. Take note, we will still have a missing value (more on this later).\n\nbake(ga_rec, new_data = NULL)\n\n# A tibble: 14 × 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14      324.\n 2 2020-12-15      323 \n 3 2020-12-16      324 \n 4 2020-12-17      203 \n 5 2020-12-18      215 \n 6 2020-12-19      215 \n 7 2020-12-20       NA \n 8 2020-12-21       55 \n 9 2020-12-22       55 \n10 2020-12-23      256 \n11 2020-12-24       32 \n12 2020-12-25       73 \n13 2020-12-26       19 \n14 2020-12-27       46 \n\n\nWe now have a complete data set. But, how were these imputed values calculated? If you peek at step_impute_roll()’s arguments, you’ll notice it contains a statistic argument set to median. It should be pretty intuitive that we are calculating the median here, but the median of what? It’s the median of our window we set in the function, which was window = 3.\n\nargs(step_impute_roll)\n\nfunction (recipe, ..., role = NA, trained = FALSE, columns = NULL, \n    statistic = median, window = 5, skip = FALSE, id = rand_id(\"impute_roll\")) \nNULL\n\n\nWe need to be aware of some important notes regarding our calcuation of the median here. Let’s put our baked data side-by-side with our old data, just so it’s easier to see what was imputed and how it was calculated.\n\nbaked_ga &lt;- bake(ga_rec, new_data = NULL) |&gt;\n  rename(new_rev = total_rev) |&gt;\n  left_join(\n    data_ga |&gt; rename(old_rev = total_rev)\n  )\n\nJoining with `by = join_by(event_date)`\n\n\nLet’s start with the tails, the missing values at row 1 and 14. Since these values lack a value above and below, they default to using the series 1:3 and 12:14 for imputation. When making the calculation for the window, only complete values are passed to the calculation. Take for example the missing value at row 1. The 324 imputed value comes from calculating the median between 323 and 324.\n\n# We're rounding up here for the imputed value \nmedian(c(323, 324))\n\n[1] 323.5\n\n\nThe same calculation is being done for the 14th value, which looks like this:\n\nmedian(c(73, 19))\n\n[1] 46\n\n\nThis brings up the interesting case for the imputed NA value at row 7, or the lack there of. Since all the values within the window are NA, an NA is imputed (i.e., you can’t calculate a median with no known values). To fix this, we would need to expand our window to include more values. We do this by setting window = 5 within the function.\n\nbaked_ga &lt;- recipe(~., data = data_ga) |&gt;\n  update_role(event_date, new_role = \"date_ref\") |&gt;\n  step_impute_roll(total_rev, window = 5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# Now we have complete values, since we expanded the window\nbaked_ga\n\n# A tibble: 14 × 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14     269  \n 2 2020-12-15     323  \n 3 2020-12-16     324  \n 4 2020-12-17     203  \n 5 2020-12-18     215  \n 6 2020-12-19     209  \n 7 2020-12-20     135  \n 8 2020-12-21     156. \n 9 2020-12-22      55  \n10 2020-12-23     256  \n11 2020-12-24      32  \n12 2020-12-25      73  \n13 2020-12-26      19  \n14 2020-12-27      52.5\n\n\nWhat about other functions to calculate values? To do this, step_impute_roll() has the statistic argument. Say instead of the median we want to calculate our imputed value using the mean. We just do the following:\n\nga_mean_rec &lt;- recipe(~., data_ga) |&gt;\n  update_role(event_date, new_role = \"ref_date\") |&gt;\n  step_impute_roll(total_rev, window = 5, statistic = mean) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nga_mean_rec\n\n# A tibble: 14 × 2\n   event_date total_rev\n   &lt;date&gt;         &lt;dbl&gt;\n 1 2020-12-14      266.\n 2 2020-12-15      323 \n 3 2020-12-16      324 \n 4 2020-12-17      203 \n 5 2020-12-18      215 \n 6 2020-12-19      209 \n 7 2020-12-20      135 \n 8 2020-12-21      156.\n 9 2020-12-22       55 \n10 2020-12-23      256 \n11 2020-12-24       32 \n12 2020-12-25       73 \n13 2020-12-26       19 \n14 2020-12-27       95 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAccording to the docs, the statistic function you use should:\n\ncontain a single argument for the data to compute the imputed value.\nreturn a double precision value.\n\nThey also note only complete values will be passed to the functions specified with the statistic argument.\n\n\nThat’s a wrap for step_imputation_*() functions. Next we’re going to focus on step functions to decorrelate predictors.\n\n\nDay 13 - Use step_corr() to remove highly correlated predictors\nStarting today, I’m going to begin highlighting step_*() functions useful for performing decorrelation preprocessing steps. These include steps to filter out highly correlated predictors, using principal component analysis, or other model-based methods.\nI’m starting out simple here by focusing on the use of step_corr(). According to the docs, step_corr() will\n\npotentially remove variables that have large absolute correations with other variables.\n\nUsing some threshold value, step_corr() will identify column combinations where a minimum number of columns are removed and all the absolute correlations between columns are below a specified threshold.\n\n\n\n\n\n\nImportant\n\n\n\nstep_corr() will potentially remove columns.\n\n\nBefore using step_corr() let’s highlight some of the important arguments of the function.\n\nthreshold - used as the absolute correlation cut off value. The function defaults to .9. This is the only tuning parameter for the function.\nmethod - the method used to make correlation calculations, defaults to pearson.\n\nLet’s get our data together. For today’s example, I’m going back to our penguins data. You can learn more about this data by running ?penguins in your console or by reviewing my past examples using this data.\n\ndata(penguins, package = \"modeldata\")\n\npenguins\n\n# A tibble: 344 × 7\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie  Torgersen           39.1          18.7               181        3750 male  \n 2 Adelie  Torgersen           39.5          17.4               186        3800 female\n 3 Adelie  Torgersen           40.3          18                 195        3250 female\n 4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193        3450 female\n 6 Adelie  Torgersen           39.3          20.6               190        3650 male  \n 7 Adelie  Torgersen           38.9          17.8               181        3625 female\n 8 Adelie  Torgersen           39.2          19.6               195        4675 male  \n 9 Adelie  Torgersen           34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n\nglimpse(penguins)\n\nRows: 344\nColumns: 7\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, …\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torger…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0, 37.8, 37.8, 41…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2, 17.1, 17.3, 17…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186, 180, 182, 191, 198…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, 4250, 3300, 3700, 32…\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male, NA, NA, NA, NA, fe…\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\n\n\n\nNow we create our testing training split.\n\nset.seed(20240113)\npenguins_split &lt;- initial_split(penguins, prop = .8)\n\npenguins_tr &lt;- training(penguins_split)\npenguins_te &lt;- testing(penguins_split)\n\nQuickly, let’s use corrr::correlate() to explore correlations between variables in our training data. The code looks like this:\n\ncorrelate(penguins_tr)\n\nNon-numeric variables removed from input: `species`, `island`, and `sex`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 4 × 5\n  term              bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;                      &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 bill_length_mm            NA            -0.229             0.646       0.591\n2 bill_depth_mm             -0.229        NA                -0.582      -0.477\n3 flipper_length_mm          0.646        -0.582            NA           0.872\n4 body_mass_g                0.591        -0.477             0.872      NA    \n\n\nYou’ll notice a highly positive correlation between flipper_length_mm and body_mass_g (.872). Although I’m not making a causal argument here, it would seem feasible to assume that as a penguin’s flippers get longer, their body mass would increase. The question now is, if we were using this data to train a model, would the inclusion of both these variables improve our model? Or, could we simplify model estimation by eliminating one of these variables? Perhaps we can achieve the same amount of accuracy while also specifying the most parsimonious model.\nHere we’ll create our recipe to address these two, highly correlated variables within our data.\n\npenguins_rec &lt;- recipe(~., data = penguins_tr) |&gt;\n  step_corr(all_numeric_predictors(), threshold = .8) |&gt;\n  prep()\n\npenguins_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 7\n\n\n\n\n\n── Training information \n\n\nTraining data contained 275 data points and 11 incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Correlation filter on: flipper_length_mm | Trained\n\n\nThe prepped recipe now tells us the flipper_length_mm variable will be removed once we bake the data. This is a good thing to keep an eye on, verifying the step removed expected columns.\nWhen you drill down into the step using tidy(), you’ll notice a tibble that highlights what column will be removed once we bake() the recipe.\n\ntidy(penguins_rec)\n\n# A tibble: 1 × 6\n  number operation type  trained skip  id        \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt; &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;     \n1      1 step      corr  TRUE    FALSE corr_q9cC1\n\ntidy(penguins_rec, number = 1)\n\n# A tibble: 1 × 2\n  terms             id        \n  &lt;chr&gt;             &lt;chr&gt;     \n1 flipper_length_mm corr_q9cC1\n\n\n\nbaked_penguin &lt;- bake(penguins_rec, new_data = NULL)\n\nbaked_penguin\n\n# A tibble: 275 × 6\n   species island    bill_length_mm bill_depth_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Gentoo  Biscoe              50.5          15.9        5400 male  \n 2 Gentoo  Biscoe              NA            NA            NA &lt;NA&gt;  \n 3 Gentoo  Biscoe              50            15.3        5550 male  \n 4 Adelie  Torgersen           40.2          17          3450 female\n 5 Adelie  Biscoe              40.5          17.9        3200 female\n 6 Adelie  Torgersen           37.8          17.1        3300 &lt;NA&gt;  \n 7 Gentoo  Biscoe              48.2          14.3        4600 female\n 8 Gentoo  Biscoe              55.1          16          5850 male  \n 9 Adelie  Dream               37.2          18.1        3900 male  \n10 Gentoo  Biscoe              48.7          15.7        5350 male  \n# ℹ 265 more rows\n\n\nYou can verify this highly correlated variable is mitigated by checking out the correlations between variables in our baked data set:\n\ncorrelate(baked_penguin)\n\nNon-numeric variables removed from input: `species`, `island`, and `sex`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 4\n  term           bill_length_mm bill_depth_mm body_mass_g\n  &lt;chr&gt;                   &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 bill_length_mm         NA            -0.229       0.591\n2 bill_depth_mm          -0.229        NA          -0.477\n3 body_mass_g             0.591        -0.477      NA    \n\n\nSo there you have it, our first step_*() function to perform decorrelation preprocessing. Tomorrow I’ll be focusing on step_pca(), which will use principal components analysis to manage correlations among predictors.\n\n\nDay 14 - Perform dimension reduction with step_pca()\nFor today, I’m overviewing step_pca(). step_pca() performs dimension reduction using principal components analysis. Dimension reduction seeks to combine predictors into latent predictors, while also retaining as much information from the full data set as possible. Principal component analysis can seem like an advanced topic, but the @statquest YouTube channel has a good step-by-step video on how it’s performed, in general terms.\nWe’ll use the mlc_churn data from the ‘modeldata’ package for today’s examples. This data is an artificial data set representing customer churn (i.e., the loss of customers to a service). You can learn more about this data by running ?mlc_churn within your console. But here’s some basic data exploration code to get a sense of what’s included within the data.\n\ndata(mlc_churn, package = \"modeldata\")\n\nglimpse(mlc_churn)\n\nRows: 5,000\nColumns: 20\n$ state                         &lt;fct&gt; KS, OH, NJ, OH, OK, AL, MA, MO, LA, WV, IN, RI, IA, MT, IA, …\n$ account_length                &lt;int&gt; 128, 107, 137, 84, 75, 118, 121, 147, 117, 141, 65, 74, 168,…\n$ area_code                     &lt;fct&gt; area_code_415, area_code_415, area_code_415, area_code_408, …\n$ international_plan            &lt;fct&gt; no, no, no, yes, yes, yes, no, yes, no, yes, no, no, no, no,…\n$ voice_mail_plan               &lt;fct&gt; yes, yes, no, no, no, no, yes, no, no, yes, no, no, no, no, …\n$ number_vmail_messages         &lt;int&gt; 25, 26, 0, 0, 0, 0, 24, 0, 0, 37, 0, 0, 0, 0, 0, 0, 27, 0, 3…\n$ total_day_minutes             &lt;dbl&gt; 265.1, 161.6, 243.4, 299.4, 166.7, 223.4, 218.2, 157.0, 184.…\n$ total_day_calls               &lt;int&gt; 110, 123, 114, 71, 113, 98, 88, 79, 97, 84, 137, 127, 96, 88…\n$ total_day_charge              &lt;dbl&gt; 45.07, 27.47, 41.38, 50.90, 28.34, 37.98, 37.09, 26.69, 31.3…\n$ total_eve_minutes             &lt;dbl&gt; 197.4, 195.5, 121.2, 61.9, 148.3, 220.6, 348.5, 103.1, 351.6…\n$ total_eve_calls               &lt;int&gt; 99, 103, 110, 88, 122, 101, 108, 94, 80, 111, 83, 148, 71, 7…\n$ total_eve_charge              &lt;dbl&gt; 16.78, 16.62, 10.30, 5.26, 12.61, 18.75, 29.62, 8.76, 29.89,…\n$ total_night_minutes           &lt;dbl&gt; 244.7, 254.4, 162.6, 196.9, 186.9, 203.9, 212.6, 211.8, 215.…\n$ total_night_calls             &lt;int&gt; 91, 103, 104, 89, 121, 118, 118, 96, 90, 97, 111, 94, 128, 1…\n$ total_night_charge            &lt;dbl&gt; 11.01, 11.45, 7.32, 8.86, 8.41, 9.18, 9.57, 9.53, 9.71, 14.6…\n$ total_intl_minutes            &lt;dbl&gt; 10.0, 13.7, 12.2, 6.6, 10.1, 6.3, 7.5, 7.1, 8.7, 11.2, 12.7,…\n$ total_intl_calls              &lt;int&gt; 3, 3, 5, 7, 3, 6, 7, 6, 4, 5, 6, 5, 2, 5, 6, 9, 4, 3, 5, 2, …\n$ total_intl_charge             &lt;dbl&gt; 2.70, 3.70, 3.29, 1.78, 2.73, 1.70, 2.03, 1.92, 2.35, 3.02, …\n$ number_customer_service_calls &lt;int&gt; 1, 1, 0, 2, 3, 0, 3, 0, 1, 0, 4, 0, 1, 3, 4, 4, 1, 3, 1, 1, …\n$ churn                         &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, yes, no, no, no, no,…\n\nskim(mlc_churn)\n\n\nData summary\n\n\nName\nmlc_churn\n\n\nNumber of rows\n5000\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nstate\n0\n1\nFALSE\n51\nWV: 158, MN: 125, AL: 124, ID: 119\n\n\narea_code\n0\n1\nFALSE\n3\nare: 2495, are: 1259, are: 1246\n\n\ninternational_plan\n0\n1\nFALSE\n2\nno: 4527, yes: 473\n\n\nvoice_mail_plan\n0\n1\nFALSE\n2\nno: 3677, yes: 1323\n\n\nchurn\n0\n1\nFALSE\n2\nno: 4293, yes: 707\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\naccount_length\n0\n1\n100.26\n39.69\n1\n73.00\n100.00\n127.00\n243.00\n▂▇▇▂▁\n\n\nnumber_vmail_messages\n0\n1\n7.76\n13.55\n0\n0.00\n0.00\n17.00\n52.00\n▇▁▂▁▁\n\n\ntotal_day_minutes\n0\n1\n180.29\n53.89\n0\n143.70\n180.10\n216.20\n351.50\n▁▃▇▅▁\n\n\ntotal_day_calls\n0\n1\n100.03\n19.83\n0\n87.00\n100.00\n113.00\n165.00\n▁▁▇▇▁\n\n\ntotal_day_charge\n0\n1\n30.65\n9.16\n0\n24.43\n30.62\n36.75\n59.76\n▁▃▇▅▁\n\n\ntotal_eve_minutes\n0\n1\n200.64\n50.55\n0\n166.38\n201.00\n234.10\n363.70\n▁▂▇▅▁\n\n\ntotal_eve_calls\n0\n1\n100.19\n19.83\n0\n87.00\n100.00\n114.00\n170.00\n▁▁▇▇▁\n\n\ntotal_eve_charge\n0\n1\n17.05\n4.30\n0\n14.14\n17.09\n19.90\n30.91\n▁▂▇▅▁\n\n\ntotal_night_minutes\n0\n1\n200.39\n50.53\n0\n166.90\n200.40\n234.70\n395.00\n▁▃▇▃▁\n\n\ntotal_night_calls\n0\n1\n99.92\n19.96\n0\n87.00\n100.00\n113.00\n175.00\n▁▁▇▆▁\n\n\ntotal_night_charge\n0\n1\n9.02\n2.27\n0\n7.51\n9.02\n10.56\n17.77\n▁▃▇▃▁\n\n\ntotal_intl_minutes\n0\n1\n10.26\n2.76\n0\n8.50\n10.30\n12.00\n20.00\n▁▃▇▃▁\n\n\ntotal_intl_calls\n0\n1\n4.44\n2.46\n0\n3.00\n4.00\n6.00\n20.00\n▇▅▁▁▁\n\n\ntotal_intl_charge\n0\n1\n2.77\n0.75\n0\n2.30\n2.78\n3.24\n5.40\n▁▃▇▃▁\n\n\nnumber_customer_service_calls\n0\n1\n1.57\n1.31\n0\n1.00\n1.00\n2.00\n9.00\n▇▅▁▁▁\n\n\n\n\n\nLet’s create our training and testing split.\n\nset.seed(20240114)\nchurn_split &lt;- initial_split(mlc_churn, prop = .8)\n\nchurn_tr &lt;- training(churn_split)\nchurn_te &lt;- testing(churn_split)\n\nBefore we apply the step_pca() function, we need to first center and scale our data. To do this, we’ll first use step_normalize() on all numeric variables within the data set. Normalizing will place all numeric data on the same scale, which is required step to complete before performing principal components analysis.\nPost center and scaling of our variables, we’ll use step_pca() on all_numeric() predictors. We use the num_comp = 3 argument to specify how many components will be outputted from our principal components analysis.\nLet’s prep() and tidy() our recipe to peek under the hood to get a better sense of what’s happening with this recipe step.\n\nchurn_rec &lt;- recipe(churn ~ ., data = churn_tr) |&gt;\n  update_role(state, area_code, new_role = \"ref_var\") |&gt;\n  update_role(ends_with(\"plan\"), new_role = \"plan_var\") |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric_predictors(), num_comp = 3) |&gt;\n  prep()\n\nchurn_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 15\nplan_var:   2\nref_var:    2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4000 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: account_length and number_vmail_messages, ... | Trained\n\n\n• PCA extraction with: account_length, number_vmail_messages, total_day_minutes, ... | Trained\n\n\nIt’s also important to point out that the tidy() method for step_pca() has two type options:\n\ntype = \"coef\"\ntype = \"variance\"\n\nEach of these options modify what gets printed to the console. I’ve added some comments below on what gets outputted for each.\n\ntidy(churn_rec)\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_7hokR\n2      2 step      pca       TRUE    FALSE pca_BXk23      \n\n# Output the variable loadings for each component\ntidy(churn_rec, number = 2, type = \"coef\")\n\n# A tibble: 225 × 4\n   terms                     value component id       \n   &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 account_length         0.0192   PC1       pca_BXk23\n 2 number_vmail_messages -0.00296  PC1       pca_BXk23\n 3 total_day_minutes      0.312    PC1       pca_BXk23\n 4 total_day_calls       -0.000110 PC1       pca_BXk23\n 5 total_day_charge       0.312    PC1       pca_BXk23\n 6 total_eve_minutes     -0.462    PC1       pca_BXk23\n 7 total_eve_calls        0.0230   PC1       pca_BXk23\n 8 total_eve_charge      -0.462    PC1       pca_BXk23\n 9 total_night_minutes    0.426    PC1       pca_BXk23\n10 total_night_calls      0.00724  PC1       pca_BXk23\n# ℹ 215 more rows\n\n# Output the variance each component accounts for\ntidy(churn_rec, number = 2, type = \"variance\")\n\n# A tibble: 60 × 4\n   terms    value component id       \n   &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 2.07          1 pca_BXk23\n 2 variance 2.02          2 pca_BXk23\n 3 variance 1.98          3 pca_BXk23\n 4 variance 1.94          4 pca_BXk23\n 5 variance 1.06          5 pca_BXk23\n 6 variance 1.04          6 pca_BXk23\n 7 variance 1.01          7 pca_BXk23\n 8 variance 0.988         8 pca_BXk23\n 9 variance 0.977         9 pca_BXk23\n10 variance 0.965        10 pca_BXk23\n# ℹ 50 more rows\n\n\nGreat, let’s bake our recipe. You’ll notice step_pca() reduced all numeric data into our three components, PC1, PC2, and PC3.\n\nbake(churn_rec, new_data = NULL)\n\n# A tibble: 4,000 × 8\n   state area_code     international_plan voice_mail_plan churn    PC1     PC2    PC3\n   &lt;fct&gt; &lt;fct&gt;         &lt;fct&gt;              &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 IN    area_code_408 no                 no              no    -0.358  1.03   -1.30 \n 2 AZ    area_code_415 no                 yes             no     3.04   2.20   -1.28 \n 3 IL    area_code_415 no                 no              no     1.17  -2.36    0.697\n 4 IN    area_code_415 no                 no              no    -0.127  0.811   1.14 \n 5 FL    area_code_415 no                 no              no     0.248  2.06   -0.311\n 6 NM    area_code_510 yes                no              no    -1.28   0.0720  1.02 \n 7 DE    area_code_415 no                 no              no    -1.95   0.250  -0.553\n 8 SD    area_code_408 no                 no              no     0.624  1.36   -0.344\n 9 DE    area_code_510 no                 no              no    -1.13  -0.663  -0.804\n10 CT    area_code_415 no                 yes             no    -2.60   0.817   0.939\n# ℹ 3,990 more rows\n\n\nConstraining by component works, but step_pca() also provides a threshold argument. In other words, we can specify the total variance that should be covered by components, and step_pca() will create the required number of components based on this threshold. Let’s say we want our PCA to create a number of components to cover 70% of the variability in our variables. We can do this by using the following recipe code:\n\nchurn_rec_thresh &lt;- recipe(churn ~ ., data = churn_tr) |&gt;\n  update_role(state, area_code, new_role = \"ref_var\") |&gt;\n  update_role(ends_with(\"plan\"), new_role = \"plan_var\") |&gt;\n  step_normalize(all_numeric()) |&gt;\n  step_pca(all_numeric_predictors(), threshold = .7) |&gt;\n  prep()\n\nchurn_rec_thresh\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 15\nplan_var:   2\nref_var:    2\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4000 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: account_length and number_vmail_messages, ... | Trained\n\n\n• PCA extraction with: account_length, number_vmail_messages, total_day_minutes, ... | Trained\n\n\nLet’s tidy() our recipe to see what’s happening here.\n\ntidy(churn_rec_thresh)\n\n# A tibble: 2 × 6\n  number operation type      trained skip  id             \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          \n1      1 step      normalize TRUE    FALSE normalize_8WvPy\n2      2 step      pca       TRUE    FALSE pca_Aj4UJ      \n\n# variable loadings\ntidy(churn_rec_thresh, number = 2, type = \"coef\")\n\n# A tibble: 225 × 4\n   terms                     value component id       \n   &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 account_length         0.0192   PC1       pca_Aj4UJ\n 2 number_vmail_messages -0.00296  PC1       pca_Aj4UJ\n 3 total_day_minutes      0.312    PC1       pca_Aj4UJ\n 4 total_day_calls       -0.000110 PC1       pca_Aj4UJ\n 5 total_day_charge       0.312    PC1       pca_Aj4UJ\n 6 total_eve_minutes     -0.462    PC1       pca_Aj4UJ\n 7 total_eve_calls        0.0230   PC1       pca_Aj4UJ\n 8 total_eve_charge      -0.462    PC1       pca_Aj4UJ\n 9 total_night_minutes    0.426    PC1       pca_Aj4UJ\n10 total_night_calls      0.00724  PC1       pca_Aj4UJ\n# ℹ 215 more rows\n\n# variance accounted for\ntidy(churn_rec_thresh, number = 2, type = \"variance\")\n\n# A tibble: 60 × 4\n   terms    value component id       \n   &lt;chr&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;    \n 1 variance 2.07          1 pca_Aj4UJ\n 2 variance 2.02          2 pca_Aj4UJ\n 3 variance 1.98          3 pca_Aj4UJ\n 4 variance 1.94          4 pca_Aj4UJ\n 5 variance 1.06          5 pca_Aj4UJ\n 6 variance 1.04          6 pca_Aj4UJ\n 7 variance 1.01          7 pca_Aj4UJ\n 8 variance 0.988         8 pca_Aj4UJ\n 9 variance 0.977         9 pca_Aj4UJ\n10 variance 0.965        10 pca_Aj4UJ\n# ℹ 50 more rows\n\n\nNow, we’ll bake our churn_rec_thresh recipe.\n\nbake(churn_rec_thresh, new_data = NULL)\n\n# A tibble: 4,000 × 12\n   state area_code     international_plan voice_mail_plan churn    PC1     PC2    PC3    PC4    PC5\n   &lt;fct&gt; &lt;fct&gt;         &lt;fct&gt;              &lt;fct&gt;           &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 IN    area_code_408 no                 no              no    -0.358  1.03   -1.30   0.613  0.402\n 2 AZ    area_code_415 no                 yes             no     3.04   2.20   -1.28   1.06   1.04 \n 3 IL    area_code_415 no                 no              no     1.17  -2.36    0.697  1.05   0.218\n 4 IN    area_code_415 no                 no              no    -0.127  0.811   1.14   1.14   0.177\n 5 FL    area_code_415 no                 no              no     0.248  2.06   -0.311 -2.28  -1.20 \n 6 NM    area_code_510 yes                no              no    -1.28   0.0720  1.02   1.05  -1.12 \n 7 DE    area_code_415 no                 no              no    -1.95   0.250  -0.553  1.80  -0.712\n 8 SD    area_code_408 no                 no              no     0.624  1.36   -0.344 -0.717 -1.42 \n 9 DE    area_code_510 no                 no              no    -1.13  -0.663  -0.804 -0.993 -0.159\n10 CT    area_code_415 no                 yes             no    -2.60   0.817   0.939 -1.09   2.53 \n# ℹ 3,990 more rows\n# ℹ 2 more variables: PC6 &lt;dbl&gt;, PC7 &lt;dbl&gt;\n\n\nTo meet our threshold, the step_pca() recipes step calculated and returned seven principal components, PC1 - PC7.\nstep_pca() makes it pretty easy to do dimension reduction utilizing principal components analysis. Give it a try.\nAnother day down. See you tomorrow.\n\n\nDay 15 - Use step_ica() for signal extraction\nHere we are, the halfway point 🎉.\nToday, I’m overviewing the use of step_ica(). This step is used to make transformations in signal processing. In general terms, independent component analysis (ICA) is a dimensionality reduction technique that attempts to isolate signal from noise.\nBefore reviewing this step, I had to do some research on what ICA is and how it is used. Here are a few sources I found useful to gain an intuitive sense of this step:\n\nMaking sense of independent component analysis Cross Validated post.\nIntroduction to ICA: Independent Component Analysis by Jonas Dieckmann\nIndependent Component Analysis (ICA) and Automated Component Labeling — EEG Example by Bartek Kulas\n\nAcross many of these sources, two examples are commonly presented to more clearly explain the purpose of ICA. First, the cocktail party problem, where we can isolate individual conversations among many during a party. The second comes from audio recording. Take for example a situation where you have multiple microphones. These microphones are being used to capture the sound of several audio sources (e.g., various instruments), and you’re attempting to isolate the sound from a single source. Both are situations where you’re trying to isolate the signal from surrounding noise. Indeed, these are simplified examples. The linked sources above provide more sophisticated explanations.\nBefore we use step_ica(), we’ll need some data. I’m using a portion of the data from the Independent Component Analysis (ICA) and Automated Component Labeling — EEG Example. This data comes from the use of Electroencephalography (EEG). It was collected to examine EEG correlates of a genetic predisposition to alcoholism, and it was made available via Kaggle.\n\n\n\n\n\n\nWarning\n\n\n\nI am not an EEG professional, and I rarely work with this type of data. The approach I highlight here may not be best practice.\n\n\nThis data represents EEG sensor measurements of electrical activity during an experimental session for one participant. There are multiple sensors, which collect data intended to observe six known brain wave patterns (again, I’m not an expert here, so this is certainly a more complex topic then I’m making it out to be here). Let’s use step_ica() to see if we can transform this data into six different signals.\n\ndata_eeg &lt;- \n  read_csv(\n    here(\"blog/posts/2024-01-01-30-days-challenge-tidymodels-recipes/data_eeg.csv\")\n  ) |&gt;\n  clean_names() |&gt;\n  rename(id = x1)\n\nNew names:\nRows: 16384 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): sensor position, subject identifier, matching condition, name dbl (6): ...1, trial number,\nsample num, sensor value, channel, time\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types\nor set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nglimpse(data_eeg)\n\nRows: 16,384\nColumns: 10\n$ id                 &lt;dbl&gt; 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, …\n$ trial_number       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ sensor_position    &lt;chr&gt; \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"FP1\", \"…\n$ sample_num         &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2…\n$ sensor_value       &lt;dbl&gt; -8.921, -8.433, -2.574, 5.239, 11.587, 14.028, 11.587, 6.704, 1.821, -1…\n$ subject_identifier &lt;chr&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"…\n$ matching_condition &lt;chr&gt; \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"S1 obj\", \"…\n$ channel            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ name               &lt;chr&gt; \"co2a0000364\", \"co2a0000364\", \"co2a0000364\", \"co2a0000364\", \"co2a000036…\n$ time               &lt;dbl&gt; 0.00000000, 0.00390625, 0.00781250, 0.01171875, 0.01562500, 0.01953125,…\n\nskim(data_eeg)\n\n\nData summary\n\n\nName\ndata_eeg\n\n\nNumber of rows\n16384\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nsensor_position\n0\n1\n1\n3\n0\n64\n0\n\n\nsubject_identifier\n0\n1\n1\n1\n0\n1\n0\n\n\nmatching_condition\n0\n1\n6\n6\n0\n1\n0\n\n\nname\n0\n1\n11\n11\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nid\n0\n1\n8228.00\n4748.27\n5.00\n4116.50\n8228.00\n12339.50\n16451.0\n▇▇▇▇▇\n\n\ntrial_number\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.0\n▁▁▇▁▁\n\n\nsample_num\n0\n1\n127.50\n73.90\n0.00\n63.75\n127.50\n191.25\n255.0\n▇▇▇▇▇\n\n\nsensor_value\n0\n1\n1.99\n7.51\n-39.83\n-2.23\n1.22\n5.40\n51.9\n▁▂▇▁▁\n\n\nchannel\n0\n1\n31.50\n18.47\n0.00\n15.75\n31.50\n47.25\n63.0\n▇▇▇▇▇\n\n\ntime\n0\n1\n0.50\n0.29\n0.00\n0.25\n0.50\n0.75\n1.0\n▇▇▇▇▇\n\n\n\n\n\nI’ll have to do some data wrangling here to work with the data first, though. Specifically, I need to go from long to wide data. This will give each EEG sensor measurement its own column.\n\ndata_eeg &lt;- data_eeg |&gt;\n  select(sensor_position, sensor_value, time) |&gt;\n  pivot_wider(\n    names_from = sensor_position, \n    values_from = sensor_value\n  )\n\ndata_eeg\n\n# A tibble: 256 × 65\n      time   FP1    FP2      F7     F8    AF1    AF2     FZ    F4     F3   FC6    FC5    FC2    FC1\n     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 0       -8.92  0.834 -19.8    8.15  -2.15   1.13  -0.071 3.41  -0.092 4.83  -2.43   0.488  0.824\n 2 0.00391 -8.43  3.28  -12.5    1.80  -2.15   0.641 -0.559 1.46   0.397 6.30  -4.38  -0.977  0.824\n 3 0.00781 -2.57  5.72    1.15  -2.59  -1.66  -0.336 -1.05  0.478 -1.07  5.81  -5.36  -1.46   0.336\n 4 0.0117   5.24  7.67   14.8   -4.55  -0.682 -0.824 -0.559 0.966 -3.51  3.37  -5.36   0     -0.641\n 5 0.0156  11.6   9.62   20.7   -5.04   2.25   0.641  0.905 1.94  -5.46  1.41  -0.966  1.95  -1.13 \n 6 0.0195  14.0   9.62   17.3   -5.52   5.18   3.57   2.37  2.92  -4.49  0.437  6.85   3.42  -1.62 \n 7 0.0234  11.6   8.65    8.96  -4.55   6.64   6.01   3.84  1.94  -1.07  0.926 13.7    3.42  -1.13 \n 8 0.0273   6.70  5.23    0.173 -0.641  5.18   6.99   4.32  0.478  3.33  1.90  15.6    1.95   0.824\n 9 0.0312   1.82  1.32   -3.73   5.71   1.76   5.52   3.35  0.478  6.74  1.90  11.7    0.977  3.75 \n10 0.0352  -1.11 -2.10   -2.27  10.6   -1.66   2.59   1.88  1.94   7.23  1.90   3.92   0.977  5.22 \n# ℹ 246 more rows\n# ℹ 51 more variables: T8 &lt;dbl&gt;, T7 &lt;dbl&gt;, CZ &lt;dbl&gt;, C3 &lt;dbl&gt;, C4 &lt;dbl&gt;, CP5 &lt;dbl&gt;, CP6 &lt;dbl&gt;,\n#   CP1 &lt;dbl&gt;, CP2 &lt;dbl&gt;, P3 &lt;dbl&gt;, P4 &lt;dbl&gt;, PZ &lt;dbl&gt;, P8 &lt;dbl&gt;, P7 &lt;dbl&gt;, PO2 &lt;dbl&gt;, PO1 &lt;dbl&gt;,\n#   O2 &lt;dbl&gt;, O1 &lt;dbl&gt;, X &lt;dbl&gt;, AF7 &lt;dbl&gt;, AF8 &lt;dbl&gt;, F5 &lt;dbl&gt;, F6 &lt;dbl&gt;, FT7 &lt;dbl&gt;, FT8 &lt;dbl&gt;,\n#   FPZ &lt;dbl&gt;, FC4 &lt;dbl&gt;, FC3 &lt;dbl&gt;, C6 &lt;dbl&gt;, C5 &lt;dbl&gt;, F2 &lt;dbl&gt;, F1 &lt;dbl&gt;, TP8 &lt;dbl&gt;, TP7 &lt;dbl&gt;,\n#   AFZ &lt;dbl&gt;, CP3 &lt;dbl&gt;, CP4 &lt;dbl&gt;, P5 &lt;dbl&gt;, P6 &lt;dbl&gt;, C1 &lt;dbl&gt;, C2 &lt;dbl&gt;, PO7 &lt;dbl&gt;, PO8 &lt;dbl&gt;,\n#   FCZ &lt;dbl&gt;, POZ &lt;dbl&gt;, OZ &lt;dbl&gt;, P2 &lt;dbl&gt;, P1 &lt;dbl&gt;, CPZ &lt;dbl&gt;, nd &lt;dbl&gt;, Y &lt;dbl&gt;\n\n\nIn addition, step_ica() requires a few packages to be installed. This includes dimRed and fastICA. If these are not installed beforehand, an error will be returned.\nNow we create our recipe. Just like principal components analysis, we need to center and scale our data. We achieve this by applying step_center() and step_scale() to all_numeric() variables. Since we’re attempting to create columns to represent the six brain waves, we’ll set num_comp to 6.\n\neeg_rec &lt;- recipe(~., data = data_eeg) |&gt;\n  update_role(time, new_role = \"time_ref\") |&gt;\n  step_center(all_numeric()) |&gt;\n  step_scale(all_numeric()) |&gt;\n  step_ica(all_numeric(), num_comp = 6) |&gt;\n  prep()\n\nLet’s tidy() our recipe to get a better sense of what’s happening here. You’ll notice it’s very similar to principal components analysis.\n\ntidy(eeg_rec, number = 3)\n\n# A tibble: 390 × 4\n   terms component     value id       \n   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    \n 1 AF1   IC1       -0.0446   ica_xymfX\n 2 AF1   IC2        0.0123   ica_xymfX\n 3 AF1   IC3        0.00928  ica_xymfX\n 4 AF1   IC4        0.000524 ica_xymfX\n 5 AF1   IC5       -0.0280   ica_xymfX\n 6 AF1   IC6       -0.0415   ica_xymfX\n 7 AF2   IC1       -0.0489   ica_xymfX\n 8 AF2   IC2        0.0667   ica_xymfX\n 9 AF2   IC3       -0.00168  ica_xymfX\n10 AF2   IC4       -0.0385   ica_xymfX\n# ℹ 380 more rows\n\n\nNow let’s bake our data and see what the result of the step_ica() function.\n\nbaked_eeg &lt;- bake(eeg_rec, new_data = NULL) |&gt;\n  bind_cols(data_eeg |&gt; select(time))\n\nFor the heck of it, let’s plot our baked data. Do you see any of the common brain wave types in these plots? Remember, this is only one participant, so if additional data were included, the patterns might become more apparent.\n\nwalk(\n  baked_eeg |&gt; select(-time), \n  ~plot(baked_eeg$time, .x, type = \"line\") \n)\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first character\n\n\n\n\n\nThere you have it, another step_*() function to try out. See you tomorrow.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2024,\n  author = {Berke, Collin K},\n  title = {30 Day Tidymodels `Recipes` Challenge},\n  date = {2024-01-01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2024. “30 Day Tidymodels `Recipes`\nChallenge.” January 1, 2024."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "",
    "text": "Photo by Markus Winkler"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-purpose-of-this-blog-series",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The purpose of this blog series",
    "text": "The purpose of this blog series\nThis series of blogposts will focus on creating forecasts using Google Analytics 4 data. Specifically, this series overviews the steps and methods involved when developing forecasts of time series data. This blog series begins with a post overviewing the wrangling, visualization, and exploratory analysis involved when working with time series data. The primary focus of the exploratory analysis will be to identify interesting trends for further analysis and application within forecasting models. Then, subsequent posts will focus on developing different forecasting models. The primary goal of this series is to generate a forecast of online order completions on the Google Merchandise store."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#a-quick-disclaimer",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "A quick disclaimer",
    "text": "A quick disclaimer\nAnother intention of this series is to document and organize my learning and practice of time series analysis. Although I try my best to perform and report valid and accurate analysis, I will most likely get something wrong at some point in this series. I’m not an expert in this area. However, it’s my hope that this series can be a supplement to others who may be learning and practicing time series analysis. In fact, seeing somebody (i.e., myself) do something wrong might be a valuable learning experience for someone else, even if that someone is my future self. If I got something wrong, I would greatly appreciate the feedback and will make the necessary changes.\nThroughout the series, I will document the resources I used to learn the process involved when generating forecasting models. I highly suggest using these as the primary source to learn this subject, especially if you intend to use this type of analysis in your own work. Specifically, the process and methods detailed in this series are mostly inspired by the Forecasting: Principles and Practice online textbook by Rob J Hyndman and George Athanasopoulos, and it utilizes several packages to wrangle, visualize, and forecast time series data (e.g., tsibble; fable; and feasts). I am very thankful to the authors and contributors for making these materials open-source."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Setup",
    "text": "Setup\nThe following is the setup steps needed to perform this exploratory analysis.\n\n# Libraries needed\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(fable)\nlibrary(feasts)\nlibrary(fuzzyjoin)\nlibrary(bigrquery)\nlibrary(glue)\nlibrary(GGally)\nlibrary(scales)\nbq_auth()\n\n## Replace with your Google Cloud `project ID`\nproject_id &lt;- 'your.google.project.id'\n\n\n## Configure the plot theme\ntheme_set(\n  theme_minimal() +\n    theme(\n       plot.title = element_text(size = 14, face = \"bold\"),\n       plot.subtitle = element_text(size = 12),\n       panel.grid.minor = element_blank()\n    )\n)"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The data",
    "text": "The data\nAs was used in previous posts, Google Analytics 4 data for the Google Merchandise Store are used for the examples below. Data represents website usage from 2020-11-01 to 2021-12-31. Google’s Public Datasets initiative makes this data open and available for anyone to use (as long as you have a Google account and have access to Google Cloud resources). Data are stored in Google BigQuery, a data analytics warehouse solution, and are exported using a SQL like syntax. Details on how this data were exported can be found in this GitHub repository. More about the data can be found here."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Export the data",
    "text": "Export the data\nThe first step in the process was to export all page_view events. To do this, the following SQL code was submitted to BigQuery using the bigrquery package. Keep in mind Google charges for data processing performed by BigQuery. Each Google account–at least since the writing of this post–had a free tier of usage. If you’re following along and you don’t have any current Google Cloud projects attached to your billing account, this query should be well within the free usage quota. However, terms of service may change at anytime, so this might not always be the case. Nevertheless, it is best to keep informed about the data processing pricing rates before submitting any query to BigQuery.\nselect \n    event_date,\n    user_pseudo_id,\n    event_name,\n    key,\n    value.string_value\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nCROSS JOIN UNNEST(event_params)\nwhere \n   event_name = 'page_view' and \n   _TABLE_SUFFIX between '20201101' and '20211231' and \n   key = 'page_location'\nThe query returns a data set with 1,350,428 rows and the following columns:\n\nevent_date - represents the date the website event took place.\nuser_pseudo_id - represents a unique User ID.\nevent_name - The name of the event specified by Google Analytics. In our case this will just be page_view given the filtering criteria.\nkey - represents the page_location dimension from the data. This column should only contain page_location.\nstring_value - represents the page to which the event took place. In other words, the page path a page_view event was counted.\n\nThis query returns a lot of data. Thus, the analysis’ scope needed to be narrowed to make the exploratory analysis more manageable. To do this, top-level pages were identified and data wrangling procedures were performed to reduce the data down to pages relevant to the exploratory analysis.\n\nNarrowing the analysis’ scope to relevant pages\nThe overall aim of the series is to forecast Order Completed page views. As part of this, relevant pages that could be used to improve forecasting models needed to be a part of the exploratory analysis. However, this is challenging given the sheer amount of pages being represented within the data. Some pages relevant to the analysis, others, not so much. Given the number of possible pages, a decision was made to only examine key, top-level pages. The question is, then, what pages should be considered relevant to the analysis?\n\n\nDetermining top-level pages\nThe navigation bar of the Google Merchandise Store was used to determine the top-level pages. Indeed, it’s reasonable to expect the navigation bar is designed to drive users to key areas of the site. Developers won’t waste valuable navbar real-estate for content users would consider useless and/or irrelevant (i.e., these are developers at Google, so they mostly likely have a good idea of how users use a website). With this in mind, the following pages were identified as potential candidates for further analysis.\n\nNew products\nApparel\nLifestyle\nStationery\nCollections\nShop by Brand\nSale (i.e., Clearance)\nCampus Collection\n\nThe checkout flow is another key component of any e-commerce website. Indeed, a main goal of the site is to convert visits into order completions. As such, pages related to the checkout flow might be another area of interest in the analysis. It’s challenging to piece together the checkout flow by just looking at the data. So, I purchased a few products to observe the checkout flow and track the different pages that came up. The checkout flow–at least when I made my purchase–went in this specific order:\n\nReview basket\nSign in (I wasn’t signed into my Google account)\nReview my information\nPayment info\nOrder review\nOrder completed\n\nAlthough these pages were identified as potential candidates for further analysis, it’s important to recognize the Google Merchandise store is not static, and thus the design and layout may have changed from the dates the data represents vs. when I went through the checkout flow. Regardless, these initial observations provided a starting point to help narrow the analysis’ focus."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Filtering out homepage events",
    "text": "Filtering out homepage events\nNow that the analysis’ scope had been narrowed to top-level pages, events associated with homepage views were filtered out to reduce the number of events within the data. To do this, the regex_filter variable was created using the glue() function from the glue package, which was then applied within a filter statement.\n\nregex_page_filter &lt;- glue(\n  \"(\",\n  \"^https://www.googlemerchandisestore.com/$|\",\n  \"^https://googlemerchandisestore.com/$|\",\n  \"^https://shop.googlemerchandisestore.com/$\",\n  \")\"\n  )\n\nThe variable contained multiple regex expressions, as several page paths in the data represented home page visits. Defining the variable in this way ensured the filter excluded all data associated with homepage visits.\nOnce the filter statement was set up, the str_to_lower() function from the stringr package was used to convert all the page paths to lower case. The following code chunk demonstrates how these operations were performed.\n\nga4_pagepaths &lt;- ga4_pageviews %&gt;% \n  filter(!str_detect(string_value, regex_page_filter)) %&gt;% \n  mutate(string_value = str_to_lower(string_value))\n\nThe filtering resulted in a reduced data set (i.e., ~1 million rows). Since the intent was to further narrow the analysis’ scope, additional filtering was performed. Specifically, the data were filtered to return a data set containing the top-level pages identified previously.\nAnother variable–similar to regex_filter–was created and used to filter the data further. Given the number of pages, though, a filtering join would be more appropriate (e.g., semi-join). The problem is the join operation needed to filter the data needed to be based on several regular expressions.\nA semi-join using a regular expression is not supported with dplyr’s joins, so the regex_semi_join() function from David Robinson’s {fuzzyjoin} package was used. This package provides a set of join operations based on inexact matching. A separate data set (tracked_data), containing the regular expressions was then created, imported into the session, and used within the join operation. A dplyr::left_join() was then used to include this data within a tidy dataset. The following chunk provides example code to perform these operations.\n\ntracked_pages &lt;- read_csv(\"tracked_pages.csv\")\n\ntop_pages_data &lt;- ga4_pagepaths %&gt;% \n  mutate(\n    string_value = str_remove(\n      string_value,'https://shop.googlemerchandisestore.com/')) %&gt;% \n  regex_semi_join(tracked_pages, by = c(\"string_value\" = \"page_path\")) %&gt;% \n  regex_left_join(tracked_pages, by = c(\"string_value\" = \"page_path\"))\n\nAt this point, the data is more manageable and easier to work with. At the start, the initial export contained around 1.6 million rows. By narrowing the focus of the analysis and performing several data wrangling steps to filter the data, the final tidy data set contained around 320,000 rows.\nGiven the limited amount of storage available and how this post is hosted makes authentication into BigQuery challenging, I opted to not integrate the extraction steps into the rendering steps and to exclude the full data with this post. However, I included the filtered data set in a .rds file to conserve space. I imported this file by running the following code chunk to continue the exploratory analysis. I would skip this step and just directly export the data from BigQuery if this analysis was performed outside the forum of a blog post.\n\ntop_pages_data &lt;- readRDS(\"top_pages_data.rds\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Data exploration",
    "text": "Data exploration\nWith data in a tidy format, the exploratory analysis and further identification of relevant series for forecasts of Order Completed page views can take place. One area of possible exploration is to identify which pages generate a significant amount of traffic. Indeed, it’s possible that pages with a lot of traffic might also result in more order completions: more traffic indicates more interest; more interest could mean more orders.\nA few questions to answer:\n\nWhich top-level pages have the most unique users?\nWhat pages get the most traffic (i.e., page views)?\n\nA simple bar plot is created to answer these questions. Here’s the code to create these plots, using the ggplot2 package.\n\npage_summary &lt;- top_pages_data %&gt;% \n  group_by(page) %&gt;% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n()\n  ) %&gt;% \n  arrange(desc(unique_users))\n\n\nggplot(page_summary, aes(x = unique_users, y = reorder(page, unique_users))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by users\",\n       subtitle = \"Apparel page viewed by a significant amount of users\",\n       y = \"\",\n       x = \"Users\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       )\n\n\n\n\n\nggplot(page_summary, aes(x = views, y = reorder(page, views))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by views\",\n       subtitle = \"Apparel and basket pages generate significant amount of views\",\n       y = \"\", \n       x = \"Views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\nApparel clearly seems to not only have received a significant amount of users, but a high number of page views as well. It’s also interesting to note that the basket had nearly half the amount of users compared to apparel, but the amount of page views was similar. It’s also apparent, at least with the data available, that more users browsed clearance then they did new items during this period. Just looking at the current summary for the period, apparel might be a potential time series to include within forecasting models of order completions.\nAlthough the apparel page is a likely candidate for the forecasting models, supplemental data should be examined to justify its inclusion. For instance, actual purchase/financial data could provide further justification of the business case and value of focusing on this specific area within future analyses. For instance, apparel may drive a lot of traffic, but it may not be an area where much revenue or profit is generated. Thus, the focus on more money generating/profitable products may be better candidates to improve the accuracy of our forecasting models. Despite this, actual purchase and financial data are not available. As a result, this is not explored further."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Create time series data visualizations",
    "text": "Create time series data visualizations\nVisualizing the time series is the next step in the exploratory analysis. This step will be further helpful in identifying potential time series that may be of value in creating a forecast of Order Completed page views.\n\nConvert the tibble into a tsibble\nThe top_pages_data tibble is now converted to an object that contains temporal structure. To do this, the as_tsibble() function from the {tsibble} package is used. This package provides a set of tools to create and wrangle tidy temporal data. Before the temporal structure could be mapped to the data set, a few wrangling steps were performed: 1). the event_date column was converted into a date variable; and 2). data were aggregated to count the number of unique_users and views. The following code chunk contains an example of these steps.\n\npages_of_interest &lt;- c(\"Apparel\", \n                       \"Campus Collection\", \n                       \"Clearance\", \n                       \"Lifestyle\", \n                       \"New\", \n                       \"Order Completed\", \n                       \"Shop by Brand\")\n\ntidy_trend_data &lt;- top_pages_data %&gt;% \n  mutate(event_date = parse_date(event_date, \"%Y%m%d\")) %&gt;% \n  group_by(event_date, page) %&gt;% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n(), \n    .groups = \"drop\"\n  ) %&gt;% \n  as_tsibble(index = event_date, key = c(\"page\")) %&gt;% \n  filter(page %in% pages_of_interest)\n\nAt this point, several trend plots could be created using the ggplot2 package. However, the feasts package provides a convenient wrapper function to quickly make trend visualizations of tsibble objects, autoplot(). The outputted plot was then formatted to improve readability.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of page views\", \n       subtitle = \"Apparel drove a significant amount of views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.title = element_blank())\n\n\n\n\nggplot2’s facet_wrap() function was used to create a plot for each series. Splitting the plots into separate entities allowed for a clearer view of the characteristics within each series.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(.~page, scales = \"free_y\")  +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plots of page views\", \n       subtitle = \"Various characteristics are present within each series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Identify notable features using time plots",
    "text": "Identify notable features using time plots\nApparel again emerges as a potential series to include within the forecasting models, as this page generates a significant amount of traffic. Despite the sheer amount of traffic to the apparel page, though, other time series peak interest. Specifically, the Campus Collection, Clearance, Lifestyle, and New pages all have some interesting characteristics that could be used to improve forecasting models. The following plots contain the isolated trends. A description of the characteristics within each trend is provided.\n\nApparel page’s characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Apparel\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Apparel page views\", \n       subtitle = \"Series exhibits positive trend; slight cyclic patterns; no seasonal patterns present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA clear positive trend.\nThe series contains some cyclic elements and very little indication of a seasonal pattern. However, with a greater amount of points, a seasonal pattern might be revealed (e.g., holiday season shopping).\n\n\n\nCampus Collection page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Campus Collection\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Campus Collection page views\", \n       subtitle = \"Cyclic behavior present within the series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA slight positive trend is present up until the middle of the series. Towards the middle of the series, no real trend is present.\nGiven the variation is not of a fixed frequency, this series exhibits some cyclical behavior.\n\n\n\nClearance page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Clearance\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Clearance page views\", \n       subtitle = \"Slight trend components are present; weekly seasonality is also present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA slight upward trend towards the middle of the series, followed by a steep downward trend, and then a slight upward trend towards the end of the series is present.\nThe series also has a clear seasonal pattern, which seems to be weekly in nature. Perhaps products are moved to clearance on a weekly basis.\n\n\n\nLifestyle page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Lifestyle\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Lifestyle page views\", \n       subtitle = \"Trend not clear in this series; some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nTrend is not clear in this series.\nThere is some strong cyclic behavior being exhibited with limited seasonality with the time frame available.\n\n\n\nNew page’s notable characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"New\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of New page views\", \n       subtitle = \"No trend present; some some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nNo real trend is present.\nStrong cyclic behavior is present within the series. Some seasonality is present. Indeed, this is similar to the Clearance series, as the seasonality seems to be weekly. Perhaps new products are released each week.\n\n\n\nShop by brand characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Shop by Brand\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Shop by Brand page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nThe trend here seems to be positive from the start, then declines sharply, and then exhibits a slight positive trend towards the end of the series.\nThere also seems to be some slight cyclicity with very little seasonality.\n\n\n\nOrder completed characteristics\n\ntidy_trend_data %&gt;% \n  filter(page == \"Order Completed\") %&gt;% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Order Completed page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nThe trend from the start seems to be positive, up until the middle of the series. From there, a steep decline is present. Towards the end of the series, there is a subtle positive trend.\nTowards the beginning of the series, there seems to be some strong cyclicity. Towards the end of the series, there seems to be more of a seasonal pattern within the data. This cyclicity may be due to the time of year which this data represents, the holiday season.\n\nSince this analysis is focused on creating a forecast for order completions, additional work needed to be done to identify potential series that may improve the forecasts. To do this, several scatter plots were created to help identify variables that relate to order completions.\nBefore additional exploratory plots can be created, though, additional data wrangling steps needed to be taken. Specifically, the data was transformed from a long format to a wide format, where the page variable is turned into several numeric columns within the transformed data set. The following code chunk was used to perform this task.\n\ntidy_trend_wide &lt;- tidy_trend_data %&gt;% \n  select(-unique_users) %&gt;% \n  mutate(page = str_replace(str_to_lower(page), \" \", \"_\")) %&gt;% \n  pivot_wider(names_from = page, \n              values_from = views, \n              names_glue = \"{page}_views\")\n\nWith data in a wide format, the ggpairs() function from the GGally package was used to create a matrix of scatterplots and correlation estimates for the various series within the dataset.\nHere is the code to perform this analysis and output the matrix of plots.\n\ntidy_trend_wide %&gt;% \n  ggpairs(columns = 2:8)\n\n\n\n\nThe scatterplots and correlations output revealed some interesting relationships. For one, although previous exploratory analysis revealed apparel generated high volumes of views, the correlation analysis revealed a slight negative relationship with order completions. However, five variables seem to be highly correlated with order completions: Clearance (.877), Campus Collection (.846), Lifestyle (.769), New (.753), and Shop by Brand (.659). Evidence points to these series as being potentially valuable components of a forecasting model of Order Completed page views."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Narrow the focus further",
    "text": "Narrow the focus further\nAt this point, this post transitions into examining just the Order Completed page views, as this is the time series intended to be forecasted within future analyses done in this series of blog posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Lag plots",
    "text": "Lag plots\nIt’s time to shift focus onto exploring the characteristics of the outcome variable of future forecasting models, order completions, in more depth. The next step, then, is to examine for any lagged relationships present within the Order Completed page views time series.\nThe gg_lag function from the feats package makes it easy to produce the lag plots. Here the tidy_trend_wide data will be used.\n\ntidy_trend_wide %&gt;% \n  gg_lag(order_completed_views, geom = \"point\")\n\n\n\n\nThe plots provide little evidence that any lagged relationships–positive or negative–are present within this time series. Thus, no further steps were taken to account for lagged relationships at this point in the analysis."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nExploring for autocorrelation is the next step. A correlogram is created to explore for this characteristic within the series. A correlogram “measures the linear relationship between lagged values of a time series” (Hyndman and Athanasopoulos 2021). The ACF is first calculated for each value within the series. This value is then plotted according to it’s corresponding lag values. The ACF() function from the feasts package was used to calculate these values. The resulting data object is then passed along to the autoplot() function, which creates the correlogram for the data. Here is what the code looks like, along with the outputted plot.\n\ntidy_trend_wide %&gt;% \n  ACF(order_completed_views, lag_max = 28) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Inspect the correlogram",
    "text": "Inspect the correlogram\nThe correlogram clearly shows the data is not a white noise series. Moreover, the plot reveals several structural characteristics within the time series.\n\nThe correlogram, given the smaller lags are large, positive, and seem to decrease with each subsequent lag, which suggests the series contains some type of trend.\nThe plot also reveals a slight scalloped shape (i.e., peaks and valleys at specific intervals), which suggests some seasonality occurring within the process. Indeed, it seems peaks occur every seven days (e.g., lags 7 and 14). Thus, a slight weekly seasonality may be present within the time series.\n\nGiven these structural characteristics of the series, future forecasting steps will need to account for these issues. This topic will be further discussed in future posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "href": "blog/posts/2022-03-03-exploratory-trend-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "title": "Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Trend Decomposition",
    "text": "Trend Decomposition\nThe final exploratory analysis step is to split the series into its several components. This includes the seasonal, trend, and remainder components. Here an additive decomposition is performed. Transformations were not applied to this data before decomposition was performed.\nAn argument could be made to transform this time series using some mathematical operation. Indeed, transforming the series may improve forecasts generated from the data (Hyndman and Athanasopoulos 2021). However, this analysis doesn’t have access to a complete series of data. Having more data could lead to more informed decisions on the appropriate application of transformations. A full year or multiple years worth of data would be preferred. Interpretability is also a concern, as transformations would need to be converted back to the original scale once the forecast was created. Thus, it was decided that transformations were not going to be applied to the data. More about transforming the series can be referenced here.\nThe series was broken down into its multiple components: seasonal, trend-cycle, and remainder (Hyndman and Athanasopoulos 2021). Decomposing the series allows for more to be learned about the underlying structure of the time series. As a result, allowing for structural components of the time series that could improve forecasting models to be identified. Several functions from the {feasts} and {fabletools} packages simplified the decomposition process.\nFirst, the trend components are calculated using the STL() and model() functions. STL() decomposes the trend. The model() function creates a mabel object of estimates. The components() function is then used to view the model object.\n\norder_views_dcmp &lt;- tidy_trend_wide %&gt;% \n  model(stl = STL(order_completed_views))\n\ncomponents(order_views_dcmp)\n\n# A dable: 92 x 7 [1D]\n# Key:     .model [1]\n# :        order_completed_views = trend + season_week + remainder\n   .model event_date order_completed_views trend season_week remainder season_adjust\n   &lt;chr&gt;  &lt;date&gt;                     &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1 stl    2020-11-01                    14  47.8      -37.6      3.80           51.6\n 2 stl    2020-11-02                    73  47.9       12.9     12.2            60.1\n 3 stl    2020-11-03                    69  47.9       23.1     -1.99           45.9\n 4 stl    2020-11-04                    47  48.2       11.8    -13.0            35.2\n 5 stl    2020-11-05                    28  48.5       -4.93   -15.5            32.9\n 6 stl    2020-11-06                    62  48.7       13.0      0.285          49.0\n 7 stl    2020-11-07                    34  48.9      -17.9      3.00           51.9\n 8 stl    2020-11-08                    32  51.7      -37.9     18.2            69.9\n 9 stl    2020-11-09                    58  54.5       12.4     -8.95           45.6\n10 stl    2020-11-10                    70  56.6       22.7     -9.33           47.3\n# ℹ 82 more rows\n\n\nPlotting the decomposition is done by piping the output from the components() function to autoplot(). The visualization will contain the original trend, the trend component, the seasonal component, and the remainder of the series after the trend and seasonal components are removed.\n\ncomponents(order_views_dcmp) %&gt;% \n  autoplot() +\n  labs(x = \"Event Date\")\n\n\n\n\nScanning the components outputted by the decomposition, a few conclusions were drawn. Looking at the trend component, it seems a steady upward trend takes place from the start to the middle of the series. Then, a sharp negative trend followed by a slight increase towards the tail end of the series is present. Indeed, this might be additional seasonality that might become more apparent if additional data were available.\nThe seasonal component is also interesting here, as some type of cyclic weekly pattern seems to be present. This includes less traffic around the beginning of the week and weekends, where the majority of this cyclic pattern occurs during the week. It’s also interesting to note a consistent drop occured on most Thursdays of the week."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "",
    "text": "Photo by T K\nI had the great fortune of being a presenter at this year’s PBS TechCon conference. The focus of my talk was to introduce attendees to the principles of tidy data and discuss a data pipeline project my team has been working on at Nebraska Public Media. Here’s the session description:\nAs part of my talk, I mentioned having put together a curated list of resources others could use to learn more about the topics covered. This list can be found in the following section of this blog post. If you’re interested in discussing these topics further, please reach out."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nTidy Data paper published in the Journal of Statistical Software written by Hadley Wickham\nTidy Data Chapter published in the open source R for Data Science book written by Hadley Wickham and Garrett Grolemun\nData Organization: Organizing Data in Spreadsheets post by Karl Broman\nData Organization: Organizing Data in Spreadsheets paper published in The American Statistician written by Karl Broman and Kara Woo\nTidy data section in Data Management in Large-Scale Education Research training modules written by Crystal Lewis\nTidy Data presented by Hadley Wickham\nTowards Data Science post published on Medium summarizing tidy data written by Benedict Neo"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Join a Community",
    "text": "Join a Community\n\nR for Data Science Online Learning Community\n\nJoin the Slack workspace\n@Collin Berke to get a hold of me in the workspace"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Open source workflow management tool",
    "text": "Open source workflow management tool\n\nApache Airflow"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "30 day tidymodels recipes challenge\n\n\n\n\n\n\n\nmachine learning\n\n\nfeature engineering\n\n\ntidymodels\n\n\ndata wrangling\n\n\n\n\nLearning how to use the recipes package, one day at a time\n\n\n\n\n\n\nJan 1, 2024\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nMessing around with tidymodels: Learning how to fit a binary classification model using NCAA Big Ten women’s volleyball data\n\n\n\n\n\n\n\ntutorial\n\n\ntidymodels\n\n\nclassification\n\n\ndecision tree\n\n\nlogistic regression\n\n\n\n\nUsing tidymodels to predict wins and losses for volleyball matches\n\n\n\n\n\n\nDec 7, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n2023 data science rig: Set up and configuration\n\n\n\n\n\n\n\ntutorial\n\n\n\n\nOverviewing and reflecting on my current data science setup.\n\n\n\n\n\n\nJan 29, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nFlattening Google Analytics 4 data\n\n\n\n\n\n\n\nBigQuery\n\n\nsql\n\n\ndata wrangling\n\n\n\n\nLet’s deep dive into working with Google Analytics data stored in BigQuery.\n\n\n\n\n\n\nSep 20, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nExploratory analysis of Google Analytics 4 data for forecasting models\n\n\n\n\n\n\n\nforecasting\n\n\n\n\nExploring Google Analytics 4 data for forecasting models.\n\n\n\n\n\n\nMar 3, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nShiny summary tiles\n\n\n\n\n\n\n\nshiny\n\n\n\n\nBuilding custom metric summary tiles for Shiny.\n\n\n\n\n\n\nDec 30, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n2021 PBS TechCon: Your Data is Disgusting!\n\n\n\n\n\n\n\ntalks\n\n\n\n\nI was fortunate to be invited to present about topics I’m passionate about: tidy data and data pipelines.\n\n\n\n\n\n\nOct 19, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nImplementing a next and back button in Shiny\n\n\n\n\n\n\n\nshiny\n\n\n\n\nTaking the time to understand a challenging question from Mastering Shiny.\n\n\n\n\n\n\nSep 12, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nIntro Post\n\n\n\n\n\n\n\npersonal\n\n\n\n\nHello World!, my name is Collin, and this is my blog.\n\n\n\n\n\n\nApr 2, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html",
    "href": "til/posts/2023-03-14-vim-substitution/index.html",
    "title": "Find and replace in Vim",
    "section": "",
    "text": "Today I learned how to find and replace in Vim. I’ve found knowing a few variations of the substitute (:s or su for short) command to be a powerful skill to quickly and efficiently edit code and text within a file. By knowing a few simple command variations, you can greatly improve your productivity. You just have to know the different patterns and when to apply them.\nThis TIL post aims to highlight some of the basics of using Vim’s :s command. My intention is to get you up and running quickly. As such, this post provides several simple examples applying the command to some practical use cases. Although most of the examples use the R programming language, these concepts can be applied to any programming language or text editing task.\nThis post focuses on the basics. Indeed, the substitute command provides a lot of utility and different options to perform various find and replace editing tasks. If you’re looking to learn more advanced features, I suggest reading the docs (:help substitute). I also provide some additional links to other resources throughout and at the end of the post if you’re interested in learning more."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#the-basics",
    "title": "Find and replace in Vim",
    "section": "The basics",
    "text": "The basics\n:s can be used to find each occurance of a text string, and replace it with another text string. Say I have a character vector basket, and it contains an assortment of fruit. However, what if I want to replace the first apple in my basket with an orange using :s? First, I need to move my cursor to the line I want to find the first string. Then, I can enter the following into the command prompt to find the first instance of the string orange and replace it with the string apple:\n:s/orange/apple\nHere is what this looks like in action.\n\nHowever, what if I don’t want any oranges, and instead I just want apples rather than orangesin my basket. I can append the previous command with g to replace all instances of orange with apple. The g flag indicates to Vim that I want to replace globally to the current line. In other words, replace all instances on the current line.\n:s/orange/apple/g\nBelow is what this will look like in your editor.\n\nWant to find and replace text globally to the line and including multiple lines, then add % to the beginning of the command.\n:%s/orange/apple/g\n\n% is really useful if you want to refactor code efficiently within a file. Check out these two examples, one more contrived, the other a more practical, common application.\n:%s/power/horsepower/g\n:%s/data/cars_data/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#confirming-replacement",
    "title": "Find and replace in Vim",
    "section": "Confirming replacement",
    "text": "Confirming replacement\nNot sure what all will be replaced and would rather go through each replacement step-by-step? Add c to the end of your command. Adding this flag will make Vim prompt you to confirm each replacement.\n:%s/orange/apple/gc\n\nIn the prompt, you’ll see something like replace with apple (y/n/a/q/l/^E/^Y). You’ll select the option that fulfills the action you want to perform. Here is a list of what each selection does:\n\ny - substitute this one match and move to the next (if any).\nn - skip this match and move to the next (if any).\na - substitue all (and it’s all matches) remaining matches.\nq - quit out of the prompt.\nl - subsitute this one match and quit. l is synonymous with “last”.\n^E - or Ctrl-e will scroll up.\n^Y - or Ctrl-y will scroll down.\n\nThe example above only highlights the use of y, so I suggest experimenting with each selection to get a feel for what they do."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-by-range",
    "title": "Find and replace in Vim",
    "section": "Replacing by range",
    "text": "Replacing by range\nTake a look at the command pattern again, specifically the first portion, [range]:\n:[range]s[ubstitute]/{pattern}/{string}/[flags] count\nThe s command provides functionality to scope the find and replace operation to a specific part of your file. Indeed, this functionality was highlighted earlier when we passed % in an earlier command. % just indicated to Vim that we wanted to find and replace all lines in the file. However, we can be more specific.\nSay we now have a much larger basket, one that can hold both fruits and veggies. In the R programming language, this can be modeled using a tribble from the tribble package.\nWhat if we wanted to find the first two instances of carrots in our basket and replace it with kale. This can be done by passing a range at the start of the :s command. In this specific instance, I want to find and replace the carrots on lines 5 and 7 with the string kale, but I don’t want to change the one on line 8. To do this, I can run the following command:\n:5,7s/carrot/kale/g\n\nAnother variation is to start on your current line . and specify to Vim how many additional lines I would like to find and replace in the range. Keep in mind . represents the current line your cursor is located currently within the file. Once you postion your cursor, your command will look something like this:"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-from-current-location-to-n-lines",
    "title": "Find and replace in Vim",
    "section": "Replacing from current location to n lines",
    "text": "Replacing from current location to n lines\n:.,+2s/carrot/kale/g\n\nWhat if I had a basket with even more fruits and veggies, and I just wanted to start at my current location and replace all instances that follow? We can use the $ in the range input. The use of the dollar sign indicates to Vim that we want to replace starting at line 8 and go to the end of the file."
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-to-end-of-file",
    "title": "Find and replace in Vim",
    "section": "Replacing to end of file",
    "text": "Replacing to end of file\n:8,$s/carrot/kale/g\n\nOr, if you want to start from the current line and replace to the end of the file, you can do the following:\n:.,$s/carrot/kale/g"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replacing-using-visual-mode",
    "title": "Find and replace in Vim",
    "section": "Replacing using visual mode",
    "text": "Replacing using visual mode\nWe can also use visual mode to set the range of the find and replace operation. Just enter visual mode v or visual line mode Shift-v, highlight the range you want your find and replace operation to be applied, enter into command mode with :, and then enter your find and replace statement. Doing this will start the command line off with '&lt;,'&gt;, and you’ll just need to enter the rest of the command, the {pattern} and {string} portions.\n:'&lt;,'&gt;s/carrot/kale`"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#use-objects-in-your-search-buffer",
    "title": "Find and replace in Vim",
    "section": "Use objects in your search buffer",
    "text": "Use objects in your search buffer\nYour previous search history can also be used to do find and replace. Let’s go back to our miles-per-gallon plot example again. First I’ll hover my cursor over the word I want to replace and hit *. Now we can use the search value in our subsititution command. All I need to do is leave the {pattern} blank in the command. The command will look like this:\n:%s//horsepower"
  },
  {
    "objectID": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "href": "til/posts/2023-03-14-vim-substitution/index.html#replace-with-whats-under-your-cursor",
    "title": "Find and replace in Vim",
    "section": "Replace with what’s under your cursor",
    "text": "Replace with what’s under your cursor\nTo keep things simple, let’s go back to our first basket example. Specifically, let’s say I want to modify the string strawberry with the string banana, but use my cursor position to do this. First I have to make sure the cursor is hovering over the word I want to use for my replacement. Then, I enter the below command. When you see &lt;c-r&gt;&lt;c-w&gt;, this means you actually hit Ctrl-R and Ctrl-W on your keyboard. You’ll notice the string banana is populated into the command for us.\n%s/strawberry/&lt;c-r&gt;&lt;c-w&gt;"
  },
  {
    "objectID": "til/posts/2023-10-14-edit-old-commit/index.html",
    "href": "til/posts/2023-10-14-edit-old-commit/index.html",
    "title": "Edit an older unpushed commit",
    "section": "",
    "text": "Photo by Yancy Min\n\n\nToday I learned how to edit older unpushed commit messages using git rebase.\nI’ve been attempting to be better about linking git commits to specific GitHub issues. Although I try to be disciplined, I forget to reference the issue in the commit message from time-to-time. Luckily, I researched and came upon a solution. The purpose of this post is to briefly document what I’ve learned.\nA quick note: I am not a Git Fu master. The approach I share here (which I learned from a Stack Overflow post) worked for a small project not intended to be in production. In fact, there may be better approaches to solve this problem given your specific situation. I for sure want to avoid receiving angry messages where someone applied what is discussed, and it took down a critical, in production system. Thus, make sure you are aware of what these commands will do to your commit history before applying them.\n\nThe problem\nLet’s take a look at a log from a practice repo I created. I’m using git’s --pretty=format flag here to simplify the printed output for this post; a simple git log will also return the same information but in a more verbose way.\ngit log --pretty=format:\"%h %s %n%b\"\nThis returns the following log information. Printed to the console is a log containing the various commit’s abbreviated SHA-1 values, subjects, and message bodies.\n31964b0 fix-found_bug\n- #1\n\nf8256d6 feat-you_get_the_point\n- #1\n\nb1b99e9 feat-another_awesome_new_feat\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nShoot! I forgot to tag the b1b99e9 commit as being related to issue #1. How can I edit before I push?\n\n\nThe solution\ngit rebase can be used here to edit the past commit message. Again, keep in mind these commits have not been pushed to the remote repository.\nFirst, we need to target the commit we want to edit. git rebase, with the --interactive flag, and the abbreviated SHA-1 value of the commit to be edited is used to do this:\ngit rebase --interactive b1b99e9~\nThis command will open our system’s default text editor. In it should be something like the following:\npick b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n\n# Rebase 5d9b87c..31964b0 onto 5d9b87c (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup [-C | -c] &lt;commit&gt; = like \"squash\" but keep only the previous\n#                    commit's log message, unless -C is used, in which case\n#                    keep only this commit's message; -c is same as -C but\n#                    opens the editor\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n#         create a merge commit using the original merge commit's\n#         message (or the oneline, if no original merge commit was\n#         specified); use -c &lt;commit&gt; to reword the commit message\n# u, update-ref &lt;ref&gt; = track a placeholder for the &lt;ref&gt; to be updated\n#                       to this position in the new commits. The &lt;ref&gt; is\n#                       updated at the end of the rebase\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\nYou’ll notice the instructions and different options (formatted as comments) are plentiful. I have yet to explore what all these operations can do (maybe a future post). But here, we are focused on editing a past commit message.\nThe next step in the process was a little confusing. With a bit of reading of the Stack Overflow post and a little experimentation, I found out we need to manually change any pick to edit for any commit intended to be edited in the currently open file. Our file will look something like this:\nedit b1b99e9 feat-another_awesome_new_feat\npick f8256d6 feat-you_get_the_point\npick 31964b0 fix-found_bug\n...\nWe save the file and close our editor. Once back in the terminal, we’ll be on the commit targeted for edits. To make our edits, submit the following to the terminal:\ngit commit --amend\nOnce ran, the text editor will be opened to the commit message we targeted for edits. We’ll then make our changes, save them, and exit the text editor.\nNow, we need to return to the previous HEAD commit. To do this, we run the following command in our terminal:\ngit rebase --continue\n\n\nRewriting history\nLet’s look at the log and view our changes. We can do that again by submitting the following to our terminal:\ngit log --pretty=format:\"%h %s %n%b\"\nBelow is what gets printed.\n709c173 fix-found_bug\n- #1\n\ne0ed7ba feat-you_get_the_point\n- #1\n\n179be4a feat-another_awesome_new_feat\n- #1\n\n5d9b87c feat-awesome_new_feature\n- #1\n\nee8e97b Initial commit\nSuccess! All our commits are now associated with issue #1. However, take a moment to compare the SHA-1 values from our previous log with the current log. Notice anything different? The SHA-1 values for both our edited commit message and all its children have been modified. We have just re-written part of our commit history.\nImportant point: You can break repos doing this if you’re not careful. This re-writing of history should only be applied in cases with unpushed commit messages and when you’re not collaborating on a branch with other people. If you make edits to your history using this approach, you’ll want to make sure to avoid using commands like git push --force. See the original Stack Overflow post for more detail.\n\n\nWrap-up\nSo there you have it. A little Git Fu magic to help edit past, unpushed commit messages.\nIf you know a better approach or if my Git Fu is way off, let me know. I have far from mastered git.\nHappy rebasing!\n\n\nResources to learn more\n\nHow do I modify a specific commit? Stack Overflow post submitted by Sam Liao and top answer from ZelluX\nGit Rebase Interactive :: A Practical Example YouTube tutorial from EdgeCoders\n7.6 Git Tools - Rewriting History from the git documentation\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@misc{berke2023,\n  author = {Berke, Collin K},\n  title = {Edit an Older Unpushed Commit},\n  date = {2023-10-14},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBerke, Collin K. 2023. “Edit an Older Unpushed Commit.”\nOctober 14, 2023."
  },
  {
    "objectID": "til/index.html",
    "href": "til/index.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Here you’ll find posts related to things I’ve learned recently.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nCombine plots using patchwork\n\n\n\n\n\n\n\ndata visualization\n\n\n\n\nNeed to add two or more plots together? Use the patchwork package\n\n\n\n\n\n\nDec 23, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nUsing base::tempdir() for temporary data storage\n\n\n\n\n\n\n\ndata wrangling\n\n\nworkflow\n\n\nproductivity\n\n\n\n\nNeed to store data in a place that’s not persistent, use a temporary directory\n\n\n\n\n\n\nNov 3, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nCalculating correlations with corrr\n\n\n\n\n\n\n\ndata analysis\n\n\nexploratory analysis\n\n\ndata visualization\n\n\n\n\nUse the corrr package to calculate and visualize correlations\n\n\n\n\n\n\nOct 22, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nEdit an older unpushed commit\n\n\n\n\n\n\n\ngit\n\n\nGitHub\n\n\n\n\nUse git rebase to edit previous commit messages\n\n\n\n\n\n\nOct 14, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\nFind and replace in Vim\n\n\n\n\n\n\n\nvim\n\n\nneovim\n\n\nproductivity\n\n\n\n\nImproving productivity by using Vim’s :substitute command\n\n\n\n\n\n\nFeb 24, 2023\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  }
]