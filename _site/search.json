[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Collin K. Berke, Ph.D.",
    "section": "",
    "text": "I’m a media research analyst, data enthusiast, and news, sports, and podcast aficianado.\nProfessionally, I use data, audience measurement, and marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. I am particularly interested in the use and development of open-source statistical software (i.e. R) to achieve this goal, and gaining a broader understanding of the role these tools play in media, digital, and marketing analytics. I also adjunct university courses on the side.\nListening to NPR, watching PBS (especially NOVA), and college football and baseball are my jam.\n\n\nWant to know more about what I’m currently working on, reading, or mastering? Check out the now page.\n\n\n\n\nPh.D. in Media and Communication, 2017, Texas Tech University\nM.A. in Communication Studies, 2013, The University of South Dakota\nBachelor of Science, 2011, The University of South Dakota\n\n\n\n\n\nDigital/Marketing analytics\nAudience measurement\nMedia testing\nR\nData engineering\n\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html",
    "title": "Shiny summary tiles",
    "section": "",
    "text": "Effective reporting tools include user interface (UI) elements to quickly and effectively communicate summary metrics. Shiny, a free software package written in the R statistical computing language, provides several tools to communicate analysis and insights. Combining several of these elements together, a developer can create user interface elements that clearly communicate important summary metrics (e.g., Key Performance Indicators) to an application’s users.\nThis post details the steps to create the following simple Shiny application. Specifically, this post overviews the use of Shiny’s built-in functions to create simple summary metric tiles. In addition, this post describes how to add styling to UI elements by applying custom css to a Shiny application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#the-reactive-graph",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#the-reactive-graph",
    "title": "Shiny summary tiles",
    "section": "The reactive graph",
    "text": "The reactive graph\nAlthough this app is simple and most of the elements can be easily managed, it’s always good practice to see the big picture of the app by plotting out a reactive graph first. It’s also good to have the intended reactive graph available as a quick reference, just in case unexpected results and/or behaviors are displayed while developing the application, and as a method for identifying any situations where computing resources are not being used efficiently.\nBelow is the reactive graph for the application to be developed:\n\n\n\n\n\n\n\nReactive graph for summary metric tiles\n\n\n\n\nAgain, a really simple application–one input (date), a reactive expression (data()), and five outputs (users; page_view; session_start; purchase; and event_date). The graph also details the dependencies clearly, where the outputs are dependent on the reactive data() object–which in cohort with the outputs–depends on the date input."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#the-setup",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#the-setup",
    "title": "Shiny summary tiles",
    "section": "The setup",
    "text": "The setup\nThe first step is to import the R packages used within the application. The following code chunk contains the packages used for the application. A brief description of each is included.\n\nlibrary(shiny) # The Shiny app library\nlibrary(readr) # Import data\nlibrary(dplyr) # Pipe and data manipulation\nlibrary(tidyr) # Tidying data function\nlibrary(purrr) # Used for functional programming\nlibrary(glue)  # Used for string interpolation\n\nMany of these packages are part of the tidyverse, and thus the import could be simplified to just running library(tidyverse). Be aware this may bring in unused, unneeded libraries. There is nothing wrong with this approach. However, I opted to be more verbose with this example, so as to be clear about what libraries are utilized within the example application and to have more control on what packages were imported by the application."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#application-layout",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#application-layout",
    "title": "Shiny summary tiles",
    "section": "Application layout",
    "text": "Application layout\nThe next step is to code the layout of the UI. To keep the design simple, a sidebar will contain the application’s inputs, while the outputs will be placed within the main panel of the application. The general skeleton of the layout looks like this:\n\nui <- fluidPage(\n   # Inputs\n   sidebarLayout(\n      sidebarPanel()\n   ),\n   # Outputs\n   mainPanel(\n      # Summary tiles\n      fluidRow(),\n      br(),\n      # Data information output\n      fluidRow()\n   )\n)\n\nThere’s nothing too fancy about this code, outside of it establishing the general layout of the application, so not much else will be said about what each element does here. However, Chapter 6 of Mastering Shiny discusses application layout if a more detailed description is needed."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#the-date-input",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#the-date-input",
    "title": "Shiny summary tiles",
    "section": "The date input",
    "text": "The date input\nThe app requirements state users need to have the ability to modify the dates to which the data represents, and the summary metric tiles will change based on this user input. However, the app will not have any user input upon startup, so it needs to default to the most recent date within the data. To meet these requirements, we use the following code:\n\n# Code excluded for brevity\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    )\n\nThe shiny:dateInput() function is used to create the HTML for the input, which resides in the application’s sidebar. The function’s id argument is given the value of date, which will establish a connection to elements within the server. More on this later. Then, a string value of Select a date for summary: is passed along to the label argument. This value will be displayed above the date input in the UI.\nSince the app won’t have an initial user input upon the startup of the application, max(ga4_date$event_date) is passed along to the value argument. This will default the input to the most recent date within the data. In addition, the functions max and min arguments are passed similar calls. However, in the case of the min argument the base R min() function is used on the ga4_data$event_date."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#first-iteration-of-the-summary-metric-tiles",
    "title": "Shiny summary tiles",
    "section": "First iteration of the summary metric tiles",
    "text": "First iteration of the summary metric tiles\n\nThe server side\n\nThe reactive data() object\nBefore the summary metrics can be displayed in the UI, the application needs data to create the outputs. In addition, since this data will be dependent on users’ input (i.e., the user can select a new date which subsequently changes the summary metric tile), this object needs to be reactive. To do this, the following code is added to the server side of the application.\n\nserver <- function(input, output, session) {\n   data <- reactive({\n      ga4_data %>% filter(.data[[\"event_data\"]] == input$date)\n   })\n}\n\nIn practical terms, this code just filters the data for the date being passed along as the input$date object.\nAgain, this object could be the most recent date within the data, the date set by the max argument in the dateInput() function, or it could be based on a user’s modification of the date input. Since this code was wrapped inside of the reactive({}) function, Shiny will be listening for any changes made to the to the input$date object. Any changes that occur will result in the data() reactive expression to be modified, followed by new output values being displayed via the UI.\nOne other key concept is being exhibited here, tidy evaluation, specifically data-masking. Since technically dplyr::filter() is being used inside of a function, an explicit reference to the data is required. Thus, .data[[\"event_data\"]] notation is used to make it explicit on what data will be filtered. The specifics on how to use data-masking in the context of a Shiny app is beyond the scope of this post. However, the previously linked materials provide a more detailed description of these concepts.\n\n\nThe outputs\nLooking back at the reactive graph, the application requires five outputs to be in the server. These outputs will just be simple text outputs, so the use of the shiny::renderText() function will be sufficient to meet our requirements. The format() function is also applied to comma format any outputs that contain numbers (e.g., 2,576 vs 2576). Here is what the server looks like currently:\n\nserver <- function(input, output, session) {\n  data <- reactive({\n    filter(ga4_date, .data[[\"event_date\"]] == input$date)\n  })\n  \n  output$users <- renderText(format(data()$users, big.mark = ','))\n  output$page_view <- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start <- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase <- renderText(format(data()$purchase, big.mark = ','))\n  output$date <- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nAs part of the functionality requirements, the app needed some UI element informing users what date is being represented in the summary tiles. The output$date object was included to meet this requirement. The output$date object, aside from using the renderText() function, includes the use of the glue::glue() function to make the outputted message more informative.\nThe {glue} package is used to manipulate string literals with the use of the curly braces (e.g., {}). When applied here, the {data()$event_date} is evaluated as an R call, its value becomes appended to the string, and the whole string is then outputted to the application’s UI.\n\n\n\nBack to the UI\nNow that there are five elements being outputted from the server, UI elements need to be included to display the rendered outputs.\nWhen making early design decisions about the application’s layout, it was decided these elements were going to reside within the main panel of the application. Another decision made was to keep the summary metric tile elements on the same row, so as to seem as though they are related to one another (i.e., related KPIs). As for the UI element informing the user on the date the summary metric tiles represent, it was decided that this element would be placed on its own row.\nTo achieve the intended design, additional Shiny layout functions were applied to the application’s code. This includes using the fluidRow() and column() functions to achieve the wanted UI organization. The following code was used to achieve the placement of the summary tiles within the application’s layout:\n\nmainPanel(\n   fluidRow(\n      column(),\n      column(),\n      column()\n   ),\n   br(),\n   fluidRow()\n)\n\nAs for the design of the summary metric tiles, each tile needed to include some type of title followed by the text representing the metric. To achieve this, the shiny::div() function was used. This function creates an individual HTML tag that outputs the text being passed along into the function. Directly below the title element, the textOutput() function is used to display the outputs coming from the application’s server. The code for one summary metric tile would look like the following:\n\ncolumn(3,\n       div(\"Unique Users\"),\n       textOutput(\"users\")\n       )\n\nBy combining these elements, the application code in its current state can be seen here:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\n# Import data -------------------------------------------------------------\n\nga4_data <- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n  )\n  \n# UI ----------------------------------------------------------------------\n\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        column(3,\n               div(\"Users\"),\n               textOutput(\"users\")\n        ),\n        column(3,\n               div(\"Page Views\"),\n               textOutput(\"page_view\")\n        ),\n        column(3,\n               div(\"Session Starts\"),\n               textOutput(\"session_start\")\n        ),\n        column(3,\n               div(\"Purchases\"),\n               textOutput(\"purchase\")\n        )    \n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver <- function(input, output, session) {\n  data <- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  # Text output\n  output$users <- renderText(format(data()$users, big.mark = ','))\n  output$page_view <- renderText(format(data()$page_view, big.mark = ','))\n  output$session_start <- renderText(format(data()$session_start, big.mark = ','))\n  output$purchase <- renderText(format(data()$purchase, big.mark = ','))\n  output$date <- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nIndeed, this code works and meets the functionality requirements. However, it’s quite verbose and contains a lot of redundant, repeated code. Different techniques could be applied to make the application more eloquent and efficient in its design. The goal of the next few sections, then, will be to simplify the application through the development of functions and applying functional programming principles.\n\nSimplifying the outputs\nReviewing the server, most of the outputs are created through the use of repeated patterns of the same code. This breaks the DRY principle (Don’t Repeat Yourself) of software development. Both functions and the application of functional programming principles will be applied to address this issue.\nAn obvious pattern used to create the outputs is output$foo <- renderText(format(bar, big.mark = ',')). This pattern could be converted into a function, and then this function could be used to iterate over the several reactive objects (e.g., data()$users) with the use of a {purrr} function. Since the side-effects are intended to be used rather than outputting a list object from our iteration, purrr::walk() will do the trick.\nUtilizing this strategy simplifies our code to the following:\n\nc('users', 'page_view', 'session_start', 'purchase') %>% \n    walk(~{output[[.x]] <- renderText(format(data()[[.x]], big.mark = ','))})\n\nIndeed, I can’t take full credit for this solution. Thanks goes to @Kent Johnson in the R4DS Slack channel for helping me out.\nThe output$date object was left out of this simplification of the code. Certainly, the function could be made to be more general and flexible to handle this repetition of the renderText() function. However, this would be over engineering a solution to the problem."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#back-to-the-ui-1",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#back-to-the-ui-1",
    "title": "Shiny summary tiles",
    "section": "Back to the UI",
    "text": "Back to the UI\nFunctions and functional programming principles will now be used to address these same issues on the UI side of the application. Much of the repetition occurs with the use of the following pattern:\n\ncolumn(3,\n       div(\"Metric Title\"),\n       textOutput(\"metric_output\")\n       )\n\nIndeed, this pattern is applied four times. Since it was copied and pasted more than twice and breaks the DRY principle, it would be best to convert it into a function and iterate it using functional programming tools."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#simplifying-the-ui-with-functional-programming",
    "title": "Shiny summary tiles",
    "section": "Simplifying the UI with functional programming",
    "text": "Simplifying the UI with functional programming\nA helper function, make_summary_tile(), is added to the setup section of the application. The function looks like this:\n\nmake_summary_tile <- function(title, text_output){\n  column(2,\n         div(title),\n         textOutput(text_output)\n  )\n}\n\nThere’s nothing too fancy or complicated about this function. It simply generalizes the pattern applied within the UI side of the first iteration of our application. As for placement, this function could be defined at the top of the application file or in a separate .R file embedded in a R/ sub-folder. Both strategies would make the function available for the app. Deciding which to use comes down to the intended organizational structure of the application.\nThe next step is to apply functional programming to iterate the make_summary_tile() function over the text outputs. Since the function requires two inputs, title and text_output, they were placed inside of a tibble to improve organization of the inputs being passed to the function through pmap().\n\n# Defined in the Setup section\ntiles <- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Used within the UI\npmap(tiles, make_summary_tile)\n\nWhat once required sixteen line’s of code was cut in half to eight (including the explicit definition of the inputs). In addition, coding the tiles using functional programming also makes it more flexible, where summary tiles could be easily added or taken away.\nDoing this would require some slight modification to the make_summary_tile() helper function, though. That is, a width argument would need to be added to the function, so the column width could be set to accommodate the number of outputs for the UI. There are lots of different options that could be explored here. At this point, though, the solution meets the functionality requirements.\nIn its current state, the application code looks like this:\n\n# Setup -------------------------------------------------------------------\n\nlibrary(shiny)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(glue)\nlibrary(purrr)\n\nmake_summary_tile <- function(header, text_output){\n  column(2,\n         div(header),\n         textOutput(text_output)\n  )\n}\n\ntiles <- tribble(\n  ~header         , ~text_output,\n  \"Users\"         , \"users\",\n  \"Page Views\"    , \"page_view\",\n  \"Session Starts\", \"session_start\",\n  \"Purchases\"     , \"purchase\"\n)\n\n# Import data -------------------------------------------------------------\n\nga4_data <- read_csv(\n  \"ga4_data.csv\", \n  col_types = list(event_date = col_date(\"%Y%m%d\"))\n)\n\n# UI ----------------------------------------------------------------------\n\nui <- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      dateInput(\"date\",\n                \"Select a date for summary:\",\n                value = max(ga4_data$event_date),\n                max = max(ga4_data$event_date),\n                min = min(ga4_data$event_date)\n      )\n    ),\n    mainPanel(\n      fluidRow(\n        h2(\"Summary report\"),\n        pmap(tiles, make_summary_tile)\n      ),\n      br(),\n      fluidRow(\n        textOutput(\"date\")    \n      )\n    ) \n  )\n)\n\n# Server ------------------------------------------------------------------\n\nserver <- function(input, output, session) {\n  data <- reactive({\n    filter(ga4_data, .data[[\"event_date\"]] == input$date)\n  })\n  \n  c('users', 'page_view', 'session_start', 'purchase') %>% \n    walk(~{output[[.x]] <- renderText(format(data()[[.x]], big.mark = ','))})\n  \n  output$date <- renderText(glue(\"Estimates represent data from {data()$event_date}\"))\n}\n\nshinyApp(ui, server)\n\nThe application works, meets the functionality requirements, and now is written in a way that reduces repetition and redundant patterns within the code. However, the summary metric tiles just blend into the UI, and nothing about the styling communicates they contain important information.\nSince these elements are meant to highlight key, important summary metrics, they need to be styled in a way that creates contrast between themselves and the application’s background. The next section focuses on applying custom CSS to give some contrast between these elements and the application’s background."
  },
  {
    "objectID": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "href": "blog/posts/2021-12-30-shiny-series-metric-summary-tiles/index.html#creating-the-www-folder-and-css-file",
    "title": "Shiny summary tiles",
    "section": "Creating the www folder and CSS file",
    "text": "Creating the www folder and CSS file\nSince the design opted for a file-based CSS approach, a separate www sub-folder in the application’s main project directory needs to be created. Once created, the custom CSS file will be placed inside this folder. The placement of this file can be seen in this Github repo.\nThe purpose of this folder is to make the file available to the web browser when the application starts. Placement of this file is critical. If it is not placed in the www sub-folder, then the CSS file will not be available when the application starts, and any custom styling will not be applied.\nOnce the www sub-folder is created, you can create a CSS file for the application in Rstudio by clicking File, hovering over New File, and selecting CSS File. Save the file in the www sub-folder and give it an informative name. In the case of this example, the file is named app-styling.css.\nThe main goal of the styling will be to create some contrast between the summary metric tiles and the application’s background. Specifically, CSS will be used to create a container that is a different color from the application’s background and includes some shading to make it seem like the element is hovering above the application’s main page. To do this, the app-styling.css file includes the following:\n#summary-tile{\n  font-size: 25px;\n  color:White;\n  text-align: center;\n  margin: 10px;\n  padding: 5px;\n  background-color: #0A145A;\n  border-radius: 15px;\n  box-shadow: 0 5px 20px 0 rgba(0,0,0, .25);\n  transition: transform 300ms;\n}\nA detailed description on how to create CSS selectors is outside the scope of this post. However, in general terms, this selector sets several values for multiple CSS properties by defining the id, #summary-tile within the file. More about this process of creating different CSS selectors can be found here.\nNow it’s just a matter of modifying the code to call this file and pass these style values to the summary tiles within the application. The following code is added to the ui side of our application to include our app-styling.css file:\n\ntags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"app-styling.css\"))\n\nSince the styling is being applied to the summary metric tiles, the make_summary_tile() function is modified to bring in the CSS elements. A css_id argument is added to the function.\n\nmake_summary_tile <- function(header, text_output, css_id){\n  column(2,\n         div(header),\n         textOutput(text_output),\n         id = css_id\n  )\n}\n\nNow that we made this modification to the make_summary_tile(), its application in the UI is also modified. Specifically, the #summary-tile CSS element is explicitly called in pmap(). To do this, the code is modified like this:\n\npmap(tiles, ~make_summary_tile(\n          header = ..1, text_output = ..2, css_id = \"summary-tile\"))\n\nThe header, text_output, and css_id arguments are now explicitly defined in the pmap() call. To refer to the first two elements in the tiles data object, the ..1 (i.e., header column) and the ..2 (i.e., text_output column) are used. Check out the pmap() docs on how to apply the ..1, ..2 (?pmap) for more information."
  },
  {
    "objectID": "blog/posts/2021-04-02-intro-post/index.html",
    "href": "blog/posts/2021-04-02-intro-post/index.html",
    "title": "Intro Post",
    "section": "",
    "text": "The purpose of this blog\nThe purpose of this blog is to serve as a location for me to express my thoughts on topics I find interesting. To be honest, I don’t expect this to be a really niche blog with one focused, clear purpose. Most likely it will be data analysis and/or visualization focused, which I will apply to develop posts in areas I find interesting: open-source software, media/marketing analytics, data analysis, sports, media, etc. My purpose may become more refined once I find my voice.\n\n\nThe inspiration and motivation to do this blog\nI have spent countless hours reading, re-reading, bookmarking, and Googling multiple topics regarding the use of the statistical computing programming language called R. Much of this time has been spent accessing useful, open-source, and free content that has aided me professionally, and it has contributed to my deeper understanding of the topics I find interesting. I couldn’t even begin to describe how grateful I am for those who have spent time organizing and drafting content others find useful. In fact, it’s the #Rstats community that has motivated me to put this blog together, as I have seen how helpful and open it is to aiding in the development of others.\nI now feel I am in a place of not only being a consumer but a producer of this information. I will never be an expert in this area, as there is too much to learn for just one person. As one of my favorite podcasts (i.e., Make Me Smart With Kai and Molly) states in every episode, “None of us is as smart as all of us.” Thus, I feel it is time to start organizing and drafting content others will hopefully find useful, at the very least amusing. Even if this blog helps one person, that will be enough motivation to keep me working on it.\n\n\nWhat’s up next?\nNot sure. I’m just excited I got this blog up and running. Most likely I’ll do something sports related. Who knows–stay tuned.\n\n\nReferences & Acknowledgements\n\nI make every attempt to properly cite information from other sources. If I have failed to properly attribute credit to a source, please kindly let me know."
  },
  {
    "objectID": "blog/posts/2022-12-22-test/index.html",
    "href": "blog/posts/2022-12-22-test/index.html",
    "title": "Test Blog",
    "section": "",
    "text": "This is a post test."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "BigQuery\n\n\nsql\n\n\ndata wrangling\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nforecasting\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2022\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntalks\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npersonal\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2021\n\n\nCollin K. Berke, Ph.D.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Collin K. Berke, Ph.D. is a media research analyst in public media. Professionally, he uses data and media/marketing research methods to answer questions on how to best reach and engage audiences–towards the goal of enriching lives and engaging minds with public media content and services. He is especially interested in the use and development of open-source statistical software (i.e. Rstats) to achieve this goal, and developing a broader understanding the role these tools can play in media, digital, and marketing analytics.\nHe has experience using different software, third-party services, and programming languages from developing several analytics projects, both in industry and academia. In regards to programming languages, he has developed projects using R, SQL, $bash, and a little bit of Python. He also has extensive experience using different analytics solutions. For data warehousing, he mostly uses database tools like Postgres and those in the Google Cloud Platform ecosystem (e.g., Google BigQuery). When it comes to automating workflows and data pipelines, he has experience implementing and working with Apache Airflow. He also has extensive experience using third-party tools and software to analyze, wrangle data, and communicate his analyses (e.g., Google Analytics, Google Sheets, Excel, Google Data Studio, and R Shiny). Most of his current work is industry related.\nCollin also serves as an adjunct instructor for the University of Nebraska-Lincoln and Southeast Community College, where he teaches courses in sports data visualization and analysis and communication specific courses. He holds a M.A. in Communication Studies from the The University of South Dakota and a Ph.D. in Media and Communication from Texas Tech University. He has also published and contributed to the publication of several academic journal articles.\nCollin is a self-proclaimed news, sports, and podcast junkie. He really enjoys listening to NPR, watching PBS (especially NOVA), and indulging in college football and baseball. At times, he will write blog posts on topics he finds interesting in these areas.\nCheck out the Now page to see what Collin is currently reading and working on."
  },
  {
    "objectID": "about.html#note-about-this-site",
    "href": "about.html#note-about-this-site",
    "title": "About",
    "section": "Note about this site",
    "text": "Note about this site\nThe views expressed on this site are my own, and they do not reflect the views of my employer, professional and/or community groups I hold membership. Any analyses hosted on this site were done for professional development or were for fun. I make every attempt to perform valid and accurate data analysis and reporting. Unless otherwise noted, none of the content on this site has been peer-reviewed, and thus any conclusions drawn or uses stemming from this work need to take these limitations into account.\nQuarto and Netlify were used to build and deploy this site. All my blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License."
  },
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "A now page, why?"
  },
  {
    "objectID": "now.html#projects-im-working-on",
    "href": "now.html#projects-im-working-on",
    "title": "Now",
    "section": "Projects I’m working on",
    "text": "Projects I’m working on\n\nProject Conduit\nCurrently developing and maintaining a data pipeline project built using R and Python. Technology utilized includes Google Cloud resources, Docker, Apache Airflow, Google BigQuery, Google Analytics, Google Data Studio, and Shiny. The goal is to centralize and automate data processing, storage, and reporting.\n\n\nR for Data Science Online Learning Community book club facilitator\nRecently started facilitating an R for Data Science Online Learning Community online book club (check it out by joining the Slack workspace). This group is currently reading through Hadley Wickham’s Advanced R book. The group meets weekly online over Zoom. Meetings are open to anyone who is a part of the Slack group (Join the #book_club-advr channel to keep up with the book club). Check out the playlist of past meeting recordings here. I would love for more to join and be a part of this group.\n\n\nExperimenting with Neovim\nI’ve been experimenting more and more with Neovim for my development work. I’m becoming more comfortable with the different modes, movements, actions, and various tools for editing text and code. Still struggling through the configuration and plugin ecosystem to set up workflows that are the most productive. Going through this process has been a challenge, but has me really reflecting on how I approach my work, evaluating what is needed, not needed, and focusing on the bad habits I need to break."
  },
  {
    "objectID": "now.html#books-im-reading",
    "href": "now.html#books-im-reading",
    "title": "Now",
    "section": "Books I’m reading",
    "text": "Books I’m reading\n\nAdvanced R by Hadley Wickham\nI’m reading this book as part of the R for Data Science Online Learning Community book club. See the ‘Projects I’m working on’ section for some more detail.\n\n\nMastering Shiny by Hadley Wickham\nDoing another read through of this book, as I’m leading my work team through the material. Our aim is to start developing more Shiny applications to communicate our work and analyses. As such, we are focusing on learning the basics of Shiny app development.\n\n\nRegression and other stories by Andrew Gelman, Jennifer Hill, and Aki Vehtari\nSlowly reading through this book. It’s been some time since I’ve focused on developing my modelling skills, so I was wanting to spend some more time developing in this area.\n\n\nAbaddon’s Gate (The Expanse, 3)\nI’ve taken up reading the third book in the Expanse series. A breakneck science fiction adventure that continues to follow Earth, Mars, and the Outer Planets Alliances’ mission to explore a massive alien artifact, which seems to be a gate leading to a starless, dark dimension. Is this gate an opportunity or a threat? Or is the greatest threat among those sent to examine this paradigm shattering object?"
  },
  {
    "objectID": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "href": "now.html#a-list-of-books-ive-read-ever-since-ive-started-keeping-track",
    "title": "Now",
    "section": "A list of books I’ve read (ever since I’ve started keeping track)",
    "text": "A list of books I’ve read (ever since I’ve started keeping track)\n\nProfessional development reads\n\nR Packages by Hadley Wickham and Jenny Bryan. Check out past book club meeting recordings here.\n\n\nVim help files maintained by Carlo Teubner\n\n\nMastering Ubuntu by Jay LaCroix\n\n\nGoogle BigQuery: The Definitive Guide by Valliappa Lakshmanan and Jordan Tigani\n\n\nMastering Shiny by Hadley Wickham. Check out the past book club meeting recordings here.\n\n\nR for Data Science by Hadley Wickham and Garrett Grolemund. Check out the past book club meeting recordings here.\n\n\nDocker Deep Dive by Nigel Poulton\n\n\n\nPersonal reads\n\nCaliban’s War (The Expanse, 2) by James S.A. Corey\n\n\nLeviathon Wakes (The Expanse, 1) by James S.A. Corey\n\n\nThe Galaxy, and the Ground Within: A Novel (Wayfarers 4) by Becky Chambers\n\n\nRecord of a Spaceborn Few (Wayfarers 3) by Becky Chambers\n\n\nA Closed and Common Orbit (Wayfarers 2) by Becky Chambers\n\n\nThe Long Way to a Small, Angry Planet (Wayfarers 1) by Becky Chambers\n\n\nLast of the Breed by Louis L’Amour\n\n\nProject Hail Mary by Andy Weir\n\n\nFirebreak by Nicole Kornher-Stace\n\n\nDune Messiah by Frank Herbert\n\n\nDune by Frank Herbert\n\n\nThe Martian: A Novel by Andy Weir"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "",
    "text": "I had the great fortune of being a presenter at this year’s PBS TechCon conference. The focus of my talk was to introduce attendees to the principles of tidy data and discuss a data pipeline project my team has been working on at Nebraska Public Media. Here’s the session description:\nAs part of my talk, I mentioned having put together a curated list of resources others could use to learn more about the topics covered. This list can be found in the following section of this blog post. If you’re interested in discussing these topics further, please reach out."
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#tidy-data",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Tidy Data",
    "text": "Tidy Data\n\nTidy Data paper published in the Journal of Statistical Software written by Hadley Wickham\nTidy Data Chapter published in the open source R for Data Science book written by Hadley Wickham and Garrett Grolemun\nData Organization: Organizing Data in Spreadsheets post by Karl Broman\nData Organization: Organizing Data in Spreadsheets paper published in The American Statistician written by Karl Broman and Kara Woo\nTidy data section in Data Management in Large-Scale Education Research training modules written by Crystal Lewis\nTidy Data presented by Hadley Wickham\nTowards Data Science post published on Medium summarizing tidy data written by Benedict Neo"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#join-a-community",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Join a Community",
    "text": "Join a Community\n\nR for Data Science Online Learning Community\n\nJoin the Slack workspace\n@Collin Berke to get a hold of me in the workspace"
  },
  {
    "objectID": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "href": "blog/posts/2021-10-19-2021-pbs-techcon-your-data-is-disgusting/index.html#open-source-workflow-management-tool",
    "title": "2021 PBS TechCon: Your Data is Disgusting!",
    "section": "Open source workflow management tool",
    "text": "Open source workflow management tool\n\nApache Airflow"
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html",
    "href": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html",
    "title": "Shiny series: Implementing a next and back button",
    "section": "",
    "text": "As part of the R4DS bookclub, our cohort has been working through Hadley Wickham’s (2020) Mastering Shiny book. Chapter 4: Case study: ER injuries had an interesting, challenging exercise I couldn’t figure out. This post aims to walk through my process of trying to solve this exercise, overviewing my thought process while trying to solve it. My hope is future me will look back at this post when I need to implement such a feature in a future Shiny application. I hope others find it useful as well."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "href": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#the-initial-runtime-variables",
    "title": "Shiny series: Implementing a next and back button",
    "section": "The initial runtime variables",
    "text": "The initial runtime variables\nAs part of my testing of the actionButton() UI function, I found out the initial value being sent to the server was zero. I also found out that zero can’t be used for subsetting (i.e, nothing is gets returned to the UI). To address this issue, a variable with a reactive value of one needed to be in the environment upon runtime of the application. This is so we can use the initial value of one to return the first element of our data to our output$series in the textOutput() function in the UI when the application starts. Let’s take a look at this in action.\n\nlibrary(shiny)\n\nseries <- c(\"a\", \"b\", \"c\")\n\nui <- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver <- function(input, output, session) {\n \n  # Create a reactive value of 1 in the environment\n  place <- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series <- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nYou’ll notice a new function here in the server, reactiveVal(). According to the documentation, this function is used to create a “reactive value” object within the app’s environment. Basically, I understand this function is just creating a reactive expression where the initial value is one upon the runtime of the application, which is then used in the subsetting operation applied in the renderText() function. Great, we have partly solved the indexing issue with the use of reactiveVal(1). You’ll also notice the buttons don’t work here because there is no dependency on them as an input, but I’ll get to that here shortly by applying some observeEvents() functions.\n\nThe maximum index value\nI also needed a solution to help limit the range of values that could be used for indexing in our subsetting operation. I now had the lower value one available in the environment, however I did not have the maximum value. At this point, I needed a function to calculate the length of the data and to treat it as a reactive expression, as this number might be dynamic in the larger application, and the users’ inputs will determine what data gets displayed within the application (e.g., filtering by product code selection). We can easily calculate the length of our data using the length() function and making this a reactive expression by wrapping it with the reactive() function. Here is what this looks like with code.\n\nlibrary(shiny)\n\nseries <- c(\"a\", \"b\", \"c\")\n\nui <- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver <- function(input, output, session) {\n \n  # Determine the upper part of the subset index range\n  max_no_values <- reactive(length(series))\n  \n  # Create a reactive value of 1 in the environment\n  place <- reactiveVal(1)\n  \n  # Use this reactive value to subset our data\n  output$series <- renderText({\n    series[place()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIt’s challenging to show this value in the environment in writing, but now given the current code, I have the lower value of the range, one, and the maximum value three corresponding to the number of values in our data structure available in the environment. This is great, so now I have those two values available to help with subsetting. At this point, we also need to incorporate the two user inputs, the Back and Next buttons. However, since we know these two buttons increment by one every time they are pressed, I need to rely on some mathematical operations to control the range of values used to subset the data. Given the simplified application, I know 1, 2, or 3 is the values and range of values I need to properly apply within a subsetting operation."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#enter-the",
    "href": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#enter-the",
    "title": "Shiny series: Implementing a next and back button",
    "section": "Enter the %%",
    "text": "Enter the %%\nPart of getting this functionality to work required the use of the modulus %% and modular arithmetic. Basically, modulus is an arithmetic operation that performs a division and returns the remainder from the operation. I learned a lot about this in this article here (Busbee and Braunschweig n.d.). The R for Data Science book (Wickham and Grolemund 2017) also introduces the use of %% as well. While researching the modulus, I found many useful applications for it within programming. It’s definitely worth some more time learning of its other uses. When applied in our case, though, we needed it to keep the subsetting index within the bounds of the size of our data structure.\nI am far from a mathematician, so the following explanation of the logic behind how a modulus is applied here is going to be a little fast and loose. However, I’m going to take a crack at it. Take for example our application. On runtime, we have a reactive value place() that starts at the value one. We also know that our maximum number of values that can be used as an index for our subsetting operation is three, our max_no_values reactive (i.e., c(\"a\", \"b\", \"c\")). We can now use the modulus with these two values to limit the number we are using in the index of our subsetting based on the number of clicks by the user. Here is a simplified example using code illustrating this point.\n\nmax_no_values <- 3\n\n# User clicks the button to increment the index of the subset\n# Vector corresponds to the value outputted by the `actionButton()`\nuser_clicks <- c(0:12)\n\nuser_clicks %% max_no_values\n\n [1] 0 1 2 0 1 2 0 1 2 0 1 2 0\n\n\nEarlier in the post, we found out that we can’t use zero to subset, as nothing gets returned. So to solve our issue, we need to shift these values by adding one to the vector. Notice how that with every ‘click’ the range of these values never goes below one or exceeds three, even when a user’s click count (keep in mind every click of the actionButton() increments by one) goes above three. This is the power of the %%, as this operation keeps our index range between 1 - 3, regardless of how many times the user clicks an action button.\n\nuser_clicks %% max_no_values + 1\n\n [1] 1 2 3 1 2 3 1 2 3 1 2 3 1\n\n\nThe math is a little different for the Back button, though. However, the same principles apply.\n\n((user_clicks - 2) %% 3) + 1\n\n [1] 2 3 1 2 3 1 2 3 1 2 3 1 2\n\n\nLet’s use some print debugging here to show how the of %% works in action. I’m going to use the glue package to help make the messages sent to the console more human readable.\n\nlibrary(shiny)\nlibrary(glue)\n\nseries <- c(\"a\", \"b\", \"c\")\n\nui <- fluidPage(\n  actionButton(inputId = \"back\", label = \"Back\"),\n  actionButton(inputId = \"forward\", label = \"Next\"),\n  textOutput(\"series\")\n)\n\nserver <- function(input, output, session) {\n \n  # Determine the total number \n  max_no_values <- reactive(length(series))\n  \n  position <- reactiveVal(1)\n  \n  # These cause a side-effect by changing the place value\n  observeEvent(input$forward, {\n    position((position() %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  observeEvent(input$back, {\n    position(((position() - 2) %% max_no_values()) + 1)\n    message(glue(\"The place value is now {position()}\"))\n  })\n  \n  output$series <- renderText({\n    series[position()]\n  })\n  \n}\n\nshinyApp(ui, server)\n\nIf you click the Back and Next button and watch your console, you’ll see the position value for every click being printed. While clicking these values, you will observe a couple of things:\n\nYou’ll notice the value zero is never passed as a subsetting index value.\nThe arithmetic operations constrain our subsetting values within a range of 1 - 3, the length of our character vector.\nMultiple clicks remain in order, regardless if the user clicks the Next or Back buttons (e.g., 1, 2, 3 or 3, 2, 1).\n\nAt this point, we can get rid of our print debugging code, test our working example, and bask in our accomplishment of understanding how this solution works. The next step is to now integrate what we know into the larger application. We’ll do that here in the next section of this post."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#product-selection",
    "href": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#product-selection",
    "title": "Shiny series: Implementing a next and back button",
    "section": "Product selection",
    "text": "Product selection\nAs part of the original functionality of the app, users were given a selectInput() in the UI to filter for injuries that were the result of different products. The requirements stated the outputted narratives also needed to reflect the users’ filter selection. This functionality needed to be added back in, and it also needed to be reactive. I do this by adding the selected <- reactive(injuries %>% filter(prod_code == input$code)) near the beginning portion of the server section of the code. You’ll also notice we are using the filter() function and %>% operator here, so we need to also bring in the dplyr package (i.e., library(dplyr)).\nThere are now two areas in the server that have a dependency on the selected() reactive expression, the max_no_stories() reactive and our output$narrative object. Since our reprex was using a simplified vector of data (e.g., c(\"a\", \"b\", \"c\")), we need to modify the code to use these reactives. The biggest change is we are now passing a tibble of data rather than a character vector of data. As such, I need to use selected()$narrative to refer to the narrative vectors we want to use in our server function. Nothing else really changes, as the underlying process of determining the range of values and using a mathematical operation to limit the indexing stays the same. We are just now applying this process to a different set of data, although it is technically a reactive expression rather than an object in our environment."
  },
  {
    "objectID": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "href": "blog/posts/2021-09-12-shiny-series-implementing-a-next-and-back-button/index.html#cases-where-users-select-a-new-product-code",
    "title": "Shiny series: Implementing a next and back button",
    "section": "Cases where users select a new product code",
    "text": "Cases where users select a new product code\nGiven the functionality provided within our application, it’s reasonable to expect users would change the product code (i.e., the main purpose is to give users tools to explore the data). It’s also reasonable that the user would then expect the narrative values to change based on their product selection, and indeed we have built this functionality into the app. However, what we didn’t account for yet was what users expectations are for the order to which the new filter data will be presented. When users make a change to their filtering criteria, they would most likely expect that the updated narrative data would start at the beginning, not where their previous clicks would place them within their previously selected data. Given this expectation, I now need some code to ‘reset’ the subsetting index when a user changes their product code filter.\nWhy might this be important? Take for example if the aim of this functionality was to output the most recent injury reported for a specific product code. Our user would expect that any time they switch their product code filtering input, the displayed narrative would be the most recent reported injury, and that each subsequent click would result in a chronological walk through the narratives, either forwards or backwards. This would especially be important if the app was connected to a streaming data source that isn’t static. Moreover, you might even modify the output$narrtive object to include the date, so the user is informed on when a specific injury was treated. For the sake of keeping things simple though, we will only add the reset behavior to the app in this post.\nThis reset of the indexing value was provided in the solutions guide referenced above, and it adds another observeEvent() to make this work. Specifically, it directed me to add this code to the server section of the application:\n\nobserveEvent(input$code, {\n    place(1)\n  })\n\nHere you can see that the observeEvent() is waiting for any changes to the input$code input. When a change occurs to this input, the place(1) is run, and the subsetting index is set back to one. We now have included functionality to the app where when the user changes the product code filtering, the narrative increment index will display the first value in that subset of injuries as selected by the user."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "",
    "text": "Managing an e-commerce websites requires some ability to predict and plan for the future. Some questions a site owner might want to predict include: What content/products will users view the most this coming period (i.e., daily, weekly, etc.)? To what effect will time of year have on the content/products users view (e.g., holiday shopping)? What and how much of each product needs to be in stock to meet users’ demands? Most importantly, what types of content/product views result in more completed orders?\nA crystal ball to consult would be ideal. Unfortunately, one has not been developed, yet. And the future isn’t looking bright for such an innovation. So then, what other tools are available to assist in the planning and goal setting surrounding the development and management of an e-commerce website?\nEducated guesses are a start (i.e., hypotheses). These hypotheses could be based on some domain knowledge, further supported with historical data. Some common guesses might be: This year will be similar to last year. This quarter’s sales will be similar to last quarters. Site views will increase this quarter. XYZ will see a decrease in sales. Although domain knowledge and past experiences can be used to inform our hypotheses, other tools can be used to generate evidence to support our hypotheses. One such tool is forecasting. A forecast is a tool to help reduce uncertainty; the process and methods used to predict the future as accurately as possible, given all the information available (Hyndman and Athanasopoulos 2021).\n\n\nThis series of blogposts will focus on creating forecasts using Google Analytics 4 data. Specifically, this series overviews the steps and methods involved when developing forecasts of time series data. This blog series begins with a post overviewing the wrangling, visualization, and exploratory analysis involved when working with time series data. The primary focus of the exploratory analysis will be to identify interesting trends for further analysis and application within forecasting models. Then, subsequent posts will focus on developing different forecasting models. The primary goal of this series is to generate a forecast of online order completions on the Google Merchandise store.\n\n\n\nAnother intention of this series is to document and organize my learning and practice of time series analysis. Although I try my best to perform and report valid and accurate analysis, I will most likely get something wrong at some point in this series. I’m not an expert in this area. However, it’s my hope that this series can be a supplement to others who may be learning and practicing time series analysis. In fact, seeing somebody (i.e., myself) do something wrong might be a valuable learning experience for someone else, even if that someone is my future self. If I got something wrong, I would greatly appreciate the feedback and will make the necessary changes.\nThroughout the series, I will document the resources I used to learn the process involved when generating forecasting models. I highly suggest using these as the primary source to learn this subject, especially if you intend to use this type of analysis in your own work. Specifically, the process and methods detailed in this series are mostly inspired by the Forecasting: Principles and Practice online textbook by Rob J Hyndman and George Athanasopoulos, and it utilizes several packages to wrangle, visualize, and forecast time series data (e.g., tsibble; fable; and feasts). I am very thankful to the authors and contributors for making these materials open-source."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#setup",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Setup",
    "text": "Setup\nThe following is the setup steps needed to perform this exploratory analysis.\n\n# Libraries needed\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(lubridate)\nlibrary(fable)\nlibrary(feasts)\nlibrary(fuzzyjoin)\nlibrary(bigrquery)\nlibrary(glue)\nlibrary(GGally)\nlibrary(scales)\nbq_auth()\n\n## Replace with your Google Cloud `project ID`\nproject_id <- 'your.google.project.id'\n\n\n## Configure the plot theme\ntheme_set(\n  theme_minimal() +\n    theme(\n       plot.title = element_text(size = 14, face = \"bold\"),\n       plot.subtitle = element_text(size = 12),\n       panel.grid.minor = element_blank()\n    )\n)"
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#the-data",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "The data",
    "text": "The data\nAs was used in previous posts, Google Analytics 4 data for the Google Merchandise Store are used for the examples below. Data represents website usage from 2020-11-01 to 2021-12-31. Google’s Public Datasets initiative makes this data open and available for anyone to use (as long as you have a Google account and have access to Google Cloud resources). Data are stored in Google BigQuery, a data analytics warehouse solution, and are exported using a SQL like syntax. Details on how this data were exported can be found in this GitHub repository. More about the data can be found here."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#export-the-data",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Export the data",
    "text": "Export the data\nThe first step in the process was to export all page_view events. To do this, the following SQL code was submitted to BigQuery using the bigrquery package. Keep in mind Google charges for data processing performed by BigQuery. Each Google account–at least since the writing of this post–had a free tier of usage. If you’re following along and you don’t have any current Google Cloud projects attached to your billing account, this query should be well within the free usage quota. However, terms of service may change at anytime, so this might not always be the case. Nevertheless, it is best to keep informed about the data processing pricing rates before submitting any query to BigQuery.\nselect \n    event_date,\n    user_pseudo_id,\n    event_name,\n    key,\n    value.string_value\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nCROSS JOIN UNNEST(event_params)\nwhere \n   event_name = 'page_view' and \n   _TABLE_SUFFIX between '20201101' and '20211231' and \n   key = 'page_location'\nThe query returns a data set with 1,350,428 rows and the following columns:\n\nevent_date - represents the date the website event took place.\nuser_pseudo_id - represents a unique User ID.\nevent_name - The name of the event specified by Google Analytics. In our case this will just be page_view given the filtering criteria.\nkey - represents the page_location dimension from the data. This column should only contain page_location.\nstring_value - represents the page to which the event took place. In other words, the page path a page_view event was counted.\n\nThis query returns a lot of data. Thus, the analysis’ scope needed to be narrowed to make the exploratory analysis more manageable. To do this, top-level pages were identified and data wrangling procedures were performed to reduce the data down to pages relevant to the exploratory analysis.\n\nNarrowing the analysis’ scope to relevant pages\nThe overall aim of the series is to forecast Order Completed page views. As part of this, relevant pages that could be used to improve forecasting models needed to be a part of the exploratory analysis. However, this is challenging given the sheer amount of pages being represented within the data. Some pages relevant to the analysis, others, not so much. Given the number of possible pages, a decision was made to only examine key, top-level pages. The question is, then, what pages should be considered relevant to the analysis?\n\n\nDetermining top-level pages\nThe navigation bar of the Google Merchandise Store was used to determine the top-level pages. Indeed, it’s reasonable to expect the navigation bar is designed to drive users to key areas of the site. Developers won’t waste valuable navbar real-estate for content users would consider useless and/or irrelevant (i.e., these are developers at Google, so they mostly likely have a good idea of how users use a website). With this in mind, the following pages were identified as potential candidates for further analysis.\n\nNew products\nApparel\nLifestyle\nStationery\nCollections\nShop by Brand\nSale (i.e., Clearance)\nCampus Collection\n\nThe checkout flow is another key component of any e-commerce website. Indeed, a main goal of the site is to convert visits into order completions. As such, pages related to the checkout flow might be another area of interest in the analysis. It’s challenging to piece together the checkout flow by just looking at the data. So, I purchased a few products to observe the checkout flow and track the different pages that came up. The checkout flow–at least when I made my purchase–went in this specific order:\n\nReview basket\nSign in (I wasn’t signed into my Google account)\nReview my information\nPayment info\nOrder review\nOrder completed\n\nAlthough these pages were identified as potential candidates for further analysis, it’s important to recognize the Google Merchandise store is not static, and thus the design and layout may have changed from the dates the data represents vs. when I went through the checkout flow. Regardless, these initial observations provided a starting point to help narrow the analysis’ focus."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#filtering-out-homepage-events",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Filtering out homepage events",
    "text": "Filtering out homepage events\nNow that the analysis’ scope had been narrowed to top-level pages, events associated with homepage views were filtered out to reduce the number of events within the data. To do this, the regex_filter variable was created using the glue() function from the glue package, which was then applied within a filter statement.\n\nregex_page_filter <- glue(\n  \"(\",\n  \"^https://www.googlemerchandisestore.com/$|\",\n  \"^https://googlemerchandisestore.com/$|\",\n  \"^https://shop.googlemerchandisestore.com/$\",\n  \")\"\n  )\n\nThe variable contained multiple regex expressions, as several page paths in the data represented home page visits. Defining the variable in this way ensured the filter excluded all data associated with homepage visits.\nOnce the filter statement was set up, the str_to_lower() function from the stringr package was used to convert all the page paths to lower case. The following code chunk demonstrates how these operations were performed.\n\nga4_pagepaths <- ga4_pageviews %>% \n  filter(!str_detect(string_value, regex_page_filter)) %>% \n  mutate(string_value = str_to_lower(string_value))\n\nThe filtering resulted in a reduced data set (i.e., ~1 million rows). Since the intent was to further narrow the analysis’ scope, additional filtering was performed. Specifically, the data were filtered to return a data set containing the top-level pages identified previously.\nAnother variable–similar to regex_filter–was created and used to filter the data further. Given the number of pages, though, a filtering join would be more appropriate (e.g., semi-join). The problem is the join operation needed to filter the data needed to be based on several regular expressions.\nA semi-join using a regular expression is not supported with dplyr’s joins, so the regex_semi_join() function from David Robinson’s {fuzzyjoin} package was used. This package provides a set of join operations based on inexact matching. A separate data set (tracked_data), containing the regular expressions was then created, imported into the session, and used within the join operation. A dplyr::left_join() was then used to include this data within a tidy dataset. The following chunk provides example code to perform these operations.\n\ntracked_pages <- read_csv(\"tracked_pages.csv\")\n\ntop_pages_data <- ga4_pagepaths %>% \n  mutate(\n    string_value = str_remove(\n      string_value,'https://shop.googlemerchandisestore.com/')) %>% \n  regex_semi_join(tracked_pages, by = c(\"string_value\" = \"page_path\")) %>% \n  regex_left_join(tracked_pages, by = c(\"string_value\" = \"page_path\"))\n\nAt this point, the data is more manageable and easier to work with. At the start, the initial export contained around 1.6 million rows. By narrowing the focus of the analysis and performing several data wrangling steps to filter the data, the final tidy data set contained around 320,000 rows.\nGiven the limited amount of storage available and how this post is hosted makes authentication into BigQuery challenging, I opted to not integrate the extraction steps into the rendering steps and to exclude the full data with this post. However, I included the filtered data set in a .rds file to conserve space. I imported this file by running the following code chunk to continue the exploratory analysis. I would skip this step and just directly export the data from BigQuery if this analysis was performed outside the forum of a blog post.\n\ntop_pages_data <- readRDS(\"top_pages_data.rds\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#data-exploration",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Data exploration",
    "text": "Data exploration\nWith data in a tidy format, the exploratory analysis and further identification of relevant series for forecasts of Order Completed page views can take place. One area of possible exploration is to identify which pages generate a significant amount of traffic. Indeed, it’s possible that pages with a lot of traffic might also result in more order completions: more traffic indicates more interest; more interest could mean more orders.\nA few questions to answer:\n\nWhich top-level pages have the most unique users?\nWhat pages get the most traffic (i.e., page views)?\n\nA simple bar plot is created to answer these questions. Here’s the code to create these plots, using the ggplot2 package.\n\npage_summary <- top_pages_data %>% \n  group_by(page) %>% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n()\n  ) %>% \n  arrange(desc(unique_users))\n\n\nggplot(page_summary, aes(x = unique_users, y = reorder(page, unique_users))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by users\",\n       subtitle = \"Apparel page viewed by a significant amount of users\",\n       y = \"\",\n       x = \"Users\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       )\n\n\n\n\n\nggplot(page_summary, aes(x = views, y = reorder(page, views))) + \n  geom_col() +\n  scale_x_continuous(labels = scales::comma) +\n  labs(title = \"Top-level pages by views\",\n       subtitle = \"Apparel and basket pages generate significant amount of views\",\n       y = \"\", \n       x = \"Views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\nApparel clearly seems to not only have received a significant amount of users, but a high number of page views as well. It’s also interesting to note that the basket had nearly half the amount of users compared to apparel, but the amount of page views was similar. It’s also apparent, at least with the data available, that more users browsed clearance then they did new items during this period. Just looking at the current summary for the period, apparel might be a potential time series to include within forecasting models of order completions.\nAlthough the apparel page is a likely candidate for the forecasting models, supplemental data should be examined to justify its inclusion. For instance, actual purchase/financial data could provide further justification of the business case and value of focusing on this specific area within future analyses. For instance, apparel may drive a lot of traffic, but it may not be an area where much revenue or profit is generated. Thus, the focus on more money generating/profitable products may be better candidates to improve the accuracy of our forecasting models. Despite this, actual purchase and financial data are not available. As a result, this is not explored further."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#create-time-series-data-visualizations",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Create time series data visualizations",
    "text": "Create time series data visualizations\nVisualizing the time series is the next step in the exploratory analysis. This step will be further helpful in identifying potential time series that may be of value in creating a forecast of Order Completed page views.\n\nConvert the tibble into a tsibble\nThe top_pages_data tibble is now converted to an object that contains temporal structure. To do this, the as_tsibble() function from the {tsibble} package is used. This package provides a set of tools to create and wrangle tidy temporal data. Before the temporal structure could be mapped to the data set, a few wrangling steps were performed: 1). the event_date column was converted into a date variable; and 2). data were aggregated to count the number of unique_users and views. The following code chunk contains an example of these steps.\n\npages_of_interest <- c(\"Apparel\", \n                       \"Campus Collection\", \n                       \"Clearance\", \n                       \"Lifestyle\", \n                       \"New\", \n                       \"Order Completed\", \n                       \"Shop by Brand\")\n\ntidy_trend_data <- top_pages_data %>% \n  mutate(event_date = parse_date(event_date, \"%Y%m%d\")) %>% \n  group_by(event_date, page) %>% \n  summarise(\n    unique_users = n_distinct(user_pseudo_id),\n    views = n(), \n    .groups = \"drop\"\n  ) %>% \n  as_tsibble(index = event_date, key = c(\"page\")) %>% \n  filter(page %in% pages_of_interest)\n\nAt this point, several trend plots could be created using the ggplot2 package. However, the feasts package provides a convenient wrapper function to quickly make trend visualizations of tsibble objects, autoplot(). The outputted plot was then formatted to improve readability.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of page views\", \n       subtitle = \"Apparel drove a significant amount of views\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.title = element_blank())\n\n\n\n\nggplot2’s facet_wrap() function was used to create a plot for each series. Splitting the plots into separate entities allowed for a clearer view of the characteristics within each series.\n\nautoplot(tidy_trend_data, views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(.~page, scales = \"free_y\")  +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plots of page views\", \n       subtitle = \"Various characteristics are present within each series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#identify-notable-features-using-time-plots",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Identify notable features using time plots",
    "text": "Identify notable features using time plots\nApparel again emerges as a potential series to include within the forecasting models, as this page generates a significant amount of traffic. Despite the sheer amount of traffic to the apparel page, though, other time series peak interest. Specifically, the Campus Collection, Clearance, Lifestyle, and New pages all have some interesting characteristics that could be used to improve forecasting models. The following plots contain the isolated trends. A description of the characteristics within each trend is provided.\n\nApparel page’s characteristics\n\ntidy_trend_data %>% \n  filter(page == \"Apparel\") %>% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Apparel page views\", \n       subtitle = \"Series exhibits positive trend; slight cyclic patterns; no seasonal patterns present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA clear positive trend.\nThe series contains some cyclic elements and very little indication of a seasonal pattern. However, with a greater amount of points, a seasonal pattern might be revealed (e.g., holiday season shopping).\n\n\n\nCampus Collection page’s notable characteristics\n\ntidy_trend_data %>% \n  filter(page == \"Campus Collection\") %>% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Campus Collection page views\", \n       subtitle = \"Cyclic behavior present within the series\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA slight positive trend is present up until the middle of the series. Towards the middle of the series, no real trend is present.\nGiven the variation is not of a fixed frequency, this series exhibits some cyclical behavior.\n\n\n\nClearance page’s notable characteristics\n\ntidy_trend_data %>% \n  filter(page == \"Clearance\") %>% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Clearance page views\", \n       subtitle = \"Slight trend components are present; weekly seasonality is also present\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nA slight upward trend towards the middle of the series, followed by a steep downward trend, and then a slight upward trend towards the end of the series is present.\nThe series also has a clear seasonal pattern, which seems to be weekly in nature. Perhaps products are moved to clearance on a weekly basis.\n\n\n\nLifestyle page’s notable characteristics\n\ntidy_trend_data %>% \n  filter(page == \"Lifestyle\") %>% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Lifestyle page views\", \n       subtitle = \"Trend not clear in this series; some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nTrend is not clear in this series.\nThere is some strong cyclic behavior being exhibited with limited seasonality with the time frame available.\n\n\n\nNew page’s notable characteristics\n\ntidy_trend_data %>% \n  filter(page == \"New\") %>% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of New page views\", \n       subtitle = \"No trend present; some some strong cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nNo real trend is present.\nStrong cyclic behavior is present within the series. Some seasonality is present. Indeed, this is similar to the Clearance series, as the seasonality seems to be weekly. Perhaps new products are released each week.\n\n\n\nShop by brand characteristics\n\ntidy_trend_data %>% \n  filter(page == \"Shop by Brand\") %>% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Shop by Brand page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nThe trend here seems to be positive from the start, then declines sharply, and then exhibits a slight positive trend towards the end of the series.\nThere also seems to be some slight cyclicity with very little seasonality.\n\n\n\nOrder completed characteristics\n\ntidy_trend_data %>% \n  filter(page == \"Order Completed\") %>% \n  autoplot(views, size = 1, alpha = .8) +\n  scale_y_continuous(labels = comma) +\n  labs(y = \"Views\",\n       x = \"\",\n       title = \"Time plot of Order Completed page views\", \n       subtitle = \"Some trend components; slight cyclic behavior\",\n       caption = \"Source: Google Merchandise Store GA4 data exported from BigQuery\"\n       ) \n\n\n\n\n\nThe trend from the start seems to be positive, up until the middle of the series. From there, a steep decline is present. Towards the end of the series, there is a subtle positive trend.\nTowards the beginning of the series, there seems to be some strong cyclicity. Towards the end of the series, there seems to be more of a seasonal pattern within the data. This cyclicity may be due to the time of year which this data represents, the holiday season.\n\nSince this analysis is focused on creating a forecast for order completions, additional work needed to be done to identify potential series that may improve the forecasts. To do this, several scatter plots were created to help identify variables that relate to order completions.\nBefore additional exploratory plots can be created, though, additional data wrangling steps needed to be taken. Specifically, the data was transformed from a long format to a wide format, where the page variable is turned into several numeric columns within the transformed data set. The following code chunk was used to perform this task.\n\ntidy_trend_wide <- tidy_trend_data %>% \n  select(-unique_users) %>% \n  mutate(page = str_replace(str_to_lower(page), \" \", \"_\")) %>% \n  pivot_wider(names_from = page, \n              values_from = views, \n              names_glue = \"{page}_views\")\n\nWith data in a wide format, the ggpairs() function from the GGally package was used to create a matrix of scatterplots and correlation estimates for the various series within the dataset.\nHere is the code to perform this analysis and output the matrix of plots.\n\ntidy_trend_wide %>% \n  ggpairs(columns = 2:8)\n\n\n\n\nThe scatterplots and correlations output revealed some interesting relationships. For one, although previous exploratory analysis revealed apparel generated high volumes of views, the correlation analysis revealed a slight negative relationship with order completions. However, five variables seem to be highly correlated with order completions: Clearance (.877), Campus Collection (.846), Lifestyle (.769), New (.753), and Shop by Brand (.659). Evidence points to these series as being potentially valuable components of a forecasting model of Order Completed page views."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#narrow-the-focus-further",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Narrow the focus further",
    "text": "Narrow the focus further\nAt this point, this post transitions into examining just the Order Completed page views, as this is the time series intended to be forecasted within future analyses done in this series of blog posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#lag-plots",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Lag plots",
    "text": "Lag plots\nIt’s time to shift focus onto exploring the characteristics of the outcome variable of future forecasting models, order completions, in more depth. The next step, then, is to examine for any lagged relationships present within the Order Completed page views time series.\nThe gg_lag function from the feats package makes it easy to produce the lag plots. Here the tidy_trend_wide data will be used.\n\ntidy_trend_wide %>% \n  gg_lag(order_completed_views, geom = \"point\")\n\n\n\n\nThe plots provide little evidence that any lagged relationships–positive or negative–are present within this time series. Thus, no further steps were taken to account for lagged relationships at this point in the analysis."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#autocorrelation",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Autocorrelation",
    "text": "Autocorrelation\nExploring for autocorrelation is the next step. A correlogram is created to explore for this characteristic within the series. A correlogram “measures the linear relationship between lagged values of a time series” (Hyndman and Athanasopoulos 2021). The ACF is first calculated for each value within the series. This value is then plotted according to it’s corresponding lag values. The ACF() function from the feasts package was used to calculate these values. The resulting data object is then passed along to the autoplot() function, which creates the correlogram for the data. Here is what the code looks like, along with the outputted plot.\n\ntidy_trend_wide %>% \n  ACF(order_completed_views, lag_max = 28) %>% \n  autoplot()"
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#inspect-the-correlogram",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Inspect the correlogram",
    "text": "Inspect the correlogram\nThe correlogram clearly shows the data is not a white noise series. Moreover, the plot reveals several structural characteristics within the time series.\n\nThe correlogram, given the smaller lags are large, positive, and seem to decrease with each subsequent lag, which suggests the series contains some type of trend.\nThe plot also reveals a slight scalloped shape (i.e., peaks and valleys at specific intervals), which suggests some seasonality occurring within the process. Indeed, it seems peaks occur every seven days (e.g., lags 7 and 14). Thus, a slight weekly seasonality may be present within the time series.\n\nGiven these structural characteristics of the series, future forecasting steps will need to account for these issues. This topic will be further discussed in future posts."
  },
  {
    "objectID": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "href": "blog/posts/2022-03-03-forecasting-series-exploratory-analysis-of-google-analytics-4-data-for-forecasting-models/index.html#trend-decomposition",
    "title": "Forecasting Series: Exploratory analysis of Google Analytics 4 data for forecasting models",
    "section": "Trend Decomposition",
    "text": "Trend Decomposition\nThe final exploratory analysis step is to split the series into its several components. This includes the seasonal, trend, and remainder components. Here an additive decomposition is performed. Transformations were not applied to this data before decomposition was performed.\nAn argument could be made to transform this time series using some mathematical operation. Indeed, transforming the series may improve forecasts generated from the data (Hyndman and Athanasopoulos 2021). However, this analysis doesn’t have access to a complete series of data. Having more data could lead to more informed decisions on the appropriate application of transformations. A full year or multiple years worth of data would be preferred. Interpretability is also a concern, as transformations would need to be converted back to the original scale once the forecast was created. Thus, it was decided that transformations were not going to be applied to the data. More about transforming the series can be referenced here.\nThe series was broken down into its multiple components: seasonal, trend-cycle, and remainder (Hyndman and Athanasopoulos 2021). Decomposing the series allows for more to be learned about the underlying structure of the time series. As a result, allowing for structural components of the time series that could improve forecasting models to be identified. Several functions from the {feasts} and {fabletools} packages simplified the decomposition process.\nFirst, the trend components are calculated using the STL() and model() functions. STL() decomposes the trend. The model() function creates a mabel object of estimates. The components() function is then used to view the model object.\n\norder_views_dcmp <- tidy_trend_wide %>% \n  model(stl = STL(order_completed_views))\n\ncomponents(order_views_dcmp)\n\n# A dable: 92 x 7 [1D]\n# Key:     .model [1]\n# :        order_completed_views = trend + season_week + remainder\n   .model event_date order_completed_views trend season_week remainder season_…¹\n   <chr>  <date>                     <int> <dbl>       <dbl>     <dbl>     <dbl>\n 1 stl    2020-11-01                    14  47.8      -37.6      3.80       51.6\n 2 stl    2020-11-02                    73  47.9       12.9     12.2        60.1\n 3 stl    2020-11-03                    69  47.9       23.1     -1.99       45.9\n 4 stl    2020-11-04                    47  48.2       11.8    -13.0        35.2\n 5 stl    2020-11-05                    28  48.5       -4.93   -15.5        32.9\n 6 stl    2020-11-06                    62  48.7       13.0      0.285      49.0\n 7 stl    2020-11-07                    34  48.9      -17.9      3.00       51.9\n 8 stl    2020-11-08                    32  51.7      -37.9     18.2        69.9\n 9 stl    2020-11-09                    58  54.5       12.4     -8.95       45.6\n10 stl    2020-11-10                    70  56.6       22.7     -9.33       47.3\n# … with 82 more rows, and abbreviated variable name ¹​season_adjust\n# ℹ Use `print(n = ...)` to see more rows\n\n\nPlotting the decomposition is done by piping the output from the components() function to autoplot(). The visualization will contain the original trend, the trend component, the seasonal component, and the remainder of the series after the trend and seasonal components are removed.\n\ncomponents(order_views_dcmp) %>% \n  autoplot() +\n  labs(x = \"Event Date\")\n\n\n\n\nScanning the components outputted by the decomposition, a few conclusions were drawn. Looking at the trend component, it seems a steady upward trend takes place from the start to the middle of the series. Then, a sharp negative trend followed by a slight increase towards the tail end of the series is present. Indeed, this might be additional seasonality that might become more apparent if additional data were available.\nThe seasonal component is also interesting here, as some type of cyclic weekly pattern seems to be present. This includes less traffic around the beginning of the week and weekends, where the majority of this cyclic pattern occurs during the week. It’s also interesting to note a consistent drop occured on most Thursdays of the week."
  },
  {
    "objectID": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "href": "blog/posts/2022-09-20-flattening-google-analytics-4-data/index.html",
    "title": "Flattening Google Analytics 4 data",
    "section": "",
    "text": "Arrays, structs, and array of structs\nBefore discussing the use of these data types in GA4 data, let’s take a step back and simply define what array and struct data types are in BigQuery. A good starting point is BigQuery’s arrays and structs documentation. According to the docs,\n\nAn array is an ordered list of zero or more elements of non-Array values. Elements in an array must share the same type.\n\n\nA struct is a container of ordered fields each with a type (required) and field name (optional).\n\nBoth definitions contain technical jargon that don’t really define, in an intuitive, useful way, what these data types are and how to use them, especially in the analysis of GA4 data. So let’s break each down by bringing in additional perspectives and through the use of several simplified examples.\nWhile learning more about arrays and structs, I found several blog posts that helped me better understand these structures and how to use them. Here is a list of the ones I found to be very helpful:\n\nHow to work with Arrays and Structs in Google BigQuery by Deepti Garg\nExplore Arrays and Structs for Better Query Performance in Google BigQuery by Skye Tran\nTutorial: BigQuery arrays and structs from Sho’t left to data science\n\nI highly suggest reading all of these. In fact, much of what follows is adapted from these posts, with a few examples I created to help me better understand how these data types are structured, stored, and queried. Towards the end of the post, the techniques learned from these posts and overviewed here will be applied to GA4 data, specifically the publicly available bigquery-public-data.ga4_obfuscated_sample_ecommerce data.\n\n\nArrays\nArrays are a collection of elements of the same datatype. If you’re familiar with the R programming language, an array is similair to a vector.\nLet’s create a table containing an array of planets in our solar system as an example, and then use the INFORMATION_SCHEMA view to verify the data was entered correctly. The following code will create this table in BigQuery:\ncreate or replace table examples.array_planets_example as \nwith a as (\n   select [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"] as planets\n)\nselect planets from a;\nThe INFORMATION_SCHEMA.COLUMNS view for the array_planets_example table can be queried to verify the data was entered correctly. This table is available for every table created in BigQuery, and it contains metadata about the table and the fields within. Here is the query needed to return this information:\nselect table_name,\n   column_name,\n   is_nullable,\n   data_type\nfrom examples.INFRORMATION_SCHEMA.COLUMNS\nwhere table_name = \"array_planets_example\";\nThe returned table will contain a data_type field, where the value ARRAY<STRING> will be present. This value represents the field in the array_planets_example contains an array with a list of string values. Although this example array contains a series of string values, arrays can hold various other data types, as long as the values are the same type across the collection. Overviewing all of the different data types that can be stored in an array is beyond the scope of this post, but check out the BigQuery docs for more examples.\n\nQuerying an array\nMultiple approaches are available to query an array. The type of approach will depend on if the returned data needs to maintain its grouped, repeated structure, or if the returned data needs to be flattened. If maintaining the repeated structure is required, then a simple SELECT statement will work. Using the array_planets_example table as an example, the query applying this approach will look something like this:\nselect planets\nfrom examples.array_planets_example\nIf each element of the array is to be outputted onto its own row (i.e., denormalized), multiple approaches are available. The first approach is to use the unnest() function. Here is an example using the planets array we created earlier:\nselect planets\nfrom examples.array_planets_example,\nunnest(planets) as planets\nThe second approach is to apply a correlated join through the use of cross join unnest(). This approach looks like this:\nselect planets\nfrom examples.array_planets_example\ncross join unnest(planets) as planets\nYou’ll notice this is only slightly different than the query above, and in fact the , used in the FROM clause is short-hand for the cross join statement. The last and final approach is to use a comma-join. This is similair to our first query, but now we refer to the table name before the array name we want flattened.\nselect planets\nfrom examples.array_planets_example, array_planets_example.planets as planets;\nWhich one do you choose? It really comes down to a matter of preference. All three approaches will lead to the same result. It depends on how explicit you want the code to be.\nThere is one note to be aware of if you’re applying these conventions to other arrays outside of analyzing GA4 data. The cross join approach will exclude NULL arrays. So if you want to retain rows containing NULL arrays, you’ll need to apply a left join. More about this is described in the BigQuery docs.\nKeep these approaches top-of-mind. They will be applied to flatten some of the fields in the GA4 dataset. In other words, get comfortable with using them.\n\n\n\nStructs\nThe structs data type holds attributes in key-value pairs. Structs can hold many different data types, even structs. We will see the use of structs within structs in the GA4 data. Keeping with the solar system theme of the post, the following example code will create a table utilizing the struct data type to hold the dimensions and distances of the planets in our solar system. The data used for this table is reported here.\ncreate or replace table examples.struct_solar_system as\nwith a as (\n  select \"Mercury\" as planet,\n  struct(0.39 as au_sun, 57900000 as km_sun, 4879 as km_diameter) as dims_distance union all\n  select \"Venus\" as planet,\n  struct(0.72 as au_sun, 108200000 as km_sun, 12104 as km_diameter) as dims_distance union all\n  select \"Earth\" as planet,\n  struct(1 as au_sun, 149600000 as km_sun, 12756 as km_diameter) as dims_distance union all\n  select \"Mars\" as planet,\n  struct(1.52 as au_sun, 227900000 as km_sun, 6792 as km_diameter) as dims_distance union all\n  select \"Jupiter\" as planet,\n  struct(5.2 as au_sun, 778600000 as km_sun, 142984 as km_diameter) as dims_distance union all\n  select \"Saturn\" as planet,\n  struct(9.54 as au_sun, 1433500000 as km_sun, 120536 as km_diameter) as dims_distance union all\n  select \"Uranus\" as planet,\n  struct(19.2 as au_sun, 2872500000 as km_sun, 51118 as km_diameter) as dims_distance union all\n  select \"Neptune\" as planet,\n  struct(30.06 as au_sun, 4495100000 as km_sun, 49528 as km_diameter) as dims_distance \n)\nselect * from a;\nThis table contains two columns. A column that holds a string value for the name of the planet and a struct column that contains a list of key value pairs of distance and dimensions for each planet.\nThe INFRORMATION_SCHEMA.COLUMNS table can then be queried again to verify the datatypes for each column were inputted correctly. Here is the code to do this:\nselect \n  table_name, \n  column_name,\n  is_nullable,\n  data_type\nfrom examples.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"struct_solar_system\";\nThe returned table will contain a data_type column with two values: STRING and STRUCT<au_sun FLOAT64, km_sun INT64, km_diameter INT64>. Take notice that the STRUCT value contains information about the data types contained within.\n\nQuerying a struct\nQuerying a struct requires the use of the . operator (i.e., dot operator) in the FROM clause to flatten the table. Take for example the case where we want to return a table of only the distance of each planet from the sun in kilometers. The following query will be used:\nselect \n  planet,\n  dims_distance.km_sun\nfrom examples.struct_solar_system;\nSay a denormalized table that contains both the distance from the sun in kilometers and each planet’s diameter in kilometers is wanted. The following query would be used:\nselect \n  planet,\n  dims_distance.km_sun,\n  dims_distance.km_diameter\nfrom examples.struct_solar_system;\nWhen reviewing these two examples, observe how the dot notation is being used. In the first, our select statement contains dims_distance.km_sun, which unnests the values and gives each its own row for each planet. This is expanded in the second query, where an additional line is added to the select statement, dims_distance.km_diameter. To unnest all the values in the struct, use the following query:\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system;\nIn fact, let’s expand this query to answer the following question: which planets are the closest and farthest from our sun. Take notice how the ORDER BY portion of the query doesn’t require the dims_distance prefix for the field we want to arrange our data.\nselect \n  planet,\n  dims_distance.au_sun,\n  dims_distance.km_sun,\n  dims_distance.km_diameter,\nfrom examples.struct_solar_system\norder by km_sun;\n\n\n\nArray and structs in GA4 data\nNow that we have learned a little bit about our solar system, let’s return to Earth and the task at hand, flattening GA4 data. We just discussed how these data types are created and queried, it is now time to combine them into more complex data structures, as both of these structures are combined to create nested repeated data structures in the GA4 data. It’s best to start with an example. Specifically, let’s look at how these structures are applied in the event_params field.\nWe can start off by querying the INFORMATION_SCHEMA.COLUMNS view for one event to get an idea of its structure. The query to do this can be seen here:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS\nwhere table_name = \"events_20210131\" and column_name = \"event_params\";\nThe data type is described in the returned table’s data_type field. This field contains the following value ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>. It should be immediately apparent that both the array and struct values are being used here to create a repeated nested structure. In fact, the event_params value uses a struct within a struct. Given this structure, all the above methods will need to be employed to flatten this data.\nTo simplify this, let’s look at one instance of one event in the GA4 data. Specifically, let’s look at one instance of a page_view event. With this simplified example, we’ll go step-by-step, adding additional elements to the query needed to flatten this data.\nselect \n  event_date,\n  event_timestamp,\n  event_name,\n  event_params\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = 'page_view'\nlimit 1;\nAfter running this query, you’ll notice the output to the console is quite verbose, especially if you’re using the bq command-line tool. The verbosity of the output is due to the event_params field holding much of the data.\nThe first layer of the structure is an array, so the initial step is to use the unnest() function. The following can be done to achieve this:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param;\nYou’ll notice a nested FROM statement is being used here. This is done to limit the result set to one row, representing one page_view event for this simplified example. Later iterations of the query will eliminate this nested query.\nNow say we’re only interested in viewing the page_location parameter. We can use a where statement to filter out this information. Here is what this will look like:\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key = 'page_location';\nInterested in viewing both the page_location and page_title parameters? Use the IN operator in the WHERE clause.\nselect \n  event_date,\n  event_name,\n  key,\n  value.string_value,\n  value.int_value,\n  value.double_value, \n  value.float_value\nfrom (\n  select \n    event_date,\n    event_timestamp,\n    event_name,\n    event_params\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n  where event_name = 'page_view'\n  limit 1\n), unnest(event_params) as event_param\nwhere key in ('page_location', 'page_title');\nWanna turn the key field into columns so you only have one row for this specific event? Use BigQuery’s pivot() operator. Here is how to achieve this in a query:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    key,\n    value.string_value\n  from (\n    select \n      event_date,\n      event_timestamp,\n      event_name,\n      event_params\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nSince the string values are all we care about here, the value.string_value was the only one retained in the query. The other nested value elements were eliminated from the SELECT statement.\n\n\nCombine other nested fields in the GA4 data\nNow that the event_params field has been flattened, let’s supplement this information with additional data in the table. Moreover, this will provide another example of how to apply these steps to flatten other elements in the GA4 data. Knowing where users originate is some additional context that may add to our event analysis, so let’s add that data to our flattened data. But first, let’s get some more information on what data type is used for the geo field in the GA4 data.\nOnce again, querying the INFORMATION_SCHEMA.COLUMNS view can be used to explore the geo field’s data type. Here is what the query looks like:\nselect \n  table_name,\n  column_name,\n  is_nullable,\n  data_type\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.INFORMATION_SCHEMA.COLUMNS`\nwhere table_name = \"events_20210131\" and column_name = \"geo\";\nThe value STRUCT<continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING> is returned. Let’s write a query to return the table without first unnesting the data.\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo \nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nwhere event_name = \"page_view\"\nlimit 1;\nYou’ll notice this field contains a struct, where the dot operator will need to be applied to flatten this data. Let’s start by flattening this data and then combine it with the events_param data. For the sake of keeping the returned table simple, let’s just return the region and city fields in a denormalized form. The following will return a flattened table with these fields:\nselect\n  event_date,\n  event_name,\n  user_pseudo_id,\n  geo.region,\n  geo.city\nfrom `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\nlimit 1;\nAs expected, the table will return a flattened table containing five fields: event_date, event_name, user_pseudo_id, geo.region, and geo.city. This table was also limited to return only the first instance of the page_view in the table.\nNow, the next step is to add this geo data to our flattened event_params query. This is as simple as adding the . operator with the needed geo elements into the FROM statement. The query will now look like this:\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n    limit 1\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\nThe resulting table will contain one row with several fields representing the specific event. This is great for one event, but the next step will be to expand this denormalization to all page_view events in the table.\n\n\nExpand the unnesting to multiple page view events\nNow that we have the flattened table for one page_view event, let’s expand it to additional events. This requires a simple modification to the initial nested query, remove the limit 1 line.\nselect *\nfrom(\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    key,\n    value.string_value,\n    geo.region,\n    geo.city\n  from (\n    select \n      event_date,\n      user_pseudo_id,\n      event_name,\n      event_params,\n      geo\n    from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`\n    where event_name = 'page_view'\n  ), unnest(event_params) as event_param\n  where key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\nWe can now refactor the query to be more concise. Here is what this will look like:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20210131`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and key in ('page_location', 'page_title')\n)\npivot(max(string_value) for key in ('page_location', 'page_title'));\n\n\nApply these approaches across multiple days\nGenerating results for one day may not be enough, so there’s a few modifications that can be made to expand the final query to return additional days. This involves modifying the FROM and WHERE statements in the initial query.\nThe first step is to modify the FROM statement to use the * wildcard operator at the end of the table name. Since the GA4 tables are partitioned by day, this will allow for a range of tables to be defined within the WHERE clause. The table name will now be bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*.\nTo define the range of dates for the events (i.e., to query multiple tables), the WHERE clause will be expanded to include the use of _table_suffix. The _table_suffix is a special column used within a separate wildcard table that is used to match the range of values. Explaining the use of the wildcard table is beyond the scope of this post, but more about how this works can be found here. The WHERE clause will now look like this:\nwhere event_name = 'page_view' and\nkey in ('page_location', 'page_title') and\n_table_suffix between \"20210126\" and \"20210131\"\nYou’ll notice this statement uses the between operator, where two string values representing the date range are passed. This statement is inclusive, so it will include partitioned tables from 20210126 and 20210131, and all tables in between. Here is the query in its final form:\nselect * \nfrom (\n  select \n    event_date,\n    event_name,\n    user_pseudo_id,\n    event_params.key,\n    event_params.value.string_value,\n    geo.region,\n    geo.city\n  from `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,\n  unnest(event_params) as event_params\n  where event_name = 'page_view' and \n  key in ('page_location', 'page_title') and\n  _table_suffix between \"20210126\" and \"20210131\"\n)\npivot(max(string_value) for key in ('page_location', 'page_title'))\norder by event_date\n\n\nWrap up\nThis post started out simple by defining what arrays, structs, and array of structs data types are in BigQuery. Through the use of several examples, this post overviewed several approaches to query these different data types, specifically highlighting how to flatten each type. A second aim of this post was to show the application of these methods to the flattening of GA4 data stored in BigQuery. This included the flattening and combination of the complex, nested, repeated and nested repeated data types used in the event_params and geo fields. Finally, this post shared queries that expanded the result set across multiple days worth of data.\nIf you found this post helpful or just have interest in this type of content, I would appreciate the follow on GitHub and/or Twitter. If you have suggestions on how to improve these queries or found something that I missed, please file an issue in the repo found here.\n\n\nAdditional resources\nI spent a lot of time researching how to write, use, and query arrays and structs in BigQuery. In the process of preparing this post, I wrote a lot of example queries and followed along with BigQuery’s turtorial on working with arrays and structs. As a result, I created multiple files that I organized into the GitHub repo for this post. These might be useful as a review after reading this post, or they might be a helpful quickstart quide for your own analysis of GA4 data stored in BigQuery. These additional notes can be found here.\n\n\nAdditional references\n\nHow to work with Arrays and Structs in Google BigQuery\nExplore Arrays and Structs for Better Query Performance in Google BigQuery\nTutorial: BigQuery arrays and structs"
  }
]